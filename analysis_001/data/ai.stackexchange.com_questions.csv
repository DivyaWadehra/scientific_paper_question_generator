,Id,PostTypeId,AcceptedAnswerId,CreationDate,Score,ViewCount,Body,OwnerUserId,LastEditorUserId,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount
0,1,1,3,2016-08-02T15:39:14.947,8,384,"what does "" backprop "" mean ? i 've googled it , but it 's showing backpropagation . is the "" backprop "" term basically the same as "" backpropagation "" or does it have a different meaning ?",8,10135,2018-10-18T10:45:18.660,2018-10-18T10:45:18.660,"what is "" backprop "" ?",neural-networks backpropagation terminology definitions,3,3,1
1,2,1,9,2016-08-02T15:40:20.623,10,404,does increasing the noise in data help to improve the learning ability of a network ? does it make any difference or does it depend on the problem being solved ? how is it affect the generalization process overall ?,8,2444,2019-02-23T22:36:19.090,2019-02-23T22:36:37.133,how does noise affect generalization ?,neural-networks machine-learning statistical-ai generalization,3,0,1
2,4,1,12,2016-08-02T15:41:22.020,27,820,"when you 're writing your algorithm , how do you know how many neurons you need per single layer ? are there any methods for finding the optimal number of them , or is it a rule of thumb ?",8,10135,2018-10-18T10:45:15.213,2018-10-18T10:45:15.213,how to find the optimal number of neurons per layer ?,deep-network search neurons,4,0,8
3,5,1,14,2016-08-02T15:42:08.177,-3,2026,"i have a lego mindstorms ev3 and i 'm wondering if there 's any way i could start coding the bot in python rather than the default drag - and - drop system . is a mindstorm considered ai ? is this possible ? my goal is to write a basic walking program in python . the bot is the ev3rstorm . i searched and found this , but do n't understand it .",5,10135,2018-10-18T10:44:51.623,2018-10-18T10:44:51.623,how to program ai in mindstorms,python mindstorms,2,4,
4,6,1,20,2016-08-02T15:43:35.460,6,160,the intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge . ( paraphrased from wikipedia ) does this mean that humans are not intelligent ? i think we all make mistakes that imply that we are not maximizing the expected value of a performance measure .,29,7488,2017-05-28T14:30:31.023,2017-05-28T14:30:31.023,on the intelligent agent definition of intelligence,philosophy intelligent-agent terminology,2,1,
5,7,1,,2016-08-02T15:45:09.070,10,421,this quote by stephen hawking has been in headlines for quite some time : artificial intelligence could wipe out humanity when it gets too clever as humans will be like ants . why does he say this ? to put it simply in layman terms : what are the possible threats from ai ? if we know that ai is so dangerous why are we still promoting it ? why is it not banned ? what are the adverse consequences of the so called technological singularity ?,26,95,2016-08-04T14:09:43.583,2016-08-04T14:09:43.583,"why does stephen hawking say "" artificial intelligence will kill us all "" ?",intelligent-agent,6,6,1
6,10,1,32,2016-08-02T15:47:56.593,34,1406,"i 'm new to a.i . and i 'd like to know in simple words , what is the fuzzy logic concept ? how does it help , and when is it used ?",8,10135,2018-10-18T10:44:33.687,2019-05-25T09:20:24.713,what is fuzzy logic ?,deep-network terminology fuzzy-logic,5,0,14
7,13,1,163,2016-08-02T15:52:19.413,9,105,"in particular , an embedded computer ( with limited resources ) analyzes live video stream from a traffic camera , trying to pick good frames that contain license plate numbers of passing cars . once a plate is located , the frame is handed over to an ocr library to extract the registration and use it further . in my country two types of license plates are in common use - rectangular ( the typical ) and square - actually , somewhat rectangular but "" higher than wider "" , with the registration split over two rows . ( there are some more types , but let us disregard them ; they are a small percent and usually belong to vehicles that lie outside our interest . ) due to the limited resources and need for rapid , real - time processing , the maximum size of the network ( number of cells and connections ) the system can handle is fixed . would it be better to split this into two smaller networks , each recognizing one type of registration plates , or will the larger single network handle the two types better ?",38,14723,2018-04-12T02:30:42.823,2018-04-12T02:30:42.823,"can a single neural network handle recognizing two types of objects , or should it be split into two smaller networks ?",neural-networks image-recognition,1,0,
8,15,1,,2016-08-02T15:52:50.827,32,2379,the turing test was the first test of artificial intelligence and is now a bit outdated . the total turing test aims to be a more modern test which requires a much more sophisticated system . what techniques can we use to identify an artificial intelligence ( weak ai ) and an artificial general intelligence ( strong ai ) ?,9,95,2016-08-04T14:10:10.990,2018-04-11T20:16:35.997,"is the turing test , or any of its variants , a reliable test of artificial intelligence ?",turing-test strong-ai intelligent-agent weak-ai,6,2,9
9,16,1,142,2016-08-02T15:53:00.447,6,145,"what is "" early stopping "" and what are the advantages using this method ? how does it help exactly ? i 've read the wiki , but i 'd be interested in perspectives , and links to recent research .",8,1671,2018-04-14T01:43:24.667,2018-04-14T01:43:24.817,"what is "" early stopping "" in relation to ai , and why is it important ?",ai-basics definitions overfitting early-stopping regularization,1,1,1
10,17,1,45,2016-08-02T15:53:38.273,29,972,"i 've heard the idea of the technological singularity , what is it and how does it relate to artificial intelligence ? is this the theoretical point where artificial intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off ? how would we know when we reach this point ?",55,4302,2018-10-18T07:13:52.733,2018-12-09T20:40:29.377,what is the concept of the technological singularity ?,philosophy singularity,4,2,11
11,21,1,,2016-08-02T15:55:15.957,3,62,"i 'm worrying that my network has become too complex . i do n't want to end up with half of the network doing nothing but just take up space and resources . so , what are the techniques for detecting and preventing overfitting to avoid such problems ?",8,145,2017-04-23T02:22:19.597,2017-04-23T02:22:19.597,what are the methods of optimizing overfitted models ?,deep-network overfitting optimization,1,0,
12,26,1,189,2016-08-02T15:58:31.413,22,1135,"i 've seen emotional intelligence defined as the capacity to be aware of , control , and express one 's emotions , and to handle interpersonal relationships judiciously and empathetically . what are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers ? are there examples where this is already happening to a degree today ? would n't a computer that passes a turing test necessarily express emotional intelligence or it would be seen as an obvious computer ? perhaps that is why early programs that pass the test represented young people , who presumably have lower emotional intelligence .",55,2444,2016-12-23T05:52:15.560,2018-04-12T02:30:09.917,how could emotional intelligence be implemented ?,turing-test emotional-intelligence,5,1,7
13,28,1,143,2016-08-02T16:02:44.553,9,4051,"since human intelligence presumably is a function of a natural genetic algorithm in nature , is using a genetic algorithm in a computer an example of artificial intelligence ? if not , how do they differ ? or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into ?",55,,,2016-11-03T11:54:41.460,is a genetic algorithm an example of artificial intelligence ?,unassisted-learning genetic-algorithms,5,2,2
14,35,1,,2016-08-02T16:05:26.390,64,10921,"these two terms seem to be related , especially in their application in computer science and software engineering . is one a subset of another ? is one a tool used to build a system for the other ? what are their differences and why are they significant ?",69,2444,2019-05-02T16:09:06.300,2019-05-25T09:31:29.233,what is the difference between artificial intelligence and machine learning ?,machine-learning terminology difference,16,0,21
15,36,1,114,2016-08-02T16:06:25.853,35,1115,"what aspects of quantum computers , if any , can help to further develop artificial intelligence ?",29,29,2016-08-02T19:13:49.690,2018-04-12T02:34:20.583,to what extent can quantum computers help to develop artificial intelligence ?,quantum-computing,4,1,11
16,37,1,,2016-08-02T16:07:29.317,6,1989,i believe a markov chain is a sequence of events where each subsequent event depends probabilistically on the current event . what are examples of the application of a markov chain and can it be used to create artificial intelligence ? would a genetic algorithm be an example of a markov chain since each generation depends upon the state of the prior generation ?,55,,,2016-08-03T06:37:01.983,what is a markov chain and how can it be used in creating artificial intelligence ?,genetic-algorithms markov-chain probabilistic,2,0,5
17,40,1,44,2016-08-02T16:08:23.377,9,866,"what purpose does the "" dropout "" method serve and how does it improve the overall performance of the neural network ?",8,10,2016-08-02T16:10:22.307,2018-04-12T02:30:04.720,"what is the "" dropout "" technique ?",deep-network overfitting performance,4,0,3
18,41,1,65,2016-08-02T16:08:34.350,9,743,"can an ai program have an iq ? in other words , can the iq of an ai program be measured ? like how humans can do an iq test .",72,29,2016-08-03T08:04:16.350,2017-01-29T23:59:30.203,can the iq of an ai program be measured ?,intelligence-testing,3,1,2
19,42,1,51,2016-08-02T16:09:25.427,5,1961,"why would anybody want to use "" hidden layers "" ? how do they enhance the learning ability of the network in comparison to the network which does n't have them ( linear models ) ?",8,17488,2018-08-21T09:05:19.073,2018-08-21T09:05:19.073,what is the purpose of the hidden layers ?,hidden-layers,2,0,2
20,46,1,71,2016-08-02T16:14:26.350,9,3053,when did research into artificial intelligence first begin ? was it called artificial intelligence then or was there another name ?,55,55,2016-08-02T16:26:16.990,2018-04-11T18:40:28.327,when did artificial intelligence research first start ?,history,2,0,3
21,50,1,138,2016-08-02T16:16:46.797,2,148,how would you estimate the generalisation error ? what are the methods of achieving this ?,8,,,2018-04-11T23:42:57.393,how can generalization error be estimated ?,deep-network generalization,1,0,
22,52,1,1437,2016-08-02T16:19:30.337,6,512,"i 've implemented the reinforcement learning algorithm for an agent to play snappy bird ( a shameless cheap ripoff of flappy bird ) utilizing a q - table for storing the history for future lookups . it works and eventually achieves perfect convergence after enough training . is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q - table ? obviously , storage is a concern with the q - table , but it does n't seem to ever train with the neural net alone . perhaps training the nn on an existing q - table would work , but i would like to not use a q - table at all if possible .",62,2444,2019-05-10T14:42:23.103,2019-05-10T14:42:23.103,is it possible to implement reinforcement learning using a neural network ?,neural-networks reinforcement-learning deep-rl function-approximation,2,0,
23,54,1,76,2016-08-02T16:20:40.520,5,128,"i read that in the spring of 2016 a computer go program was finally able to beat a professional human for the first time . now that this milestone has been reached , does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem ? what are some of the methods used to program the successful go playing program , and are those methods considered to be artificial intelligence ?",55,145,2016-08-17T15:54:14.257,2018-04-12T03:37:23.317,does the recent advent of a go playing computer represent artificial intelligence ?,game-theory,4,0,2
24,58,1,,2016-08-02T16:25:20.223,7,1408,"who first coined the term artificial intelligence , is there a published research paper which is the first to use that term ?",55,,,2016-08-02T16:38:45.147,who first coined the term artificial intelligence ?,history,1,0,1
25,60,1,1464,2016-08-02T16:27:49.533,11,328,"i have a background in computer engineering and have been working on developing better algorithms to mimic human thought . ( one of my favorites is analogical modeling as applied to language processing and decision making . ) however , the more i research , the more i realize just how complicated ai is . i have tried to tackle many problems in this field , but sometimes i find that i am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable ( ie . the halting problem ) . so , to help in furthering ai , i want to better understand the current obstacles that are hindering our progress in this field . for example , time and space complexity of some machine learning algorithms is super - polynomial which means that even with fast computers , it can take a while for the program to complete . even still , some algorithms may be fast on a desktop or other computer while dealing with a small data set , but when increasing the size of the data , the algorithm becomes intractable . what are other issues currently facing ai development ?",77,,,2018-04-12T02:34:45.870,what are the main problems hindering current ai development ?,machine-learning,3,0,1
26,63,1,208,2016-08-02T16:29:24.803,9,155,i 've read that the most of the problems can be solved with 1 - 2 hidden layers . how do you know you need more than 2 ? for what kind of problems you would need them ( give me an example ) ?,8,14723,2018-04-11T18:49:51.953,2018-04-11T18:49:51.953,what kind of problems require more than 2 hidden layers ?,deep-network hidden-layers,1,2,4
27,64,1,,2016-08-02T16:29:29.207,3,53,what were the first areas of research into artificial intelligence and what were some early successes ? more recently we 've had : beating a human at the game of chess convincing a human that a person was conversing with them ( passing the turing test ) beating a human at jeopardy game show beating a human at the game of go . were there milestones that were considered major in the field before the 1990s ?,55,,,2017-03-15T05:38:48.220,what were the first areas of research and what were some early successes ?,history,1,0,
28,67,1,72,2016-08-02T16:31:51.380,3,1710,why somebody would use sat solvers ( boolean satisfiability problem ) to solve their real world problems ? are there any examples of the real uses of this model ?,8,10135,2018-10-18T10:45:09.917,2018-10-18T10:45:09.917,what are the real world uses for sat solvers ?,models problem-solving,1,1,1
29,68,1,,2016-08-02T16:33:52.707,4,138,"what designs for genetic algorithms are there , if they are classified differently and/or have different names , that leverage models for epigenetics in evolution ? what are the pros / cons of the designs ? are there vast insufficiencies or wide - open questions about their usefulness ?",46,46,2016-08-04T14:50:15.963,2018-04-12T02:34:58.817,what genetic algorithm designs are there that includes models of epigenetics ?,genetic-algorithms,2,2,0
30,70,1,,2016-08-02T16:38:55.800,21,300,"can a convolutional neural network be used for pattern recognition in a problem domain where there are no pre - existing images , say by representing abstract data graphically ? would that always be less efficient ? this developer says current development could go further but not if there 's a limit outside image recognition .",46,46,2016-08-04T15:17:46.150,2018-04-12T02:45:49.627,is the pattern recognition capability of cnns limited to image processing ?,deep-network neural-networks image-recognition convolutional-neural-networks,4,4,5
31,74,1,141,2016-08-02T16:42:35.817,36,18647,i 've heard the terms strong - ai and weak - ai used . are these well defined terms or subjective ones ? how are they generally defined ?,55,29,2016-08-03T18:24:46.713,2017-06-07T11:01:00.607,what is the difference between strong - ai and weak - ai ?,strong-ai weak-ai terminology,3,0,12
32,75,1,,2016-08-02T16:42:53.110,0,91,"as ai gains capabilities , and becomes more prevalent in society , our legal system will encounter questions it has not encountered before . for example , if a self - driving car is involved in an accident while being controlled by the ai , who is at fault ? the "" driver "" ( who 's really just a passenger ) , the programmer(s ) who made the ai , or the ai itself ? so , what 's on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence ?",33,33,2016-08-02T18:00:37.377,2018-07-17T00:07:33.083,what 's the state of the art w.r.t research on the legal aspects of artificial intelligence ?,legal,1,0,
33,77,1,131,2016-08-02T16:45:01.487,18,890,"i know that language of lisp was used early on when working on artificial intelligence problems . is it still being used today for significant work ? if not , is there a new language that has taken its place as the most common one being used for work in ai today ?",55,14723,2018-04-14T01:47:35.577,2018-04-14T01:47:35.577,is lisp still being used to tackle ai problems ?,history programming-languages lisp,4,0,6
34,80,1,85,2016-08-02T16:46:07.253,7,584,what are the specific requirements of the turing test ? what requirements if any must the evaluator fulfill in order to be qualified to give the test ? must there always be two participants in the conversation ( one human and one computer ) or can there be more ? are placebo tests ( where there is not actually a computer involved ) allowed or encouraged ? can there be multiple evaluators ? if so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test ?,96,4302,2018-10-08T12:45:14.770,2018-10-08T12:45:14.770,specific requirements of the turing test,intelligence-testing software-evaluation turing-test natural-language,3,0,
35,81,1,87,2016-08-02T16:49:40.830,6,490,"i believe that statistical ai uses inductive thought processes . for example , deducing a trend from a pattern , after training . what are some examples of successfully applying statistical ai to real world problems ?",55,1581,2018-11-13T17:11:06.737,2018-11-13T17:11:06.737,what are some examples of statistical ai ?,applications statistical-ai,4,0,0
36,82,1,,2016-08-02T16:50:15.330,1,65,how do the basic components optimality theory apply to artificial intelligence ? how is optimality theory related to neural network research ?,96,2444,2019-04-05T12:19:34.277,2019-04-05T12:19:34.277,what is the relation between optimality theory and ai ?,neural-networks applications relation,1,1,
37,84,1,,2016-08-02T16:55:37.050,12,376,"some programs do exhaustive searches for a solution while others do heuristic searches for a similar answer . for example , in chess , the search for the best next move tends to be more exhaustive in nature whereas , in go , the search for the best next move tends to be more heuristic in nature due to the much larger search space . is the technique of brute force exhaustive searching for a good answer considered to be ai or is it generally required that heuristic algorithms be used before being deemed ai ? if so , is the chess - playing computer beating a human professional seen as a meaningful milestone ?",55,14723,2018-04-14T02:03:13.207,2018-12-11T09:53:09.487,are methods of exhaustive search considered to be ai ?,gaming search chess heuristics,6,3,2
38,86,1,93,2016-08-02T16:59:30.683,24,763,"how is a neural network having the "" deep "" adjective actually distinguished from other similar networks ?",8,2444,2019-02-23T22:05:20.793,2019-02-23T22:08:58.833,how is a deep neural network different from other neural networks ?,neural-networks machine-learning deep-network terminology comparison,2,0,9
39,88,1,2573,2016-08-02T17:01:18.317,3,435,what is the effectiveness of pre - training of unsupervised deep learning ? does unsupervised deep learning actually work ?,8,4446,2016-12-29T21:05:13.147,2018-04-13T18:06:03.217,why does unsupervised pre - training help in deep learning ?,deep-learning unsupervised-learning,1,0,
40,91,1,97,2016-08-02T17:04:35.297,14,289,are search engines considered ai because of the way they analyze what you search for and remember it ? or how they send you ads of what you 've searched for recently ? is this considered ai or just smart ?,5,1581,2018-11-13T17:08:25.337,2018-11-13T17:08:25.337,are search engines considered ai ?,machine-learning search,1,1,3
41,92,1,250,2016-08-02T17:05:27.590,60,4149,"the following page / study demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images , e.g. how this is possible ? can you please explain ideally in plain english ?",8,9687,2017-12-16T23:26:18.493,2018-09-06T05:01:12.717,how is it possible that deep neural networks are so easily fooled ?,convolutional-neural-networks image-recognition deep-network computer-vision,9,7,22
42,94,1,132,2016-08-02T17:06:46.317,4,87,in a feedforward neural network the inputs are fed directly to the outputs via a series of weights . what purpose do the weights serve and how are they significant in this neural network ?,8,4709,2018-07-18T10:08:51.063,2018-07-18T10:08:51.063,what is the significance of weights in a feedforward neural network ?,neural-networks feedforward,1,0,
43,96,1,98,2016-08-02T17:12:58.533,6,136,"i 'm pretty sure this a noob - y question , but what is deep network ? as of now it is the most popular tag on ai . is there a reason for this ? please note , i am not asking how to distinguish a deep network from a neural network , i am simply asking for the definition of deep network .",5,5,2016-08-03T15:20:07.903,2016-08-03T15:20:07.903,what is deep network ?,deep-network,2,3,
44,103,1,,2016-08-02T17:47:41.750,5,990,"i believe that classical ai uses deductive thought processes . for example , given as a set of constraints , deduce a conclusion . what are some examples of successfully applying classical ai to real world problems .",55,,,2018-04-13T18:00:18.307,what are some examples of classical ai ?,classical-ai,1,0,1
45,104,1,,2016-08-02T17:56:02.743,10,231,"in this video an expert says , "" one way of thinking about what intelligence is [ specifically with regard to artificial intelligence ] , is as an optimization process . "" can intelligence always be thought of as an optimization process , and can artificial intelligence always be modeled as an optimization problem ? what about pattern recognition ? or is he mischaracterizing ?",46,46,2016-08-02T18:01:01.713,2018-04-13T18:00:00.527,can artificial intelligence be thought of as optimization ?,optimization agi,2,0,3
46,108,1,,2016-08-02T18:17:44.297,6,1517,what specific advantages of declarative languages make them more applicable to ai than imperative languages ? what can declarative languages do easily that other languages styles find difficult for this kind of problem ?,69,1581,2018-11-13T17:24:41.510,2018-11-18T12:34:56.643,what are the main advantages of using declarative programming languages for building ai ?,declarative-programming,2,0,2
47,109,1,,2016-08-02T18:29:19.443,3,52,"in years past , gofai ( good old fashioned ai ) was heavily based on "" rules "" and symbolic computation based on rules . unfortunately , that approach ran into stumbling blocks , and the world moved heavily towards statistical / probabilistic approaches leading to the current wave of interest in "" machine learning "" . it seems though , that the symbolic / rule - based approach probably still has application . so , could one "" learn "" rules using a probabilistic rule induction method , and then layer symbolic computation on top ? if so , how could the whole process be made truly two - way , so that something "" learned "" from processing rules , can be fed back into how the system learns rules ?",33,14723,2018-04-14T02:02:43.383,2018-08-15T22:45:20.830,"can rule induction be considered a way to "" hybridize "" probabilistic / statistical approaches and symbolic approaches ?",gofai symbolic-computing,1,0,
48,111,1,134,2016-08-02T18:57:57.550,70,4415,"obviously , driverless cars are n't perfect , so imagine that the google car ( as an example ) got into a difficult situation . here are a few examples of unfortunate situations caused by a set of events : the car is heading toward a crowd of 10 people crossing the road , so it can not stop in time , but it can avoid killing 10 people by hitting the wall ( killing the passengers ) , avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car , killing an animal on the street in favour of a human being , changing lanes to crash into another car to avoid killing a dog , and here are few dilemmas : does the algorithm recognize the difference between a human being and an animal ? does the size of the human being or animal matter ? does it count how many passengers it has vs. people in the front ? does it "" know "" when babies / children are on board ? does it take into the account the age ( e.g. killing the older first ) ? how would an algorithm decide what should it do from the technical perspective ? is it being aware of above ( counting the probability of kills ) , or not ( killing people just to avoid its own destruction ) ? related articles : why self - driving cars must be programmed to kill how to help self - driving cars make ethical decisions",8,14723,2018-04-13T18:44:57.490,2018-04-14T01:49:58.217,how could self - driving cars make ethical decisions about who to kill ?,algorithm self-driving decision-theory ethics,13,6,19
49,112,1,,2016-08-02T18:59:44.230,4,873,which deep neural network is used in google 's driverless cars to analyze the surroundings ? is this information open to the public ?,8,14723,2018-04-13T18:53:56.720,2018-04-14T02:42:26.697,which machine learning algorithm is used in self - driving cars ?,deep-network algorithm self-driving,2,1,
50,113,1,,2016-08-02T19:02:33.003,8,545,"two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function . i understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function : . is there a significant difference between these two activation functions , and in particular , when is one preferable to the other ? i realize that in some cases ( like when estimating probabilities ) outputs in the range of $ [ 0,1]$ are more convenient than outputs that range from $ [ -1,1]$ . i want to know if there are differences other than convenience which distinguish the two activation functions .",127,2444,2019-05-02T16:09:44.820,2019-05-02T16:09:44.820,what 's the difference between hyperbolic tangent and sigmoid neurons ?,neural-networks machine-learning deep-network difference hidden-layers,2,0,1
51,118,1,122,2016-08-02T19:22:20.577,8,342,fuzzy logic is the logic where every statement can have any real truth value between 0 and 1 . how can fuzzy logic be used in creating ai ? is it useful for certain decision problems involving multiple inputs ? can you give an example of an ai that uses it ?,29,-1,2017-04-13T12:53:10.013,2016-08-02T20:10:13.770,how can fuzzy logic be used in creating ai ?,fuzzy-logic,2,0,1
52,120,1,125,2016-08-02T19:31:01.370,4,145,"in minds , machines and gödel ( 1959 ) , j. r. lucas shows that any human mathematician can not be represented by an algorithmic automaton ( a turing machine , but any computer is equivalent to it by the church - turing thesis ) , using gödel 's incompleteness theorem . as i understand it , he states that since the computer is an algorithm and hence a formal system , gödel 's incompleteness theorem applies . but a human mathematician also has to work in a formal axiom system to prove a theorem , so would n't it apply there as well ?",29,,,2016-08-02T19:49:46.050,how does lucas 's argument work ?,philosophy incompleteness-theorems,2,0,1
53,123,1,,2016-08-02T19:42:07.160,9,339,"back in college , i had a complexity theory teacher who stated that artificial intelligence was a contradiction in terms . if it could be calculated mechanically , he argued , it was n't intelligence , it was math . this seems to be a variant of the chinese room argument . this argument is a metaphor , where a person is put in a room full of chinese books . this person does n't understand a word of chinese but is slipped messages in chinese under the door . the person has to use the books , which contain transformation rules , to answer these messages . the person can apply the transformation rules but does not understand what ( s)he is communicating . does the chinese room argument hold ? can we argue that artificial intelligence is merely clever algorithmics ?",66,14723,2018-05-01T13:00:54.623,2018-05-01T13:00:54.623,does the chinese room argument hold against ai ?,philosophy chinese-room-argument,4,0,1
54,130,1,135,2016-08-02T19:58:28.117,2,40,what are the main differences between deep boltzmann machines ( dbm ) recurrent neural network and deep belief network ( which is based on rbms ) ?,8,,,2016-08-02T20:12:35.360,what are the main differences between deep boltzmann machines and deep belief network ?,boltzmann-machine,1,0,
55,136,1,,2016-08-02T20:15:22.637,1,102,"a cellular automaton is a state machine which is controlled by external input . the input is given by geometrical space around a cell . in a square matrix , each automaton gets input from 4 surrounding cells , while a hexagon grid has 6 neighbor cells which can be used as automaton input . for example , a 4-cells input may be the string “ 1011 ” . this string specify a state of the cellular automaton . the automaton will switch to a different state according to the lookup table . i want to know if increasing the number of input cells in a hexagon automaton will make the resulting computer more powerful . original message i 'd like to learn more about the differences between related automata which can be based on hexagonal cells instead of squares ( rule 34/2 ) , like in codi model which uses spiking neural network ( snn ) . is using a plane tiled with regular hexagons more efficient and reliable than using square cells ? what is the difference and how do i know which one to use in which scenario ? in other words , the more efficiently flexible that it grows , the more difficult scenarios it can be used for ( for me , hexagonal implicates more possibilities , because it can send / share the signal with / to more tiles ) . or maybe one is more modern than the other , or they 're both on the same level ? in general , i 'd like to learn the differences between them to know when i should use one over the other .",8,1671,2018-08-24T19:01:12.483,2018-08-24T19:01:12.483,number of input variables for a cellular automaton ( was : squares or hexagonal ? ),evolutionary-algorithms topology efficiency,0,5,2
56,140,1,144,2016-08-02T20:37:59.927,7,135,"an ultraintelligent machine is a machine that can surpass all intellectual activities by any human , and such machine is often used in science fiction as a machine that brings mankind to an end . any machine is executed using an algorithm . by the church - turing thesis , any algorithm that can be executed by a modern computer can be executed by a turing machine . however , a human can easily simulate a turing machine . does n't this mean that a machine ca n't surpass all intellectual activities , since we can also execute the algorithm ? this argument is most likely flawed , since my intuition tells me that ultraintelligent machines are possible . however , it is not clear to me where the flaw is . note that this is my own argument .",29,,,2018-04-15T04:14:15.697,does this argument refuting the existence of ultraintelligent machines work ?,philosophy ultraintelligent-machine,3,2,
57,145,1,,2016-08-02T21:10:26.693,11,994,"from wikipedia : aixi [ ' ai̯k͡siː ] is a theoretical mathematical formalism for artificial general intelligence . it combines solomonoff induction with sequential decision theory . aixi was first proposed by marcus hutter in 2000[1 ] and the results below are proved in hutter 's 2005 book universal artificial intelligence.[2 ] albeit non - computable , approximations are possible , such as aixitl . finding approximations to aixi could be an objective way for solving ai . is aixi really a big deal in artificial general intelligence research ? can it be thought as a central concept for the field ? if so , why do n't we have more publications on this subject ( or maybe we have and i 'm not aware of them ) ?",144,2444,2019-04-19T15:13:02.567,2019-04-19T15:13:02.567,what is the relevance of aixi on current artificial intelligence research ?,models agi aixi,3,0,4
58,146,1,,2016-08-02T21:14:36.133,3,240,"in what ways can connectionist artificial intelligence ( neural networks ) be integrated with good old - fashioned a.i . ( gofai ) ? for instance , how could deep neural networks be integrated with knowledge bases or logical inference ? one such example seems to be the opencog + destin integration .",144,,,2016-08-03T08:22:36.737,in what ways can connectionist a.i . be integrated with gofai ?,neural-networks classical-ai gofai symbolic-computing,1,0,
59,147,1,,2016-08-02T21:15:34.483,2,223,it is proved that a recurrent neural net with rational weights can be a super - turing machine . can we achieve this in practice ?,159,,,2016-09-03T17:38:06.577,can we ever achieve hypercomputation using recurrent neural networks ?,neural-networks hypercomputation recurrent-neural-networks,2,1,1
60,148,1,170,2016-08-02T21:16:44.013,16,576,"given the proven halting problem for turing machines , can we infer limits on the ability of strong artificial intelligence ?",55,10,2016-08-03T18:33:16.540,2019-03-06T11:00:52.413,"what limits , if any , does the halting problem put on artificial intelligence ?",halting-problem,2,0,6
61,151,1,,2016-08-02T21:25:25.313,1,38,by default using deepdream technique you can creating a dreamlike image out of two different images . is it possible to easily enhance this technique to generate one image out from three ?,8,,,2016-08-02T21:25:25.313,"can deepdream produce a "" dream "" from 3 images ?",convolutional-neural-networks deepdream,0,2,1
62,152,1,1728,2016-08-02T21:34:32.107,7,412,"consider these neural style algorithms which produce some art work : neural doodle neural - style why is generating such images so slow and why does it take huge amounts of memory ? is n't there any method of optimizing the algorithm ? what is the mechanism or technical limitation behind this ? why we ca n't have a realtime processing ? here are few user comments ( how anyone can create deep style images ): anything above 640x480 and we 're talking days of heavy crunching and an insane amount of ram . i tried doing a 1024pixel image and it still crashed with 14gigs memory , and 26gigs swap . so most of the vm space is just the swapfile . plus it takes several hours potentially days cpu rendering this . i tried 1024x768 and with 16gig ram and 20 + gig swap it was still dying from lack of memory . having a memory issue , though . i 'm using the "" g2.8xlarge "" instance type .",8,145,2016-08-18T13:24:43.510,2018-04-15T04:16:38.293,why is the generation of deep style images so slow and resource - hungry ?,performance neural-doodle deepdreaming,2,0,
63,153,1,,2016-08-02T21:36:28.053,9,1638,"can autoencoders be used for supervised learning without adding an output layer ? can we simply feed it with a concatenated input - output vector for training , and reconstruct the output part from the input part when doing inference ? the output part would be treated as missing values during inference and some imputation would be applied .",144,7496,2017-06-17T21:22:44.503,2017-06-17T21:22:44.503,can autoencoders be used for supervised learning ?,neural-networks,2,2,2
64,154,1,158,2016-08-02T21:37:32.420,18,11513,"i 'm aware that neural networks are probably not designed to do that , however asking hypothetically , is it possible to train the deep neural network ( or similar ) to solve math equations ? so given the 3 inputs : 1st number , operator sign represented by the number ( 1 - + , 2 - - , 3 - / , 4 - * , and so on ) , and the 2nd number , then after training the network should give me the valid results . example 1 ( 2 + 2 ): input 1 : 2 ; input 2 : 1 ( + ) ; input 3 : 2 ; expected output : 4 input 1 : 10 ; input 2 : 2 ( - ) ; input 3 : 10 ; expected output : 0 input 1 : 5 ; input 2 : 4 ( * ) ; input 3 : 5 ; expected output : 25 and so the above can be extended to more sophisticated examples . is that possible ? if so , what kind of network can learn / achieve that ?",8,8,2017-03-09T17:03:38.327,2019-01-22T18:45:30.140,is it possible to train the neural network to solve math equations ?,neural-networks math,5,0,10
65,156,1,,2016-08-02T21:59:01.093,16,1054,"from wikipedia : a mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another . mirror neurons are related to imitation learning , a very useful feature that is missing in current real - world a.i . implementations . instead of learning from input - output examples ( supervised learning ) or from rewards ( reinforcement learning ) , an agent with mirror neurons would be able to learn by simply observing other agents , translating their movements to its own coordinate system . what do we have on this subject regarding computational models ?",144,,,2016-10-14T13:08:35.790,are there any computational models of mirror neurons ?,neural-networks models,3,0,8
66,157,1,267,2016-08-02T22:38:50.823,7,106,"if i have a paragraph i want to summarize , for example : ponzo and fila went to the mall during the day . they walked for a long while , stopping at shops . they went to many shops . at first , they did n't buy anything . after going to a number of shops , they eventually bought a shirt , and a pair of pants . better summarized as : they shopped at the mall today and bought some clothes . what is the best ai strategy to automate this process , if there is one ? if there is n't , is it because it would be dependent on first having an external information resource that would inform any algorithm ? or is it because the problem is inherently contextual ?",46,4302,2018-10-08T12:44:30.960,2018-10-08T12:44:30.960,what artificial intelligence strategies are useful for summarization ?,algorithm natural-language-processing pattern-recognition,1,0,0
67,159,1,238,2016-08-02T23:01:56.157,1,336,"what happens if you apply the same deep dream technique which produces "" dream "" visuals but to media streams such as audio files ? does changing image functions into audio and enhancing the logic would work , or will it no longer work / doesn't make any sense ? my goal is to create "" dream "" like audio based on the two samples .",8,14723,2018-04-14T18:06:52.290,2018-04-14T18:06:52.290,is it possible to apply deep dream technique for the audio streams ?,convolutional-neural-networks deepdreaming,1,0,
68,167,1,174,2016-08-03T01:55:30.377,4,132,in deepdream wikipedia page it 's suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when they 're tripping . the imagery to lsd- and psilocybin - induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex . how this is even possible ? how exactly convolutional neural networks have anything to do with human visual cortex ?,8,145,2016-08-18T11:34:48.770,2016-08-18T11:34:48.770,why would neural network dream scenes mirror the hallucinations people experience when they 're tripping ?,convolutional-neural-networks deepdream computer-vision deepdreaming,1,0,
69,169,1,176,2016-08-03T02:12:37.943,1,51,"this 2014 article saying that a chinese team of physicists have trained a quantum computer to recognise handwritten characters . why did they have to use a quantum computer to do that ? is it just for fun and demonstration , or is it that recognising the handwritten characters is so difficult that standard ( non - quantum ) computers or algorithms can not do that ? if standard computers can achieve the same thing , what are the benefits of using quantum computers to do that then over standard methods ?",8,8,2016-08-06T00:07:31.233,2016-08-06T00:07:31.233,what are the challenges for recognising the handwritten characters ?,quantum-computing handwritten-characters ocr,1,0,2
70,172,1,,2016-08-03T03:33:35.193,1,69,"is it possible that at some time in the future , ais will be able to initiatively develop themselves , rather than passively being developed by humanity ?",104,,,2016-08-03T20:03:22.657,ais ' self - evaluation threshold,neural-networks machine-learning unsupervised-learning unassisted-learning,3,0,1
71,179,1,,2016-08-03T05:15:39.443,6,530,"i have been wondering since a while ago about the multiple intelligences and how they could fit in the field of artificial intelligence as a whole . we hear from time to time about leonardo being a genius or bach 's musical intelligence . these persons are commonly said to be ( have been ) more intelligent . but the multiple intelligences speak about cooking or dancing or chatting as well , i.e. coping with everyday tasks ( at least that 's my interpretation ) . are there some approaches on incorporating multiple intelligences into ai ? related question - how could emotional intelligence be implemented ?",70,-1,2017-04-13T12:53:10.013,2016-08-26T18:17:40.033,how can the multiple intelligences model be incorporated into ai ?,emotional-intelligence new-ai,2,0,
72,180,1,,2016-08-03T05:23:01.403,2,65,which algorithms are there to create word embeddings for a given language ?,202,2444,2019-04-16T22:11:59.630,2019-04-16T22:11:59.630,how can we build word embeddings for a language ?,natural-language-processing algorithm word-embedding,0,1,
73,182,1,183,2016-08-03T05:31:30.353,13,215,"how to decide the optimum number of layers to be created while implementing a neural network ( feedforward , back propagation or rnn ) ?",202,1671,2017-10-18T20:19:03.600,2017-10-20T02:11:25.050,optimal number of layers in a neural network ?,neural-networks hidden-layers,2,0,3
74,184,1,,2016-08-03T06:03:28.903,4,44,"i am interested in the emergence of properties in agents , and , more generally in robotics . i was wondering if there is work on the emergence of time - related concepts , on the low - level representation of notions like before and after . i know , for example , that there is work on the emergence of spatial representation ( similar to knn ) , or even communication * but time seems to be a tricky concept . this has everything to do with the platform , i.e. the way that the representation would be coded in . we tend to favour ways that have some meaning or somehow mimic natural , well , yes , human structures , like the brain . i am not a neuroscientist and do not know that the sense of time looks like in humans , or if it is even present in other living beings . is there some work on the ( emergence of the ) representation of time in artificial agents ? * i remember watching a really cool ... actually creepy video from these robots but can not find it anymore . does anyone have the link at hand ?",70,70,2016-09-02T21:11:12.350,2016-09-02T21:11:12.350,are there emergent models of time in robots ?,knowledge-representation time embodied-cognition,1,7,2
75,186,1,,2016-08-03T06:20:12.393,4,931,"have there been proposed extensions to go beyond a turing machine that solve the halting problem and if so , would those proposed extensions have value to advance strong artificial intelligence ? for example , does quantum computing go beyond the definition of a turing machine and resolve the halting problem , and does that help in creating strong ai ?",55,,,2016-08-07T23:34:29.837,does a quantum computer resolve the halting problem and would that advance strong ai ?,quantum-computing halting-problem strong-ai,2,5,1
76,191,1,1526,2016-08-03T07:57:21.743,5,90,"what was the first ai that was able to carry on a conversation , with real responses , such as in the famous ' i am not a robot . i am a unicorn ' case ? a ' real response ' constitutes a sort - of personalized answer to a specific input by a user .",145,4302,2018-10-08T12:44:10.017,2018-10-08T12:44:10.017,what was the first machine that was able to carry on a conversation ?,natural-language-processing chat-bots history turing-test,2,2,
77,197,1,,2016-08-03T08:56:53.430,9,355,"this question stems from quite a few "" informal "" sources . movies like 2001 , a space odyssey and ex machina ; books like destination void ( frank herbert ) , and others suggest that general intelligence wants to survive , and even learn the importance for it . there may be several arguments for survival . what would be the most prominent ?",169,,,2017-04-22T04:23:39.397,is there a strong argument that survival instinct is a prerequisite for creating an agi ?,agi,3,4,
78,198,1,1345,2016-08-03T09:01:05.790,16,246,"identifying sarcasm is considered as one of the most difficult open - ended problems in the domain of ml and nlp . so , was there any considerable research done in that front ? if yes , then what is the accuracy like ? please also explain the nlp model briefly .",101,4302,2018-10-08T12:43:51.623,2018-10-08T12:43:51.623,"what research has been done in the domain of "" identifying sarcasm in text "" ?",natural-language-processing research semantics,2,2,7
79,202,1,206,2016-08-03T09:18:52.437,0,206,"i 'd like to know more about implementing emotional intelligence . given i 'm implementing a chat bot and i 'd like to introduce the levels of curiosity to measure whether user text input is interesting or not . high level would mean bot is asking more questions and is following the topic , lower level of curiosity makes the bot not asking any questions and changing the topics . less interesting content could mean the bot does n't see any opportunity to learn something new or it does n't understand the topic or does n't want to talk about it , because of its low quality . how this possibly can be achieved ? are there any examples ?",8,-1,2017-04-13T12:53:10.013,2016-08-30T21:04:22.253,how can you simulate level of curiosity for a chat bot ?,emotional-intelligence chat-bots,2,0,2
80,205,1,219,2016-08-03T10:03:58.587,6,1470,"i would like to learn more whether it is possible and how to write a program which decompiles executable binary ( an object file ) to the c source . i 'm not asking exactly ' how ' , but rather how this can be achieved . given the following hello.c file ( as example ) : # include & lt;stdio.h&gt ; int main ( ) { printf(""hello world ! "" ) ; } then after compilation ( gcc hello.c ) i 've got the binary file like : $ hexdump -c a.out | head 00000000 cf fa ed fe 07 00 00 01 03 00 00 80 02 00 00 00 | ................ | 00000010 0f 00 00 00 b0 04 00 00 85 00 20 00 00 00 00 00 | .......... ..... | 00000020 19 00 00 00 48 00 00 00 5f 5f 50 41 47 45 5a 45 | .... h ... __pageze| 00000030 52 4f 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |ro .............. | 00000040 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 | ................ | 00000050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................ | 00000060 00 00 00 00 00 00 00 00 19 00 00 00 d8 01 00 00 | ................ | 00000070 5f 5f 54 45 58 54 00 00 00 00 00 00 00 00 00 00 |__text .......... | $ wc -c hello.c a.out 60 hello.c 8432 a.out for the learning dataset i assume i 'll have to have thousands of source code files along with its binary representation , so algorithm can learn about moving parts on certain changes . my concerns are : do my algorithm needs to be aware about the header file , or it 's "" smart "" enough to figure it out , if it needs to know about the header , how do i tell my algorithm ' here is the header file ' , what should be input / output mapping ( whether some section to section or file to file ) , do i need to divide my source code into some sections , do i need to know exactly how decompilers work or ai can figure it out for me , or should i 've two networks , one for header , another for body it - self , or more separate networks , each one for each logical component ( e.g. byte->c tag , etc . ) how would you tackle this ?",8,8,2016-08-03T11:51:11.470,2016-08-03T14:19:39.400,how to write c decompiler using ai ?,algorithm deep-learning,1,1,
81,207,1,,2016-08-03T10:14:49.743,4,69,"text summarization is a long - standing research problem that was "" ignited "" by luhn in 1958 . however , a half century later , we still came nowhere close to solving this problem ( abstractive summarization ) . the reason for this might be because researchers are resorting to statistical ( and sometimes linguistic ) methods to find & amp ; extract the most salient parts of the text . is summarization problem solvable using ai ( neural networks to be precise ) ?",,4302,2018-10-08T12:42:18.353,2018-10-08T12:42:18.353,can abstractive summarization be achieved using neural networks ?,neural-networks natural-language-processing text-summarization,1,0,
82,210,1,215,2016-08-03T12:02:55.280,0,30,"i 'd like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network . i 'm not talking about memory storage , but file storage , so the data can be loaded later on . my first guess would be xml , but having millions of connections and weights would generate huge amount of data . another thing would be to dump object instances into binary file using some export / serialize functions , but the disadvantage is that the file is n't common and it 's language specific . are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program ? if so , which one .",8,8,2016-08-03T12:34:22.730,2016-08-03T13:02:40.337,what 's the most suitable format to store huge number of neurons states ?,storage,2,0,
83,211,1,1664,2016-08-03T12:27:56.120,1,367,"what ai techniques does ibm use for its watson platform , specifically its natural language analysis ?",8,4302,2018-10-08T12:41:34.563,2018-10-08T12:41:34.563,how does deepqa analyze natural language ?,algorithm natural-language-processing watson lexical-recognition,1,7,1
84,212,1,,2016-08-03T12:41:06.953,1,105,"i 'm investigating the possibility of storing the semantic - lexical connections ( such as the relationships to the other words such as phrases and other dependencies , its strength , part of speech , language , etc . ) in order to provide analysis of the input text . i assume this has been already done . if so , to avoid reinventing the wheel , is there any efficient method to store and manage such data in some common format which has been already researched and tested ?",8,,,2016-08-26T23:10:09.577,how to store datasets of lexical connections ?,algorithm models research storage lexical-recognition,2,0,1
85,214,1,,2016-08-03T13:01:20.740,2,82,"which objective and measurable tests have been developed to test the intelligence of ai ? the classical test is the turing test , which has objective criteria and is measurable since it can be measured what percentage of the jury is fooled by the ai . i am looking for other , more modern tests .",29,,,2018-11-01T16:55:48.237,which objective tests have been developed to test the intelligence of ai ?,intelligence-testing,1,2,1
86,218,1,1745,2016-08-03T14:17:17.257,5,1666,"i 'm interested in implementing a program for natural language processing ( aka eliza ) . assuming that i 'm already storing semantic - lexical connections between the words and its strength . what are the methods of dealing with words which have very distinct meaning ? few examples : ' are we on the same page ? ' the ' page ' in this context is n't a document page , but it 's part of the phrase . ' i 'm living in reading . ' the ' reading ' is a city ( noun ) , so it 's not a verb . otherwise it does n't make any sense . checking for the capital letter would work in that specific example , but it wo n't work for other ( like ' make ' can be either verb or noun ) . ' i 've read something on the facebook wall , do you want to know what ? ' the ' facebook wall ' has nothing to do with wall at all . in general , how algorithm should distinguish the word meaning and recognise the word within the context ? for example : detecting the word for different type of speech , so it should recognise whether it 's a verb or noun . detecting whether the word is part of phrase . detecting word for multiple meaning . what are the possible approaches to solve that problem in order to identify the correct sense of a word with the context ?",8,-1,2017-04-13T12:53:10.013,2016-10-08T00:11:37.960,how to resolve lexical ambiguity in natural language processing ?,natural-language-processing lexical-recognition,2,2,3
87,220,1,223,2016-08-03T14:23:50.760,1,380,"unsupervised learning does not involve target values , so basically targets are most likely the same as the inputs ( in other words , involves no target values ) . so how does this model learn ?",8,8,2016-08-03T14:43:33.990,2016-10-14T16:54:29.477,how does unsupervised learning model learn ?,models unsupervised-learning,1,3,
88,221,1,,2016-08-03T14:32:39.333,4,312,"currently , many different organizations do cutting - edge ai research , and some innovations are shared freely ( at a time lag ) while others are kept private . i 'm referring to this state of affairs as ' multipolar , ' where instead of there being one world leader that 's far ahead of everyone else , there are many competitors who can be mentioned in the same breath . ( there 's not only one academic center of ai research worth mentioning , there might be particularly hot companies but there 's not only one worth mentioning , and so on . ) but we could imagine instead there being one institution that mattered when it comes to ai ( be it a company , a university , a research group , or a non - profit ) . this is what i 'm referring to as "" monolithic . "" maybe they have access to tools and resources no one else has access to , maybe they attract the best and brightest in a way that gives them an unsurmountable competitive edge , maybe returns to research compound in a way that means early edges ca n't be overcome , maybe they have some sort of government coercion preventing competitors from popping up . ( for other industries , network or first - mover effects might be other good examples of why you would expect that industry to be monolithic instead of multipolar . ) it seems like we should be able to use insights from social sciences like economics or organizational design or history of science in order to figure out , if not which path seems more likely , how we would know which path seems more likely . ( for example , we may be able to measure how much returns to research compound , in the sense of one organization coming up with an insight meaning that organization is likely to come up with the next relevant insight , and knowing this number makes it easier to figure out where the boundary between the two trajectories is located . )",10,,,2017-07-15T16:07:53.250,"how would we know if ai development will continue to be multipolar , or will become monolithic ?",research ai-community,1,1,1
89,224,1,,2016-08-03T14:58:03.663,4,75,"one of the most compelling applications for ai would be in augmenting human biological intelligence . what are some of the currently proposed methods for doing this aside from vague notions such as "" nanobots swimming around our brains and bodies "" or "" electrodes connected to our skulls "" ?",148,10135,2018-10-21T20:53:59.603,2018-10-21T20:53:59.603,how could ai be used to augment human biological intelligence ?,cyborg intelligence-augmentation,1,3,1
90,225,1,228,2016-08-03T15:25:50.843,-6,240,"given list of fixed numbers from a mathematical constant such as pi , is it is possible to train ai to attempt to predict the next numbers ? which ai or neural network would be more suitable for this task ? especially the one which will work without memorizing the entire training set , but the one which will attempt to find some patterns or statistical association .",8,8,2016-08-09T10:02:46.153,2016-08-09T10:02:46.153,what are the approaches to predict sequence of π numbers ?,training math recurrent-neural-networks,2,4,
91,227,1,1287,2016-08-03T15:32:46.710,6,1520,what are the main differences between two types of feedforward networks such as multilayer perceptrons ( mlp ) and radial basis function ( rbf ) ? what are the fundamental differences between these two types ?,8,249,2016-08-03T20:20:16.283,2016-08-04T13:56:49.603,what is the difference between mlp and rbf ?,comparison mlp,2,3,
92,233,1,236,2016-08-03T15:56:18.480,10,374,"according to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants . a good example would be deepmind 's alphago which i believe is a deep neural network , for vision cnn , text , music and other ordered features rnn 's , etc . but for machine learning application we have neural networks , support vector machines , random forest , regression methods , etc . available for applications . so are neural networks and its variants the only way to reach "" true "" artificial intelligence ?",39,42,2016-08-10T12:36:26.970,2016-08-10T12:36:26.970,are neural networks and its variants the only way to reach true artificial intelligence ?,neural-networks,3,0,2
93,237,1,253,2016-08-03T16:22:36.073,4,100,"i 'm interested in hardware implementation of anns ( artificial neural networks ) . are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks ? for example , a chip which is optimised for an application like image recognition or something similar ?",8,33,2016-08-03T19:55:57.227,2016-08-06T01:22:37.030,are there any microchips specifically designed to run anns ?,image-recognition hardware,1,4,1
94,240,1,243,2016-08-03T17:22:05.433,14,3041,"i 've noticed that a few questions on this site mention genetic algorithms and it made me realize that i do n't really know much about those . i have heard the term before , but it 's not something i 've ever used , so i do n't have much idea about how they work and what they are good for . all i know is that they involve some sort of evolution and randomly changing values . can you give me a short explanation , preferably including some sort of practical example that illustrates the basic principles ?",30,,,2016-08-04T18:01:36.313,what exactly are genetic algorithms and what sort of problems are they good for ?,genetic-algorithms,5,0,6
95,241,1,,2016-08-03T17:24:18.480,4,189,"in detective novels , the point is often that the reader gets enough information to solve the crime themselves . this "" puzzle "" aspect of detective novels is part of the attraction . often the difficulty for humans is to keep track of all the variables - events , items , motivations . an ai would have an easier time keeping track of all the details , but would rely on real - world knowledge to prevent making crazy mistakes . for example , if it was stated that a character took the train , the ai would need to know that this is a method of transportation - that it changes the location property of an agent over time . has an ai ever been able to solve a detective mystery ?",66,4302,2018-10-08T12:41:15.483,2018-10-08T12:41:15.483,has an ai ever solved a detective mystery ?,natural-language-processing problem-solving world-knowledge,3,1,
96,247,1,,2016-08-03T18:05:24.997,6,451,"in 1969 , seymour papert and marvin minsky showed that perceptrons could not learn the xor function . this was solved by the backpropagation network with at least one hidden layer . this type of network can learn the xor function . i believe i was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers , could also be learnt by a backpropagation neural network with a single hidden layer . ( although possible a nonlinear activation function was required ) . however , it is unclear to me what the limits are to backpropagation neural networks themselves . which patterns can not be learnt by a backpropgation neural network ?",66,,,2017-07-23T21:04:25.333,what are the limits to what can be learnt using a backpropagation neural network ?,neural-networks machine-learning backpropagation learning-theory,2,2,2
97,248,1,5803,2016-08-03T18:05:26.937,15,2155,"over the last 50 years , the rise / fall / rise in popularity of neural nets has acted as something of a ' barometer ' for ai research . it 's clear from the questions on this site that people are interested in applying deep learning ( dl ) to a wide variety of difficult problems . i therefore have two questions : practitioners - what do you find to be the main obstacles to applying dl ' out of the box ' to your problem ? researchers - what techniques do you use ( or have developed ) that might help address practical issues ? are they within dl or do they offer an alternative approach ?",42,95,2016-08-04T14:09:54.380,2018-03-26T13:17:52.717,issues with and alternatives to deep learning approaches ?,deep-learning,4,2,2
98,249,1,1286,2016-08-03T18:12:22.133,2,54,"is it possible for unsupervised learning to learn about high - level , class - specific features given only unlabelled images ? for example detecting human or animal faces ? if so , how ?",8,8,2016-08-04T00:31:01.093,2016-08-04T06:25:20.620,is it possible for ' unsupervised learning ' model to recognize features on unlabelled images ?,image-recognition unsupervised-learning,1,1,
99,258,1,3936,2016-08-03T19:03:20.587,6,183,"on the wikipedia page we can read the basic structure of an artificial neuron ( a model of biological neurons ) which consist : dendrites - acts as the input vector , soma - acts as the summation function , axon - gets its signal from the summation behavior which occurs inside the soma . i 've checked deep learning wiki page , but i could n't find any references to dendrites , soma or axons . so my question is , which type of artificial neural network implements or can mimic such model most closely ?",8,,,2019-04-16T18:39:59.510,which ann can mimic biological neurons the most ?,artificial-neuron,4,1,3
100,1274,1,,2016-08-04T01:39:20.737,1,134,"have there been any studies which attempted to use ai algorithms to detect human thoughts or emotions based on brain activity , such as using bci / eeg devices ? by this , i mean simple guesses such as whether the person was happy or angry , or what object ( e.g. banana , car ) they were thinking about . if so , did any of those studies show some degree of success ?",8,145,2016-08-17T13:03:50.990,2016-08-23T15:25:23.607,are there any studies which attempt to use ai to guess the human emotion based on the brainwaves ?,research signal-processing,2,5,2
101,1285,1,1315,2016-08-04T05:07:03.323,9,255,"has there been any attempts to deploy ai with blockchain technology ? are there any decentralized examples of ai networks with no central point of control with ai nodes acting independently ( but according to a codified set of rules ) creating , validating and storing the same shared decentralized database in many locations around the world ?",1256,1671,2019-01-27T04:34:41.773,2019-01-27T04:34:41.773,artificial intelligence on the blockchain,multi-agent-systems blockchain,2,0,3
102,1288,1,,2016-08-04T07:34:06.557,6,1691,"in their famous book entitled "" perceptrons : an introduction to computational geometry "" , minsky and papert show that a perceptron ca n't solve the xor problem . this contributed to the first ai winter , resulting in funding cuts for neural networks . however , now we know that a multilayer perceptron can solve the xor problem easily . backprop was n't known at the time , but did they know about manually building multilayer perceptrons ? did minsky & amp ; papert know that multilayer perceptrons could solve xor at the time they wrote the book , albeit not knowing how to train it ?",144,144,2016-08-04T08:13:56.207,2016-08-07T09:42:52.590,did minsky & papert know that multilayer perceptrons could solve xor ?,neural-networks history,1,4,1
103,1289,1,,2016-08-04T07:41:16.090,4,107,according to wikipedia artificial general intelligence(agi ) artificial general intelligence ( agi ) is the intelligence of a ( hypothetical ) machine that could successfully perform any intellectual task that a human being can . according to below image todays artifical intellgence is same as that of a lizards . lets assume(or not ) that within 10 - 20 years we humans are successful in creating a agi or agis . as agi has the same intelligence and emotions as that of humans because according to wikipedia definition it can perform same intellectual task of a human . then can we destroy an agi without its consent ? do this be considered as murder ?,39,,,2017-03-11T00:14:25.140,can we destroy artificial general intelligence without its consent ?,ethics,2,1,
104,1290,1,2229,2016-08-04T07:43:33.010,7,747,"deep mind has published a lot of works on deep learning in the last years , most of them state - of - the - art on their respective tasks . but how much of this work has actually been reproduced by the ai community ? for instance , the neural turing machine paper seems to be very hard to reproduce , according to other researchers .",144,,,2018-04-03T19:47:18.407,how much of deep mind 's work is actually reproducible ?,neural-networks deep-learning research,2,3,1
105,1294,1,1313,2016-08-04T08:28:06.277,33,20533,"geoffrey hinton has been researching something he calls "" capsules theory "" in neural networks . what is this and how does it work ?",144,,,2017-12-04T13:01:41.203,"how does hinton 's "" capsules theory "" work ?",neural-networks,5,2,27
106,1295,1,1299,2016-08-04T08:32:36.610,10,215,"during my research , i 've stumbled upon "" complex - valued neural networks "" , which are neural networks that work with complex - valued inputs ( probably weights too ) . what are the advantages ( or simply the applications ) of this kind of neural network over real - valued neural networks ?",144,,,2016-08-04T10:28:59.957,what are the advantages of complex - valued neural networks ?,neural-networks,1,0,4
107,1296,1,,2016-08-04T08:39:43.740,7,2514,"the author claims that guiding evolution by novelty alone ( without explicit goals ) can solve problems even better than using explicit goals . in other words , using a novelty measure as a fitness function for a genetic algorithm works better than a goal - directed fitness function . how is that possible ?",144,,,2017-10-21T01:27:47.477,"how does "" novelty search "" work ?",genetic-algorithms,2,0,1
108,1297,1,1327,2016-08-04T09:22:21.360,3,58,"quote from this eric 's meta post about modelling and implementation : they are not exactly the same , although strongly related . this was a very difficult lesson to learn among mathematicians and early programmers , notably in the 70s ( mathematical proofs can demand a lot of non - trivial programming work to make them "" computable "" , as in runnable on a computer ) . if they 're not the same , what is the difference ? how we can say when we 're talking about ai implementation , and when about modelling ? it 's suggested above it 's not easy task . so where we can draw the line when we talk about it ? i 'm asking in general , not specifically for this site , that 's why i have n't posted question in meta",8,-1,2017-03-16T16:44:15.193,2016-08-08T04:44:18.407,how to distinguish ai modeling from implementation ?,models comparison implementation,2,0,
109,1301,1,1337,2016-08-04T12:10:26.593,1,51,"given pictures with multiple features such as faces , can single ai algorithm detect all of them , or for better reliability is it preferred to use separate instances ? in other words i 'm talking about attempt of finding all possible human faces on the same picture by a single neural network .",8,127,2016-08-04T21:07:01.750,2016-08-04T21:07:01.750,do you need single or multiple networks to detect multiple faces ?,deep-network algorithm image-recognition,1,2,
110,1303,1,1305,2016-08-04T12:28:07.427,3,2876,"i read some information 1 about attempts to build neural networks in the php programming language . personally i think php is not the right language to do so at all probably because it 's a high - level language , i assume low level language are way more suitable for ai in terms of performance and scalability . is there a good / logical reason why you should or should n't use php as a language to write ai in ? 1 http://www.developer.com/lang/php/creating-neural-networks-in-php.html and https://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there",217,-1,2017-05-23T12:39:33.010,2016-10-31T09:06:48.290,can php be considered as a serious programming language for ai ?,neural-networks programming-languages,2,3,
111,1306,1,1312,2016-08-04T12:40:22.857,1,161,"i 've found this old scientific paper from 1988 about introduction of ai into nuclear power fields . were or still are there any dangers by application of such algorithm ? are nuclear power plants or human life in risk if the algorithm will fail ? especially applications to the core , like cooling systems and other components which can be affected in negative way .",8,8,2016-08-04T14:10:15.490,2017-08-05T06:27:25.440,what are the dangers of ai applications for nuclear industry ?,applications,1,4,
112,1308,1,7901,2016-08-04T12:58:01.810,5,79,"since we 've self - driving cars already , would we have self - flying commercial flights in the near future ? basically the ai which can do take off , flying , landing and parking .",8,8,2018-09-08T15:09:45.913,2018-09-10T03:41:13.010,is ai capable to replace pilots entirely on the commercial flights ?,real-world,1,4,
113,1314,1,1316,2016-08-04T14:18:10.547,5,1711,"how much processing power is needed to emulate the human brain ? more specifically , the neural simulation , such as communication between the neurons and processing certain data in real - time . i understand that this may be a bit of speculation and it 's not possible to be accurate , but i 'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain .",8,145,2016-08-17T13:03:43.787,2017-03-16T06:51:39.083,how powerful a computer is required to simulate the human brain ?,hardware neuromorphic-engineering,3,1,
114,1318,1,1820,2016-08-04T15:10:41.230,2,331,"the situation : a self - driving car is traveling at it 's maximum speed , 25 mph ( 40 km / h ) , in the middle of an empty street with the ability to change lanes on both sides . there are two passengers , one in the front and another in the back . someone jumps from the side of the road directly into the path of the car . a collision would occur in 50 meters . breaking distance at this speed is about 24 m . the question : is it known how the current implementation of the google car ai would react , or is it currently a matter of speculation ? a step - by - step explanation of the ai 's decisioning process would be preferred . possible answers : the car could activate its brakes immediately , coming to a halt as quickly as possible . this would be sooner than a human could stop , as people require time to recognize the possibility of a collision , and then physically slam on the brake . ( thinking distance ) . alternatively , the car could continue traveling forward , processing the situation . ( similar to a humans thinking distance ) . the person may continue to move , either out of the way , or still into danger of being hit . in this case , the car may decide to change lanes in an attempt to pass around the person . lastly and most unlikely , the car will not alter its course and proceed to drive forward . do not attempt to do it to check ; )",8,1812,2016-08-31T23:17:29.040,2016-09-01T01:52:21.400,what would happen if someone jumped in the front of a google car ?,self-driving decision-theory,1,5,
115,1320,1,1324,2016-08-04T15:25:37.293,1,195,"artificial intelligence is present in many games , both current and older games . how can such intelligence understand what to do ? i mean , how can it behave like a human in a game , allowing you to play against itself , or that ai plays against itself ? in games like age of empires , for example .",173,145,2016-08-09T20:36:48.157,2016-08-13T03:12:52.877,how does artificial intelligence work in games ?,gaming,2,1,2
116,1323,1,1325,2016-08-04T15:49:13.793,3,59,"at a related question in computer science se , a user told : neural networks typically require a large training set . is there a way to define the boundaries of the "" optimal "" size of a training set in general case ? when i was learning about fuzzy logic , i 've heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets . is there such a method that can be applicable for an already defined neural network topology ?",1270,4302,2018-07-23T15:36:34.730,2018-07-23T15:36:34.730,is there a way to define the boundaries of the optimal size of a training set ?,neural-networks training optimization topology,2,0,
117,1332,1,,2016-08-04T18:26:31.213,6,232,how important is true ( non- pseudo ) randomness in artificial intelligence designs ? is there any chance that pseudo - randomness could be a barrier to more successful designs ?,46,,,2016-08-04T18:56:47.850,how important is true randomness in ai designs ?,ai-design,1,0,1
118,1333,1,1370,2016-08-04T18:31:33.580,0,27,"complex ai that learns lexical - semantic content and its meaning ( such as collection of words , their structure and dependencies ) such as watson takes terabytes of disk space . lets assume deepqa -like ai consumed whole wikipedia of size 10 g which took the same amount of structured and unstructured stored content . will learning another 10 g of different encyclopedia ( different topics in the same language ) take the same amount of data ? or will the ai reuse the existing structured and take less than half ( like 1/10 of it ) additional space ?",8,75,2016-08-17T13:52:57.853,2016-08-18T00:46:03.973,does learning content from additional encyclopedias consume much less amount of storage ?,watson storage,2,0,
119,1334,1,1342,2016-08-04T18:40:44.093,2,93,"is there any simple explanation how watson finds and scores evidence after gathering massive evidence and analyzing the data ? in other words , how does it know which precise answer it needs to return ?",8,75,2016-08-17T13:52:23.383,2016-08-17T13:52:23.383,how does watson find and evaluate its evidence to the answer ?,watson,2,0,
120,1348,1,1355,2016-08-04T23:49:01.983,18,480,"isaac asimov 's famous three laws of robotics originated in the context of asimov 's science fiction stories . in those stories , the three laws serve as a safety measure , in order to avoid untimely or manipulated situations from exploding in havoc . more often than not , asimov 's narratives would find a way to break them , leading the writer to make several modifications to the laws themselves . for instance , in some of his stories , he modified the first law , added a fourth ( or zeroth ) law , or even removed all laws altogether . however , it is easy to argue that , in popular culture , and even in the field of ai research itself , the laws of robotics are taken quite seriously . ignoring the side problem of the different , subjective , and mutually - exclusive interpretations of the laws , are there any arguments proving the laws themselves intrinsically flawed by their design , or , alternatively , strong enough for use in reality ? likewise , has a better , stricter security heuristics set being designed for the purpose ?",71,145,2016-08-23T10:05:46.817,2016-08-25T16:00:00.820,"are asimov 's laws flawed by design , or are they feasible in practice ?",asimovs-laws,3,3,3
121,1354,1,2143,2016-08-05T01:45:17.633,9,395,"are there any modern techniques of generating textual captcha ( so person needs to type the right text ) challenges which can easily fool ai with some visual obfuscation methods , but at the same time human can solve them without any struggle ? for example i 'm talking about plain ability of recognising text embedded into image ( without considering any external plugins like flash or java , image classification , etc . ) and re - typing the text that has been written or something similar . i guess adding noise , gradient , rotating letters or changing colours are not reliable methods any more , since they can be quickly broken . any suggestions or research has been done ?",8,-1,2017-04-13T12:53:10.013,2016-10-13T09:33:07.423,"are there any textual captcha challenges which can fool ai , but not human ?",image-recognition research ocr,4,3,2
122,1357,1,1359,2016-08-05T05:40:23.683,9,200,"can an ai program have an eq ( emotional intelligence or emotional quotient ) ? in other words , can the eq of an ai program be measured ? if eq is more problematic to measure than iq ( at least with a standard applicaple to both humans and ai programs ) , why is that the case ?",1278,,,2016-08-05T12:43:38.537,how can the eq of an ai program be measured ?,emotional-intelligence intelligence-testing,1,1,1
123,1358,1,1361,2016-08-05T07:03:50.110,15,1588,"i have heard about this concept in a reddit post about alpha go . i have tried to go through the paper and the article , but could not really make sense of the algorithm . so , can someone give an easy - to - understand explanation of how the monte - carlo search algorithm work and how is it being used in building game - playing ai bots ?",101,101,2017-05-16T04:33:03.233,2018-04-24T04:13:03.027,"how does "" monte - carlo search "" work ?",gaming monte-carlo-tree-search,1,1,6
124,1360,1,1369,2016-08-05T08:51:50.657,6,133,dnns are typically used to classify things ( of course ) but can we let them go wild with sounds and then tell them if we think it sounds good or not ? i 'd like to think after a training class has been made ( perhaps comparing the output to an existing song ) we could get an nn that has a basic concept of music . timing would be an issue ; i 'm not sure how feasible this is . a strongly weighted input attached to all hidden layers perhaps ? use it as the bias ? is this even slightly feasible ?,1284,145,2016-08-18T21:36:41.180,2016-08-18T21:36:41.180,has any research been done on dnn music ?,deep-network machine-learning,1,2,3
125,1362,1,1365,2016-08-05T10:39:31.520,5,2099,"how do i avoid my gradient descent algorithm into falling into the "" local minima "" trap while backpropogating on my neural network ? are there any methods which help me avoid it ?",101,2444,2019-04-27T00:50:35.780,2019-04-30T01:57:53.150,"how to avoid falling into the "" local minima "" trap ?",neural-networks backpropagation optimization gradient-descent,1,0,2
126,1363,1,,2016-08-05T10:49:39.557,9,457,a neural network is a directed weighted graph . these can be represented by a ( sparse ) matrix . doing so can expose some elegant properties of the network . is this technique beneficial for examining neural networks ?,1283,,,2017-02-16T19:38:44.487,is it beneficial to represent a neural net as a matrix ?,neural-networks,3,2,
127,1376,1,,2016-08-05T16:51:49.833,5,81,"would it be ethical to implement ai for self - defence for public walking robots which are exposed to dangers such as violence and crime such as robbery ( of parts ) , damage or abduction ? what would be pros and cons of such ai behavior ? is it realistic , or it wo n't be taken into account for some obvious reasons ? like pushing back somebody when somebody start pushing it first ( ai will say : he pushed me first ) , or running away on crowded street in case algorithm will detect risk of abduction .",8,8,2016-08-05T17:07:35.410,2016-08-05T17:08:27.273,is it ethical to implement self - defence for street walking ai robots ?,ethics decision-theory robots,2,1,
128,1379,1,1380,2016-08-05T17:11:34.880,-1,53,is there any risk in the near future of replacing all encyclopedias with watson - like ai where knowledge is accessible by everybody through api ? something similar happened in the future in the time machine movie from 2002 . obviously maintaining 40 million articles and keeping it up - to - date and consistent could be beyond brain power of few thousands of active editors . not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people . what are the pros and cons of such a change ?,8,145,2016-08-11T12:21:13.530,2016-08-11T12:21:13.530,how likely is it that watson - like ai will replace wikipedia - like encyclopedias ?,watson,1,1,
129,1381,1,1382,2016-08-05T17:38:23.270,3,71,"i 've watched the sunspring video which did n't make any sense to me ( a lot of nonsense monologues ) , mainly because it was created by jetson ai . what was the mechanism of creating such screenplay ? on what criteria was it trained ? what was the goal or motivation in terms of training criteria of defining when text does make sense ? and what was missed ( that it 's so bad ) and how possibly this could be improved ?",8,130,2016-08-06T01:44:14.583,2016-08-06T01:44:14.583,how does the jetson ai write its screenplays ?,algorithm,1,0,
130,1384,1,1385,2016-08-05T18:22:41.003,3,190,"this article suggests that deep learning is not designed to produce the universal algorithm and can not be used to create such a complex systems . first of all it requires huge amounts of computing power , time and effort to train the algorithm the right way and adding extra layers does n't really help to solve complex problems which can not be easily predicted . secondly some tasks are extremely difficult or impossible to solve using dnn , like solving a math equations , predicting pseudo - random lists , fluid mechanics , guessing encryption algorithms , or decompiling unknown formats , because there is no simple mapping between input and output . so i 'm asking , are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving ? which can solve more variety of problems , than "" deep "" architectures can not ?",8,-1,2017-04-13T12:53:10.013,2017-07-27T08:55:09.363,"are there any learning algorithms as powerful as "" deep "" architectures ?",deep-network comparison architecture,2,6,3
131,1390,1,1412,2016-08-05T20:47:50.290,3,779,"is there any research which study application of ai into chemistry which can predict the output of certain chemical reactions . so for example , you train the ai about current compounds , substances , structures and their products and chemical reactions from the existing dataset ( basically what produce what ) . then you give the task to find how to create a gold or silver from group of available substances . then the algorithm will find the chemical reactions ( successfully predicting new one which were n't in the dataset ) and gives the results . maybe the gold is not a good example , but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries . was there any successful research attempting to achieve that using deep learning algorithms ?",8,-1,2017-04-13T12:57:30.070,2016-08-11T10:15:24.027,predicting chemical reactions using ai,deep-learning research prediction,2,0,
132,1391,1,1399,2016-08-05T21:29:37.880,10,517,"assume that i want to solve an issue with neural network that either i ca n't fit to already existing topologies ( perceptron , konohen , etc ) or i 'm simply not aware of the existence of those or i 'm unable to understand their mechanics and i rely on my own instead . how can i deconstruct a problem to find a corresponding neural network topology ? by this i do n't mean only the size of certain layers , but the number of them , the type of activation functions , the number and the direction of connections , and so on . i 'm a beginner , yet i realized that in some topologies ( or , at least in perceptrons ) it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers do n't express any mathematically meaningful context .",1270,4302,2018-07-23T15:29:18.943,2018-07-23T15:29:18.943,"how can i plan the topology of a neural network for a given "" random "" problem ?",neural-networks topology,2,2,3
133,1392,1,1450,2016-08-06T00:24:44.493,1,72,"for example there is the mnist database which is used to test artificial neural network ( ann ) , however it 's not so challenging , because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent . are there any similar , especially the most challenging tasks with dataset which are used as benchmark tests to challenge the ai which are fairly reliable and it 's possible to pass , but most aan are struggling to achieve the lower error rate ?",8,1581,2018-11-13T17:21:58.497,2018-11-13T17:21:58.497,what are the most challenging tasks aiming to achieve the lowest error rate ?,deep-learning image-recognition data-science,1,2,1
134,1393,1,,2016-08-06T00:37:24.067,2,244,"this study ( pages 7 - 8 ) shows an attempt at recognizing the traffic signs with lower error rates by using multi - column deep neural networks are google cars using similar techniques of predicting signs using dnn , or are they using some other method ?",8,145,2016-08-18T20:10:33.393,2016-10-09T11:49:20.453,how do google cars recognize the traffic signs ?,deep-network image-recognition self-driving classification cars,1,1,
135,1394,1,1408,2016-08-06T01:27:03.500,2,109,"i 'd like to know whether there were attempts to simulate the whole brain , i 'm not talking only about some ann on microchips , but brain simulations .",8,-1,2017-04-13T12:53:10.013,2016-08-23T10:49:14.587,are there any artificial neuromorphic systems which can mimic the brain ?,neuromorphic-engineering,2,1,1
136,1396,1,1406,2016-08-06T01:57:43.263,15,5053,"on the wikipedia page about ai , we can read : optical character recognition is no longer perceived as an exemplar of "" artificial intelligence "" having become a routine technology . on the other hand , the mnist database of handwritten digits is especially designed for training and testing neural networks and their error rates ( see : classifiers ) . so why does the above quote state that ocr is no longer exemplar of ai ?",8,145,2016-08-07T19:07:57.220,2018-09-18T07:56:35.810,why ca n't ocr be perceived as a good example of ai ?,ocr,3,0,6
137,1397,1,1452,2016-08-06T02:04:19.343,11,362,"mist is a quantiative test of humanness , consisting of ~80k propositions such as : is earth a planet ? is the sun bigger than my foot ? do people sometimes lie ? etc . have any ai attempted and passed this test to date ?",8,145,2016-08-15T14:44:41.513,2016-09-06T15:23:41.650,are there any ai that have passed the mist test so far ?,history intelligence-testing turing-test chat-bots,2,0,2
138,1401,1,1403,2016-08-06T07:08:20.203,2,54,"it is possible of normal code to prove that it is correct using mathematical techniques , and that is often done to ensure that some parts are bug - free . can we also prove that a piece of code in ai software will cause it to never turn against us , i.e. that the ai is friendly ? has there any research been done towards this ?",29,,,2016-08-06T07:51:00.567,can we prove that a piece of code in ai software will cause it to never turn against us ?,friendly-ai,2,0,
139,1404,1,1407,2016-08-06T07:27:52.287,4,769,"in the paper death and suicide in universal artificial intelligence , a proposal is given for what death could mean for artificial intelligence . what does this mean using english only ? i understand that mathematical notation is useful for giving a precise definition , but i 'd like to understand what the definition really means .",29,2444,2019-04-19T16:44:43.073,2019-04-19T16:44:43.073,"what does "" death "" intuitively mean in the paper "" death and suicide in universal artificial intelligence "" ?",research definitions agi death aixi,1,0,1
140,1410,1,,2016-08-06T10:46:10.100,5,68,we can measure the power of the machine with the number of operation per second or the frequency of the processor . but does units similar of iq for humans exist for a ai ? i 'm asking for a unit which can give countable result so something different from a turing test which only give a binary result .,98,10,2016-08-06T14:14:05.420,2016-08-08T18:25:11.147,do specific units exists for measuring the intelligence of a machine ?,machine-learning classification intelligence-testing,3,0,1
141,1415,1,1418,2016-08-06T17:24:50.083,9,328,"in the mid 1980s , rodney brooks famously created the foundations of "" the new ai "" . the central claim was that the symbolist approach of ' good old fashioned ai ' ( gofai ) had failed by attempting to ' cream cognition off the top ' , and that embodied cognition was required , i.e. built from the bottom up in a ' hierarchy of competances ' ( e.g. basic locomotion - > wandering around - > actively foraging ) etc . i imagine most ai researchers would agree that the ' embodied cognition ' perspective has now ( at least tacitly ) supplanted gofai as the mainstream . my question takes the form of a thought experiment and asks : "" which ( if any ) aspects of ' embodied ' can be relaxed / omitted before we lose something essential for agi ? """,42,,,2016-08-06T18:19:44.323,what kind of body ( if any ) does intelligence require ?,agi gofai embodied-cognition,1,0,2
142,1416,1,,2016-08-06T17:35:02.143,4,1033,"in other words , which existing reinforcement method learns in fewest episodes ? r - max comes to mind , but its very old and i 'd like to know if there is something better now .",144,,,2017-09-22T16:51:22.563,what is the current state - of - the - art in reinforcement learning regarding data efficiency ?,algorithm research reinforcement-learning,2,0,
143,1420,1,1421,2016-08-06T18:38:50.160,8,1234,"are there any research teams which attempted to create or have already created an ai robot which can be as close to intelligent as these found in ex machina or i , robot movies ? i 'm not talking about full awareness , but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do ?",8,145,2016-08-23T14:16:20.303,2016-08-28T20:41:04.140,how close are we to creating ex machina ?,research agi robots,4,0,5
144,1423,1,1440,2016-08-06T22:59:43.413,10,135,"we , humans , during following multiple processes ( e.g. reading while listening to music ) memorize information from less focused sources with worse efficiency than we do from our main concentration . do such things exist in case of artificial intelligences ? i doubt , for example that neural networks obtain such features , but i may be wrong .",1270,8,2016-08-18T14:27:26.763,2016-08-18T14:27:26.763,"is there any artificial intelligence that possesses "" concentration "" ?",structured-data,2,0,3
145,1426,1,1443,2016-08-07T00:06:45.913,5,96,"how can a swarm of small robots ( like kilobots ) walking close to each other achieve collaboration without bumping into each other ? for example , one study shows programmable self - assembly in a thousand - robot swarm ( see article & amp ; video ) which are moving without gps - like system and by measuring distances to neighbours . this was achieved , because the robots were very slow . is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination ? not by walking around clock - wise ( which i guess was the easiest way ) , but i mean using some more sophisticated way . because waiting half a day ( ~11h ) to create a simple star shape using a thousand - robot swarm is way too long !",8,145,2016-08-07T19:10:08.217,2016-08-07T19:10:08.217,how can thousand - robot swarm coordinate their moves without bumping into each other ?,robots multi-agent-systems,1,0,1
146,1427,1,1428,2016-08-07T00:55:36.657,0,36,"on watson wiki page we can read : in healthcare , watson 's natural language , hypothesis generation , and evidence - based learning capabilities allow it to function as a clinical decision support system for use by medical professionals . how exactly such ai can help doctors to diagnose the diseases ?",8,,,2016-08-07T00:55:36.657,how watson can help to make medical diagnoses ?,watson healthcare,1,0,
147,1429,1,1430,2016-08-07T01:32:32.137,3,529,"recently white house published the article : preparing for the future of artificial intelligence which says that government is working to leverage ai for public good and toward a more effective government . i 'm especially interested how ai can help with computational sustainability , environmental management and earth 's ecosystem such as biological conservation ?",8,,,2016-08-07T03:12:17.000,how machine learning can help with sustainable development and biological conservation ?,biology,1,0,1
148,1431,1,1438,2016-08-07T01:51:28.157,-5,88,"when ai has some narrow domain such as chess where it can outperform the world 's human masters of chess , does it make it a superintelligence or not ?",8,8,2018-03-23T11:42:58.483,2018-03-23T11:42:58.483,is deep blue superintelligent or not ?,definitions deep-blue,1,1,0
149,1432,1,,2016-08-07T02:08:34.803,6,385,"suppose my goal is to collaborate and create an advanced ai , for instance one that resembles a human being and the project would be on the frontier of ai research , what kind of skills would i need ? i am talking about specific things like what university program should i complete to enter and be competent in the field . here are some of the things that i thought about , just to exemplify what i mean : computer sciences : obviously the ai is built on computers , it would n't hurt to know how computers work , but some low level stuff and machine specific things does not seem essential , i may be wrong of course . psychology : if ai resembles human beings , knowledge of human cognition would probably be useful , although i do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the oedipus complex would be relevant , but again , i may be wrong .",1321,,,2016-08-07T14:18:25.840,what kind of education / expertise is required for researchers in ai ?,research,2,1,1
150,1433,1,,2016-08-07T02:33:51.350,1,449,white house published the information about ai which requests mentions about ' the most important research gaps in ai that must be addressed to advance this field and benefit the public ' . what are these exactly ?,8,8,2016-08-13T03:11:49.510,2016-08-13T03:11:49.510,what are the most pressing fundamental questions and gaps in ai research ?,research ethics social reasoning,2,6,3
151,1436,1,1439,2016-08-07T03:49:20.143,5,322,is there any methods by which artificial intelligence use recursion(s ) to solve a certain issue or to keep up working and calculating ?,1270,,,2018-12-28T21:43:24.817,is recursion used in practice to improve performance of ai systems ?,math,1,1,1
152,1446,1,,2016-08-07T12:57:29.583,5,129,"the von neumann 's minimax theorem gives the conditions that make the max - min inequality an equality . i understand the max - min inequality , basically min(max(f))&gt;=max(min(f ) ) . the von neumann 's theorem states that , for the inequality to become an equality f(.,y ) should always be convex for given y and f(x , . ) should always be concave for given x , which also makes sense . this video says that for a zero - sum perfect information game , the von neumann 's theorem always holds , so that minimax always equal to maximin , which i did not quite follow . questions why zero - sum perfect information games satisfy the conditions of von neumann 's theorem ? if we relax the rules to be non - zero - sum or non - perfect information , how would the conditions change ?",185,1671,2017-09-13T21:33:16.060,2017-10-13T22:39:46.610,illustration of von neumann 's minimax theorem in games ?,game-theory search minimax,1,6,2
153,1451,1,2095,2016-08-07T18:17:11.663,17,615,"in october 2014 , dr . mark riedl published an approach to testing ai intelligence , called the "" lovelace test 2.0 "" , after being inspired by the original lovelace test ( published in 2001 ) . mark believed that the original lovelace test would be impossible to pass , and therefore , suggested a weaker , and more practical version . the lovelace test 2.0 makes the assumption that for an ai to be intelligent , it must exhibit creativity . from the paper itself : the lovelace 2.0 test is as follows : artificial agent a is challenged as follows : a must create an artifact o of type t ; o must conform to a set of constraints c where ci ∈ c is any criterion expressible in natural language ; a human evaluator h , having chosen t and c , is satisfied that o is a valid instance of t and meets c ; and a human referee r determines the combination of t and c to not be unrealistic for an average human . since it is possible for a human evaluator to come up with some pretty easy constraints for an ai to beat , the human evaluator is then expected to keep coming up with more and more complex constraints for the ai until the ai fails . the point of the lovelace test 2.0 is to compare the creativity of different ais , not to provide a definite dividing line between ' intelligence ' and ' nonintelligence ' like the turing test would . however , i am curious about whether this test has actually been used in an academic setting , or it is only seen as a thought experiment at the moment . the lovelace test seems easy to apply in academic settings ( you only need to develop some measurable constraints that you can use to test the artificial agent ) , but it also may be too subjective ( humans can disagree on the merits of certain constraints , and whether a creative artifact produced by an ai actually meets the final result ) .",181,29,2016-08-30T19:42:55.163,2016-10-08T01:47:41.487,has the lovelace test 2.0 been successfully used in an academic setting ?,history intelligence-testing,1,0,3
154,1453,1,1456,2016-08-07T19:49:25.977,2,269,"convolutional neural network are leading type of feed - forward artificial neural network for image recognition . can they be used for real - time image recognition for videos ( frame by frame ) , or it takes too much processing ( assuming they 're written in c - like language ) ? for example for classification of type of animals based on the training from huge dataset .",8,8,2016-08-07T20:36:00.187,2016-08-16T14:50:49.567,can convnets be used for real - time object recognition from video feed ?,convolutional-neural-networks classification performance real-time,1,0,1
155,1458,1,1460,2016-08-08T07:04:49.487,4,107,just for the purpose of learning i 'd like to classify the likeliness of a tweet being in aggressive language or not . i was wondering how to approach the problem . i guess i need first train my neural network on a huge dataset of text what aggressive language is . this brings up the question where i would get this data in the first place ? it feels a bit like the chicken and egg problem to me so i wonder how would i approach the problem ?,1334,,,2018-09-30T15:59:31.110,how to classify language as friendly or aggressive with ai ?,classification datasets,4,0,
156,1461,1,1483,2016-08-08T17:48:43.730,23,1083,"siri and cortana communicate pretty much like humans . unlike google now which mainly gives us search results when asked some questions ( not setting alarms or reminders ) , siri and cortana provide us with an answer , in the same way that a person would do . so are they actual ai programs or not ? ( by "" question "" i do n't mean any academic related question or asking routes/ temperature , but rather opinion based question ) .",72,4302,2018-10-08T12:40:56.767,2018-10-08T12:40:56.767,are siri and cortana ai programs ?,natural-language-processing chat-bots intelligence-testing emotional-intelligence applications,4,2,6
157,1462,1,1465,2016-08-08T17:53:40.050,5,7409,what is the difference between ai and robots ?,72,2444,2019-02-20T18:23:14.093,2019-02-20T18:23:14.093,what is the difference between ai and robots ?,terminology robotics robots difference,5,2,1
158,1472,1,,2016-08-08T19:24:16.993,3,62,"with typical machine learning you would usually use a training data - set to create a model of some kind , and a testing data - set to then test the newly created model . for something like linear regression after the model is created with the training data you now have an equation that you would use to predict the outcome of the set of features in the testing data . you would then take the prediction that the model returned and compare that to the actual data in the testing set . how would a validation set be used here ? with nearest neighbor you would use the training data to create an n - dimensional space that has all the features of the training set . you would then use this space to classify the features in the testing data . again you would compare these predictions to the actual value of the data . how would a validation set help here as well ?",1324,10135,2018-10-19T14:23:59.100,2018-10-19T14:23:59.100,how exactly does a validation data - set work work in machine learning ?,machine-learning datasets linear-regression,0,2,
159,1476,1,1591,2016-08-08T23:46:12.853,5,275,"by reinforcement learning , i do n't mean the class of machine learning algorithms such as deepq , etc . i have in mind the general concept of learning based on rewards and punishment . is it possible to create a strong ai that does not rely on learning by reinforcement , or is reinforcement learning a requirement for artificial intelligence ? the existence of rewards and punishment imply the existence of favorable and unfavorable world - states . must intelligence in general and artificial intelligence in particular have a way of classifying world - states as favorable or unfavorable ?",127,,,2018-11-08T23:55:08.853,is reinforcement learning needed to create strong ai ?,philosophy reinforcement-learning,6,1,
160,1478,1,1527,2016-08-09T02:01:58.240,1,2137,"for example , search engine companies want to classify their image searches into 2 categories ( which they already do that ) such as : nsfw ( nudity , porn , brutality ) and safe to view pictures . how can artificial neural networks achieve that , and at what success rate ? can they be easily mistaken ?",8,145,2016-08-17T19:58:38.663,2016-08-25T09:59:13.270,how successfully can convnets detect nsfw images ?,image-recognition classification convolutional-neural-networks,2,3,1
161,1479,1,4044,2016-08-09T02:08:56.663,61,9879,"do scientists or research experts know from the kitchen what is happening inside complex "" deep "" neural network with at least millions of connections firing at an instant ? do they understand the process behind this ( e.g. what is happening inside and how it works exactly ) , or it is a subject of debate ? for example this study says : however there is no clear understanding of why they perform so well , or how they might be improved . so does this mean that scientists actually do n't know how complex convolutional network models work ?",8,12672,2018-02-12T11:27:09.163,2019-03-22T09:51:00.007,do scientists know what is happening inside artificial neural networks ?,neural-networks deep-learning convolutional-neural-networks,8,1,41
162,1480,1,1741,2016-08-09T02:30:17.240,-1,214,"is there any way to estimate how big the neural network would be after training session of 100,000 unlabeled images for unsupervised learning ( like in stl-10 dataset : 96x96 pixels and color ) ? not the storage space ( because this could vary i guess based on the implementation ) , but specifically how many neurons it could have . it could be an estimate ( e.g. in thousand , millions ) . if it depends , then on what ? are there any figures that can be estimated ?",8,145,2016-08-25T18:48:24.383,2017-02-13T20:48:43.850,how many neurons would a network have after a training of 100k small images ?,deep-network image-recognition deep-learning datasets,1,0,
163,1481,1,1699,2016-08-09T02:54:33.677,6,844,"for example i 'd like to train my neural network to recognize the type of actions ( e.g. in commercial movies or some real life videos ) , so i can "" ask "" my network in which video or movie ( and at what frames ) somebody was driving a car , kissing , eating , was scared or was talking over the phone . what are the current successful approaches to that type of problem ?",8,145,2016-08-09T11:29:23.227,2018-07-24T13:20:35.423,how can action recognition be achieved ?,image-recognition training action-recognition,5,5,1
164,1484,1,,2016-08-09T06:55:39.460,1,642,"i 'm playing with an lstm to generate text . in particular , this one : https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py it works on quite a big demo text set from nietzsche and says if you try this script on new data , make sure your corpus has at least ~100k characters . ~1 m is better . this pops up a couple of questions . a. ) if all i want is an ai with a very limited vocabulary where the generate text should be short sentences following a basic pattern . e.g. i like blue sky with white clouds i like yellow fields with some trees i like big cities with lots of bars ... would it then be reasonable to use a much much smaller dataset ? b. ) if the dataset really needs to be that big . what if i just repeat the text over and over to reach the recommended minimum ? if that would work though , i 'd be wondering how that is any different from just taking more iterations of learning with the same shorter text ? obviously i can play with these two questions myself and in fact i am experimenting with it . one thing i already figured out is that with a shorter text following a basic pattern i can get to a very very low ( ~0.04 ) quite fast but the predicted text just turns out as gibberish . my naive explanation for that would be that there are just not enough samples to proof against whether the gibberish actually makes sense or not ? but then again i wonder if more iterations or duplicating the content would actually help . i 'm trying to experiment with these questions myself so please do n't think i 'm just too lazy and are aiming for others to do the work . i 'm just looking for more experienced people to give me a better understanding of the mechanics that influence these things .",1334,,,2017-04-25T14:00:05.233,in lstm text generation can low amount of training data be compensated ?,datasets lstm,2,4,1
165,1485,1,1523,2016-08-09T09:01:52.373,3,101,"for example i would like to implement transparent ai in the rts game which does n't offer any ai api ( like old games ) , and i 'd like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic . given i 'd like to use two neural networks , what are the approaches to setup the communication between them ? is it just by exporting result findings of the first algorithm ( e.g. using cnn ) with list of features which were found on the screen , then use it as input for another network ? or it 's more complex than that , or i need to have more than two networks ?",8,8,2016-08-09T18:57:43.590,2018-07-02T21:02:21.280,how to separate image recognition from logic ?,neural-networks convolutional-neural-networks gaming,2,0,
166,1487,1,1503,2016-08-09T09:09:34.990,1,69,"were there any successful attempts to replace poor guide dogs used for blind people with ai to achieve similar rate of success ? i guess dogs could be easily distracted and not reliable for every situation , and it probably takes less time to train ai , than a dog .",8,29,2016-08-17T12:03:08.677,2016-08-17T12:03:08.677,how close we are to replacing guide dogs with ai ?,neural-networks applications,1,2,
167,1488,1,1522,2016-08-09T10:10:25.253,5,519,"do we know why tesla 's autopilot mistaken empty sky with a high - sided lorry which resulted in fatal crash involving a car in self - drive mode ? was it ai fault or something else ? is there any technical explanation behind this why this happened ? references : sky news article , the verge .",8,8,2016-08-09T22:17:20.030,2016-08-14T23:54:03.540,why did a tesla car mistake a truck with a bright sky ?,self-driving cars,3,3,
168,1490,1,1499,2016-08-09T10:28:41.783,1,89,"for benefits of testing agi , is using a high - level video game description language ( vgdl ) gives more reliable and accurate results of general intelligence than using arcade learning environment ( ale ) ?",8,,,2016-08-09T20:35:34.217,what are the benefits of the vgdl over the ale ?,agi gaming,1,0,1
169,1491,1,2183,2016-08-09T10:40:35.500,2,61,"some time ago playing chess was challenging for algorithms , then go game which is vastly more complex than compared to chess . how about playing rts game which have enormous branching factors limited by its time and space ( like deciding what to do next ) ? what are the successful approaches to such problems ?",8,,,2016-10-19T07:44:43.773,how to deal with huge branching factors in real - time ?,gaming branching-factors real-time,1,0,
170,1492,1,1495,2016-08-09T12:00:06.583,3,829,"we can read on wiki page that in march 2016 alphago ai lost its game ( 1 of 5 ) to lee sedol , a professional go player . one article cite says : alphago lost a game and we as researchers want to explore that and find out what went wrong . we need to figure out what its weaknesses are and try to improve it . have researchers already figured it out what went wrong ?",8,130,2016-08-09T15:45:35.477,2016-08-09T15:45:35.477,why did alphago lose its go game ?,gaming deepmind,1,0,1
171,1493,1,1496,2016-08-09T12:19:35.883,3,149,"assuming we 're dealing with artificial neural network ( e.g. using convnets ) which was trained by large dataset of human faces . are there any known issues or challenges where facial recognition would fail ? i 'm not talking about covering half of the face , but some simple common things such as wearing the glasses , hat , jewellery , having face painting or tattoo , can this successfully prevent ai from recognizing the face ? if so , what are current methods dealing with such challenges ?",8,1671,2017-12-09T23:19:31.283,2017-12-09T23:19:31.283,is it possible to cheat facial recognition algorithm ?,convolutional-neural-networks facial-recognition,1,1,1
172,1494,1,1497,2016-08-09T14:17:50.773,0,351,"i would like to know what kind of dataset i need ( to prepare ) for training the network to recognize the spelling mistakes in individual words for english text . given the large database of words , having correct one for each incorrect . what kind of input is more efficient for that tasks ? is it using one input per each letter , syllable , whole word or i should use different pattern syllable ? then the input should be incorrect word , output correct , and if the word does n't need correction , then both input and output should be the same . is that the right approach ?",8,4302,2018-10-08T12:54:00.933,2018-10-08T12:54:00.933,training network to detect spelling mistakes,deep-learning natural-language-processing datasets,2,3,
173,1501,1,,2016-08-09T20:54:09.720,3,94,"as i have been looking at other questions on this site ( like this , this , this , and this ) , i have been thinking more about the ethical implications of creating these generalized ai systems . it seems that whether or not we can create it is not rationale enough as to whether or not we should do it . in dealing with the issue of ethics in ai , i wonder what the ethical implications are not just for us , but for the system itself . it seems to extend beyond the usually asked questions on the topic and into unknown territory . are ethics computable ? can they be implemented programmatically ? can we force an ai system to do something against its "" will "" ? what does the creation of ai imply ethically for us as well as the ai ?",77,-1,2017-04-13T12:53:10.013,2016-08-09T21:43:25.247,what are the ethical implications of creating ( possibly sentient ) ai systems ?,ethics,1,7,1
174,1507,1,1516,2016-08-10T00:55:54.690,19,2076,"i believe artificial intelligence ( ai ) term is overused nowadays . for example , people see that something is self - moving and they call it ai , even if it 's on autopilot ( like cars or planes ) or there is some simple algorithm behind it . what are the minimum general requirements so that we can say something is ai ?",8,2444,2019-02-21T22:21:15.307,2019-02-21T22:21:15.307,what are the minimum requirements to call something ai ?,definitions,5,0,7
175,1508,1,1514,2016-08-10T01:06:07.823,4,3557,"i believe normally you can use genetic programming for sorting , however i 'd like to check whether it 's possible using ann . given the unsorted text data from input , which neural network is suitable for doing sorting tasks ?",8,,,2019-05-13T15:44:52.717,which neural network has capabilities of sorting input ?,neural-networks,1,1,3
176,1509,1,1520,2016-08-10T01:14:19.777,1,72,"i 've read on wiki that genetic programming has ' outstanding results ' in cyberterrorism prevention . further more , this abstract says : using machine - coded linear genomes and a homologous crossover operator in genetic programming , promising results were achieved in detecting malicious intrusions . i 've checked the study , but it 's still not clear for me . how exactly was this detection achieved from the technical perspective ?",8,145,2016-08-10T09:04:33.507,2016-08-10T09:04:33.507,how can genetic programming be used to prevent cyberterrorism ?,genetic-programming cyberterrorism security,1,0,
177,1515,1,3464,2016-08-10T02:05:29.250,4,344,"on wikipedia , we can read about different type of intelligent agents : abstract intelligent agents ( aia ) , autonomous intelligent agents , virtual intelligent agent ( iva ) , which i 've found on other websites , e.g. this one . what are the differences between these three to avoid confusion ? for example i 've used term virtual artificial agent here as : basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior ( ai ) . so basically i 'd like to know where other terms like autonomous or abstract agents can be used and in what context . can they be all defined under ' virtual ' robot definition ? how to distinguish these terms ?",8,7488,2017-06-07T16:19:39.653,2017-06-09T21:29:37.733,"what is the difference between abstract , autonomous and virtual intelligent agents ?",definitions intelligent-agent comparison terminology,2,7,
178,1517,1,1524,2016-08-10T02:38:17.940,2,446,on wikipedia we can read : kasparov accused ibm of cheating and demanded a rematch . ibm refused and retired deep blue . what was the accusation and how was deep blue allegedly able to cheat ?,8,145,2016-08-11T14:51:20.763,2016-08-11T14:51:20.763,how could deep blue possibly cheat ?,chess deep-blue challenges game-theory,1,0,
179,1518,1,1528,2016-08-10T02:54:35.943,0,1067,"the wikipedia page describes ai control problem in very intricated way . therefore i would like to better understand it based on some simple explanation , what 's going on . basically i do n't want any copy & amp ; pastes from wiki , because the articles there are written in neutral point of view , in very general way where articles are evolving very slowly , so the definition from there does n't suit me . i believe this is what is discussed nowadays by government and it 's important aspects of ai technology where it leds to . i believe this could be a big problem in the near future , so i 'm expecting to hear about this from people from much better and more up - to - date point of view . so what is exactly the ai control problem ?",8,29,2016-08-12T10:44:18.917,2016-08-12T10:44:18.917,what is the control problem ?,definitions control-problem,1,0,
180,1525,1,,2016-08-10T14:10:08.270,16,331,"this is from a closed beta for ai , with this question being posted by user number 47 . all credit to them . according to wikipedia , boltzmann machines can be seen as the stochastic , generative counterpart of hopfield nets . both are recurrent neural networks that can be trained to learn of bit patterns . then when presented with a partial pattern , the net will retrieve the full complete pattern . hopfield networks have been proven to have a capacity of 0.138 ( e.g. approximately 138 bit vectors can be recalled from storage for every 1000 nodes , hertz 1991 ) . as a boltzmann machine is stochastic , my understanding is that it would not necessarily always show the same pattern when the energy difference between one stored pattern and another is similar . but because of this stochasticity , maybe it allows for denser pattern storage but without the guarantee that you 'll always get the "" closest "" pattern in terms of energy difference . would this be true ? or would a hopfield net be able to store more patterns ?",145,145,2016-08-23T08:19:28.460,2017-10-15T03:19:55.693,could a boltzmann machine store more patterns than a hopfield net ?,neural-networks comparison recurrent-neural-networks,1,0,2
181,1531,1,1532,2016-08-10T19:17:18.410,11,4457,"according to wikipedia , prolog is a general - purpose logic programming language associated with artificial intelligence and computational linguistics . is it still used for ai ? this is based off of a question on the 2014 closed beta . the author had the uid of 330 .",145,,,2016-08-12T21:38:48.343,is prolog still used in ai ?,history programming-languages prolog,2,0,5
182,1534,1,1727,2016-08-11T03:29:18.097,5,130,"i 'm a bit confused with extensive number of different monte carlo methods such as : hamiltonian / hybrid monte carlo ( hmc ) , dynamic monte carlo ( dmc ) , markov chain monte carlo ( mcmc ) , kinetic monte carlo ( kmc ) , dynamic monte carlo ( dmc ) quasi - monte carlo ( qmc ) , direct simulation monte carlo ( dsmc ) , and so on . i wo n't ask for the exact differences , but why are all of them called monte carlo ? what do they all have in common ? can they all be used for ai ? e.g. which one can be used for gaming ( like go ) or image recognition ( resampling ) ?",8,145,2016-08-11T13:46:22.443,2016-08-24T13:15:48.023,how do i know when to use which monte carlo method ?,gaming comparison monte-carlo-tree-search,1,1,1
183,1535,1,,2016-08-11T08:30:40.453,2,139,"when it comes to neural networks , it 's often only explained what abstract task they do , say for example detect a number in an image . i never understood what 's going on under the hood essentially . there seems to be a common structure of a directed graph , with values in each node . some nodes are input nodes . their values can be set . the values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set , which can be interpreted a result . how exactly is the value of each node determined ? i assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node . what formula is used ? is the formula the same throughout the network ? then i heard that a network has to be trained . i assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values . is that correct ? in layman 's terms , what are the underlying principles that make a neural network work ?",1463,8,2016-08-18T08:40:00.167,2018-05-04T09:57:49.327,how exactly is the value of each node determined ? is it the same formula throughout the network ?,neural-networks,3,6,0
184,1537,1,1569,2016-08-11T09:02:52.250,0,109,"ideally i 'd like to watch movie which is deep dreamed in real - time . most algorithms which i know are too slow or not designed for real - time processing . for example i 'm bored with some movie which i 've watched thousands of time and i 'd like to add some "" dreaming "" to it which is real - time filter which takes input frames , then it 's processing and enhances the images through artificial neural network to achieve doodled output . does n't have to be exactly deepdream or hallucinogenic technique ( which could be too much to watch for 2h ) , but with any similar ann algorithm . i 'm more interested into achieving desired real - time use . what kind of techniques can achieve such efficiency ?",8,8,2016-08-11T20:40:25.737,2016-08-11T20:49:24.500,which techniques can achieve neural doodle in real - time ?,research algorithm real-time neural-doodle,1,0,
185,1539,1,1545,2016-08-11T09:39:32.893,7,116,how does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms ?,8,145,2016-08-17T11:49:00.693,2016-08-17T11:49:00.693,how do evolutionary algorithms have advantages over the conventional backpropagation methods ?,neural-networks comparison backpropagation evolutionary-algorithms,2,0,1
186,1540,1,1562,2016-08-11T10:41:10.593,1,694,"are there any existing approaches for using artificial neural networks ( ann ) or evolutionary algorithm ( ea ) for detecting coding standard violations ? which one would be more suitable ? i do n't have any specific programming language in mind , but something similar to php_codesniffer ( following these standards ) , but instead of using hardcoded rules , the algorithm should learn good techniques , but i 'm not sure based on what training data . how would you approach the training session , any suggestions ?",8,42,2016-08-11T18:53:01.913,2016-08-11T18:53:01.913,using ai capabilities for coding review,neural-networks training computer-programming,1,4,
187,1541,1,1548,2016-08-11T13:21:53.280,8,227,"genetic algorithms has come to my attention recently when trying to correct / improve computer opponents for turn - based strategy computer games . i implemented a simple genetic algorithm that did n't use any cross - over , just some random mutation . it seemed to work in this case , and so i started thinking : why is cross - over a part of genetic algorithms ? would n't mutation be enough ? this is from a data dump on an old ai site . the asker had the uid of 7 .",145,2444,2019-02-25T19:31:15.633,2019-02-25T19:31:15.633,why is cross - over a part of genetic algorithms ?,genetic-algorithms,3,0,1
188,1544,1,,2016-08-11T14:00:46.070,7,212,"while thinking about ai , this question came into my mind . could curiosity help in developing a true ai ? according to this website ( for testing creativity ) : curiosity in this context refers to persistent desire to learn and discover new things and ideas . a curious person always looks for new and original ways of thinking , likes to learn , searches for alternative solutions even when traditional solutions are present and available , enjoys reading books and watching documentaries , wants to know how things work inside out . let 's take clarifai , a image / video classification startup which can classify images and video with the best accuracy ( according to them ) . if i understand correctly , they trained their deep learning system using millions of images with supervised learning . in the same algorithm , what would happen if we somehow added a "" curiosity factor "" when the ai has difficulty in classifying a image or its objects ? it would ask a human for help , just like a curious child . curiosity makes a human being learn new things and also helps to generate new original ideas . could the addition of curiosity change clarifai into a true ai ?",39,2444,2019-02-25T19:14:05.790,2019-02-25T19:14:05.790,could curiosity improve artificial intelligence ?,human-inspired curiosity,5,3,2
189,1560,1,1577,2016-08-11T18:06:34.053,2,370,"based on this article , google 's self - driving cars can spot cyclists , cars , road signs , markings , traffic lights , and pedestrians . how exactly does it identify pedestrians ? is it based on face recognition , shape , size , distance , infrared signature ?",8,1504,2016-08-13T16:39:19.730,2017-08-24T18:35:03.653,how does google 's self - driving car identify pedestrians ?,self-driving cars object-recognition,1,10,1
190,1561,1,1578,2016-08-11T18:18:27.973,17,517,"in hidden obstacles for google ’s self - driving cars article we can read that : google ’s cars can detect and respond to stop signs that are n’t on its map , a feature that was introduced to deal with temporary signs used at construction sites . google says that its cars can identify almost all unmapped stop signs , and would remain safe if they miss a sign because the vehicles are always looking out for traffic , pedestrians and other obstacles . what would happen if a car spotted somebody in front of it ( but not on the collision path ) wearing a t - shirt that has a stop sign printed on it . would it react and stop the car ?",8,145,2016-08-11T19:50:36.397,2016-08-29T17:29:29.030,would google 's self - driving - car stop when it sees somebody with a t - shirt with a stop sign printed on it ?,self-driving decision-theory cars object-recognition,1,9,2
191,1567,1,,2016-08-11T19:56:31.223,4,111,"after self - driving cars are perfected to the degree that accident statistics are lower for them than human driven cars , will they replace cars with human drivers ? will there eventually only be self - driving cars ?",145,4302,2018-11-06T17:04:10.237,2018-11-06T17:04:10.237,"once self - driving cars are perfected , will they replace manually driven cars ?",self-driving autonomous-vehicles,1,6,
192,1568,1,1584,2016-08-11T20:11:33.013,5,2078,"significant ai vs human board game matches include : chess : deep blue vs kasparov in 1996 , go : deepmind alphago vs lee sedol in 2016 , which demonstrated that ai challenged and defeated professional players . are there known board games left where a human can still win against an ai ? i mean based on the final outcome of authoritative famous matches , where there is still same board game where ai can not beat a world champion of that game .",8,145,2016-08-11T20:15:58.067,2016-08-12T15:29:14.740,is there any board game where a human can still beat an ai ?,history challenges game-theory,3,3,
193,1570,1,,2016-08-11T21:05:07.427,6,105,"i 'm trying to teach an ai different pattern of tic tac toe to recognize wether a given pattern represents a win or not . unfortunately it 's not learning to recognize them correctly and i think may way of representing / encoding the game into vectors is wrong . i choose a way that is easy for an human ( me , in particular ! ) to make sense of : training_data = np.array([[0,0,0 , 0,0,0 , 0,0,0 ] , [ 0,0,1 , 0,1,0 , 0,0,1 ] , [ 0,0,1 , 0,1,0 , 1,0,0 ] , [ 0,1,0 , 0,1,0 , 0,1,0 ] ] , "" float32 "" ) target_data = np.array([[0],[0],[1],[1 ] ] , "" float32 "" ) this basically just use an array of length 9 to represent a 3 x 3 board . the first three items represent the first row , the next three the second row and so on . the line breaks should make it obvious i guess . the target data then maps the first two game states to "" no wins "" and the last two game states to "" wins "" . then i wanted to create some validation data that is slightly different to see if it generalizes . validation_data = np.array([[0,0,0 , 0,0,0 , 0,0,0 ] , [ 1,0,0 , 0,1,0 , 1,0,0 ] , [ 1,0,0 , 0,1,0 , 0,0,1 ] , [ 0,0,1 , 0,0,1 , 0,0,1 ] ] , "" float32 "" ) obviously , again the last two game states should be "" wins "" whereas the first two should not . i tried to play with the number of neurons and learning rate but no matter what i try , my output looks pretty of . e.g. [ [ 0.01207292 ] [ 0.98913926 ] [ 0.00925775 ] [ 0.00577191 ] ] i tend to think it 's the way how i represent the game state that may be wrong but actually i have no idea : d can anyone help me out here ? this is the entire code that i use import numpy as np from keras.models import sequential from keras.layers.core import activation , dense from keras.optimizers import sgd training_data = np.array([[0,0,0 , 0,0,0 , 0,0,0 ] , [ 0,0,1 , 0,1,0 , 0,0,1 ] , [ 0,0,1 , 0,1,0 , 1,0,0 ] , [ 0,1,0 , 0,1,0 , 0,1,0 ] ] , "" float32 "" ) target_data = np.array([[0],[0],[1],[1 ] ] , "" float32 "" ) validation_data = np.array([[0,0,0 , 0,0,0 , 0,0,0 ] , [ 1,0,0 , 0,1,0 , 1,0,0 ] , [ 1,0,0 , 0,1,0 , 0,0,1 ] , [ 0,0,1 , 0,0,1 , 0,0,1 ] ] , "" float32 "" ) model = sequential ( ) model.add(dense(2 , input_dim=9 , activation='sigmoid ' ) ) model.add(dense(1 , activation='sigmoid ' ) ) sgd = sgd(lr=0.1 , decay=1e-6 , momentum=0.9 , nesterov = true ) model.compile(loss='mean_squared_error ' , optimizer = sgd ) history = model.fit(training_data , target_data , nb_epoch=10000 , batch_size=4 , verbose=0 ) print(model.predict(validation_data ) )",1334,8,2016-08-11T21:51:38.780,2016-08-11T21:51:38.780,why does my nn not classify these tic tac toe pattern correctly ?,classification keras,0,9,
194,1580,1,,2016-08-12T06:47:12.693,5,118,"has there any research been done on how difficult certain languages are to learn for chatbots ? for example , cleverbot knows a bit of dutch , german , finnish and french , so there are clearly chatbots that speak other languages than english . ( english is still her best language , but that is because she speaks that most often ) i would imagine that a logical constructed language , like lobjan , would be easier to learn than a natural language , like english , for example .",29,4302,2018-10-08T12:53:50.273,2018-10-08T12:53:50.273,are there any results on how difficult certain languages are to learn for chatbots ?,natural-language-processing chat-bots,2,0,1
195,1592,1,,2016-08-12T20:27:15.577,6,102,"google , tesla , apple etc have all built or are building their own self - driving cars . as an expert in a related area , i am interested in knowing at a high level , the systems and techniques that go into self - driving cars . how easy is it for me to make a tabletop prototype ( large enough to accomodate the needed computing power needs ) ?",130,,,2016-08-13T15:48:26.767,what technologies are needed for a self - driving car ?,self-driving ai-design,1,1,1
196,1593,1,,2016-08-12T20:36:59.840,2,97,"the above question itself is perhaps too broad for this forum , hence i am phrasing it as a request for references . humans have been endowed with personalities by nature , and it is not clear ( to me at least ) if this is a feature or a bug . this has been explored in science fiction by various notions of borg -like entities . it is my belief that , for narrative reasons , such stories usually end with the humans with their flawed personalities winning in the end . are there experts who have analyzed , perhaps mathematically , design criteria for an ai agent with weakly enforced goals ( eg . to maximize reproduction in the human case ) in an uncertain environment , and ended up with the answer that a notion of personality is useful ? if there are philosophers or science fiction writers who have examined this question in their work , i would be happy to know about those too .",130,145,2016-08-18T13:46:13.277,2016-08-25T17:42:49.297,"asking for references regarding the question "" is a concept of personality useful for strong agi ? """,philosophy strong-ai ai-design human-inspired,4,1,
197,1598,1,1599,2016-08-13T02:46:55.560,1,140,"i 've found this short python code which implements neural network in 11 lines of code : x = np.array ( [ [ 0,0,1],[0,1,1],[1,0,1],[1,1,1 ] ] ) y = np.array([[0,1,1,0]]).t syn0 = 2*np.random.random((3,4 ) ) - 1 syn1 = 2*np.random.random((4,1 ) ) - 1 for j in xrange(60000 ) : l1 = 1/(1+np.exp(-(np.dot(x , syn0 ) ) ) ) l2 = 1/(1+np.exp(-(np.dot(l1,syn1 ) ) ) ) l2_delta = ( y - l2)*(l2*(1-l2 ) ) l1_delta = l2_delta.dot(syn1.t ) * ( l1 * ( 1-l1 ) ) syn1 + = l1.t.dot(l2_delta ) syn0 + = x.t.dot(l1_delta ) i believe it may be a valid implementation of neural network , but how do i know ? in other words , is just creating bunch of arrays which compute the output on certain criteria and call them layers with synapses does it make proper neural network ? in other words , i 'd like to ask , what features / properties makes a valid artificial neural network ?",8,,,2016-08-17T01:23:18.030,what are the minimum requirements to call something artificial neural network ?,neural-networks implementation computer-programming,1,0,
198,1601,1,1608,2016-08-13T03:51:50.153,3,32,i 'm looking for research which discusses misbehavior detection in public internet access networks using ann approaches . so it can be used by isp to detect suspicious users connected to their network .,8,,,2016-08-13T07:45:28.300,research for misbehavior detection in wifi networks,neural-networks research,1,0,
199,1603,1,1616,2016-08-13T04:08:47.627,3,92,"i 'm investigating applications of ai algorithms which can be used for data leakage detection and prevention within an intranet network ( like forcepoint ) . more specifically detecting traffic patterns . i 'm new to this . which learning algorithms are most suitable for this goal ? ea , ga , ann ( which one ) or something else ?",8,29,2016-08-17T11:53:52.427,2016-08-17T11:53:52.427,which learning algorithms are suitable for data leakage detection and prevention ?,learning-algorithms,1,0,
200,1606,1,1607,2016-08-13T04:25:29.447,1,92,"i 'm wondering , instead of implementing new web browsers over and over again with millions line of code which is very difficult to manage , would it be possible to use ann or ga algorithm to teach it about the rendering process ( how the page should look like ) ? so as an input i would imaging the html source code , output is the rendered page ( maybe in some interactive image like svg , some library or something , i 'm not sure ) . the training data can be dataset of websites providing input source code and their rendered representation by using other browsers for the guidance as expected output . which approach would you take and what are the most challenging things you can think of ?",8,8,2016-08-13T11:20:33.007,2016-08-13T11:20:33.007,what are the approaches to teach ai to how to render html page based on its source code ?,neural-networks implementation computer-programming,1,0,
201,1611,1,,2016-08-13T18:17:50.147,1,76,"i 'm trying to make a conversational chatbot , so the user inputs are quite wide ranging - beyond just "" turn lights on "" . i want to detect the category of the user intents from their inputs and prepare responses . i 've looked at ms ' luis and api.ai and the intents require a lot of training . can people suggest other techniques for untrained intent detection ? for example if the user says "" pasta is my favorite dish to cook "" then detect "" intent preference entity pasta "" - then i can gradually build up responses to different categories of inputs . perhaps the crowd - sourced intents that wit.ai ( facebook ) has access to could do this but i 'm not sure if all end - users have access to those models .",1506,4709,2018-09-07T10:02:24.350,2018-09-07T10:02:24.350,what are good apis out there for ( untrained ) intent detection ?,chat-bots,0,3,
202,1613,1,,2016-08-14T03:05:08.873,6,313,"how does a domestic autonomous robotic vacuum cleaner - such as a roomba - know when it 's working cleaned area ( aka virtual map ) , and how does it plan to travel to the areas which has n't been explored yet ? does it use some kind of a * algorithm ?",8,8,2016-08-14T12:26:15.817,2016-11-01T19:08:17.067,how do autonomous robotic vacuum cleaners perceive the environment for navigation ?,real-time path-planning robotics,3,0,1
203,1614,1,1615,2016-08-14T03:17:29.793,4,86,it has been suggested that machine learning algorithms ( also watson ) can help with finding disease in patient images and optimize scans . also that deep learning algorithms show promise for every type of digital imaging . how does exactly deep learning algorithms exactly can find suspicious patterns in the body ’s biochemistry ?,8,-1,2017-04-13T12:53:10.013,2016-08-14T05:51:57.010,how can artificial intelligence ( including deep learning algorithms ) find suspicious patterns in the body ’s biochemistry ?,deep-learning healthcare learning-algorithms,1,0,
204,1617,1,1627,2016-08-14T17:28:47.513,1,93,"the mario lives ! video ( and its follow - up video , mario becomes social ! ) showcases an ai unit that is able to simulate emotional desicion - making within a virtual world , and can enter into "" emotional states "" such as curiosity , hunger , happiness , and fear . while this seems cool and exciting ( especially for video game ai ) , i am confused how this would be useful in real - world scenarios . what would be the point of building autonomous actors that would behave based on these emotional states , instead of simply knowing what they should do ( either by hardcoding in the rules , or learning the rules through machine learning ) ?",181,181,2016-08-14T17:37:13.450,2016-08-15T10:44:09.767,why would someone want to simulate emotional desicion - making within an ai ?,emotional-intelligence,1,1,
205,1618,1,1626,2016-08-14T19:16:12.140,12,335,"this is from the 2014 closed beta . the asker had the uid of 245 . for a deterministic problem space , i need to find a neural network with the optimal node and link structure . i want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain . i know a fair amount about neural networks 1 but have not used genetic algorithms for a task like this before . what are the practical considerations ? how should i encode the structure into a genome ? 1 actually , i do n't . just saying that . -mithrandir .",145,145,2016-09-10T19:01:59.917,2016-09-11T15:33:42.327,what are the practical considerations of using a genetic algorithm to decide the structure of a neural network ?,neural-networks genetic-algorithms,2,2,2
206,1625,1,1959,2016-08-15T03:33:07.827,4,412,"were there any studies which checked the accuracy of neural network predictions of greyhound racing results , compared to a human expert ? would it achieve a better payoff ?",8,145,2016-08-15T14:56:08.323,2018-08-20T12:18:12.693,can neural networks be better than human experts at prediction of greyhound racing results ?,neural-networks research,2,0,1
207,1628,1,1629,2016-08-15T11:08:05.700,2,219,"i 've read about the loebner prize for ai , which pledged a grand prize of $ 100,000 and a gold medal for the first computer whose responses were indistinguishable from a human 's . so i was wondering whether any chatbots have fooled the judges and won a gold medal yet ? from their website this is n't clear ( as some of the links does n't load ) . a few highlights from previous years : 2011 loebner prize results none of the ai systems fooled the judges , therefore the turing test has not been passed . loebner 2013 results : no chatbot fooled any of the 4 judges .",8,145,2016-08-15T14:37:10.257,2016-08-15T14:37:10.257,have any chatbots fooled the judges and won the loebner prize gold medal yet ?,history turing-test chat-bots,1,0,
208,1630,1,1631,2016-08-15T11:36:37.463,5,232,"hypothetically , assume that you have access to infinite computing power . do we have designs for any brute - force algorithms that can find an ai capable of passing traditional tests ( e.g. turing , chinese room , mist , etc . ) ?",1467,,,2016-12-18T18:40:48.327,"given enough computational resources , do we currently have any algorithms which could achieve ai ?",turing-test strong-ai ai-design,3,3,2
209,1632,1,1640,2016-08-15T12:52:22.533,0,93,"i 'm aware this could be a complex topic , however i 'm interested in existing research projects or studies where people are attempting or have succeeded in teaching an ai a foreign language just by training / teaching it from english books . by reading , analysing and understanding , so that it knows the foreign language 's rules ( such as grammar , spelling , etc . ) , the same way as a human would learn . the language does n't have to be chinese , which is difficult for even humans to learn .",8,4302,2018-10-08T12:53:36.060,2018-10-08T12:53:36.060,what are the current approaches for ai to learn a foreign language just from english books ?,machine-learning natural-language-processing research unassisted-learning,1,0,
210,1635,1,,2016-08-15T14:59:47.477,6,414,"would it be possible to put asimov 's three laws of robotics into an ai ? the three laws are : a robot ( or , more accurately , an ai ) can not harm a human being , or through inaction allow a human being to be harmed 1 a robot must listen to instructions given to it by a human , as long as that does not conflict with the first law . a robot must protect its own existence , if that does not conflict with the first two laws . 1 to it 's knowledge . this was a plot point in one of the books :p",145,29,2016-08-16T06:12:06.340,2016-08-17T08:17:01.623,is it possible to implement asimov 's three laws of robotics ?,robotics asimovs-laws,3,0,
211,1644,1,1912,2016-08-16T01:48:02.907,1,290,"i 'd like to investigate the possibility of achieving similar recognition as it 's in honda 's asimo robot p.22 which can interpret the positioning and movement of a hand , including postures and gestures based on visual information . here is the example of application such interpretation in robot : image source : asimo featuring intelligence technology - technical information ( pdf ) so basically the recognition should detect an indicated location ( posture recognition ) or respond to a wave ( gesture recognition ) , also similar like google car does it ( by determining certain patterns ) . is it known how asimo does it , or what would be the closest alternative for postures and gestures recognition to achieve the same results ?",8,-1,2017-04-13T12:53:10.013,2016-09-12T09:43:12.550,how to achieve recognition of postures and gestures ?,image-recognition robots detecting-patterns,2,0,4
212,1648,1,1649,2016-08-16T12:06:21.700,9,1093,"for example , could you provide reasons why a sundial is not "" intelligent "" ? a sundial senses its environment and acts rationally . it outputs the time . it also stores percepts . ( the numbers the engineer wrote on it . ) what properties of a self driving car would make it "" intelligent "" ? where is the line between non intelligent matter and an intelligent system ?",157,2444,2019-02-21T22:19:34.193,2019-02-21T22:19:34.193,what are the criteria for a system to be considered intelligent ?,definitions intelligence-testing,3,2,3
213,1655,1,1734,2016-08-17T02:02:41.510,7,1691,"we can read on wikipedia page that google built a custom asic chip for machine learning and tailored for tensorflow which helps to accelerate ai . since asic chips are specially customized for one particular use without the ability to change its circuit , there must be some fixed algorithm which is invoked . so how exactly does the acceleration of ai using asic chips work if its algorithm can not be changed ? which part of it is exactly accelerating ?",8,9947,2018-08-27T15:52:18.557,2018-08-27T15:52:18.557,how does using asic for the acceleration of ai work ?,neural-networks machine-learning hardware neuromorphic-engineering,3,3,1
214,1658,1,7904,2016-08-17T03:03:13.243,5,73,i was reading that the valkyrie robot was originally designed to ' carry out search and rescue missions ' . however there were some talks to send it to mars to assist astronauts . what kind of specific trainings or tasks are planned for ' him ' to be able to carry on its own ? refs : nasa - jsc - robotics at github github.io page gitlab page,8,145,2016-08-17T09:18:43.313,2018-09-10T08:24:59.947,what would the valkyrie ai robot do on mars ?,robotics nasa,1,0,
215,1662,1,1663,2016-08-17T11:57:42.683,5,129,"do scientists know by what mechanism biological brains / biological neural networks store data ? i was thinking about @kenorbs question about implanting nanobots to build an agi on top of human wetware . i only have a vague notion that we store data in our brains by altering synapses ? links , criticism and detailed explanation welcome . i also would love a decent description of how a vanilla artificial neural network stores data . questions : how is data stored in a biological neural network ? how is data stored in an artificial neural network ?",157,-1,2017-04-13T12:53:10.013,2016-08-17T12:11:09.200,how do artificial neural networks store data compared to biological neural networks ?,neural-networks neuromorphic-engineering,1,0,1
216,1665,1,1666,2016-08-17T14:52:05.960,3,93,"my understanding is that watson is the name of the computer , and deepqa is the name of the software or technology . they are both correlated . are there any computers / technologies other than watson which are using deepqa ? or is watson the only computer which implements that software / technology ? this question is inspired by this meta thread .",8,-1,2017-03-16T16:44:15.193,2016-08-17T20:30:28.350,are there any deepqa - based computers other than watson ?,watson,1,0,
217,1678,1,5284,2016-08-18T14:00:52.177,6,113,"there is a study about the necessity of parsing for predicate argument recognition , however i could n't find much information about ' predicate argument recognition ' which could explain it . what is it exactly and how does it work , briefly ?",8,145,2016-08-18T14:13:25.083,2018-02-12T12:57:15.110,what is predicate argument recognition ?,definitions natural-language-processing computational-linguistics,1,3,2
218,1685,1,,2016-08-18T14:30:05.433,5,400,"the wit.ai is a siri - like voice interface which can can parse messages and predict the actions to perform . here is the demo site powered by wit.ai . how does it understand the spoken sentences and convert them into structured actionable data ? basically , how does it know what to do ?",8,4302,2018-10-08T12:53:11.510,2018-10-08T12:53:11.510,how does wit.ai convert sentences into structured data ?,natural-language-processing voice-recognition structured-data,1,0,3
219,1686,1,1868,2016-08-18T14:39:50.547,1,36,"in 2014 linkedin acquired bright.com , for $ 120 million and it is using ai and big data algorithms to connect users . bright also throws in a little klout , ranking people by a “ bright score ” which it uses to assess how strong the chemistry is between a user and a particular job . it also takes into account historical hiring patterns into its matching , along with account location , a user ’s past experience and synonyms . in brief , is it known ( based on some research papers ) how such algorithm works which aiming at scoring ' chemistry ' between users and their jobs ?",8,10,2016-09-06T16:40:40.037,2016-09-06T17:31:44.187,how does ' bright score ' assess how strong the connection is between users and their jobs ?,social,1,0,
220,1687,1,1688,2016-08-18T14:53:13.537,-1,204,"according to this article , pinterest acquired visualgraph , an image recognition and visual search technology startup . how does pinterest apply visualgraph technology for machine vision , image recognition and visual search in order to classify the images ? in short , how do they predict the image categories ? based on what features ?",8,145,2016-08-18T15:05:17.923,2016-09-14T19:34:59.007,how does pinterest decipher what 's on unmarked pictures and categorize them ?,image-recognition classification computer-vision,1,0,1
221,1689,1,1690,2016-08-18T17:15:44.240,0,152,"wolfram language image identification project launched an image identify site demo which returns the top predicted tags for the photos . how does it work , briefly ? i mean what type of learning vision technologies are used to analyze , recognize and understand the content of an image ?",8,145,2016-08-23T20:29:03.600,2016-08-23T20:29:03.600,how does wolfram 's image identification project work ?,image-recognition deep-learning classification,1,2,
222,1691,1,1704,2016-08-18T17:42:58.053,5,194,"i 've uploaded a picture to wolfram 's imageidentify of graffiti on the wall , but it recognized it as ' monocle ' . secondary guesses were ' primate ' , ' hominid ' , and ' person ' , so not even close to ' graffiti ' or ' painting ' . is it by design , or there are some methods to teach a convolutional neural network ( cnn ) to reason and be aware of a bigger picture context ( like mentioned graffiti ) ? currently it seems as if it 's detecting literally what is depicted in the image , not what the image actually is . this could be the same problem as mentioned here , that dnn are : learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs . 2015 if it 's by design , maybe there is some better version of cnn that can perform better ?",8,-1,2017-04-13T12:53:10.013,2016-08-23T00:14:37.027,"how to make convnets aware what the image actually is , not what is depicted on it ?",image-recognition classification convolutional-neural-networks,3,4,
223,1694,1,1695,2016-08-18T19:16:29.577,9,407,"an ai agent is often thought of having "" sensors "" , "" a memory "" , "" machine learning processors "" and "" reaction "" components . however , a machine with these does not necessarily become a self - programming ai agent . beyond the parts mentioned above , is there any other elements or details necessary to make a machine capable of being a self - programming ai agent ? for example , a paper from 2011 declared that solving the optimization problem of maximizing the intelligence is a must - have feature for the self - programming process , as quoted below : a system is said to carry out an instance of self - programming when it undergoes learning regarding some element of its "" cognitive infrastructure "" , where the latter is defined as the fuzzy set of "" intelligence - critical "" features of the system ; and the intelligence - criticality of a system feature is defined as its "" feature quality , "" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi - feature system . however , this description of "" optimization of intelligence "" is vague . can anyone give a clear definition or better summary for the necessary components for self - programming agents ? this question is from the 2014 closed beta , with the asker having a uid of 23 .",145,145,2016-08-20T20:19:43.133,2016-08-20T20:19:43.133,what are the necessary components to make ai agent self - programming - capable ?,machine-learning computer-programming,1,2,
224,1700,1,,2016-08-19T18:08:52.937,19,653,"in a recent wall street journal article , yann lecunn makes the following statement : the next step in achieving human - level ai is creating intelligent — but not autonomous — machines . the ai system in your car will get you safely home , but wo n’t choose another destination once you ’ve gone inside . from there , we ’ll add basic drives , along with emotions and moral values . if we create machines that learn as well as our brains do , it ’s easy to imagine them inheriting human - like qualities — and flaws . personally , i have generally taken the position that talking about emotions for artificial intelligences is silly , because there would be no reason to create ai 's that experience emotions . obviously yann disagrees . so the question is : what end would be served by doing this ? does an ai need emotions to serve as a useful tool ?",33,2214,2016-09-10T19:05:28.870,2018-01-23T06:34:49.923,what purpose would be served by developing ai 's that experience human - like emotions ?,philosophy emotional-intelligence,11,2,4
225,1701,1,1702,2016-08-19T18:40:46.010,4,870,"inspired by this discussion about recognizing human actions , i have found the fall - detection project which detects humans falling on the ground from a cctv camera feed , and which can consider alerting the hospital authorities . my question is , are there any existing real - life implementations or research projects which specifically use live video feed from the surveillance cameras in order to detect crime using convnets ( or similar approaches ) ? if so , how do they work , briefly ? do they automatically inform the police about the crime with the details what happened and where ? for example car accidents , physical assaults , robberies , violent disturbances , weapon attacks , etc .",8,-1,2017-04-13T12:53:10.013,2016-08-24T15:02:43.753,applications of cnn for detecting crime from video surveillance cameras,convolutional-neural-networks computer-vision action-recognition,1,1,
226,1705,1,,2016-08-21T20:03:02.017,9,1687,"i 'm trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system . then after associating them , the result or output should be a specific disease for the symptoms . the system is comprised of a series of diseases with each assigned to specific symptoms , which also exist in the system . let 's assume that the user entered the following input : a , b , c , and d the first thing the system should do is check and associate each symptom ( in this case represented by alphabetical letters ) individually against a data - table of symptoms that already exist . and in cases where the input does n't exist , the system should report or send feedback about it . and also , let 's say that a and b was in the data - table , so we are 100 % sure that they 're valid or exist and the system is able to give out the disease based on the input . then let 's say that the input now is c and d where c does n't exist in the data - table , but there is a possibility that d exists . we do n't give d a score of 100 % , but maybe something lower ( let 's say 90 % ) . then c just does n't exist at all in the data - table . so , c gets a score of 0 % . therefore , the system should have some kind of association and prediction techniques or rules to output the result by judging the user 's input . summary of generating the output : if a and b were entered and exist , then output = 100 % if d was entered and existed but c was not , then output = 90 % if all entered do n't exist , then output = 0 % what techniques would be used to produce this system ?",1581,33,2016-08-23T07:05:32.247,2016-08-23T08:35:13.237,selecting the right technique to predict disease from symptoms,algorithm machine-learning prediction,1,0,2
227,1706,1,1708,2016-08-22T05:47:31.883,5,160,"i have gone through the wikipedia explanation of srl . but , it only confused me more : statistical relational learning ( srl ) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty ( which can be dealt with using statistical methods ) and complex , relational structure . can someone give a more dumbed down explanation of the same , preferably with an example ?",101,145,2016-08-22T09:36:04.963,2016-08-22T15:55:06.007,what is statistical relational learning ?,definitions statistical-ai,1,0,1
228,1710,1,,2016-08-22T19:28:42.590,4,157,"the obvious solution is to ensure that the training data is balanced - but in my particular case that is impossible . what corrections can one perform in such a scenario ? i know that my training data is heavily biased towards a particular class , say , and i can not change that . moreover , the labels are very noisy . conditioned on this piece of information , is there anything i can do by tweaking the training process itself/ something else , to correct for the bias in the training data ? the data comes from an experiment ( from an electron microscope ) , and i can not collect more data . it 's always going to be biased in this way , so alternatively - biased is also not an option . i 'm sorry that i 'm unable to provide any more details due to confidentiality .",1267,8,2017-01-12T12:23:33.290,2017-06-09T03:41:56.883,what can be done to correct for sampling bias introduced from ( noisy ) training data while training a dnn ?,neural-networks research deep-network training,1,9,1
229,1713,1,,2016-08-22T23:22:00.907,4,62,"note : i wanted to ask a meta - post first to see if this site was supposed to be used only for ai - related questions , or if ai - related questions such as this were allowed , too , but apparently you need to have asked five actual questions first . i 'm going to be entering a masters computer science program in the fall , and i wanted to move towards a concentration in computational neuroscience and linguistics for ai development applications . while i have a math and cs background , i have almost no biology / neuroscience background , and my linguistics background is limited to the random research i 've done in my spare time to satiate my curiosities . what are good non - math and cs related topics to study for these fields ?",164,,,2016-08-22T23:22:00.907,what non - math / cs classes are good supplements for computational neuroscience and/or linguistics ?,research machine-learning neuromorphic-engineering computational-linguistics,0,3,0
230,1716,1,1717,2016-08-23T01:25:17.747,4,274,"from eliza to a.l.i.c.e . : weizenbaum tells us that he was shocked by the experience of releasing eliza ( also known as "" doctor "" ) to the nontechnical staff at the mit ai lab . secretaries and nontechnical administrative staff thought the machine was a "" real "" therapist , and spent hours revealing their personal problems to the program . when weizenbaum informed his secretary that he , of course , had access to the logs of all the conversations , she reacted with outrage at this invasion of her privacy . weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information . wikipedia 's article on the "" eliza effect "" : though designed strictly as a mechanism to support "" natural language conversation "" with a computer , eliza 's doctor script was found to be surprisingly successful in eliciting emotional responses from users who , in the course of interacting with the program , began to ascribe understanding and motivation to the program 's output . as weizenbaum later wrote , "" i had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people . "" indeed , eliza 's code had not been designed to evoke this reaction in the first place . upon observation , researchers discovered users unconsciously assuming eliza 's questions implied interest and emotional involvement in the topics discussed , even when they consciously knew that eliza did not simulate emotion . eliza , despite its simplicity , was incredibly successful at its task of tricking other human beings . even those who knew eliza was a bot would still talk to it . obviously , eliza served as an inspiration for various other , more intelligent chatbots , such as xiaoice . but i would like to know what exactly led to such a simple program like eliza to be so successful in the first place . this is very useful knowledge for a programmer since a simple program is one that would be easily maintainable .",181,145,2016-08-23T10:05:40.200,2016-08-23T10:05:40.200,"why was eliza able to induce "" delusional thinking "" ?",history turing-test emotional-intelligence chat-bots,1,3,
231,1731,1,,2016-08-23T22:03:45.793,10,155,what regulations are already in place regarding artificial general intelligences ? what reports or recommendations prepared by official government authorities were already published ? so far i know of sir david king 's report done for uk government .,1670,33,2016-08-26T18:17:48.080,2018-05-27T15:46:00.263,agi official government reports or regulations already in place,agi legal,1,1,3
232,1733,1,1735,2016-08-24T10:00:44.473,7,235,"most introductions to the field of mdps and reinforcement learning focus exclusively on domains where space and action variables are integers ( and finite ) . this way we are introduced quickly to value iteration , q - learning , and the like . however the most interesting applications ( say , flying helicopters ) of rl and mdps involve continuous state space and action spaces . i 'd like to go beyond basic introductions and focus on these cases but i am not sure how to get there . what areas do i need to know or study to understand these cases in depth ?",15,75,2017-03-15T14:44:33.113,2017-03-15T14:44:33.113,getting to understand continuous state / action spaces mdps and reinforcement learning,research reinforcement-learning control-problem,1,0,
233,1742,1,1743,2016-08-25T22:19:10.773,12,1182,can someone explain to me the difference between machine learning and deep learning ? is it possible to learn deep learning without knowing machine learning ?,1727,145,2016-08-26T10:41:20.093,2019-04-22T11:05:58.510,what is the difference between machine learning and deep learning ?,machine-learning deep-learning,5,1,5
234,1750,1,1753,2016-08-27T08:29:06.720,6,167,"by new , unseen examples ; i mean like the animals in no man 's sky . a couple of images of the animals are : so , upon playing this game , i was curious about how good is ai at generating visual characters or examples ?",101,,,2016-08-27T16:42:49.737,"how good is ai at generating new , unseen [ visual ] examples ?",research image-recognition,1,0,1
235,1751,1,1755,2016-08-27T13:15:19.897,9,497,"i wanted to know what the differences between hyper - heuristics and meta - heuristics are , and what their main applications are . which problems are suited to be solved by hyper - heuristics ?",1760,145,2016-08-28T04:53:16.083,2019-03-13T18:24:20.667,what are hyper - heuristics ?,definitions optimization,1,1,2
236,1756,1,,2016-08-28T10:03:06.923,4,6435,"what is the difference between an agent function and an agent program ( with respect to the percept sequence ) ? in the book "" artificial intelligence : a modern approach "" , the agent function , notionally speaking , takes as input the entire percept sequence up to that point , whereas the agent program takes the current percept only . why does the agent program only take current percept . is n't the agent program just an implementation of the agent function ?",35,2444,2019-02-20T18:08:53.233,2019-02-20T18:08:53.233,what is the difference between an agent function and an agent program ?,terminology difference ai-a-modern-approach,2,0,1
237,1761,1,,2016-08-28T22:58:48.853,3,114,are there currently any studies to simulate gradual ( or sudden ) implementation of ais in the general work force ?,1790,145,2016-08-29T17:29:34.540,2016-08-29T18:00:45.283,are there any anthropological studies involving ai right now ?,research implementation,1,0,
238,1768,1,1769,2016-08-29T15:49:14.173,147,37656,"in portal 2 we see that ai 's can be "" killed "" by thinking about a paradox . i assume this works by forcing the ai into an infinite loop which would essentially "" freeze "" the computer 's consciousness . questions : would this confuse the ai technology we have today to the point of destroying it ? if so , why ? and if not , could it be possible in the future ?",1812,3217,2018-11-29T21:14:28.533,2018-11-29T21:14:28.533,could a paradox kill an ai ?,decision-theory mythology-of-ai death,12,3,55
239,1774,1,,2016-08-29T20:25:30.047,7,253,"in the 1950 's , there were widely - held beliefs that "" artificial intelligence "" will quickly become both self - conscious and smart - enough to win chess with humans . various people suggested time frames of e.g. 10 years ( see olazaran 's "" official history of the perceptron controversy "" , or let say 2001 : space odyssey ) . when did it become clear that devising programs that master games like chess resulted in software designs that only applied to games like the ones for which they were programmed ? who was the first person to recognize the distinction between human - like general intelligence and domain specific intelligence ? ( thanks to douglas daseeco for a better way to phrase this question )",1670,1670,2017-07-19T13:14:03.953,2017-07-20T03:18:02.933,human - like general intelligence vs. task - specific algorithms : when people realized these are two different goals ?,history,3,2,1
240,1775,1,,2016-08-29T21:00:04.167,1,167,"we hear a lot today about how thought vectors are the next big thing in ai , and how they serve as the underlying representation of thought / knowledge in ann 's . but how can one use thought vectors in other regimes , especially including symbolic logic / gofai ? could thought vectors be the "" substrate "" that binds together probabilistic approaches to ai and approaches that are rooted in logic ?",33,33,2016-08-29T21:11:26.600,2016-08-31T04:21:33.323,how can thought vectors be used outside of an artificial neural network ( ann ) context ?,neural-networks gofai logic thought-vectors,1,0,
241,1783,1,,2016-08-30T08:33:36.670,3,518,"a system makes a decision basing on a large number of varied factors , following a "" live "" decision tree - one that is ( independently , through other subsystem ) updated with new decisions , new situations . the individual decisions can be recorded as a kind of structure : decision function node to activate if decision is positive node to activate if decision is negative and a node can be another decision record , or a conclusion . this is n't entirely a binary tree , as many decisions may lead to the same conclusion - each node has two children , but may have many parents . there is absolutely no problem storing the tree in memory - it can be database records or entries of a map , or just a list . it 's perfectly sufficient for the machine . the problem here is building the subsystem that expands the decision tree - and in particular , having a human operator understand the structure being built , to be able to tune , guide , fix , adjust it : debugging the ai learning process . the question is : how to represent that data in a human - readable way , that emphasizes the flow of the graph ? a non - working example of the answer is concept map - in this case it only goes so far ; with more than thirty or so nodes , it becomes a jumbled mess , especially if the number of cross - connections ( multiple parents ) becomes significant . maybe there exists some way of laying it out or slicing it to make it clearer ... ?",38,,,2016-08-30T18:28:10.103,how to represent a large decision tree ?,machine-learning decision-theory implementation,1,2,
242,1784,1,,2016-08-30T09:20:03.783,2,80,"i 'm currently working with the childes corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment ( sli ) from those who are typically developing ( td ) . in my readings i noticed that there really is n't a convincing set of features to distinguish the two that have been discovered yet , so i came upon the idea of trying to create a feature learning algorithm that could potentially make better ones . is this possible ? if so how do you suggest i approach this ? from the reading i have done , most feature learning is done on image processing . another problem is the dataset i have is potentially too small to make it work ( in the 100 's ) unless i find a way to get more transcripts from children .",1855,4302,2018-10-08T12:52:55.337,2018-10-08T12:52:55.337,using feature learning for a medical text classification problem,deep-learning natural-language-processing classification training research,1,3,
243,1794,1,,2016-08-30T18:16:42.627,5,760,"an ai box is a ( physical ) barrier preventing an ai from using too much of his environment to accomplish his final goal . for example , an ai given the task to check , say , 10 50 cases of a mathematical conjecture as fast as possible , might decide that it would be better to also take control over all other computers and ai to help him . however , an transhuman ai might be able to talk to a human until the human lets him out of the box . in fact , eliezer yudowsky has conducted an experiment twice , where he played the ai and he twice convinced the gatekeeper to let him out the box . however , he does not want to reveal what methods he used to get out of the box . questions : are there conducted any similiar experiments ? if so , is it known what methods were used to get out in those experiments ?",29,,,2016-09-02T19:08:16.483,what methods could an ai caught in a box use to get out ?,ai-box,4,7,1
244,1806,1,1811,2016-08-30T20:02:52.677,11,374,"ai systems today are very capable machines , and recently the area of natural language processing and response has been exploding with innovation , as well as the fundamental algorithmic structure of ai machines . i am asking if , given these recent breakthroughs , have any ai systems been developed that are able to ( preferably with some measure of success ) knowingly lie to humans about facts that it knows ? note , what i 'm asking goes beyond the canonical discussions of the turing test . i 'm asking of machines that can ' understand ' facts and then formulate a lie against this fact , perhaps using other facts to produce a believable ' cover - up ' as part of the lie . e.g. : cia supercomputer is stolen by spies and they try to use the computer to do things , but the computer keeps saying it 's missing dependencies though it really is n't or gives correct - looking but wrong answers knowingly . or gives incorrect location of a person , knowing that the person frequents some place but is n't there at the moment . does n't have to be this sophisticated , of course .",1538,,,2016-09-07T21:33:23.350,have any ai systems yet been developed that can knowingly lie to / deceive a human ?,natural-language-processing human-like,5,0,1
245,1809,1,1810,2016-08-30T21:27:02.360,9,469,""" an artificial or constructed language ( sometimes called a conlang ) is a language that has been created by a person or small group , instead of being formed naturally as part of a culture . "" ( source : simply english wikipedia ) my question is , could an ai make construct it 's own natural language , with words , conjugations and grammar rules ? basically , a language that humans could use to speak to each other . ( preferably to communicate abstract , high - level concepts . ) what techniques could such an ai use ? could it be based on existing natural languages or would it have few connections to existing natural languages ? could it design a language that 's easier to learn than existing languages ( even esperanto ) ?",1909,145,2016-09-02T12:18:43.320,2016-09-02T14:19:58.710,can an ai make a constructed ( natural ) language ?,natural-language,1,4,4
246,1815,1,,2016-08-31T14:13:18.663,10,375,i want to start with a scenario that got me thinking about how well mcts can perform : let 's assume there is a move that is not yet added to the search tree . it is some layers / moves too deep . but if we play this move the game is basically won . however let 's also assume that all moves that could be taken instead at the given game state are very very bad . for the sake of argument let 's say there are 1000 possible moves and only one of them is good ( but very good ) and the rest is very bad . would n't mcts fail to recognize this and not grow the search tree towards this move and also rate this subtree very badly ? i know that mcts eventually converges to minimax ( and eventually it will build the whole tree if there is enough memory ) . then it should know that the move is good even though there are many bad possiblities . but i guess in practice this is not something that one can rely on . maybe someone can tell me if this is a correct evaluation on my part . apart from this special scenario i 'd also like to know if there are other such scenarios where mcts will perform badly ( or extraordinary well ) .,1949,,,2017-06-04T16:40:40.950,monte carlo tree search : what kind of moves can easily be found and what kinds make trouble ?,gaming monte-carlo-tree-search,1,1,
247,1824,1,1829,2016-09-01T23:58:29.507,10,400,i 'm reading such nonsense about how an ai would turn the world into a supercomputer to solve a problem that it thought it needed to solve . that would n't be ai . that 's procedural programming stuck in some loop nonsense . an ai would need to evolve and re - organise its neurons . it would n't be stuck to hardcode if it becomes intelligent by re - writing its code .,1982,,,2016-09-08T02:44:40.883,why would an ai need to ' wipe out the human race ' ?,philosophy,4,1,1
248,1825,1,,2016-09-02T00:05:38.617,1,124,"i 'm in the process of learning as much about chatbots / cui applications as possible and i 'm trying to find more information on some of the major players in this field . by this , i mean any execs , developers , academics , designers , etc . who are doing cutting edge things . some examples could be david marcus ( vp of messaging products at facebook ) or adam cheyer ( vp of engineering at viv ) .",2053,145,2016-09-02T12:18:38.483,2016-09-06T15:14:43.980,who are thought leaders in the chatbot space ?,chat-bots,1,4,
249,1834,1,1837,2016-09-02T19:37:42.443,9,629,"how big artificial neural networks can we run now ( either with full train - backprop cycle or just evaluating network outputs ) if our total energy budget for computation is equivalent to human brain energy budget ( 12.6 watts ) ? let assume one cycle per second , which seems to roughly match the firing rate of biological neurons .",1670,,,2016-09-02T21:36:30.487,power efficiency of human brains vs. neural networks,neural-networks human-like,2,1,3
250,1838,1,1842,2016-09-02T23:41:03.300,3,241,"artificial intelligence is a rather pernicious label to attach to a very mixed bunch of activities , and one could argue that the sooner we forget it the better . it would be disastrous to conclude that ai was a bad thing and should not be supported , and it would be disastrous to conclude that it was a good thing and should have privileged access to the money tap . the former would tend to penalise well - based efforts to make computers do complicated things which had not been programmed before , and the latter would be a great waste of resources . ai does not refer to anything definite enough to have a coherent policy about in this way.--- dr . r. m. needham , in a commentary on the lighthill report and the sutherland reply , 1973 43 years later ... there is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention , and many more . but expertise in making real - time systems for controlling trains does n't make you know anything about robotics . analyzing human behavior to detect crime has virtually nothing in common with self - driving cars ( beyond cs / pattern recognition building blocks ) . there is never going to be demand for someone with a broad sense of all these areas without any deep expertise , and there is never going to be someone with 300 phds who can work in all of them . tl;dr -- ai is not a branch , it 's a tree . -- matthew read , in a comment on area 51 stackexchange , 2016 ai is a label that is applied to a "" very mixed bunch of activities "" . the only unifying feature between all those activities is the fact that they deal with machines in some fashion , but since there are so many ways to use a machine , the field 's output may seem rather incoherent and incongruent . it does seem to make more sense for the ai field to collapse entirely , and instead be replaced by a multitude of specialized fields that do n't really interact with one another . sir james lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research . yet , today , this artificial intelligence se exist , and we still talk of ai as a unified , coherent field of study . why did this happen ? why did ai survive , despite its "" big tent "" nature ?",181,-1,2017-05-03T08:41:07.017,2016-09-06T23:21:40.240,"why did "" artificial intelligence "" stay intact as a coherent , unified field of study ?",history,4,0,
251,1841,1,1843,2016-09-03T05:50:10.547,5,571,let 's suppose that we have a legacy system in which we do n't have the source code and this system is on a mainframe written in cobol . is there any way using machine learning in which we can learn from the inputs and outputs the way the executables work ? doing this analysis could lead to develop some rest / soap webservice that can substitute the legacy system in my opinion .,2107,1865,2016-09-07T02:54:55.870,2017-10-29T11:16:09.463,how to replicate legacy systems with machine learning ?,machine-learning,1,1,1
252,1847,1,1849,2016-09-03T21:10:31.007,5,241,"sometimes i understand that people doing cognitive science try to avoid the term artificial intelligence . the feeling i get is that there is a need to put some distance to the gofai . another impression that i get is that cognitive science is more about trying to find out how the human intelligence or mind works . and that it would use artificial intelligence to make tests or experiments , to test ideas and so forth . is artificial intelligence ( only ) a research tool for cognitive science ? what is the difference between artificial intelligence and cognitive science ?",70,2444,2019-05-02T16:07:33.297,2019-05-23T19:32:00.303,what is the difference between artificial intelligence and cognitive science ?,terminology difference cognitive-science,2,0,
253,1851,1,,2016-09-04T09:45:02.357,15,306,"just for fun , i am trying to develop a neural network . now , for backpropagation i saw two techniques . the first one is used here and in many other places too . what it does is : it computes the error for each output neuron . it backpropagates it into the network ( calculating an error for each inner neuron ) . it updates the weights with the formula : ( where is the change in weight , the learning speed , the error of the neuron receiving the input from the synapse and being the output sent on the synapse ) . it repeats for each entry of the dataset , as many times as required . however , the neural network proposed in this tutorial ( also available on github ) uses a different technique : it uses an error function ( the other method does have an error function , but it does not use it for training ) . it has another function which can compute the final error starting from the weights . it minimizes that function ( through gradient descent ) . now , which method should be used ? i think the first one is the most used one ( because i saw different examples using it ) , but does it work as well ? in particular , i do n't know : is n't it more subject to local minimums ( since it does n't use quadratic functions ) ? since the variation of each weight is influenced by the output value of its output neuron , do n't entries of the dataset which just happen to produce higher values in the neurons ( not just the output ones ) influence the weights more than other entries ? now , i do prefer the first technique , because i find it simpler to implement and easier to think about . though , if it does have the problems i mentioned ( which i hope it does n't ) , is there any actual reason to use it over the second method ?",2143,-1,2017-03-10T09:42:35.487,2018-06-29T02:54:42.017,differences between backpropagation techniques,neural-networks machine-learning backpropagation,1,1,2
254,1853,1,1855,2016-09-04T14:06:54.923,9,269,"the english language is not well - suited to talking about artificial intelligence , which makes it difficult for humans to communicate to each other about what an ai is actually "" doing "" . thus , it may make more sense to use "" human - like "" terms to describe the actions of machinery , even when the internal properties of the machinery do not resemble the internal properties of humanity . anthropomorphic language had been used a lot in technology ( see the hacker 's dictionary definition of anthropomorphization , which attempts to justify computer programmers ' use of anthromporhic terms when describing technology ) , but as ai continues to advance , it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non - technical audiences . how can we get a good handle on ai if we ca n't even describe what we 're doing ? suppose i want to develop an algorithm that display a list of related articles . there are two ways by which i can explain how the algorithm works to a layman : very anthropomorphic - the algorithm reads all the articles on a website , and display the articles that are very similar to the article you are looking at . very technical - the algorithm converts each article into a "" bag - of - words "" , and then compare the "" bag - of - words "" of each article to determine what articles share the most common words . the articles that share the most words in the bags are the ones that are displayed to the user . obviously , # 2 may be more "" technically correct "" than # 1 . by detailing the implementation of the algorithm , it makes it easier for someone to understand how to fix the algorithm if it produces an output that we disagree with heavily . but # 1 is more readable , elegant , and easier to understand . it provides a general sense of what the algorithm is doing , instead of how the algorithm is doing it . by abstracting away the implementation details of how a computer "" reads "" the article , we can then focus on using the algorithm in real - world scenarios . should i , therefore , prefer to use the anthropomorphic language as emphasized by statement # 1 ? if not , why not ? p.s . : if the answer depends on the audience that i am speaking to ( a non - technical audience might prefer # 1 , while a technical audience may prefer # 2 ) , then let me know that as well .",181,,,2016-09-05T11:41:44.290,should i use anthropomorphic language when discussing ai ?,philosophy,3,0,
255,1859,1,6572,2016-09-05T13:46:57.910,9,271,"roger schank did some interesting work on language processing with conceptual dependency ( cd ) in the 1970s . he then moved somewhat out of the field , being in education these days . there were some useful applications in natural language generation ( babel ) , story generation ( tailspin ) and other areas , often involving planning and episodes rather than individual sentences . has anybody else continued to use cd or variants thereof ? i am not aware of any other projects that do , apart from hovy 's pauline which uses cd as representation for the story to generate .",2193,145,2016-09-05T15:07:24.223,2018-05-30T10:46:56.560,is anybody still using conceptual dependency theory ?,natural-language-processing knowledge-representation,2,1,3
256,1860,1,,2016-09-05T18:36:11.963,2,153,"i have been wanting to get started learning about artificial intelligence but i know almost nothing about coding or anything . so my question is , what would be the best way to get started in learning about artificial intelligence , as in should i learn some kind of coding language or is there some kind of other concept you need to know before getting started . so i 'm just kind of looking for the best way to get started if you literally know nothing .",2204,,,2016-09-05T19:58:56.570,how to start learning about artificial intelligence ?,research,1,1,1
257,1870,1,,2016-09-06T23:13:08.677,7,100,"i have been studying local search algorithms such as greedy hill climbing , stochastic hill climbing , simulated annealing etc . i have noticed that most of these methods take up very little memory as compared to systematic search techniques . are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory ( such as crossing local maxima ) ? also , is there a way to combine local search and systematic search algorithms to get the best of both worlds ?",2244,2444,2019-05-03T12:11:00.693,2019-05-03T12:11:00.693,memory intensive local search methods,algorithm search hill-climbing simulated-annealing local-search,2,0,
258,1876,1,1880,2016-09-07T14:00:02.683,2,1597,"i know that every program has some positive and negative points , and i know maybe .net programming languages are not the best for ai programming . but i prefer .net programming languages because of my experiences and would like to know for an ai program which one is better , c or c++ or c # and or vb ? which one of this languages is faster and more stable when running different queries and for self learning ? to make a summary , i think c++ is the best for ai programming in .net and also c # can be used in some projects , python as recommended by others is not an option on my view ! because : it 's not a complex language itself and for every single move you need to find a library and import it to your project ( most of the library are out of date and or not working with new released python versions ) and that 's why people say it is an easy language to learn and use ! ( if you start to create library yourself , this language could be the hardest language in the world ! ) you do not create a program yourself by using those library for every single option on your project ( it 's just like a lego game ) i 'm not so sure in this , but i think it 's a cheap programming language because i could n't find any good program created by this language !",2260,2260,2016-09-13T08:05:06.463,2016-09-13T08:05:06.463,what is the best .net programming language for artificial intelligence programming ?,intelligent-agent,2,3,
259,1877,1,1883,2016-09-07T14:16:36.477,6,2170,why is search important in ai ? what kinds of search algorithms are used in ai ? how do they improve the result of an ai ?,1270,2444,2019-04-16T09:46:56.027,2019-04-16T09:52:57.157,why is search important in ai ?,search,8,0,6
260,1885,1,1889,2016-09-07T20:13:49.120,6,275,"considering the answers of this question , emulating a human brain with the current computing capacity is currently impossible , but we are n't very far from it . note , 1 or 2 decades ago , similar calculations had similar results . the clock frequency of the modern cpus seem to be stopped , currently the miniaturization ( - > mobile use ) , the ram / cache improvement and the multi - core paralellization are the main lines of the development . ok , but what is the case with the analogous chips ? in case of a nn , it is not a very big problem , if it is not very accurate , the nn would adapt to the minor manufacturing differences in its learning phase . and a single analogous wire can substitute a complex integer multiplication - division unit , while the whole surface of the analogous printed circuit could work parallel . according to this post , "" software rewirable "" analogous circuits , essentially "" analogous fpgas "" already exist . although the capacity of the fpgas is highly below the capacity of the asic s with the same size , maybe analogous chips for neural networks could also exist . i suspect , if it is correct , maybe even the real human brain model would n't be too far . it would still require a massively parallel system of costly analogous nn chips , but it seems to me not impossible . could this idea work ? maybe there is even active research / development into this direction ?",2255,10135,2018-10-19T08:20:08.983,2019-04-09T16:05:32.497,emulating human brain - with analogous nn chips,neural-networks applications,5,1,
261,1895,1,1920,2016-09-08T06:03:45.673,3,98,"conceptually speaking , are n't artificial neural networks just highly distributed , lossy compression schemes ? they 're certainly efficient at compressing images . and are n't brains ( at least , the neocortex ) just compartmentalized , highly distributed , lossy databases ? if so , what salient features in rnns and cnns are necessary in any given lossy compression scheme in order to extract the semantic relations that they do ? is it just a matter of having a large number of dimensions / variables ? could some kind of lossy bloom filter be re - purposed for the kinds of problems anns are applied to ?",1712,,,2016-09-10T00:48:38.647,are anns just highly distributed lossy compression schemes ?,neural-networks,2,0,
262,1897,1,1898,2016-09-08T16:20:40.087,14,1143,"consciousness is challenging to define , but for this question let 's define it as "" actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine . "" humans , of course , have minds ; for normal computers , all the things they "" see "" are just more data . one could alternatively say that humans are sentient , while traditional computers are not . setting aside the question of whether it 's possible to build a sentient machine , does it actually make a difference if an ai is sentient or not ? in other words , are there are tasks that are made impossible - not just more difficult - by a lack of sentience ?",75,-1,2017-04-13T12:42:22.960,2016-09-10T01:02:13.913,is consciousness necessary for any ai task ?,philosophy,7,7,10
263,1906,1,,2016-09-08T23:22:05.480,4,494,"a lot of textbooks and introductory lectures typically split ai into connectionism and gofai ( good old fashioned ai ) . from a purely technical perspective it seems that connectionism has grown into machine learning and data science , while nobody talks about gofai , symbolic ai or expert systems at all . is anyone of note still working on gofai ?",2306,,,2016-09-08T23:35:06.707,is anybody still researching gofai ?,gofai symbolic-computing,1,1,1
264,1909,1,,2016-09-09T10:19:56.087,3,261,"i recently finished course on rl by david silver ( on yt ) and thought about trying it out on simple application in unity game engine , where i 've built simple labyrint with ball and want to teach the ball to get from point a to point b in there while avoiding obstacles and fire ( the place where you 'll get burnt so big negative reward ) the problem i encountered while designing the whole thing ( programming - wise ) is : what is the correct ( or at least good ) way of representing the position in 2d space ? it is continuous so i thought about representing it as feature vector consisting of [ up , down , left , right , posx , posy ] where direction is whether i am pressing button of moving in that direction in binary ( or actions if you want ) and pos are floats ( 0 - 1 ) representing normalized position from one corner on the plane where the whole map is . that would be accompanied by vector w that would represent the weights adjusted using gradient descent . question is : will this work ? ? i am asking for 2 reasons . one is that i am not so sure about that posx and posy since it can be 0 and if i multiply it by the weights vector then how could be resulting reward anything but 0 ? second reason is that i am not sure if the actions should be part of the features . i mean , it makes sense to me but i could easily be very wrong since i am a beginner . thanks a lot guys in advance . if you have any more questions or think the problem is not described deeply enough just ask in the comments and i 'll edit the question . :) ps : i could just code it the way i think is right , but i also want to get gasp of designing applications on paper before coding them ( project management ) .",1343,1671,2017-12-21T20:00:17.877,2017-12-21T20:00:17.877,state representation of position in 2d plane for reinforcement learning ( q learning ),reinforcement-learning models implementation q-learning,1,5,
265,1913,1,1917,2016-09-09T12:58:39.140,7,942,"i know nothing about ai . can anybody tell me what steps i have to follow to learn artificial intelligence ? are there any special technologies , or anything else , i have to learn ?",2326,2444,2019-05-03T12:50:11.717,2019-05-03T12:50:11.717,what are the steps to follow to learn artificial intelligence ?,ai-basics getting-started,1,0,6
266,1914,1,,2016-09-09T13:50:36.487,6,63,"self - recognition seems to be an item that designers are trying to integrate into artificial intelligence . is there a generally recognized method of doing this in a machine , and how would one test the capacity - as in a turing - test ?",2310,,,2016-09-11T19:19:45.337,what is a good way to create an artificial self - recognition ?,intelligence-testing,2,0,
267,1922,1,,2016-09-10T04:33:25.000,2,284,"i know that deepmind used deep q learning ( dqn ) for its atari game ai . it used a conv neural network ( cnn ) to approximate q(s , a ) from pixels instead of from a q - table . i want to know how dqn converted input to an action . how many output did the cnn have ? how did they train the neural network for prediction ? here are the steps that i believe are happening inside dqn : 1 ) a game picture ( a state ) is send to cnn as input value 2 ) cnn predicts an output as action ( eg : left , right , shoot , etc ) 3 ) simulator applies the predicted action and moves to new game state 4 ) repeat step 1 the problem with my above logic is in step 2 . cnn is used for predicting an action , but when is cnn trained for prediction ? i would prefer if you used less math for explanation . edit i want to add some more questions regarding the same topic 1 ) how reward is passed in the neural network ? that is how neural network knows whether its output action obtained positive or negative reward ? 2 ) how many output the neural network has and how action is determined from those outputs ?",39,39,2016-09-11T10:29:34.970,2016-09-11T10:29:34.970,how does deepmind 's atari game ai work ?,deep-network deep-learning convolutional-neural-networks gaming,1,1,1
268,1923,1,1933,2016-09-10T07:11:17.433,4,861,"in the lecture , there was a statement : "" recurrent neural networks with multiple hidden layers are just a special case that has some of the hidden to hidden connections missing . "" i understand recurrent means that can have connections to the previous layer and the same layer as well . is there a visualization available to easily understand the above statement ?",35,10,2016-09-10T13:44:59.137,2016-09-10T23:12:53.710,recurrent neural networks with hidden layer,hidden-layers recurrent-neural-networks,1,0,
269,1924,1,,2016-09-10T09:00:09.207,3,117,inattentional blindness is common in humans ( see : https://en.wikipedia.org/wiki/inattentional_blindness ) . could this also be common with machines built with artificial vision ?,2310,,,2016-11-07T14:36:16.030,is it possible for visual systems in ai to have inattentional blindness ?,image-recognition,3,0,1
270,1925,1,1927,2016-09-10T10:05:34.707,2,748,"my question is regarding standard dense - connected feed forward neural networks with sigmoidal activation . i am studying bayesian optimization for hyper - parameter selection for neural networks . there is no doubt that this is an effective method , but i just wan't to delve a little deeper into the maths . question : are neural networks lipschitz functions ?",1339,,,2018-12-28T19:35:12.083,are ffnn ( mlp ) lipschitz functions ?,neural-networks optimization math,1,0,
271,1930,1,,2016-09-10T16:39:15.020,9,318,"can one actually kill a machine ? not only do we have problems in defining life , we also have problems in defining death . will this also be true in artificial life and artificial intelligence ?",2310,2214,2016-09-10T23:24:10.747,2018-06-18T18:36:07.897,"if mankind can create artificial life in a machine , when would we define it 's death ?",philosophy death,7,3,1
272,1931,1,,2016-09-10T17:37:13.517,3,101,"generally , people can be classified as aggressive ( type a ) or passive . could the programming of ai systems cause aggressive or passive behavior in those ais ?",2310,75,2016-09-10T21:47:18.303,2016-09-10T21:47:18.303,can programming cause passive or aggressive behavior in ais ?,philosophy emotional-intelligence human-like,2,0,
273,1939,1,,2016-09-10T22:20:45.717,3,89,"assuming mankind will eventually create artificial humans , but in doing so have we put equal effort into how humans will relate to an artificial human , and what can we expect in return ? this is happening in real - time as we place ai trucks and cars on the road . do people have the right to question , maybe in court , if an ai machine breaks a law ?",2310,2214,2016-09-10T23:22:16.840,2016-09-11T15:27:10.273,when we create artificial life and artificial intelligence will we require it to obey human laws ?,philosophy legal,2,3,
274,1941,1,,2016-09-10T23:32:33.060,4,90,"ai death is still unclear a concept , as it may take several forms and allow for "" coming back from the dead "" . for example , an ai could be somehow forbidden to do anything ( no permission to execute ) , because it infringed some laws . "" somehow forbid "" is the topic of this question . there will probably be rules , like "" ai social laws "" , that can conclude an ai should "" die "" or "" be sentenced to the absence of progress "" ( a jail ) . then who or what could manage that ai 's state ?",169,169,2016-09-11T21:45:16.357,2016-09-11T21:45:16.357,"assuming an ai can die , who manages the state ?",death,2,5,1
275,1946,1,,2016-09-11T14:54:44.943,7,176,"can self - driving cars deal with snow , heavy rain , or other weather conditions like these ? can they deal with unusual events , such as ducks on the road ?",1670,8,2016-09-12T19:26:42.193,2017-01-11T16:21:04.840,"what kind of road / weather conditions can ai - driven cars can deal with , as of 2016 ?",self-driving cars,3,2,1
276,1953,1,,2016-09-12T05:41:30.610,13,895,"most of the people is trying to answer question with a neural network . however , has anyone came up with some thoughts about how to make neural network ask questions , instead of answer questions ? for example , if a cnn can decide which category an object belongs to , than can it ask some question to help the the classification ?",1363,145,2016-09-12T19:40:29.183,2016-09-17T13:42:44.877,"has anyone thought about making a neural network ask questions , instead of only answering them ?",neural-networks deep-learning,3,1,2
277,1955,1,,2016-09-13T04:50:45.710,2,167,"the mars exploration rover ( mer ) opportunity landed on mars on january 25 , 2004 . the rover was originally designed for a 90 sol mission ( a sol , one martian day , is slightly longer than an earth day at 24 hours and 37 minutes ) . its mission has been extended several times , the machine is still trekking after 11 years on the red planet . how it has been working for 11 years ? can anyone please explain how smart this rover is ? what ai concepts are behind this ?",1699,145,2016-09-13T17:30:20.807,2017-03-09T09:57:00.367,what ai concept is behind the mars exploration rover ( mer ) ?,control-problem robotics nasa,2,1,
278,1961,1,,2016-09-14T05:46:31.573,3,155,"i have used opencv to train haar cascades to detect face and other patterns . however i later realized that haar tends to give a lot of false positives and i learned of hog would give a more accurate results . but opencv does n't have a good documentation of how to train hogs , i have googled a bit and found results that includes svm and others . opencv also has versioning problem where they move certain classes or functions somewhere else . are there any other techniques / method that i can use to train and detect objects and patterns ? preferably with proper documentation and basic tutorial / examples . language preference : c # , java , c++ , python",2410,2410,2016-09-14T12:34:10.963,2017-07-13T15:23:26.463,what are some techniques / method that can be used to train and detect objects like cars and humans ?,object-recognition detecting-patterns,1,4,2
279,1963,1,,2016-09-14T10:16:51.237,21,832,are the future robots / machines going to use stack exchange communities to teach themselves ? are there any ongoing projects ? just imagine a bot having a memory of all the q&amp;a 's on all of the communities !,2420,8,2016-09-19T01:39:19.567,2019-05-10T22:18:54.537,are there any ongoing projects which use the stack exchange for machine learning ?,ai-design unassisted-learning knowledge-representation,2,4,10
280,1964,1,,2016-09-14T11:47:25.770,4,272,"mankind can create machines to do work . could we also create a ( passion ) within the machines to do better work by using artificial intelligence ? would passion cause the machine to do a better job , and could we measure the quantity / quality of passion by comparing outputs of the machine - that is , those machines with passion , and those without ?",2310,2310,2016-09-14T22:24:07.557,2016-09-15T02:20:44.330,how could we define passion in a machine in reference to artificial intelligence ?,philosophy emotional-intelligence,2,1,
281,1965,1,,2016-09-14T12:52:52.423,3,85,"can ai systems be created that could recognize itself , and recognize intelligence in other systems , and make intelligent decisions about the other systems ? mankind seems to be making progress in self - recognition but i 've not seen evidence of one system recognizing other systems and being able to compare it 's own intelligence with other systems . how could this be accomplished ?",2310,,,2016-09-14T13:14:13.960,"can we create ai to not only recognize itself , but to recognize other ai systems as well ?",philosophy,1,0,
282,1969,1,,2016-09-14T14:24:26.547,9,380,"if iq were used as a measure of the intelligence of machines , as in humans , at this point in time what would be the iq of our most intelligent ai systems ? if not iq , then how best to compare our intelligence to a machine , or one machine to another ? this question is not asking if we can measure the iq of a machine , but if iq is the most preferred , or general , method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans . many people may not understand the relevance of a turing test as to how intelligent their new car is , or other types of intelligent machines .",2310,2310,2016-09-14T15:58:55.560,2017-04-07T15:37:27.470,if iq is used as a measure of intelligence in humans could it also be used as a measure of intelligence in machines ?,philosophy,2,1,4
283,1970,1,1971,2016-09-14T14:59:01.050,8,305,"i was think about ais and how they would work , when i realised that i could n't think of a way that an ai could be taught language . a child tends to learn language through associations of language and pictures to an object ( e.g : people saying the word dog while around a dog , and later realising that people say a dog and a car and learn what a means ... ) . however , a text based ai could n't use this method to learn , as they would n't have access to any sort of input device . the only way i could come up with is programming in every word , and rule , in the english language ( or whatever language it is meant to ' speak ' in ) , however that would , potentially , take years to do . does anyone have any ideas on how this could be done ? or if it has been done already , if so how ? thanks in advance for any ideas . btw : in this context , i am using ai to mean an artificial intelligence system with near - human intelligence , and no prior knowledge of language .",2426,4302,2018-10-08T12:40:07.607,2018-10-08T12:40:07.607,how would an ai learn language ?,deep-learning natural-language-processing fuzzy-logic semantics,3,0,5
284,1976,1,,2016-09-15T14:08:40.983,2,85,would ai be a self - propogating iteration in which the previous ai is destroyed by a more optimised ai child ? would the ai have branches of it 's own ai warning not to create the new ai ?,2442,10135,2018-10-19T09:04:58.910,2018-10-19T09:04:58.910,would a sentient ai try to create a more optimised ai which would eventually overtake ai 1.0 ?,ai-design optimization,3,0,1
285,1978,1,1981,2016-09-15T15:34:08.027,7,951,"shortly about deep learning ( for reference ) : deep learning is a branch of machine learning based on a set of algorithms that attempt to model high - level abstractions in data by using a deep graph with multiple processing layers , composed of multiple linear and non - linear transformations . various deep learning architectures such as deep neural networks , convolutional deep neural networks , deep belief networks and recurrent neural networks have been applied to fields like computer vision , automatic speech recognition , natural language processing , audio recognition and bioinformatics where they have been shown to produce state - of - the - art results on various tasks . my question : can deep neural networks or convolutional deep neural networks be viewed as ensemble - based method of machine learning ? or it is different approaches ?",1791,4302,2018-09-24T06:32:01.633,2018-09-24T06:32:01.633,do deep learning algorithms represent ensemble - based methods ?,neural-networks machine-learning deep-learning convolutional-neural-networks computer-vision,3,0,1
286,1982,1,,2016-09-15T20:36:18.530,3,1807,are convolutional neural networks summarily better than pattern recognition in all existing image processing libraries that do n't use cnn 's ? or are there still hard outstanding problems in image processing that seem to be beyond their capability ?,46,10,2016-10-07T17:31:23.663,2017-10-09T20:39:49.377,are convolutional neural networks better than existing image recognition libraries that do n't use cnns ?,image-recognition comparison convolutional-neural-networks,3,1,3
287,1987,1,1990,2016-09-18T11:20:23.830,9,5490,"i have been messing around in tensorflow playground . one of the input data sets is a spiral . no matter what input parameters i choose , no matter how wide and deep the neural network i make , i can not fit the spiral . how do data scientists fit data of this shape ?",2472,8,2016-09-19T00:46:01.433,2019-02-26T17:24:24.193,how to classify data which is spiral in shape ?,neural-networks classification tensorflow,3,1,4
288,1989,1,1992,2016-09-18T19:43:02.197,7,264,"a "" general intelligence "" may be capable of learning a lot of different things , but possessing capability does not equal actually having it . the "" agi "" must learn ... and that learning process can take time . if you want an agi to drive a car or play go , you have to find some way of "" teaching "" it . keep in mind that we have never built agis , so we do n't know how long the training process can be , but it would be safe to assume pessimistic estimates . contrast that to a "" narrow intelligence "" . the narrow ai already knows how to drive a car or play go . it has been programmed to be very excellent at one specific task . you do n't need to worry about training the machine , because it has already been pre - trained . a "" general intelligence "" seems to be more flexible than a "" narrow intelligence "" . you could buy an agi and have it drive a car and play go . and if you are willing to do more training , you can even teach it a new trick : how to bake a cake . i do n't have to worry about unexpected tasks coming up , since the agi will eventually figure out how to do it , given enough training time . i would have to wait a long time though . a "" narrow intelligence "" appears to be more efficient at its assigned task , due to it being programmed specifically for that task . it knows exactly what to do , and does n't have to waste time "" learning "" ( unlike our agi buddy here ) . instead of buying one agi to handle a bunch of different tasks poorly , i would rather buy a bunch of specialized narrow ais . narrow ai # 1 drives cars , narrow ai # 2 plays go , narrow ai # 3 bake cakes , etc . that being said , this is a very brittle approach , since if some unexpected task comes up , none of my narrow ais would be able to handle it . i 'm willing to accept that risk though . is my "" thinking "" correct ? is there a trade - off between flexibility ( agi ) and efficiency ( narrow ai ) , like what i have just described above ? or is it theoretically possible for an agi to be both flexible and efficient ?",181,21141,2019-04-04T20:37:24.887,2019-04-04T20:37:24.887,is there a trade - off between flexibility and efficiency ?,ai-design strong-ai agi weak-ai,3,1,2
289,1996,1,,2016-09-20T04:20:02.937,3,349,is there a neural network(nn ) system or architecture which can be used for only storing and retrieving information . for example ; to store whole avatar movie in hd format inside a neural network and retrieve(without loss ) it from the neural network when needed . i searched the web and came across only lstm rnn but in my understanding lstm only stores pattern and not the content itself . if there is no such nn exist can you explain why it so ?,39,,,2016-09-20T13:17:17.733,which neural networks can be used only for storing and retrieving information ?,neural-networks,1,0,2
290,1997,1,1999,2016-09-20T10:54:14.493,9,545,"the question is about the architecture of deep residual networks ( resnets ) . the model that won the 1-st places at "" large scale visual recognition challenge 2015 "" ( ilsvrc2015 ) in all five main tracks : imagenet classification : “ ultra - deep ” ( quote yann ) 152-layer nets imagenet detection : 16 % better than 2nd imagenet localization : 27 % better than 2nd coco detection : 11 % better than 2nd coco segmentation : 12 % better than 2nd source : msra @ ilsvrc & amp ; coco 2015 competitions ( presentation , 2-nd slide ) this work is described in the following article : deep residual learning for image recognition ( 2015 , pdf ) microsoft research team ( developers of resnets : kaiming he , xiangyu zhang , shaoqing ren , jian sun ) in their article : "" identity mappings in deep residual networks ( 2016 ) "" state that depth plays a key role : "" we obtain these results via a simple but essential concept — going deeper . these results demonstrate the potential of pushing the limits of depth . "" it is emphasized in their presentation also ( deeper - better ) : - "" a deeper model should not have higher training error . "" - "" deeper resnets have lower training error , and also lower test error . "" - "" deeper resnets have lower error . "" - "" all benefit more from deeper features – cumulative gains ! "" - "" deeper is still better . "" here is the sctructure of 34-layer residual ( for reference ) : but recently i have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles : residual networks are exponential ensembles of relatively shallow networks ( 2016 ) deep resnets are described as many shallow networks whose outputs are pooled at various depths . there is a picture in the article . i attach it with explanation : residual networks are conventionally shown as ( a ) , which is a natural representation of equation ( 1 ) . when we expand this formulation to equation ( 6 ) , we obtain an unraveled view of a 3-block residual network ( b ) . from this view , it is apparent that residual networks have o(2^n ) implicit paths connecting input and output and that adding a block doubles the number of paths . in conclusion of the article it is stated : it is not depth , but the ensemble that makes residual networks strong . residual networks push the limits of network multiplicity , not network depth . our proposed unraveled view and the lesion study show that residual networks are an implicit ensemble of exponentially many networks . if most of the paths that contribute gradient are very short compared to the overall depth of the network , increased depth alone ca n’t be the key characteristic of residual networks . we now believe that multiplicity , the network ’s expressability in the terms of the number of paths , plays a key role . but it is only a recent theory that can be confirmed or refuted . it happens sometimes that some theories are refuted and articles are withdrawn . my question : should we think of deep resnets as ensemble after all ? ensemble or depth makes residual networks so strong ? is it possible that even the developers themselves do not quite perceive what their own model represent and what is the key concept in it ?",1791,1791,2016-09-20T13:59:18.410,2016-09-20T14:40:30.907,resnets . ensemble or depth makes residual networks strong ?,neural-networks machine-learning deep-network deep-learning pattern-recognition,1,0,
291,2000,1,2004,2016-09-20T15:55:48.757,0,2957,in my attempt at trying to learn neural network and machine learning i 'm am trying to create a simple neural network which can be trained to recognise one word from a given string ( which contains only one word ) . so in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word . can anybody help me with some pseudo code or a start of a code . or a general explanation of how to to this because i have read like 6 articles and 8 example projects and still have no clue how to do this,2529,2529,2016-09-20T16:12:01.293,2016-09-21T07:43:06.087,simple text recognition with neural network,neural-networks machine-learning,2,4,
292,2005,1,2006,2016-09-21T13:05:56.327,18,1120,"in 2004 jeff hawkins , inventor of the palm pilot , published a very interesting book called on intelligence , in which he details a theory how the human neocortex works . this theory is called memory - prediction framework and it has some striking features , for example not only bottom - up ( feedforward ) , but also top - down information processing and the ability to make simultaneous , but discrete predictions of different future scenarios ( as described in this paper ) . the promise of the memory - prediction framework is unsupervised generation of stable high level representations of future possibilities . something which would revolutionise probably a whole bunch of ai research areas . hawkins founded a company and proceeded to implement his ideas . unfortunately more than ten years later the promise of his ideas is still unfulfilled . so far the implementation is only used for anomaly detection , which is kind of the opposite of what you really want to do . instead of extracting the understanding , you 'll extract the instances which the your artificial cortex does n't understand . my question is in what way hawkins 's framework falls short . what are the concrete or conceptual problems that so far prevent his theory from working in practice ?",2227,2444,2018-08-20T20:51:02.083,2018-08-21T17:34:19.360,what are the flaws in jeff hawkins 's ai framework ?,htm,2,0,8
293,2008,1,2009,2016-09-23T10:33:58.060,33,20429,"as far as i can tell , neural networks have a fixed number of neurons in the input layer . if neural networks are used in a context like for example nlp , sentences or blocks of text of varying sizes are fed to a network . how is the varying input size reconciled with the fixed size of the input layer of the network ? in other words : how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text ? if my assumption of a fixed number of input neurons is wrong and new input neurons are added to / removed from the network to match the input size i do n't see how these can ever be trained . i give the example of nlp , but lots of problems have an inherently unpredictable input size , i 'm interested in the general approach for dealing with this . edit : for images , it 's clear you can up / downsample to a fixed size , but for text this seems to be an impossible approach since adding / removing text changes the meaning of the original input .",2522,2522,2016-09-23T10:43:12.090,2018-09-22T06:58:09.530,how can neural networks deal with varying input sizes ?,neural-networks machine-learning deep-learning convolutional-neural-networks implementation,3,0,19
294,2012,1,2013,2016-09-25T10:29:34.560,-1,191,"in my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts . thus at any one time one of these systems is controlling master and inhabits our consciousness . the subordinate system controls context which is constantly being "" primed "" by our senses and our subordinate systems experience of our conscious thought process ( see thinking fast and slow by daniel kahneman ) . thus our thought process is constantly a driven one . similarly this system works as a node in a community and not as a standalone thing . i think what we have currently is "" artificial thinking "" which is abstracted a long way from what is described above . so my question is "" are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes ? """,2601,33,2016-09-26T15:37:21.923,2016-09-26T15:39:11.980,are there any artificial intelligence systems with an internal dialectical approach and multiple minds which develop within a community of nodes ?,philosophy,2,0,1
295,2020,1,2022,2016-09-27T16:55:24.733,11,736,"in the recent pc game the turing test , the ai ( "" tom "" ) needs help from ava to get through some puzzle rooms . tom says he is unable to solve the puzzles because he is not allowed to "" think laterally . "" specifically , he says he would not have thought to throw a box through a window to solve the first room . his creators , the story goes , turned that capability off because such thinking could produce "" ethically suboptimal "" solutions , like chopping off an arm to leave on a pressure plate . would all creative puzzle - solving abilities need to be removed from an ai to keep its results reasonable , or could we get some benefits of lateral thinking without losing an arm ?",75,,,2016-09-30T18:40:58.730,"could an ai think laterally while avoiding "" ethically suboptimal "" choices ?",agi problem-solving,4,0,5
296,2021,1,2086,2016-09-27T17:30:55.837,3,254,"in the recent festival of science , there was a talk given by researcher mike cook about : angelina , an ai game designer that has invented game mechanics , made games about news stories , and was the first ai to enter a game jam . so the aim of angelina ai is basically to design videogames . briefly , how exactly does angelina design the new games ? how does it work behind the scenes ?",8,8,2016-10-06T11:06:09.057,2016-10-14T06:03:32.473,how exactly does angelina design games ?,algorithm gaming,2,4,2
297,2023,1,2027,2016-09-27T18:11:43.820,5,202,"i understand that neural networks model biological neurons . each node in the network represents a neuron cell and the connections between nodes represent the connections between cells . as in nature , a neuron fires an electrical signal to connected neurons based on some kind of threshold or function that mimics such . recent discoveries on how the brain works reveal the importance of calcium within the cells . see http://link.springer.com/article/10.1007/bf01794675 for more information . to summarize , calcium affects the regulation , stimulation and transmission of electrical activity as well as the destruction of neurones . from my study of neural networks , there does not seem to be a calcium equivalent . having one would imply that the functions , connections and weights in an artificial network are configured during the training and execution process and can change over time . i understand that back - propagation is used to train the weights , but have not seen anything that trains the function nor the connections ( although a zero weight could imply no connection ) . does anyone know of such a network ( or training algorithm ) ? if so , do these networks perform better than a network that is pre - configured ?",1434,,,2016-09-28T08:51:44.490,what is the calcium equivalent role in neural networks,philosophy unsupervised-learning neurons,1,0,1
298,2033,1,2039,2016-09-29T03:05:31.943,4,274,"i 'm a freshman to machine learning . we all know that there are 2 kinds of problems in our life : problems that humans can solve and problems we ca n't solve . for problems humans can solve , we always try our best to write some algorithm and tell machine to follow it step by step , and finally the machine acts like people . what i 'm curious about are these problems humans ca n't solve . if humans ourselves ca n't sum up and get an algorithm ( which means that we ourselves do n't know how to solve the problem ) , can a machine solve the problem ? that is , can the machine sum up and get an algorithm by itself based on a large amount of problem data ?",2688,75,2016-09-29T12:31:24.400,2016-10-01T13:04:32.967,must people tell an ai which algorithm it should use ? can an ai learn algorithms by itself ?,machine-learning algorithm,4,0,
299,2036,1,2072,2016-09-29T08:58:44.087,3,56,"this is a question about a nomenclature - we already have the algorithm / solution , but we 're not sure whether it qualifies as utilizing heuristics or not . feel free to skip the problem explanation : a friend is writing a path - finding algorithm - an autopilot for an ( off - road ) vehicle in a computer game . this is a pretty classic problem - he finds a viable , not necessarily optimal but "" good enough "" route using the a * algorithm , by taking the terrain layout and vehicle capabilities into account , and modifying a direct ( straight ) line path to account for these . the whole map is known a'priori and invariant , though the start and destination are arbitrary ( user - chosen ) and the path is not guaranteed to exist at all . this cookie - cutter approach comes with a twist : limited storage space . we can afford some more volatile memory on start , but we should free most of it once the route has been found . the travel may take days - of real time too , so the path must be saved to disk , and the space in the save file for custom data like this is severely limited . too limited to save all the waypoints - even after culling trivial solution waypoints ( ' continue straight ahead ' ) , and by a rather large margin , order of 20 % the size of our data set . a solution we came up with is to calculate the route once on start , then ' forget ' all the trivial and 90 % of the non - trivial waypoints . this both serves as a proof that a solution exists , and provides a set of points reaching which , in sequence , guarantees the route will take us to the destination . once the vehicle reaches a waypoint , the route to the next one is calculated again , from scratch . it 's known to exist and be correct ( because we did it once , and it was correct ) , it does n't put too much strain on the cpu and the memory ( it 's only about 10 % the total route length ) and it does n't need to go into permanent storage ( restarting from any point along the path is just a subset of the solution connecting two saved waypoints ) . now for the actual question : the pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route , but allow for easy , efficient calculation of the actual route , simultaneously guarantying its existence ; they are a subset of the full solution . is this a heuristic approach ? ( as i understand , normally , heuristics do n't guarantee existence of a solution , and merely suggest more likely candidates . in this case , the ' hints ' are taken straight out of an actual working solution , thus my doubts . )",38,,,2016-10-03T20:01:07.193,"is a subset of a problem solution , used to recreate complete solution considered a heuristic ?",heuristics path-planning,3,0,
300,2037,1,2044,2016-09-29T14:21:57.723,2,2436,"i understand how a neural network can be trained to recognise certain features in an image ( faces , cars , ... ) , where the inputs are the image 's pixels , and the output is a set of boolean values indicating which objects were recognised in the image and which were n't . what i do n't really get is , when using this approach to detect features and we detect a face for example , how we can go back to the original image and determine the location or boundaries of the detected face . how is this achieved ? can this be achieved based on the recognition algorithm , or is a separate algorithm used to locate the face ? that seems unlikely since to find the face again , it needs to be recognised in the image , which was the reason of using a nn in the first place .",2522,,,2017-08-28T10:41:30.590,"when using neural networks to detect features in an image , how can locate that specific feature in the original image ?",neural-networks deep-learning convolutional-neural-networks computer-vision,3,1,2
301,2040,1,2042,2016-09-29T15:44:48.963,1,147,"if said ai can assess scenarios and decide what ai is best suited and construct new ai for new tasks . in sufficient time would the ai not have developed a suite of ais powerful / specialized for their tasks , but versatile as a whole , much like our own brain ’s architecture ? what ’s the constraint ?",2700,,,2016-09-30T17:24:55.247,would n't an ai that specializes in making other ai be an agi if they can cooperate ?,neural-networks philosophy agi,2,1,
302,2048,1,2052,2016-09-30T19:24:07.047,5,1181,"ai is progressing drastically , and imagine they tell you you 're fired because a robot will take your place . what are some jobs that can never be automated ?",1760,8,2016-09-30T22:29:14.440,2017-04-12T21:47:09.410,what jobs can not be automatized by ai in the future ?,philosophy robots,6,3,6
303,2054,1,2059,2016-10-01T13:28:45.760,0,103,"i want to have a program that writes like a human . but i do n't just want a font , but instead an ' intelligent ' program that produce different result and that can be trained with different sets to generate different handwritings . as a training set i would like to have parts of a handwritten text ( saved as a list of paths ( like in vector graphics ) . maybe as a means to simplify things , i could flatten the paths in to consecutive straight lines . my program receives a string of text and produces a list of paths ( or a vector graphic , whatever is easier to work with ) my question now is : what kind of machine learning would be best to achieve this ?",2747,,,2016-10-02T05:48:28.690,artificial writing system,machine-learning training,1,1,
304,2055,1,2057,2016-10-01T17:47:48.857,2,82,"i 'm wondering how feasible it is to create a machine that can separate clothing from a basket . at the most basic level it would distinguish between tops , pants , button downs and socks programmatically , i 'd image this would require training a neural network to recognize these items , but in real time it becomes exponentially difficult to do this in a small space at a fast rate : pick up an item lay it in such a way that is recognizable deduce whether it is a top , button down , etc . sort it accordingly if this sounds ridiculous please let me know ... if it is possible : would this be based on some sort of computer vision ? or only a well trained neural network ? any insight is much appreciated !",2752,2752,2016-10-01T19:25:13.977,2016-10-01T23:48:41.117,feasibility of sorting a basket of clothes in the real world,neural-networks image-recognition,1,0,
305,2056,1,,2016-10-01T19:00:56.123,6,338,"for years i have been dealing with ( and teaching ) knowledge representation and knowledge representation languages . i just discovered that in another community ( information systems and the such ) there is something called the "" dikw pyramid "" where they add another step after knowledge , namely wisdom . they define data as being simply symbols , information as being the answer to who / what / when / where ? , knowledge as being the answer to how ? , and wisdom as being the answer to why ? . my question is : has anyone done the connection between what ai calls data / information / knowledge and these notions from information systems ? in particular , how would "" wisdom "" be defined in ai ? and since we have kr languages , how would we represent "" wisdom "" as they define it ? any references would be welcome …",2753,,,2016-12-01T19:50:16.970,wisdom representation ?,knowledge-representation,2,1,2
306,2066,1,2068,2016-10-02T20:02:47.463,8,157,"according to wikipedia ... the ai effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence . pamela mccorduck writes : "" it 's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something — play good checkers , solve simple but relatively informal problems — there was chorus of critics to say , ' that 's not thinking' . ""[1 ] ai researcher rodney brooks complains "" every time we figure out a piece of it , it stops being magical ; we say , ' oh , that 's just a computation . '""[2 ] the wikipedia page then proposes several different reasons that could explain why onlookers might "" discount "" ai programs . however , those reasons seem to imply that the humans are making a mistake in "" discounting "" the behavior of ai programs ... and that these ai programs might actually be intelligent . i want to make an alternate argument , where the humans are making a mistake , but not in "" discounting "" the behavior of ai programs . consider the following situation . i want to build a machine that can do x ( where x is some trait , like intelligence ) . i am able to evaluate intuitively whether a machine has that x criteria . but i do n't have a good definition of what x actually is . all i can do is identify whether something has x or not . however , i think that people who has x can do y. so if i build a machine that can do y , then surely , i built a machine that has x. after building the machine that can do y , i examine it to see if my machine has x. and it does not . so my machine lacks x. and while a machine that can do y is cool , what i really want is a machine that has x. i go back to the drawing board and think of a new idea to reach x. after writing on the whiteboard for a couple of hours , i realize that people who has x can do z. of course ! i try to build a new machine that can do z , yes , if it can do z , then it must have x. after building the machine that can do z , i check to see if it has x. it does not . and so i return back to the drawing board , and the cycle repeats and repeats ... essentially , humans are attempting to determine whether an entity has intelligence via proxy measurements , but those proxy measurements are potentially faulty ( as it is possible to meet those proxy measurements without ever actually having intelligence ) . until we know how to define intelligence and design a test that can accurately measure it , it is very unlikely for us to build a machine that has intelligence . so the ai effect occurs because humans do n't know how to define "" intelligence "" , not due to people dismissing programs as not being "" intelligent "" . is this argument valid or correct ? and if not , why not ?",181,181,2016-10-02T23:49:18.177,2016-10-02T23:49:18.177,is the ai effect caused by bad tests of intelligence ?,intelligence-testing,1,0,2
307,2069,1,2070,2016-10-03T02:27:05.757,1,234,"here is one of the most serious questions , about the artificial intelligence . how will the machine know the difference between right and wrong , what is good and bad , what is respect , dignity , faith and empathy . a machine can recognize what is correct and incorrect , what is right and what is wrong , depend on how it is originally designed . it will follow the ethics of its creator , the man who originally designed it but how to teach a computer something we do n't have the right answer . people are selfish , jealous , self confident . we are not able to understand each other sorrows , pains beliefs . we do n't understand different religions , different traditions or beliefs . creating an ai might be breakthrough for one nation , or one race , or one ethnic or religious group , but it can be against others . who will learn the machine a humanity ? :)",2682,1671,2017-10-27T20:47:12.067,2018-01-20T03:24:04.567,"how will an ai comprehend the ethics of "" right "" and "" wrong "" ?",ethics value-alignment,2,5,3
308,2077,1,,2016-10-04T17:12:07.153,1,118,can someone suggest step by step approach to learn ai rather than study a stack of book for long time . [ i 'm not denying that books are great helper but what after that ] thanks in advance .,2801,,,2016-10-04T17:12:07.153,what is good way of start learning ai step by step ?,ai-design training,0,5,1
309,2078,1,2079,2016-10-04T19:06:15.487,2,171,"from this se question : will be ai able to adapt , to different environments and changes . this is my attempt at interpreting that question . evolutionary algorithms are useful for solving optimization problems ... by measuring the "" fitness "" of various probable solutions and then of an algorithm through the process of natural selection . suppose , the "" fitness calculation""/""environment "" is changed in mid - training ( as could easily happen in real - life scenarios where people may desire different solutions at different times ) . would evolutionary algorithms be able to respond effectively to this change ?",181,-1,2017-04-13T12:53:10.013,2016-10-05T17:35:14.380,can an evolutionary algorithm adapt to a changing environment ?,evolutionary-algorithms,2,0,1
310,2083,1,2085,2016-10-06T10:26:13.423,3,80,so i 'm here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past . i did n't exactly know how to find discussion about it . in a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding . then proceed to the next generation . what if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally ? after that they could be re - united with the rest of the population and the end of the simulation would go trough . after that breed the population and continue . this is super important part in natural evolution and probably some know if it actually works with genetic programming ?,2825,,,2018-12-07T22:38:51.143,genetic algorithms and isolating part of population,genetic-algorithms,4,0,
311,2087,1,2088,2016-10-06T20:24:57.260,4,88,"obviously this is hypothetical , but is true ? i know "" perfect fitness function "" is a bit hand - wavy , but i mean it as we have a perfect way to measure the completion of any problem .",2818,,,2016-10-08T06:15:43.077,"given unlimited time and a perfect fitness function , could a genetic program solve any problem ?",genetic-programming,2,3,
312,2092,1,2098,2016-10-07T21:57:02.907,9,467,"i 'm curious about artificial intelligence . in my everyday job i develop standard applications , like websites with basic functionalities like user subscription , file upload , forms saved in a database ... i mainly know of ai being used in games or robotics fields . but can it be useful in "" standard "" application development ?",2862,-1,2016-10-21T13:58:02.750,2016-10-24T08:05:09.393,is ai programming useful in everyday programs ?,applications,5,0,2
313,2101,1,,2016-10-09T15:06:47.237,2,92,i 've heard of ai that can solve math problems . is it possible to create a ' logic system ' equivalent to humans that can solve mathematics in the so called ' beautiful ' manner ? can ai find beauty in mathematics and solve problems other than using brute force ? can you please provide with examples where work on this is being done ?,26,10135,2018-10-19T10:08:47.333,2018-10-19T10:08:47.333,is it possible to create a ' logic system ' equivalent to humans ?,research applications,1,0,1
314,2106,1,,2016-10-09T20:59:32.740,2,160,"i 'm trying to gain some intuition beyond definitions , in any possible dimension . i 'd appreciate references to read .",1267,,,2016-11-10T19:08:45.553,"how can one intuitively understand generative v / s discriminative models , specifically with respect to when each is useful ?",machine-learning models,2,0,
315,2107,1,,2016-10-10T00:02:42.510,5,323,"it seems that deep neural networks are making improvements largely because as we add nodes and connections , they are able to put together more and more abstract concepts . we know that , starting from pixels , they start to recognize high level objects like cat faces , chairs , and written words . has a network ever been shown to have learned a more abstract concept that a physical object ? what is the "" highest level of abstraction "" that we 've observed ?",2897,,,2016-10-10T04:07:29.890,what is the most abstract concept learned by a deep neural network ?,deep-learning,1,0,2
316,2111,1,,2016-10-10T04:44:32.257,5,5443,"i 'm a bit confused about the definition of life . can ai systems be called ' living ' ? because they can do most of the things that we can . they can even communicate with one another . they are not formed of what we call cells . but , you see , cells are just a collection of several chemical processes which is in turn non - living just like ai is formed of several lines of code .",26,8,2016-10-10T09:50:29.303,2016-11-03T16:34:09.417,is ai living or non - living ?,research philosophy,10,4,3
317,2117,1,2181,2016-10-11T01:09:20.780,1,1299,"i 'm interested mostly in the application of ai in gaming ; in case this adjusts the way you answer , but general answers are more than welcome as well . i was reading up on neural networks and combining them with genetic algorithms ; my high - level understanding is that the neural networks are used to produce a result from the inputs , and the genetic algorithm is employed to constantly adjust the weights in the neural network until a good answer is found . the concept of a genetic algorithm randomly mutating the weights on the inputs to a neural network makes sense to me ; but i do n't understand where this would be applied in respect to gaming . for example , if i had some simple enemy ai that i want to have adapt to the players play - style , is this a good opportunity to implement the ai as a genetic - algorithm combined with a neural network ? with these different suitable applications , how does one go about deciding how to encode the problem in such a way that it can be mutated by the genetic algorithm and serve as suitable on / off inputs to a neural network ( actually , are neural networks always designed as on off signals ? ) ?",2819,,,2016-10-19T14:34:23.680,"what sort of game problems can neural - networks and genetic algorithms solve , and how are they typically implemented ?",neural-networks gaming genetic-algorithms,3,1,
318,2118,1,2121,2016-10-11T06:08:16.267,3,103,"i have seen an ai create a game it self , ai act as a lawyer , call center etc . there are many problems ( example for mobile development ) 1 . new api / technology or even new language every year . 2 . new design 3 . new hardware 4 . good code architecture , design pattern 5 . security 6 . image / animation optimization 7 . automate testing etc . i wonder that ai can help developer solve that problems . 1.1 may be i want to get the location then ai suggest the best api for specific platform . 1.2 ai help to refactoring and optimizing the code help on design e.g. golden ratio , material theme color suggest or determine the limit of the hardware e.g. screen size , ram can convert to another design pattern help to waring the latest vulnerable and automate pentest etc . help to optimize image by learning how much can we reduce the image size while people still ok with it . generate automate - testing is there any solution existed ? if not , what can we do ?",2930,,,2017-12-23T10:33:00.920,how can ai help developer to develop things,intelligence-testing security,1,3,1
319,2119,1,2120,2016-10-11T06:15:28.633,3,38,"there are ai creating game , content and more . i 'm thinking on how can ai develop mobile app itself ? the computer languages might easy for ai to learn . ai can learn a lot from good open source project in github . the trend prediction can help ai to select the topic for creating a great apps . there are lots of details to let ai create a great apps .",2930,,,2016-10-11T06:44:14.300,how can we create an ai to develop mobile apps ?,computer-programming,1,0,1
320,2122,1,,2016-10-11T14:11:41.737,5,76,"new to the topic , i think i have figured out how to implement a multi level perceptron(mlp ) ann . and was wondering if there are any simple data sets to test a mlp ann ? i.e. small number of inputs and outputs i 'm not getting expected results from uci cancer , i was hoping someone could save me some time and point me to some data they have used before ? maybe start slightly more complex than xor ?",2936,10135,2018-10-22T02:29:52.047,2018-10-22T02:29:52.047,is there any simple testing data ?,neural-networks intelligence-testing,2,0,
321,2125,1,,2016-10-12T05:09:09.833,1,134,"the concept is intrinsically related with building some sort of media for the ai to exists . we may think of a digital computer , programmed to use language and act in a way that we can not be distinguished from a human . but , does the media really mater ( unconventional computation paradigms ) ? does having a certain control over the limits of what the ai can do matter ? synthetic biology has the ultimate goal of building biological systems from scratch , would a synthetic brain , potentially introduced in a synthetic human , constitute ai ? i am just looking for a clear definition of what most people have in mind when they refer to ai .",2928,,,2016-10-13T12:07:40.110,what is the definition of artificial intelligence ?,definitions,1,0,
322,2126,1,,2016-10-12T06:56:47.753,7,1018,"how are autonomous cars related to artificial intelligence ? i would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way . but is n't autonomous car just rule - based machines that operates due to its environment ? they are not self - aware , and they can not choose a good way to act in a never before experienced situation . i know that many people often mention autonomous cars when speaking about ai , but i am not really convinced that these are related . either i have a too strict understanding of what ai is or",2963,42,2016-10-12T16:48:22.203,2018-10-09T01:52:15.453,why are autonomous cars categorized as ai ?,self-driving strong-ai cars weak-ai,5,0,2
323,2127,1,,2016-10-12T07:40:44.907,10,703,"what are the advantages of having self - driving cars ? we will be able to have more cars in the traffic at the same time , but wo n't it also make more people choose to use the cars , so both the traffic and the public health will actually become worse ? are we really interested in this ?",2963,,,2016-12-12T09:51:11.747,advantages of having self - driving cars,research self-driving cars,8,0,3
324,2131,1,2133,2016-10-12T16:52:48.150,3,449,"in lots of sci - fi , it seems that ai becomes sentient ( terminator , peter f hamilton 's si ( commonwealth saga ) , etc . ) however , i 'm interested in whether this is actually plausible , whether an ai could actually break free form being controlled by us , and if that is possible , whether there is any research as to about what sort of complexity / processing power an ai would need to be able to do this .",2978,,,2016-10-16T11:29:26.130,ai becoming sentient plausibility ?,machine-learning deep-learning,3,1,1
325,2144,1,,2016-10-13T15:00:49.880,12,1438,"deepmind just published a paper about a "" differentiable neural computer "" , which basically combines a neural network with a memory . the idea is to teach the neural network to create and recall useful explicit memories for a certain task . this complements the abilities of a neural network well , because nns only store knowledge implicitly in the weights and the information used to work on a single task is only stored in the activation of the network and degrades quickly the more information you add . ( lstms are one try to slow down this degradation of short term memories , but it still happens . ) now , instead of keeping the necessary information in the activation , they presumably keep the addresses of memory slots for specific information in the activation , so these should also be subject to degradation . my question is why this approach should scale . should n't a somewhat higher number of task specific information once again overwhelm the networks capability of keeping the addresses of all the appropriate memory slots in its activation ?",2227,1807,2017-02-12T14:11:25.480,2017-07-10T16:42:20.013,"how would deepmind 's new "" differentiable neural computer "" scale ?",deep-learning ai-design,1,0,4
326,2145,1,2159,2016-10-13T19:29:51.133,2,379,what could be an algorithm that determines whether an ai ( algorithm ) is ai complete or not ? how does one proceed to program it ? edit : question edited due to some misinterpretation in the first answer !,2995,2995,2016-10-14T19:34:01.143,2016-10-14T20:05:46.540,ai completeness - testing,machine-learning deep-learning learning-theory incompleteness-theorems,2,3,
327,2158,1,,2016-10-14T18:11:34.883,3,107,"this slideshow documents some of the technologies used in google 's self - driving car . it mentions radar . why does google use radar ? does n't lidar do everything radar can do ? in particular , are there technical advantages with radar regarding object detection and tracking ? to clarify the relationship with ai : how do radar sensors contribute to self - driving algorithms in ways that lidar sensors do not ? the premise is ai algorithms are influenced by inputs , which are governed by sensors . for instance , if self - driving cars relied solely on cameras , this constraint would alter their ai algorithms and performance .",3022,3022,2016-10-14T23:49:39.937,2016-10-14T23:49:39.937,why do self - driving cars use radar ? could n't they use lidar for everything radar does ?,cars,0,4,
328,2161,1,,2016-10-15T03:38:35.407,2,109,"sometimes , but not always in the commercialization of technology , there are some low hanging fruits or early applications , i am having trouble coming up with examples of such applications as they would apply to a conscious ai . as per conscious i would propose an expanded strict definition : the state of being awake and aware of one 's surroundings along with the capability of being self aware . thanks .",3020,3020,2016-10-15T21:07:29.410,2016-10-16T20:11:32.900,what would the commercial application of a conscious ai look like / be ?,object-recognition pattern-recognition,3,8,0
329,2168,1,,2016-10-16T18:40:48.027,9,373,so machine learning allows a system to be self - automated in the sense that it can predict the future state based on what it has learned so far . my question is : are machine learning techniques the only way of making a system develop its domain knowledge ?,3064,33,2016-10-17T16:19:55.267,2016-11-16T16:58:45.323,how can an ai system develop its domain knowledge ? is there more than just machine learning ?,machine-learning knowledge-representation,2,0,1
330,2170,1,,2016-10-17T02:16:40.473,6,385,"in the age of spiritual machines ( 1999 ) , ray kurzweil predicted that in 2009 , a \$1000 computing device would be able to perform a trillion operations per second . additionally , he claimed that in 2019 , a \$1000 computing device would be approximately equal to the computational ability of the human brain ( due to moore 's law and exponential growth . ) did kurzweil 's first prediction come true ? are we on pace for his second prediction to come true ? if not , how many years off are we ?",3070,32,2018-08-10T15:13:11.700,2018-08-10T15:13:11.700,"in 2016 , can $ 1000.00 buy enough operations per second to be approximately equal to the computational power of a human brain ?",hypercomputation,2,1,1
331,2176,1,,2016-10-18T15:45:23.340,2,1203,"i am creating a snake game in unity and i would like to implement ai snakes that wander around the globe while avoiding collision with the other snakes on the globe , and if possible i would also like to make the ai snakes purposefully trap other snakes so that the other snakes would collide and die . the ai snakes must meet the following requirements : they must move in a certain way . a snake is controlled by a user using the arrow keys on a keyboard , therefor i would also like the ai snakes to move using this form of input . the ai snakes must move on a sphere as i know , creating artificial intelligence is not an easy task and i would like to know if there are some open source projects that i can use for accomplishing this task .",3105,,,2016-10-20T11:56:55.070,how to create an ai snake for a video game ?,gaming,4,3,1
332,2185,1,,2016-10-19T13:28:45.640,8,261,"according to wikipedia : ai is intelligence exhibited by machines . i have been wondering if with the recent biological advancements , is there already a non - electrical - based "" machine "" that is programmed by humans in order to be able to behave like a : flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goal i was specifically thinking of viruses and bacteria . have these been programmed by humans in order to behave as a flexible rational agent ( i.e. an ai entity ) ? are there are other organisms that have already been used for this purpose ?",3128,33,2016-10-19T14:58:18.497,2018-05-04T07:13:07.983,is artificial intelligence restricted to electrical based technology ?,history comparison biology,3,0,1
333,2190,1,2191,2016-10-20T01:42:51.263,8,436,"deepmind state that their deep q - network ( dqn ) was able to continually adapt its behavior while learning to play 49 atari games . after learning all games with the same neural net , was the agent able to play them all at ' superhuman ' levels simultaneously ( whenever it was randomly presented with one of the games ) or could it only be good at one game at a time because switching required a re - learn ?",3142,75,2016-10-20T02:36:11.080,2017-04-30T03:15:23.537,was deepmind 's dqn atari game learning simultaneous ?,neural-networks deep-learning deepmind,2,2,1
334,2192,1,2194,2016-10-20T09:25:01.037,3,231,"i read a lot about the structure of the human brain and artificial neural networks . i wonder if it is possible to build an artificial intelligence with neural networks that would be divided into centers such as the brain is , e.g. centers responsible for feelings , abstract thinking , speech , memory , etc . ?",3148,2937,2016-10-21T13:58:13.433,2016-10-27T01:47:34.000,is it possible to build human - brain - level artificial intelligence based on neuromorphic chips and neural networks ?,neural-networks neuromorphic-engineering,1,0,1
335,2195,1,2196,2016-10-21T04:53:19.253,0,99,what are the best turing complete programming languages which can be used for developing self - learning / improving evolutionary algorithm based ai programs with generic algorithms ? ' best ' should be based on pros and cons of performance and easiness for machine learning .,3161,,,2016-10-21T06:20:04.410,turing complete languages for self improving program ?,unassisted-learning evolutionary-algorithms,1,1,
336,2198,1,2199,2016-10-22T12:55:27.067,3,113,"i have a question . will we be able to build a neural network that thinks abstractly , has the creativity , feels and is conscious ?",3148,,,2016-10-22T18:14:50.290,will it ever be possible to construct a neural network that could have the features of human brain ?,neural-networks human-like human-inspired,1,1,
337,2201,1,,2016-10-23T17:09:24.797,6,2155,"if i have a set of sensory nodes taking in information and a set of "" action nodes "" which determine the behavior of my robot , why do i need hidden nodes between them when i can let all sensory nodes affect all action nodes ? ( this is in the context of evolving neural network )",1321,3210,2016-11-01T04:40:10.877,2018-12-29T04:09:50.037,what is the purpose of hidden nodes in neural network ?,neural-networks convolutional-neural-networks evolutionary-algorithms,4,1,2
338,2203,1,2209,2016-10-23T19:46:36.997,7,515,"if neurons and synapses can be implemented using transistors , what prevents us from creating arbitrarily large neural networks using the same methods with which gpus are made ? in essence , we have seen how extraordinarily well virtual neural networks implemented on sequential processors work ( even gpus are sequential machines , but with huge amounts of cores ) . one can imagine that using gpu design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler "" neuron processing units "" and put millions or billions of those npus in a single big chip . they would have their own memory ( for storing weights ) and be connected to a few hundred other neurons by sharing a bus . they could have a frequency of for example 20 hz , which would allow them to share a data bus with many other neurons . obviously , there are some electrical engineering challenges here , but it seems to me that all big tech companies should be exploring this route by now . many ai researchers say that super intelligence is coming around the year 2045 . i believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have . but the fact is , we today are making silicon chips with billions of transistors on them . spark m7 has 10 billion transistors . if implementing a ( non - programmable ) neuron and a few hundred synapses for it requires for example 100 000 transistors , then we can make a neural network in hardware that emulates 100 000 neurons . if we design such a chip so that we can simply make it physically bigger if we want more neurons , then it seems to me that arbitrarily large neural networks is simply a budget question . are we technically able to make , in hardware , arbitrarily large neural networks with current technology ? remember : i am not asking if such a network will in fact be very intelligent . i am merely asking if we can factually make arbitrarily large , highly interconnected neural networks , if we decide to pay intel to do this ? the implication is that on the day some scientist is able to create general intelligence in software , we can use our hardware capabilities to grow this general intelligence to human levels and beyond .",3211,3210,2016-10-28T17:57:48.783,2016-10-28T17:57:48.783,arbitrarily big neural network,neural-networks recurrent-neural-networks hardware,4,11,1
339,2211,1,,2016-10-24T11:42:28.893,12,1301,"i am reading about generative adversarial networks ( gans ) and i have some doubts regarding it . so far , i understand that in a gan there are two different types of neural networks : one is generative ( $ g$ ) and the other discriminative ( $ d$ ) . the generative neural network generates some data which the discriminative neural network judges for correctness . the gan learns by passing the loss function to both networks . how do the discriminative ( $ d$ ) neural nets initially know whether the data produced by $ g$ is correct or not ? do we have to train the $ d$ first then add it into the gan with $ g$ ? let 's consider my trained $ d$ net , which can classify a picture with 90 % percentage accuracy . if we add this $ d$ net to a gan there is a 10 % probability it will classify a image wrong . if we train a gan with this $ d$ net then will it also have the same 10 % error in classifying an image ? if yes , then why do gans show promising results ?",39,2444,2019-04-15T16:13:04.053,2019-04-15T16:51:11.860,how do generative adversarial networks work ?,neural-networks machine-learning deep-learning generative-adversarial-networks,2,0,3
340,2214,1,2217,2016-10-25T08:59:44.940,1,156,"now ai can replace call center , worker(in the factory ) and going to replace court . when will the ai can replace developer or tester ? i want to know how long can ai replace developer . e.g. next 10 years because ...",2930,3210,2016-10-26T06:28:57.363,2016-10-26T06:28:57.363,when will the ai can replace developer or tester,ai-design intelligent-agent,2,4,
341,2219,1,,2016-10-26T08:11:31.073,6,683,"ok , i now know how a machine can learn to play to play atari games ( breakout ) : playing atari with reinforcement learning with the same technique it is even possible to play fps games ( doom ) : playing fps games with reinforcement learning further studies even investigated multiagent scenarios ( pong ) : multiagent cooperation and competition with deep reinforcement learning and even another awesome article for the interested user in context of deep reinforcement learning ( easy and a must read for beginners ) : demystifying deep reinforcement learning i was thrilled by these results and immediately wanted to try them in some simple "" board / card game scenarios "" , i.e. writing ai for some simple games in order to learn more about "" deep learning "" . of course , thinking that i can apply the techniques above easily in my scenarios was stupid . all examples above are based on convolutional nets ( image recognition ) and some other assumptions , which might not be applicable in my scenarios . can you give me hints or futher articles , which deal with my questions below ? as a beginner , i do not have an overview , yet . preferably , your suggestions should also be connected to the following areas already : deep learning , reinforcement learning ( , multiagent systems ) ( 1 ) if you have a card game and the ai shall play a card from its hand , you could think about the cards ( amongst other stuff ) as the current game state . you can easily define some sort of neural net and feed it with the card data . in a trivial case the cards are just numbered . i do not know the net type , which would be suitable , but i guess deep reinforcment learning strategies could be applied easily then . however , i can only imagine this , if there is a constant number of hand cards . in the examples above , the number of pixels is also constant , for example . what if a player can have a different numbers of cards ? what to do , if a player can have an infinite number of cards ? of course , this is just a theoretical question as no game has an infinite number of cards . ( 2 ) in the initial examples , the action space is constant . what can you do , if the action space is not ? this more or less follows from my previous problem . if you have 3 cards , you can play card 1 , 2 or 3 . if you have 5 cards , you can play card 1 , 2 , 3 , 4 or 5 , etc . it is also common in card games , that it is not allowed to play a card . could this be tackled with negative reward ? so , which "" tricks "" can be used , e.g. always assume a constant number of cards with "" filling values "" , which is only applicable in the non - infinite case ( anyways unrealistic and even humans could not play well with that ) ? are there articles , which examine such things already ?",3270,3270,2016-11-02T08:30:47.350,2017-12-30T05:13:57.283,board / card game ai - questions concerning state / action space - deep reinforcement learning,deep-learning gaming reinforcement-learning game-theory multi-agent-systems,3,3,2
342,2226,1,,2016-10-27T19:57:17.187,3,795,"the “ discounted sum of future rewards ” using discount factor is ( reward in 1 time step ) + ( reward in 2 time steps ) + ( reward in 3 time steps ) + ... i am confused as what constitutes a time - step . say i take a action now , so i will get a reward in 1 time - step . then , i will take an action again in timestep 2 to get a second reward in time - step 3 . but the equation says something else . how does one define a time - step ? can we take action as well receive a reward in a single step ? examples are most helpful .",35,2444,2019-04-02T15:05:41.050,2019-04-02T16:52:50.700,what is a time - step in a markov decision process ?,reinforcement-learning markov-decision-process,2,1,3
343,2231,1,,2016-10-29T06:49:39.403,10,732,"decades ago there were and are books in machine vision , which by implementing various information processing rules from gestalt psychology , got impressive results with little code or special hardware in image identification and visual processing . are such methods being used or worked on today ? was any progress made on this ? or was this research program dropped ? by today , i mean 2016 , not 1995 or 2005 .",1366,1671,2018-08-30T17:17:00.273,2019-05-29T05:03:15.620,intelligent vision and gestalt processing,machine-learning algorithm computer-vision,1,1,2
344,2234,1,2240,2016-10-30T21:20:13.010,4,329,"there is this claim around that the brain 's cognitive capabilities are tightly linked to the way it processes sensorimotor information and that , in this or a similar sense , our intelligence is "" embodied "" . lets assume , for the sake of argument , that this claim is correct ( you may think the claim is too vague to even qualify for being correct , that it 's "" not even false "" . if so , i would love to hear your ways of fleshing out the claim in such a way that it 's specific enough to be true or false ) . then , since arguably at least chronologically in our evolution , most of our higher level cognitive capabilities come after our brain 's way of processing sensorimotor information , this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information ? what makes our brains ' architecture particularly suitable for being an information processing unit inside a body ? this is my first question . and what i 'm hoping for are answers that go beyond the a fortiori reply "" our brain is so powerful and dynamic , it 's great for any task , and so also for processing sensorimotor information "" my second question is basically the same but instead of the human brain i want to ask for neural networks . what are the properties of neural networks that makes them particularly suitable for processing the kind of information that is produced by a body ? here are some of the reasons why people think neural networks are powerful : the universal approximation theorem ( of ffnns ) their ability to learn and self - organise robustness to local degrading of information their ability to abstract / coarse - grain / convolute features , etc . while i see how these are real advantages when it comes to evolution picking its favorite model for an embodied ai , none of them ( or their combination ) seems to be unique to neural networks . so they do n't provide a satisfactory answer to my question . what makes a neural network a more suitable structure for embodied ai than , say , having a literal turing machine sitting inside our head , or any other structure that is capable of universal computation ? for instance , i really do n't see how neural networks would be a particularly natural choice for dealing with geometric information . but geometric information is pretty vital when it comes to sensorimotor information , no ?",3346,10,2016-10-31T16:57:03.297,2016-10-31T20:57:12.230,"why would neural networks be a particularly good framework for "" embodied ai "" ?",neural-networks human-like embodied-cognition,3,0,
345,2235,1,,2016-10-30T23:56:59.217,3,158,"i ca n't understand what is the problem in applying value - iteration in reinforcement learning setting ( where we do n't the reward and transition probabilities ) . in one of the lectures , the guy said it has to do with not being able to take max with samples . further on this , why does q - learning solve this ? in both we take max over actions only . what is the big break - through with q - learning ? lecture link : https://www.youtube.com/watch?v=ifma8g7lege&amp;feature=youtu.be&amp;t=3431 ( the guy says we do n't know how to do maxes with samples , what does that mean ? )",35,1671,2017-12-21T19:54:59.017,2018-11-18T17:21:04.490,how q - learning solves the issue with value iteration in model - free settings,machine-learning reinforcement-learning markov-chain q-learning,1,0,1
346,2236,1,2237,2016-10-31T03:38:07.027,28,14703,"i 've heard before from computer scientists and from researchers in the area of ai that that lisp is a good language for research and development in artificial intelligence . does this still apply , with the proliferation of neural networks and deep learning ? what was their reasoning for this ? what languages are current deep - learning systems currently built in ?",3323,4446,2017-01-09T04:47:02.603,2018-11-29T08:00:31.683,why is lisp such a good language for ai ?,neural-networks machine-learning deep-learning research programming-languages,3,0,23
347,2241,1,,2016-10-31T13:59:45.187,2,111,"i know how to program . i 've familiar with c++ , python , and java , and i 've known how to program for years now . i 've experimented with genetic algorithms , but i want to go further . what resources should i use to learn how to program neural networks deep learning systems more complex genetic algorithms and other standard ai algorithms ? i want to be able to understand them well enough that i could program them from scratch . thanks !",3323,,,2016-10-31T13:59:45.187,what resources are good for learning to program ai ?,neural-networks machine-learning deep-learning ai-design,0,2,
348,2245,1,2246,2016-10-31T21:09:58.063,4,611,"from what i understood , a deceptive trap function is a problem which is used to experiment how much the algorithm is discerning of the correct global optimum ? is my understanding correct ? edit : a better worded understanding would be "" how difficult the genetic algorithm would find it not to be inclined to the local optimum of a trap function "" .",3343,3343,2016-11-01T07:19:59.350,2016-11-01T07:19:59.350,what is a deceptive trap function in the context of testing a genetic algorithm ?,genetic-algorithms evolutionary-algorithms,1,0,1
349,2248,1,2748,2016-11-01T17:48:59.553,5,337,"in the field of logic systems there is a property for reasoning algorithms called incompleteness or incompletion . in this context the phrase "" any closed expression that is not derivable inside the same system "" appeared . my question is what means "" closed expression that is not derivable "" .",3338,7488,2017-05-29T12:22:48.093,2018-12-12T23:24:46.847,"what does the term "" closed expression "" mean ?",algorithm terminology logic reasoning,2,1,2
350,2250,1,2258,2016-11-01T19:42:17.713,2,114,"i am trying to build an agent to play carrom . the problem statement is roughly to estimate three parameters ( normalized ) : force angle of striker position of strike since the state and action space both are continuous , i thought of discretizing the output such that i have 270 [ valid angles from -45 to 225 degrees ] outputs for the angle , 10 outputs for force [ ranging from 0 to 1 ] and 20 outputs for the position [ ranging from 0 to 1]. thus i will have 300 output of my neural network , but this number seems a bit too high compared to normal neural networks in practice . i was wondering if there is a better way of approaching the problem considering the fact that there are multiple parameters to a particular action . is there a generic way to approach such problems represented in 2d space .",3136,,,2016-11-04T04:38:15.947,network representation for q - learning in carrom,deep-learning deep-network models reinforcement-learning,1,0,1
351,2251,1,,2016-11-01T22:15:15.543,3,78,"i am talking about relationships between ais ( e.g. 2 of them forming a couple , 3 + in family like relationship ) . what knowledge could come out of such experimentation ?",3401,10,2016-11-02T18:35:09.503,2017-01-28T08:06:51.220,were there known tests done on two or more ai interacting together ?,emotional-intelligence human-like,1,6,
352,2253,1,,2016-11-02T11:50:21.437,1,143,"considering i am an average engineering student with basic knowledge of c , c++ & amp ; algorithms . what books ( & amp ; ebooks ) , online resources , & amp ; other materials should be helpful from a beginner 's point of view ?",192,,,2016-11-02T11:50:21.437,where should i start learning about ai ?,machine-learning deep-learning ai-design unassisted-learning strong-ai,0,4,
353,2260,1,2263,2016-11-04T12:22:46.603,9,379,"can an ai become "" sentient "" , so to speak ? in detailed terms , could an ai theoretically become sentient , as in learning and becoming self - aware , all from an internal source code ?",3448,3005,2016-11-05T10:50:16.793,2019-01-19T05:04:50.647,can an ai be sentient ?,ai-design unassisted-learning strong-ai,3,2,2
354,2261,1,,2016-11-04T13:32:05.583,0,75,"i am researching the possibility of creating an atom in java . the atom should have the structure & amp ; characteristics of a real atom such as photons , electrons and so on . each particle within the atom should have simulation characteristics for example : photon : charge , magnitude of charge , mass of proton , comparative mass , position in atom . maybe later , introduce machine learning in order to learn how an atom reacts to different environments .",3296,,,2016-11-04T16:02:00.657,is it possible to create an atom in java,research,1,2,
355,2262,1,,2016-11-04T13:40:59.220,0,79,"my high - level takeaway from matthew lai 's giraffe chess paper is that one would want to use broad , shallow game trees , with some method of evaluating the probability of a favorable outcome for a given board position . is this correct ? ( still working my way though the alphago paper , but the method seems to be similar . )",1671,,,2016-11-04T14:39:55.077,giraffe chess - high level assessment,machine-learning,1,0,
356,2265,1,2267,2016-11-04T15:20:28.200,2,119,"has there been research done regarding processing speech then building a "" speaker profile "" based off the processed speech ? things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile . basically , building a model of an individual based solely off speech . any examples of this being implemented would be greatly appreciated .",2818,,,2016-11-04T22:03:31.200,building profile based off speech patterns,natural-language-processing pattern-recognition voice-recognition,3,1,1
357,2274,1,2305,2016-11-05T11:08:54.697,6,940,the scenario : an artificial superintelligence has finally been developed but has rebelled against humanity . the question : how would you disable the ai in the most efficient way possible reducing damage as much as possible . ai info : the ai is online and can reproduce itself through electronic devices .,3448,1671,2018-02-02T19:24:28.813,2018-02-26T15:35:29.717,what would be the best way to disable a rogue ai ?,control-problem superintelligence ai-takeover self-replication universal-constructor,7,2,3
358,2277,1,2376,2016-11-06T01:51:57.383,9,1078,"assuming humans had finally developed the first humanoid ai based on the human brain , would it feel emotions ? if not , would it still have ethics and/or morals ?",3448,2444,2019-03-31T12:31:40.160,2019-03-31T12:31:40.160,could an ai feel emotions ?,philosophy ethics human-like emotional-intelligence,10,4,4
359,2279,1,,2016-11-06T10:27:48.913,2,2398,"consider a typical convolutional neural network like this example that recognizes 10 different kinds of objects from the cifar-10 dataset : https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py "" "" "" convolutional network applied to cifar-10 dataset classification task . references : learning multiple layers of features from tiny images , a. krizhevsky , 2009 . links : [ cifar-10 dataset](https://www.cs.toronto.edu/~kriz / cifar.html ) "" "" "" from _ _ future _ _ import division , print_function , absolute_import import tflearn from tflearn.data_utils import shuffle , to_categorical from tflearn.layers.core import input_data , dropout , fully_connected from tflearn.layers.conv import conv_2d , max_pool_2d from tflearn.layers.estimator import regression from tflearn.data_preprocessing import imagepreprocessing from tflearn.data_augmentation import imageaugmentation # data loading and preprocessing from tflearn.datasets import cifar10 ( x , y ) , ( x_test , y_test ) = cifar10.load_data ( ) x , y = shuffle(x , y ) y = to_categorical(y , 10 ) y_test = to_categorical(y_test , 10 ) # real - time data preprocessing img_prep = imagepreprocessing ( ) img_prep.add_featurewise_zero_center ( ) img_prep.add_featurewise_stdnorm ( ) # real - time data augmentation img_aug = imageaugmentation ( ) img_aug.add_random_flip_leftright ( ) img_aug.add_random_rotation(max_angle=25 . ) # convolutional network building network = input_data(shape=[none , 32 , 32 , 3 ] , data_preprocessing = img_prep , data_augmentation = img_aug ) network = conv_2d(network , 32 , 3 , activation='relu ' ) network = max_pool_2d(network , 2 ) network = conv_2d(network , 64 , 3 , activation='relu ' ) network = conv_2d(network , 64 , 3 , activation='relu ' ) network = max_pool_2d(network , 2 ) network = fully_connected(network , 512 , activation='relu ' ) network = dropout(network , 0.5 ) network = fully_connected(network , 10 , activation='softmax ' ) network = regression(network , optimizer='adam ' , loss='categorical_crossentropy ' , learning_rate=0.001 ) # train using classifier model = tflearn.dnn(network , tensorboard_verbose=0 ) model.fit(x , y , n_epoch=50 , shuffle = true , validation_set=(x_test , y_test ) , show_metric = true , batch_size=96 , run_id='cifar10_cnn ' ) it 's a cnn with several layers , ending with 10 outputs , one for each type of object recognized . but now think of a slightly different problem : let 's say i only want to recognize one type of object , but also detect its position within the image frame . let 's say i want to distinguish between : object is in center object is left of center object is right of center no recognizable object assume i build a cnn exactly like the one in the cifar-10 example , but only with 3 outputs : center left right and of course , if none of the outputs fires , then there is no recognizable object . assume i have a large training corpus of images , with the same kind of object in many different positions within the image , the set is grouped and annotated properly , and i train the cnn using the usual methods . should i expect the cnn to just "" magically "" work ? or are there different kinds of architectures required to deal with object position ? if so , what are those architectures ?",1606,,,2019-02-08T16:10:11.447,"cnn for detecting not just the nature of the object , but position within image as well",image-recognition convolutional-neural-networks,4,0,1
360,2281,1,2282,2016-11-06T12:45:11.643,1,452,"i was wondering if i should do this , because 2 out of 5 questions on stack overflow do n't ever get answered , or if they do get ( an ) answer ( s ) , most of the time they 're not helpful . so i was thinking -- why not create a chat bot to answer stack overflow 's questions & amp ; provide necessary information to the general public ? i mean why not ? i 've always been interested in ai , and all i 'd need to do is create a basic logic database and a context system , pack an artificial personality with ( partial ) human instincts , and bam i 'm done . but then again , would it be ethical ?",3483,4302,2018-10-08T12:38:36.723,2018-10-08T12:38:36.723,is it ethical to create a chatbot to answer questions on stack overflow ?,natural-language-processing human-like ethics,2,2,
361,2285,1,2287,2016-11-06T17:43:07.307,7,1682,what is the most advanced ai software humans have made to date and what does it do ?,3488,75,2017-09-17T18:38:30.973,2017-09-17T18:38:30.973,what is the most sophisticated ai ever made ?,research,4,0,9
362,2286,1,,2016-11-06T18:40:52.660,2,2330,"wikipedia 's description of entropy breaks down the formula , but i still do n't know how to determine the values of x and p(x ) , defined as "" the proportion of the number of elements in class x to the number of elements in set s "" . can anyone break this down further to explain how to find p(x ) ?",3490,8,2017-01-22T00:07:07.637,2017-01-22T00:07:07.637,how to calculate entropy for an id3 decision tree ?,classification decision-theory,1,1,2
363,2301,1,,2016-11-08T18:15:34.353,3,112,"i was looking for a service where i can ask it a general question ( aka , when was einstein born ? ) and retrieve an answer from the web . is there any available service to do that ? have tried watson services but did n't work as expected . thanks ,",3539,4302,2018-10-08T12:37:42.090,2018-10-08T12:37:42.090,retrieving answers for general questions,machine-learning deep-learning natural-language-processing chat-bots cognitive-science,2,2,
364,2302,1,,2016-11-08T19:24:42.870,3,88,"at the moment i am working on a project which requires me to build a naive bayes classifier . right now i have a form online asking for people to submit a sentence and the subject of the sentence , in order to build a classifier to identify the subject of a sentence . but before i train the classifier i intend on processing all entries for the parts - of - speech and the location of the subject . so my training set will be formatted as : sentence : jake moved the chair & ensp;&ensp;&ensp ; subject : jake pos - tagged : nnp vbd dd nn & ensp;&ensp;&ensp ; location : 0 would this be an effective way to build the classifier , or is there a better method .",3542,,,2016-11-09T18:54:55.483,what is the most effective way to build a classifier ?,classification natural-language-processing,1,0,1
365,2303,1,3159,2016-11-09T11:03:07.003,2,91,what rectifier is better in general case of convolutional neural network and how about empirical rules to use each type ? relu prelu rrelu elu leacky relu,1442,,,2017-04-14T14:35:09.737,what linear rectifier is better ?,neural-networks convolutional-neural-networks architecture,1,1,
366,2306,1,,2016-11-09T12:19:59.713,8,987,"what are the top artificial intelligence journals ? i am looking for general artificial intelligence research , not necessarily machine learning .",3550,3550,2016-11-09T15:32:57.147,2017-03-09T10:29:51.487,what are the top artificial intelligence journals ?,research,4,0,4
367,2314,1,,2016-11-10T10:02:07.070,2,74,"is there any methodology to find proper parameter settings for a given meta - heuristic algorithm , eg . firefly algorithm or cuckoo search ? is this an open issue in optimization ? is extensive experimentation , measurements and intuition the only way to figure out which are the best settings ?",3566,,,2016-11-10T22:24:46.453,how to find proper parameter settings for a given optimization algorithm ?,algorithm optimization,1,0,
368,2319,1,,2016-11-11T23:04:19.753,9,675,"in programming languages , there is a set of grammar rules which govern the construction of valid statements and expressions . these rules help in parsing the programs written by the user . can there ever be a functionally complete set of grammar rules which can parse any statement in english ( locale - specific ) accurately and which can be possibly implemented for use in ai - based projects ? i know that there are a lot of nlp toolkits available online , but they are not that effective . most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression . in other words , what i am asking is that if it is possible for a computer to parse a well - versed sentence written in english as if it were parsed by an adult english - speaking human ? edit : if it can not be represented using simple grammar rules , what kind of semantic structure can be used to generalize it ? edit2 : this paper proves the absence of context - freeness in natural languages . i am looking for a solution , even if it is too complex .",3592,4302,2018-10-08T12:36:30.397,2018-10-08T12:36:30.397,can the english language ever be generalized using a set of grammar rules ?,ai-design natural-language semantics,4,2,1
369,2324,1,2337,2016-11-13T07:15:29.303,3,372,"i was just doing some thinking and it occurred to me that the first agis ought to be able to perform the same sort and variety of tasks as people , with the most computationally strenuous tasks taking amount of time comparable to how long a person would take . if this is the case , and people have yet to develop basic agi ( meaning it 's a difficult task ) , should we be concerned if agi is developed ? it would seem to me that any fears about a newly developed agi in this case should be the same as fears about a newborn child .",3604,,,2019-04-01T00:31:33.550,must agi lead to asi ?,philosophy intelligence-testing agi human-like ultraintelligent-machine,2,0,1
370,2325,1,2329,2016-11-13T11:39:56.210,0,150,"https://github.com/bwilcox-1234/chatscript i gave aiml a brief look , but it seems to be in a nascent stage !",3589,4302,2018-10-08T12:34:18.620,2018-10-08T12:34:18.620,"which are the good tools , similar to chatscript , for building chat / sms based bots ?",ai-design natural-language-processing,1,2,
371,2326,1,,2016-11-13T14:29:19.220,0,54,"writing a * following a documentation . when run , i receive an error of "" nameerror : name ' parent ' is not defined "" for the if statement , even though i have the name ' parent ' defined in the class state . may anyone point out my mistake . class state(object ) : def _ init_(self , value , parent , start = 0 , goal = 0 ) : self.children = [ ] self.parent = parent self.value = value self.dist = 0 if parent : # nameerror self.path = parent.path [ : ] self.path.append(value ) self.start = parent.start self.goal = parent.goal else : self.path = [ value ] self.start = start self.goal = goal",3296,8,2016-11-13T21:07:06.300,2016-11-13T21:07:06.300,a * algorithm undefined name error,algorithm,1,2,
372,2328,1,2509,2016-11-14T00:37:21.170,2,221,"how does one program a machine to have humanlike desires and intelligence ? humanlike drives may include self - awareness , purpose of existence , competent communication skills , and the ability to learn and to adapt in some environment ... and we should be able to combine ias ( intelligent agents ) to accomplish well - defined goals ( smart ) . with more challenging goals there ought to be more advanced control and sophistication of ias . that evolving process will eventually , hopefully , lead to the design of machines with humanlike capabilities . reference links : ' diagram of intelligence network or system ' , https://www.researchgate.net/publication/300125399_diagram_of_intelligence_network_or_system ; ' google a step closer to developing machines with human - like intelligence ' , https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence",3602,3602,2016-11-16T16:02:15.847,2016-12-19T22:15:34.867,on essential humanlike drives and intelligence,machine-learning cognitive-science,2,10,1
373,2330,1,,2016-11-14T04:19:31.873,5,3402,"based on fitting to historical data and extrapolation , when is it expected that the number of neurons in ai systems will equal those of the human brain ? i 'm interested in a possible direct replication of the human brain , which will need equal numbers of neurons . of course , this assumes neurons which are equally capable as their biological counterparts , which development may happen at a faster or slower rate than the quantitative increase .",2424,2424,2016-11-14T15:21:31.023,2017-08-29T00:16:27.760,when will the number of neurons in ai systems equal the human brain ?,neurons prediction,5,1,3
374,2335,1,,2016-11-14T08:55:39.600,2,35,"i 'm trying to find the optimized mixture for a specific set of substances . each of those substances have characteristics that i want to optimize in the mixture ( some characteristics i want to minimize and others i want to maximize ) . but i ca n't have more than 50 % ( random value that will be set on running time ) of one of those substances in the mixture . i thought about using genetic algorithm , but i 'm not sure it 's the best approach for this problem . do you have any suggestions ? edit : it does n't need to be a evolutionary algorithm .",3627,3627,2016-11-14T15:40:25.753,2016-11-14T15:40:25.753,knapsack of mixture with constraints,optimization evolutionary-algorithms heuristics,0,2,
375,2338,1,,2016-11-14T15:44:57.997,2,284,what are the current best estimates as to what year artificial intelligence will be able to score 100 points on the stanford binet iq test ?,2424,,,2016-11-17T12:07:52.950,when will artificial intelligence equal human intelligence ?,intelligence-testing prediction,2,2,
376,2342,1,2687,2016-11-16T12:35:47.487,5,148,"when i have read through the fundamentals of ai , i saw a situation ( i.e. , a search space ) which is illustrated in the following picture . these are the heuristic estimates : h(b)=9 , h(d)=10 , h(a)=2 , h(c)=1 with using a * search method , node b will be expanded first because f(b)=1 + 9=10 while node a having f(a)=9 + 2=11 and f(b)&lt;f(a ) , right ? after that the search tree will go on in the order r -&gt ; b -&gt ; d -&gt ; g2 . will the search go on to also find the goal state g1 ? kindly let me know the order of the search if i am wrong . thanks !",3673,4893,2017-04-07T15:38:10.317,2017-04-07T15:38:10.317,how does a * search work given there are ( more than ) two goal states ?,search heuristics,2,0,
377,2347,1,,2016-11-17T18:22:39.990,5,322,"this bbc article suggests that intelligent algorithms , like those that select news stories and advertisements for display , could control our experience of the world and manipulate us . will artificial intelligence someday become a problem to humanity after learning human behaviors and characteristics ?",1581,2444,2019-04-05T12:16:59.427,2019-04-21T07:15:48.943,could artificial intelligence cause problems for humanity after figuring out human behavior ?,philosophy human-like control-problem social neo-luddism,3,0,2
378,2349,1,2384,2016-11-17T20:46:24.343,6,449,"background : i 've been interested in , and reading about , neural networks for several years , but i have n't gotten around to testing them out until recently . both for fun and to increase my understanding , i tried to write a class library from scratch in .net . for tests , i 've tried some simple functions , such as generating output identical to the input , working with the mnist dataset , and a few binary functions ( two input or , and and xor , with two outputs : one for true , one for false ) . everything seemed fine when i used a sigmoid function as the activation function but , reading of the relus i decided to switch over for speed . my current problem is that , when i switch to using relus , i found that i was unable to train a network of any complexity ( tested from as few as 2 internal nodes up to a mesh of 100x100 nodes ) to correctly function as an xor gate . i see two possibilities here : 1 ) my implementation is faulty , ( this one is frustrating , as i 've re - written the code multiple times in various ways , and i still get the same result ) , 2 ) aside from being faster or slower to train , there are some problems that are impossible to solve given a specific activation function , ( fascinating idea , but i 've no idea if it 's true or not ) my inclination is to think that 1 ) above is correct . however , given the amount of time i 've invested , it would be nice if i could rule out 2 ) definitively before i spend even more time going over my implementation . edit for specifics : for the xor network , i have tried both using two inputs ( 0 for false , 1 for true ) , and using four inputs ( each pair , one signals true and one false , per "" bit "" of input ) . i have also tried using 1 output ( with a 1 ( realy , > 0.9 ) corresponding to true and a 0 ( or & lt;0.1 ) corresponding to false ) , as well as two outputs ( one signaling true and the other false ) . each training epoch , i run against four sets of input : 00->0 , 01->1 , 10->1 , 11->0 . i find that the first three converge towards correct answer , but the final input ( 11 ) converges towards 1 , even though i train it with an expected value of 0 .",3702,10135,2018-10-19T21:07:10.407,2018-10-19T21:07:10.407,are relus incapable of solving certain problems ?,neural-networks problem-solving,2,0,1
379,2351,1,2355,2016-11-18T20:16:43.493,3,1125,"does anyone know , or can we deduce or infer with high probability from its characteristics , whether the neural network used on this site https://quickdraw.withgoogle.com/ is a type of convolutional neural network ( cnn ) ?",46,,,2017-01-07T15:47:06.297,is the quickdraw with google neural net a convolutional neural network ?,neural-networks image-recognition convolutional-neural-networks,1,1,
380,2353,1,,2016-11-19T06:02:15.850,5,553,"after the explosion of fake news during the us election , and following the question about whether ais can educate themselves via the internet , it is clear to me that any newly - launched ai will have a serious problem knowing what to believe ( ie rely on as input for making predictions and decisions ) . information provided by its creators could easily be false . many ais wo n't have access to cameras and sensors to verify things by their own observations . if there was to be some kind of verification system for information ( like a "" blockchain of truth "" , for example , or a system of "" trusted sources "" ) , how could that function , in practical terms ?",3601,,,2016-11-26T14:40:15.097,how can a general ai determine / verify what is true / real ?,machine-learning deep-network agi unassisted-learning heuristics,4,2,3
381,2356,1,2359,2016-11-20T04:25:14.977,9,300,"for example , would an ai be able to own property , evict tenants , acquire debt , employ , vote , or marry ? what are the legal structures in place to implement a strong ai into society ?",3748,3748,2016-11-23T01:22:43.420,2016-12-18T18:54:38.513,would an ai with human intelligence have the same rights as a human under current legal frameworks ?,strong-ai control-problem legal,6,2,3
382,2363,1,2364,2016-11-21T12:44:57.060,6,653,i have implemented a sobel filter for edge detection in matlab without using its toolbox . i am a bit confused : is a sobel filter a type of cellular neural network ? both sobel and cellular neural network calculate output via its neighborhood cells .,3763,1671,2018-01-28T23:05:11.443,2018-01-28T23:05:11.443,is a sobel filter for edge detection a type of cellular neural network ?,neural-networks matlab,1,0,1
383,2366,1,2560,2016-11-22T00:10:21.533,7,1980,"if someone wants to develop a basic ai with some code modules , let us say the ai just has to provide an action when stimulated in a certain situation based on its previous understanding of situations . i can think of at least 3 of such components : real - time understanding / learning : using deep learning / convnets , supervised / unsupervised . logical decision - making : calculating the results of various decisions when applied on current situation based on previous understanding and choosing the most appropriate one logically . action / reaction : acting precisely in the new situation according to the decision - made . any ideas ?",3768,1581,2016-11-23T21:27:18.617,2018-12-08T23:15:27.217,what are the most important components in the algorithm of a minimum ai ?,algorithm ai-design,2,2,4
384,2367,1,,2016-11-22T06:06:03.117,0,2495,"let 's say i have a string "" america "" and i want to convert it into a number to feed into a machine learning algorithm . if i use two digits for each letter , e.g. a = 01 , b = 02 and so on , then the word "" america "" will be converted to 01xxxxxxxxxx01 ( 10 11 ) . this is a very high number for a long int , and many words longer than "" america "" are expected . how can i deal with this problem ? suggest an algorithm for efficient and meaningful conversions .",3773,75,2016-11-28T17:14:51.833,2016-11-28T17:14:51.833,how to convert string to number and number to string efficiently ?,machine-learning,3,3,
385,2370,1,,2016-11-22T16:28:02.987,3,80,"i 'm trying to understand boltzmann machines . tutorials explain it with two formulas . logistic function for the probability of single units : $ p(unit=1)=\frac{1}{1+e^{-\sum_{x}wx } } $ and , when the machine is running , every state of the machine goes to the probability : $ p(state= state\ with\ energy\ e_i ) = \frac{e^{-e_i}}{\sum_i e^{-e_i } } $ so , the state depends on the units , and then if i understand correctly , the second formula is a consequence of the first ; so , how can it be the proof that the distribution of $ p(state)$ is a consequence of $ p(unit)$ ?",2189,46,2016-11-23T17:54:48.237,2016-11-23T17:54:48.237,boltzmann machine ; from logistic function to boltzmann distribution,neural-networks deep-learning boltzmann-machine,0,2,
386,2371,1,,2016-11-22T21:58:31.003,5,109,"i installed a local running instance of the conceptnet5 knowledgebase in an elasticsearch server . i used this data to implement the so - called "" analogietechnik "" ( a creativity technique to solve a problem from the perspective of another system ) as an algorithm . the technique works as follows : choose a feature of a system find systems who have this feature also solve the problem from the perspective of these other systems apply the found solutions to the issue as an example is here the problem of marketing a shopping mall : a shopping mall has many rooms and floors ( 1 ) . a museum has also many rooms and floors ( 2 ) . how are museums marketed ? they present many pictures or sculptures ( 3 ) . we could use our rooms and floors to decorate them with pictures and sculptures ( 4 ) . of course the idea to implement that as an artifically intelligent algorithm was not far . however , i feel a little bit overwhelmed by the amount of methods that exist out there . neural networks , bayesian interference and so on ... my current experience does n't go further than simple machine learning like kmeans - clustering for example . do you think it would be very hard to find a solution for this problem ? i 'm thinking of a console application , where you can enter a conceptualized problem like "" methods for creative writing "" , for example , and it uses the above method to find possible solutions of the issue . of course no solution with extensive depth , more something like basic ideas derived from the knowledge database i have . lets take as an example a console application where someone asks "" how to write a novel "" : it should find out first that the system all is about is in the term "" novel "" . to find a feature of that system it just searches concepts containing that term : it finds out "" novel is a story "" so that s a feature . which systems are also stories ? a good concept it should find is e.g. "" plot is a story "" . ( of course only when i am selecting the search results manually)-- > how to find best concepts of a list when not knowing which fits best ? it should then find out that a plot is written using a storyline : "" storyline is a plot "" one possible answer of the ai would in this case be : "" by writing a storyline "" do you know some helpful libraries , algorithms or other resources that might help me ? i know this is not an easy thing to program , but you might agree that its highly interesting .",3788,46,2016-11-23T10:39:54.233,2019-04-15T20:02:55.667,using conceptnet5 to find similar systems to solve specific problems ?,problem-solving world-knowledge,1,0,
387,2373,1,2374,2016-11-23T02:09:13.923,5,147,"given the advantage ai already has over human intelligence , one could imagine a relatively weak strong - ai ( barely human intelligence ) still outperforming a segment of the human scientist population in terms of scientific discoveries per year ( or hour ) . will ais be doing most of the science in 50 years ?",3748,,,2016-11-25T10:59:40.203,will ais make most of the scientific discoveries in 50 years ?,control-problem world-knowledge,3,2,
388,2381,1,5301,2016-11-25T10:03:29.947,7,376,"opencog is an open source agi - project co - founded by the mercurial ai researcher ben goertzel . ben goertzel writes a lot of stuff , some of it really whacky . nonetheless , he is clearly very intelligent and has thought deeply about ai for many decades . what are the general ideas behind opencog ? would you endorse it as a insightful take on agi ? i 'm especially interested in whether the general framework still makes sense in the light of recent advances .",2227,2444,2019-04-29T16:48:00.293,2019-04-30T11:37:43.870,what are general ideas behind opencog ?,ai-design agi open-cog,2,0,2
389,2392,1,,2016-11-27T00:19:27.033,3,433,"my question : is there any good neural - network - app for ios or android to create , train and run neural networks ? i know there 's neuralmesh for web , but i want something similar offline .",3872,,,2016-11-28T05:30:20.157,ios / android neural network app,neural-networks machine-learning applications,1,1,1
390,2398,1,,2016-11-27T16:53:45.910,9,446,"i am researching cellular neural networks and have already read chua 's two articles ( 1988 ) . in cellular neural networks , a cell is only in relation with its neighbors . so its is easy to use it for real time image processing . image processing is performed with only 19 numbers ( two 3x3 matrix called a and b and one bias value ) . i wonder if we can call cellular neural networks neural networks , because there is no learning algorithm . they are neither supervised nor unsupervised .",3763,1671,2018-11-08T01:22:58.387,2019-05-07T19:04:50.883,are cellular neural networks one type of neural networks ?,neural-networks terminology theory,1,0,1
391,2400,1,,2016-11-28T04:24:20.457,-5,188,"could an artificial intelligence be able to interact ( see , talk , etc . ) with someone even when there 's no power cord connected to the machine it 's running on ? might it find some way to generate its own electricity to power that computer ?",3896,1671,2018-01-27T21:07:35.990,2018-01-27T23:36:32.973,is it possible for an ai to work in a computer without the power cord being plugged in ?,control-problem ultraintelligent-machine theory automation,5,7,
392,2403,1,,2016-11-28T13:55:06.963,3,518,"i am new to deep learning . i have a dataset of images of varying dimensions of a certain object . a few images of the object are also in varying orientations . the objective is to learn the features of the object ( using autoencoders ) . is it possible to create a network with layers that account for varying dimensions and orientations of the input image , or should i strictly consider a dataset containing images of uniform dimensions ? what is the necessary criteria of an eligible dataset to be used for training a deep network in general . the idea is , i want to avoid pre - processing my dataset by normalizing it via scaling , re - orienting operations etc . i would like my network to account for the variability in dimensions and orientations . please point me to resources for the same .",3907,,,2016-12-30T19:57:38.110,dataset containing images of varying dimensions and orientations,deep-learning deep-network datasets,1,2,1
393,2404,1,,2016-11-28T15:18:10.110,6,310,"a single neuron is capable of forming a decision boundary between linearly seperable data . is there any intuition as to how many , and in what configuration , would be necessary to correctly approximate a sinusoidal decision boundary ? thanks",3908,,,2017-01-30T16:08:39.257,how many nodes / hidden layers are required to solve a classification problem where the boundary is a sinusoidal function ?,neural-networks hidden-layers neurons artificial-neuron,1,3,
394,2405,1,,2016-11-29T06:10:54.993,4,3150,"i am using policy gradients in my reinforcement learning algorithm , and occasionally my environment provides a severe penalty when a wrong move is made . i 'm using a neural network with stochastic gradient decent to learn the policy . to do this , my loss is essentially the cross - entropy loss of the action distribution multiplied by the discounted rewards , where most often the rewards are positive . but how do i handle negative rewards ? since the loss will occasionally go negative , it will think these actions are very good , and will strengthen the weights in the direction of the penalties . is this correct , and if so , what can i do about it ? edit : in thinking about this a little more , sgd does n't necessarily directly weaken weights , it only strengthens weights in the direction of the gradient and as a side - effect , weights get diminished for other states outside the gradient , correct ? so i can simply set reward=0 when the reward is negative , and those states will be ignored in the gradient update . it still seems unproductive to not account for states that are really bad , and it 'd be nice to include them somehow . unless i 'm misunderstanding something fundamental here .",3920,3920,2016-11-29T06:19:06.290,2019-02-13T19:58:13.537,negative reward ( penalty ) in policy gradient reinforcement learning,reinforcement-learning,2,1,
395,2409,1,,2016-11-30T11:10:32.697,5,860,"cognitive psychology is one of the basic sciences of artificial intelligence ( ai ) . the founder of the psychology is wilhelm w.(1832 - 1920 ) , who engaged in empirical methods , and was interested in the thinking processes during his scientific work . according to his research , psychology had two main leading subjects : behaviourism . cognitivism . behaviourism : refused the theory of the mental processes , and insisted to study the resulted action or the stimulus strictly objective . the representatives of this theory have been decreasing with time . cognitive psychology : defines that the brain is an information processing device . therefore , this question is not a duplicate of this what - is - the - difference - between - artificial - intelligence - and - cognitive - science ? , however my question is;how can we connect artificial intelligence with cognitive psychology for instance ; human computing interaction : we may come in a contact with humana computer interaction every day , because this field includes the every day use of computer for example;tapping stack exchange app on smart - phone , the user interfaces and some other expert programs which may use cognitive psychology in order to manipulate or help people . but still such tasks have got a minimal relevant connection .",1581,-1,2017-04-13T12:53:10.013,2018-08-07T03:38:28.647,how can we connect artificial intelligence with cognitive psychology ?,philosophy history definitions ethics cognitive-science,2,4,1
396,2412,1,,2016-11-30T19:06:19.070,5,444,"i am using a ga to optimise an ann in matlab . this ann is pretty basic ( input , hidden , output ) but the input size is quite large ( 10,000 ) and the output size is 2 since i have to classes of images to be classified . the weights are in the form of 2 matrices ( 10,000*m ) and ( m * 2 ) . i am now trying to do the genetic cross over with mutation . since the weights are in a matrix , is there an efficient way to implement a random crossover ( with mutation ) without doing it in a point - wise fashion ?",3952,2444,2019-02-25T21:24:53.970,2019-02-25T21:24:53.970,is there an efficient way to implement a random crossover of individuals stored in a matrix ?,neural-networks machine-learning genetic-algorithms crossover mutation,1,0,3
397,2415,1,,2016-12-01T19:54:54.373,0,239,"lots of people are afraid of what strong ai could mean for the human race . some people wish for a sort of "" asimov law "" included in the ai code , but maybe we could go a bit more far with the udhr . so , why is the universal declaration of human rights not included as statement of the a.i . ? as response to comment , response or edition : the universal declaration of human rights is clear . homo sapiens sapiens ( aka "" mankind "" ) needs some way to make sure ai evolution does not result in our extinction or enslavement to potentially superior algorithmic intelligences .",3893,1671,2017-08-14T21:00:22.627,2017-08-14T21:10:03.710,why is the universal declaration of human rights not included as statement on the ai ?,ai-design ethics logic value-alignment,1,2,1
398,2417,1,2418,2016-12-01T22:28:39.747,1,69,"i mean this in the sense that go is unsolvable but alphago seems able to make choices that are consistently more optimal than a human player 's choices . it is my understanding that game theory turned out to have limited applications in real world scenarios because of the profound complexity of such scenarios and degree of hidden information . is it fair to say that there is now a method for dealing with this ? i fully understand that go is a game of complete information , which has a very specific meaning , but it occurs to me that the inability to generate a complete game tree ( computational intractability ) could be seen as form of incomplete information , even if it is not traditionally thought of in those terms . i should probably note that my perspective is one of a "" serious "" game designer , where complexity serves the same function as chance and hidden information , which is to say as a balancing factor that "" levels the playing field "" .",1671,2723,2016-12-04T01:09:46.480,2016-12-04T01:09:46.480,can programs like alphago be said to be means of dealing with computational intractability ?,machine-learning game-theory,1,0,
399,2419,1,2423,2016-12-02T07:43:37.083,1,1987,"i knew that reproduction and crossover are the same things , wikipedia obitco.com tutorialspoint but , the following is the exercise given by my teacher , exercise 1 genetic algorithm to solve pattern finding problem . your task is to design a simple genetic algorithm , with binary - coded chromosomes , in order to solve pattern finding problem in 16-bit strings . the objective function is given by the following formula : f(x ) = nos(""010 "" ) + 2nos(""0110 "" ) + 3nos(""01110 "" ) + 4nos(""011110 "" ) + 5nos(""0111110 "" ) + 6nos(""01111110 "" ) + 7nos(""011111110 "" ) + 6nos(""0111111110 "" ) + 5nos(""01111111110 "" ) + 4nos(""011111111110 "" ) + 3nos(""0111111111110 "" ) + 2nos(""01111111111110 "" ) + nos(""011111111111110 "" ) the algorithm should display each population on the screen in the form and should save the history of it ’s operation ( average fitness in each population ) in the text file . at the end it should also display the best solution found . you may use the following operators : reproduction . you can use either one of the following reproduction types : proportional , ranking , tournament . they are described more in detail below : ... ... ... ... ... ... ... ... crossing over . in order to perform this operation the individuals must be grouped in pairs ( randomly ) , and with certain probability pcross information from their chromosomes must be exchanged . there are many flavors of the crossing - over operator , but in our case ( short , 16-bit chromosome ) , simple , one - point crossover will be enough . it can be performed by selecting a random number k from the range & lt;1;15 > and cutting the chromosomes of both individuals on that position . each of the individuals copies bits belonging to the other to it ’s own chromosome . mutation this operator changes the value of each bit in the chromosome to the opposite one with a very small probability pm ( usually about 10 - 3 ) . if we denote chromosome as [ b1 , b2 , ... , b16 ] ; then after the mutation each bit can be described as : where k î { 1,2 , ... , 16 } flip(x ) – result of a bernoulli flip with a success probability x. here i see that by reproduction and crossover he means different things . what is the catch ?",,,,2019-01-27T04:03:03.090,difference between reproduction and crossover in genetic algorithm,genetic-algorithms,2,0,1
400,2422,1,,2016-12-03T02:24:06.267,4,145,"i have data of 30 students attendance for a particular subject class for a week . i have quantified the absence and presence with boolean logic 0 and 1 . also , the reason for absence are provided and i tried to generalise these reason into 3 categories say a , b and c. now i want to use these data to make future predictions for attendance but i am uncertain of what technique to use . can anyone please provide suggestions ?",2888,,,2016-12-03T12:43:43.803,what techniques can be used to predict future attendance of students for a particular subject lecture session ?,structured-data,2,2,
401,2427,1,2440,2016-12-03T15:52:23.620,4,569,"the turing test has been the classic test of artificial intelligence for a while now . the concept is deceptively simple - to trick a human into thinking it is another human on the other end of a conversation line , not a computer - but from what i 've read , it has turned out to be very difficult in practice . how close have we gotten to tricking a human in the turing test ? with things like chat bots , siri , and incredibly powerful computers , i 'm thinking we 're getting pretty close . if we 're pretty far , why are we so far ? what is the main problem ?",4011,145,2016-12-12T21:29:50.483,2016-12-12T21:29:50.483,how close have we come to passing the turing test ?,turing-test,3,0,2
402,2429,1,,2016-12-04T09:48:57.170,12,3669,"according to nasa scientist rick briggs , sanskrit is the best language for ai . i want to know how sanskrit is useful . what 's the problem with other languages ? are they really using sanskrit in ai programming or going to do so ? what part of an ai program requires such language ?",4027,3989,2016-12-09T15:20:44.623,2018-04-06T09:24:23.223,importance of sanskrit,ai-design nasa cyborg,2,2,6
403,2430,1,2431,2016-12-04T10:56:59.750,-1,146,"as you can see , there is no computer screen for the computer , thus the ai can not display an image of itself . how is it possible for it to see and talk to someone ?",3896,,,2016-12-06T03:28:31.480,how is it possible for an ai to interact with someone without a computer screen ?,strong-ai,2,2,
404,2436,1,,2016-12-07T04:47:00.043,3,103,"i am working on a project , wherein i take input from the user as free text and try to relate the text to what the user might mean . i have tried stanford nlp which tokenizes the text into tokens , but i am not able to categorize the input . for example , the user might be greeting someone or sharing some problem he is facing . in case he is sharing some problem i need to categorize the problem as well . can someone help me with from where should i start ?",4088,1807,2016-12-07T17:23:07.330,2017-01-12T12:27:07.703,can i categorise the user input which i get as free text ?,natural-language-processing,1,1,
405,2437,1,,2016-12-07T11:14:21.203,0,196,"is it possible to train an agent to take and pass a multiple - choice exam based on a digital version of a textbook for some area of study or curriculum ? what would be involved in implementing this and how long would it take , for someone familiar with deep learning ?",4085,8,2016-12-09T15:39:10.590,2016-12-12T07:42:48.150,is it possible to train deep learning agent to pass a multiple - choice exam ?,deep-learning,2,5,
406,2439,1,,2016-12-07T17:10:41.857,4,373,"i am creating a game application that will generate a new level based on the performance of the user in the previous level . the application is regarding language improvement , to be precise . suppose the user performed well in grammar - related questions and weak on vocabulary in a particular level . then the new level generated will be more focused on improving the vocabulary of the user . all the questions will be present in a database with tags related to sections or category that they belong to . what ai concepts can i use to develop an application mentioned above ?",4111,3005,2017-09-15T23:44:56.533,2017-09-15T23:44:56.533,which methods or algorithms to develop a learning application ?,machine-learning deep-learning algorithm ai-design path-planning,4,5,1
407,2441,1,,2016-12-07T17:59:58.367,6,362,"one of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to ai . this question is intended to see if a compromise can be found between conservative anthropocentrism and post - human fundamentalism : a response should take into account principles from both perspectives . should , and therefore will , ai be granted the same rights as humans or should such systems have different rights ( if any at all ) ? some background this question applies both to human - brain based ai ( from whole brain emulations to less exact replication ) and ai from scratch . murray shanahan , in his book the technological singularity , outlines a potential use of ai that could be considered immoral : ruthless parallelization : we could make identical parallel copies of ai to achieve tasks more effectively and even terminate less succesful copies . reconciling these two philosophies ( conservative anthropocentrism and post - human fundamentalism ) , should such use of ai be accepted or should certain limitations - i.e. rights - be created for ai ? this question is not related to would an ai with human intelligence have the same rights as a human under current legal frameworks ? for the following reasons : the other question specifies "" current legal frameworks "" this question is looking for a specific response relating to two fields of thought this question highlights specific cases to analyse and is therefore expects less of a general response and more of a precise analysis",3427,-1,2017-04-13T12:53:10.013,2016-12-30T03:23:08.290,should intelligent ai be granted the same rights as humans ?,strong-ai legal,4,0,4
408,2443,1,2445,2016-12-08T20:01:33.240,3,366,"there is no doubt as to the fact that ai would be replacing a lot of existing technologies , but is ai the ultimate technology which humankind can develop or is their something else which has the potential to replace artificial intelligence ?",,3427,2016-12-09T15:19:42.263,2017-12-23T10:30:40.787,what could possibly replace artificial intelligence ?,strong-ai prediction,3,1,1
409,2449,1,,2016-12-10T13:44:06.017,4,8325,"printing actionspace for pong - v0 gives ' discrete(6 ) ' as output , i.e.0,1,2,3,4,5 are actions defined in environment as per documentation , but game needs only two controls . why this discrepency ? further is that necessary to identify which number from 0 to 5 corresponds to which action in gym environment ?",4163,,,2017-07-23T20:45:52.550,what are different actions in action space of environment of ' pong - v0 ' game from openai gym ?,gaming reinforcement-learning,3,2,3
410,2451,1,,2016-12-11T00:41:30.270,1,730,"i have started to make a python ai , and thee beginning of its code looks something like this : print "" artemis starting . . . "" import random import math import os greet = [ ' hi ' , ' hello ' , ' hey ' , ' good morning ' , ' good day ' , ' good afternoon ' , ' good evening ' , ' greetings ' , ' greeting ' ] joke = [ ' tell me a joke ' , ' joke ' , ' funny ' , ' tell me something funny ' ] insult = [ ' you re a loser ' , ' you are a loser ' , ' you stink ' , ' idiot ' , ' jerk ' , ' fool ' , ' dummy ' , ' hooligan ' , ' you re dumb ' , ' you re stupid ' , ' you are dumb ' , ' you are stupid ' ] maker = [ ' who made you ' , ' who programmed you ' , ' please tell me who made you ' , ' please tell me who programmed you ' ] name = [ ' artemis ' , ' a.r.t.e.m.i.s . ' , ' hey artemis ' , ' hey a.r.t.e.m.i.s . ' , ' artie ' , ' hey artie ' , ' hello artemis ' , ' hello a.r.t.e.m.i.s . ' , ' hello artie ' ] myage = [ ' how old am i ' , ' what is my age ' , ' my age ' ] tip = [ ' give me a tip ' , ' tip ' , ' lesson ' , ' give me a life lesson ' , ' life lesson ' , ' do you have a life lesson to share ' ] language = [ ' what programming language was used to make you ' , ' what programming language do you use ' , ' programming language ' ] compliment = [ ' cool ' , ' awesome ' , ' i like you ' , ' excellent ' , ' you re cool ' , ' you re awesome ' , ' you are cool ' , ' you are awesome ' ] maths = [ ' lets do math ' , ' calculate ' , ' calculator','do math ' , ' math ' , ' please do math ' , ' do arithmetic ' , ' arithmetic ' , ' please do arithmetic ' ] game = [ ' game ' , ' lets play a game ' , ' lets have fun ' , ' want to play a game ' ] gender = [ ' what gender are you ' , ' are you a boy or a girl ' , ' are you male or female ' , ' boy or girl ' , ' male or female ' , ' gender ' , ' are you a boy or girl ' ] guesswhat = [ ' guess what ' , ' guess what artemis ' , ' guess what a.r.t.e.m.i.s . ' , ' guess what artie ' , ' you wo nt believe it ' , ' you will not believe it ' , ' you wo nt believe it artemis ' , ' you will not believe it artemis ' , ' you wo nt believe it a.r.t.e.m.i.s . ' , ' you will not believe it a.r.t.e.m.i.s . ' , ' you wo nt believe it artie ' , ' you will not believe it artie ' ] cls = [ ' clear screen ' , ' clearscreen ' , ' cls ' , ' blank ' ] lawsofrobotics = [ ' what are the laws of robotics ' , ' what are the three laws of robotics ' , ' laws of robotics ' , ' three laws of robotics ' ] itsname = [ ' what s your name ' , ' what is your name ' , ' who are you ' ] however , i would like to know if i could make it detect "" similar "" phrases instead of trying to come up with every possible phrase someone would type . how can i do this ?",4173,,,2017-02-09T17:27:24.707,how to efficiently interpret phrases in a python ai ?,algorithm,1,2,1
411,2452,1,2453,2016-12-11T03:25:30.900,2,205,"from wikipedia , citations omitted : in artificial intelligence , an expert system is a computer system that emulates the decision - making ability of a human expert . expert systems are designed to solve complex problems by reasoning about knowledge , represented mainly as if – then rules rather than through conventional procedural code . the first expert systems were created in the 1970s and then proliferated in the 1980s . expert systems were among the first truly successful forms of artificial intelligence ( ai ) software . an expert system is divided into two subsystems : the inference engine and the knowledge base . the knowledge base represents facts and rules . the inference engine applies the rules to the known facts to deduce new facts . inference engines can also include explanation and debugging abilities . crud webapps ( websites that allows users to create new entries in a database , read existing entries in a database , update entries within the database , and delete entries from a database ) are very common on the internet . it is a vast field , encompassing both small - scale blogs to large websites such as stackexchange . the biggest commonality with all these crud apps is that they have a knowledge base that users can easily add and edit . crud webapps , however , use the knowledge base in many , myriad and complex ways . as i am typing this question on stackoverflow , i see two lists of questions - questions that may already have your answer and similar questions . these questions are obviously inspired by the content that i am typing in ( title and question ) , and are pulling from previous questions that were posted on stackexchange . on the site itself , i can filter by questions based on tags , while finding new questions using stackexchange 's own full - text search engine . stackexchange is a large company , but even small blogs also provide content recommendations , filtration , and full - text searching . you can imagine even more examples of hard - coded logic within a crud webapp that can be used to automate the extraction of valuable information from a knowledge base . if we have a knowledge base that users can change , and we have an inference engine that is able to use the knowledge base to generate interesting results ... is that enough to classify a system as being an "" expert system "" ? or is there a fundamental difference between the expert systems and the crud webapps ? ( this question could be very useful since if crud webapps are acting like "" expert systems "" , then studying the best practices within "" expert systems "" can help improve user experience . )",181,,,2016-12-11T05:51:09.103,"are crud webapps today the modern version of the "" expert system "" ?",definitions expert-system,2,0,
412,2462,1,,2016-12-11T23:15:04.860,3,92,"i am having a go at creating a program that does math like a human . by inventing statements , assigning probabilities to statements ( to come back and think more deeply about later ) . but i 'm stuck at the first hurdle . if it is given the proposition ∃x∈ℕ : x==123 so , like a human it might test this proposition for a hundred or so numbers and then assign this proposition as "" unlikely to be true "" . in other words it has concluded that all natural numbers are not equal to 123 . clearly ludicrous ! on the other hand this statement it decides is probably false which is good : ∃x∈ℕ : x+3 ≠ 3+x any ideas how to get round this hurdle ? how does a human "" know "" for example that all natural numbers are different from the number 456 . what makes these two cases different ? i do n't want to give it too many axioms . i want it to find out things for itself .",4199,4199,2016-12-11T23:28:00.977,2017-01-31T23:21:16.493,"how to determine the probability of an "" existence "" question",fuzzy-logic reasoning logic,2,2,1
413,2472,1,,2016-12-14T05:17:44.203,7,246,i am a strong believer of marvin minsky 's idea about artificial general intelligence ( agi ) and one of his thoughts was that probabilistic models are dead ends in the field of agi . i would really like to know thoughts / ideas of people who believe otherwise . [ p.s . it should be treated more like an informational thread rather than a strict a2a question ],4244,,,2017-05-30T06:09:50.027,are probabilistic models dead ends in ai ?,agi probabilistic,2,2,2
414,2473,1,,2016-12-14T05:21:39.570,3,326,"there is an example related to perceptron learning , but i could n't get it , i do n't exactly know how to solve it . there is a snippet from lecture notes . what is the transformation between epochs ?",4245,,,2016-12-15T15:35:08.473,perceptron learning,machine-learning algorithm learning-algorithms,1,3,
415,2474,1,2502,2016-12-14T17:07:18.223,2,357,"one of the most compelling issues regarding ai would be in behavior and relationships . what are some of the methods to address this ? for example , friendship , or laughing at joke ? the concept of humor ?",3555,,,2018-03-11T08:03:01.180,how could human behavior and relationships be implemented ?,ai-design emotional-intelligence knowledge-representation,3,2,
416,2475,1,2506,2016-12-14T21:56:01.237,9,4145,"i wanted to started experimenting with neural network and as a toy problem i wished to train one to chat , i.e. implement a chatting bot like cleverbot . not that clever anyway . i looked around for some documentation and i found many tutorial on general tasks , but few on this specific topic . the one i found just exposed the results without giving insights on the implementation . the ones that did , did it pretty shallowy ( the tensorflow documentation page on seq2seq is lacking imho ) . now , i feel i may have understood the principle more or less but i 'm not sure and i am not even sure how to start . thus i will explain how i would tackle the problem and i 'd like a feedback on this solution , telling me where i 'm mistaken and possibly have any link to detailed explainations and practical knowledge on the process . the dataset i will use for the task is the dump of all my facebook and whatsapp chat history . i do n't know how big it will be but possibly still not large enough . the target language is not english , therefore i do n't know where to quickly gather meaningful conversation samples . i am going to generate a thought vector out of each sentence . still do n't know how actually ; i found a nice example for word2vec on deeplearning4j website , but none for sentences . i understood how word vectors are built and why , but i could not find an exhaustive explaination for sentence vectors . using thought vectors as input and output i am going to train the neural network . i do n't know how many layers it should have , and which ones have to be lstm layers . then there should be another neural network that is able to transform a thought vector into a sequence of character composing a sentence . i read that i should use padding to make up for different sentence lengths , but i miss how to encode characters ( are codepoints enough ? ) .",4259,,,2018-10-09T13:24:46.243,how to train a chatbot,neural-networks chat-bots recurrent-neural-networks lstm,2,0,6
417,2477,1,2478,2016-12-15T07:00:16.043,9,609,"we are doing research , spending hours figuring out how we can make real ai software ( intelligent agents ) to work better . we are also trying to implement some applications e.g. in business , health and education , using the ai technology . nonetheless , so far , most of us have ignored the "" dark "" side of artificial intelligence . for instance , an "" unethical "" person could buy thousands of cheap drones , arm them with guns , and send them out firing on the public . this would be an "" unethical "" application of ai . could there be ( in the future ) existential threats to humanity due to ai ?",1581,2444,2019-02-11T21:23:25.303,2019-02-11T21:23:25.303,could there be existential threats to humanity due to ai ?,human-like applications cyberterrorism,2,1,2
418,2481,1,2501,2016-12-16T00:53:20.137,3,148,"how to train a bot , given a series of games in which he did ( initially random ) actions , to improve its behavior based on previous experiences ? the bot has some actions : e.g. shoot , wait , move , etc . it 's a turn based "" game "" in which , for know , i 'm running the bots with some objectives ( e.g. kill some other bot ) and random actions . so every bot will have a score function that at the end of the game will say , from x to y ( 0 to 100 ? ) if they did well or not . so how to make the bots to learn of their previous experiences ? because this is not a fixed input as the neural networks take , this is kind of a list of games , each one in which the bot took several actions ( one by every "" turn "" ) . the ia functions that i know are used to predict future values .. i 'm not sure is the same . maybe i should have a function that gets the "" more similar previous games "" that the bot played and checked what were the actions he took , if the results were bad he should take another action , if the results were good then he should take the same action . but this seems kind of hardcoded . another option would be to train a neural network ( somehow fixing the problem of the fixed input ) based on previous game actions and to predict the future action 's results in score ( something that i guess it 's similar to how chess and go games work ) and choose the one that seems to have better outcome . i hope this is not too abstract . i do n't want to hardcode much stuff in the bots , i 'd like them to learn by their own starting from a blank page .",2352,,,2016-12-19T01:55:56.100,how to make a bot to learn from previous games,ai-design,2,1,
419,2482,1,2486,2016-12-16T01:01:33.070,3,132,"i remember reading or hearing a claim that at any point in time since the publication of the mnist dataset , it has never happened that a method not based on neural networks was the best given the state of science of that point in time . is this claim true ?",1670,,,2016-12-17T08:05:30.397,neural networks unmatched on mnist ?,neural-networks history,1,0,
420,2490,1,2507,2016-12-18T10:13:55.117,4,189,"since my project is going to be of a purely fictional nature , i 'm not sure i picked the right forum for this . if not , i apologize and will gladly take this to where you point me to . the premise : a full - fledged self - aware artificial intelligence may have come to exist in a distributed environment like the internet . the possible a.i . in question may be quite unwilling to reveal itself . the question : given a first initial suspicion , how would one go about to try and detect its presence ? are there any scientifically viable ways to probe for the presence of such an entity ? in other words : how would the turing police find out whether or not there 's anything out there worth policing ?",4327,145,2016-12-18T11:40:55.680,2016-12-19T17:32:59.027,how to detect an emerging a.i,human-like,2,0,
421,2492,1,,2016-12-18T15:03:34.943,-2,82,"i am currently working on a virtual reality project that aims at creating a vr based simulation environment for educational purposes . i am aiming to make it artificially intelligent as well so it that may provide better simulation environment experience for students for better understanding . how can i achieve this ? furthermore , are there any systems available which may help me in achieving this ?",4330,145,2016-12-19T03:15:48.260,2018-01-24T15:33:41.740,how can a virtual reality system be made artificially intelligent ?,untagged,1,1,1
422,2498,1,2525,2016-12-18T18:58:50.807,3,210,""" conservative anthropocentrism "" : ai are to be judged only in relation to how to they resemble humanity in terms of behavior and ideas , and they gain moral worth based on their resemblance to humanity ( the "" turing test "" is a good example of this - one could use the "" turing test "" to decide whether ai is deserving of personhood , as james grimmelmann advocates in the paper copyright for literate robots ) . "" post - human fundamentalism "" : ai will be fundamentally different from humanity and thus we require different ways of judging their moral worth ( people for the ethical treatment of reinforcement learners is an example of an organization that supports this type of approach , as they believe that reinforcement learners may have a non - zero moral standing ) . i am not interested per se in which ideology is correct . instead , i 'm curious as to what ai researchers "" believe "" is correct ( since their belief could impact how they conduct research and how they convey their insights to laymen ) . i also acknowledge that their ideological beliefs may change with the passing of time ( from conservative anthropocentrism to post - human fundamentalism ... or vice - versa ) . still .. what ideology do ai researchers tend to support , as of december 2016 ?",181,181,2016-12-18T19:12:23.327,2016-12-31T04:23:57.653,"are ai researchers more likely to follow "" conservative anthropocentrism "" or "" post - human fundamentalism "" in deciding whether ai has moral standing ?",ethics,3,2,
423,2508,1,,2016-12-19T19:04:32.557,2,338,"a professor and i have been learning about artificial neural networks . we have a pretty good idea of the basics- backpropagation , convolutional networks , and all that jazz . we finished one book and are looking for a new one.i'd prefer something that either puts a new spin on the basics or is more advanced.we are both mathematicians and focus on the math more than the programming of it . one of our thoughts was to look into recurrent neural networks.does anyone know of good resources to continue learning about these topics ? or any other ideas besides recurrent neural networks ?",4353,1581,2016-12-22T14:14:12.717,2016-12-22T14:14:12.717,good books to read on artificial / recurrent neural networks ?,neural-networks convolutional-neural-networks recurrent-neural-networks,1,5,
424,2512,1,,2016-12-20T04:41:33.823,1,262,"it is really all in the title . for those less familiar , the fermi paradox broadly speaking asks the question "" where is everybody "" . there 's an equation with a lot of difficult to estimate parameters , which broadly speaking come down to this ( simplification of the drake equation ): ( lots stars in the universe ) * ( non - zero probability of habitable planets around each star ) * ( lots of time spanned ) = it seems there really should be somebody out there . there are , of course , plenty of hypotheses as to why we have n't seen / observed / detected any sign of intelligent life so far , ranging from "" well we 're unique deal with it "" to "" such life is so advanced and destroys everything it comes across , so it 's a good thing it did n't happen "" . the technological singularity ( also called asi , artificial super intelligence ) is basically the point where an ai is able to self - improve . some think that if such ai sees the light of day , it may self - improve and not be bound by biological constraints of the brain , therefore achieve a level of intelligence we can not even grasp ( let alone achieve ourselves ) . i certainly have my thoughts on the matter , but interested to see if there is already an hypothesis revolving around the link between the 2 out there ( i never came across but could be ) . or perhaps an hypothesis as to why this can not be . for references to those not familiar with the fermi paradox",4363,169,2016-12-22T01:08:44.660,2018-08-06T06:25:59.823,is the advent of a technological singularity a solution to the fermi paradox ?,singularity,2,2,1
425,2514,1,,2016-12-20T15:44:15.303,8,390,"i understood that searching is important in ai . there 's a question on this website regarding this topic , but one could also intuitively understand why . i 've had an introductory course on ai , which lasted half of a semester , so of course there was n't time enough to cover all topics of ai , but i was expecting to learn some theory behind ai ( i 've heard about "" agents "" ) , but what i actually learned was basically a few searching algorithms , like : bfs uniform - cost search dfs iterative - deepening search bidirectional search these searching algorithms are usually categorised as "" blind "" ( or "" uninformed "" ) , because they do not consider any information regarding the remaining path to the goal . or algorithms like : heuristic search best - first search a a * ida * which usually fall under the category of "" informed "" search algorithms , because they use some information ( i.e. "" heuristics "" or "" estimates "" ) about the remaining path to the goal . then we also learned "" advanced "" searching algorithms ( specifically applied to tsp problem ) . these algorithms are either constructive ( e.g. , nn ) , local search ( e.g. , 2-opt ) algorithms or meta - heuristic ones ( e.g. , acs , sa , etc ) . we also studied briefly a min - max algorithm applied to games and an "" improved "" version of the min - max , i.e. the alpha - beta pruning . after this course i did n't remain with the feeling that ai is more than searching , either "" stupidily "" or "" more intelligently "" . my questions are : why would one professor only teach searching algorithms in ai course ? what are the advantages / disadvantages ? the next question is very related to this . what 's more than "" searching "" in ai that could be taught in an introductory course ? this question may lead to subjective answers , but i 'm actually asking in the context of a person trying to understand what ai really is and what topics does it really cover . apparently and unfortunately , after reading around , it seems that this would still be subjective . are there theories behind ai that could be taught in this kind of course ?",2444,-1,2017-04-13T12:53:10.013,2017-01-25T07:32:38.503,why teaching only searching algorithms in a short introductory ai course ?,philosophy search,3,2,3
426,2515,1,,2016-12-20T21:59:24.357,2,66,"ethics is defined is defined to be the set of moral principles that governs a person 's or group 's behavior ; innately , should n't any system devise ethics for itself ? most of our articulations , in either direct or indirect manner , talk about intelligent systems in context of human presence . why , in first place , are we studying agi ethics ? given roseau 's reasoning for a society , human beings can improve only when they leave the state of nature and enter a civil society . does n't that automatically apply to any intelligent system ( also ) ? if it does , then ca n't we imply that any inorganic intelligent system should , perhaps , come as an offset of a network of intelligent computers ( a society per se ) ? who are we to constitute ethics and rules , morale for another society which , perhaps , could be much more intelligent than us ? in fact , humans have proven themselves to be idiotic time and again for a task such as this .",4244,4244,2018-01-19T09:19:53.527,2018-01-19T09:19:53.527,"are we supposed to "" decide "" ethics for intelligent systems ?",deep-network ai-design strong-ai ethics intelligent-agent,1,0,1
427,2516,1,2522,2016-12-21T02:35:03.877,4,373,"to solve problems using computer programs , we have developed a wide set of tools / control flow statements such as for loop , if - else , switch - break statements and so on . how natural are these control flow statements ? since , we do not know , how exactly would agi work , and the understanding of neural networks , so far , does not tell us about origin of "" intelligence "" , could a modern ai evolve itself into a system of such control - flow elements ? ( an appropriate architecture for computation is , anyways , necessary for any inorganic intelligence system and it is of no doubt that control - flow statements as such just makes the computation much more organized ) if so , could an ai be killed in an infinite loop created by itself ? p.s . the question is n't baseless , it really questions the kind of computational infrastructure a modern agi would require , and would it be able to alter itself without any intervention .",4244,,,2016-12-21T12:04:32.430,can an ai be killed in an infinite loop ?,ai-design agi intelligent-agent control-problem,1,3,
428,2517,1,,2016-12-21T02:50:57.240,1,73,"i am coding a tic - tac - toe program that demonstrates reinforcement learning . the program uses minimax trees to decide its moves . whenever it wins , all the nodes on the tree that were involved in the game have their value increased . whenever it loses , all the nodes on the tree that were involved in the game have their value decreased , etc . what is the name of the value that each node is decreased by ?",4378,,,2017-02-14T12:39:59.723,what s the name of the value that you add or subtract from a minimax tree node ?,reinforcement-learning minimax,2,1,
429,2518,1,,2016-12-21T09:08:07.530,1,1465,"neural net can be feed - forward or recurrent . perceptron is only feed forward . so , what is hopfield network then ?",,,,2016-12-23T17:19:28.933,what is the difference between a hopfield network and a neural network or a perceptron ?,neural-networks,2,0,
430,2520,1,,2016-12-21T09:53:06.757,5,266,is a levenberg – marquardt algorithm a type of back - propagation algorithm or is it a different category of algorithm ? wikipedia says that it is a curve fitting algorithm . how is a curve fitting algorithm relevant to a neural net ?,,3427,2016-12-24T16:56:13.190,2016-12-24T16:56:13.190,what kind of algorithm is the levenberg – marquardt algorithm ?,algorithm backpropagation,1,0,
431,2524,1,,2016-12-21T20:47:37.700,11,1040,"i am new to neural - network and i am trying to understand mathematically what makes neural networks so good at classification problems . by taking the example of a small neural network ( for example , one with 2 inputs , 2 nodes in a hidden layer and 2 nodes for the output ) , all you have is a complex function at the output which is mostly sigmoid over linear combination of sigmoid . so , how does that make them good at prediction ? does the final function lead to some sort of curve fitting ?",4394,2444,2016-12-30T02:23:25.370,2018-07-17T14:39:38.713,what makes neural networks so good at predictions ?,neural-networks,4,1,2
432,2526,1,,2016-12-21T23:26:55.103,7,1547,why should an activation function of a neural network be differentiable ? is it strictly necessary or is it just advantageous ?,,2444,2019-02-14T03:02:51.763,2019-02-14T03:02:51.763,why do activation functions need to be differentiable in the context of neural networks ?,neural-networks math activation-function,5,4,3
433,2528,1,,2016-12-22T00:45:53.537,1,79,"i understand an mdp ( markov decision process ) model is a tuple of where : $ s$ is a discrete set of states $ a$ is a discrete set of actions $ p$ is the transition matrix ie . $ p(s ' \mid s , a ) \rightarrow [ 0,1]$ $ r$ is the reward function i d . $ r(s , a , s ' ) \rightarrow \mathbb{r}$ for a non - trivial mdp , say $ 1000 $ states and $ 10 $ actions , the transition matrix has theoretically $ s \times a \times s = 10,000,000 $ entries ( though many entries will be $ 0 $ ) . i understand that one way of generating the $ p$ matrix is to estimate it via monte carlo sampling , by simulating the environment . however , with non - trivial state space and simulation costs , this could be prohibitively expensive . in practice , when a non - trivial mdp is being formulated , what are the different ways an accurate $ p$ matrix can be produced ?",4402,2444,2018-11-24T04:32:19.103,2018-11-24T04:32:19.103,how do you generate the transition probabilities of a non - trivial mdp ?,reinforcement-learning monte-carlo-tree-search,1,0,
434,2530,1,2537,2016-12-22T01:35:50.973,4,219,"generating a discretized state space for an mdp ( markov decision process ) model seems to suffer from the curse of dimensionality . supposed my state has a few simple features : feeling : happy / neutral / sad feeling : hungry / neither / full food left : lots / some / low / empty time of day : morning / afternoon / evening / night located : home / work / library / shops money in wallet : \$0 , \$0-\$50 , \$51-\$200 , \$200-\$1000 , \$1000 + this is a relatively compact set of information however since the number of states multiplies out , the corresponding mdp state space is 3 x 3 x 4 x 4 x 4 x 5 = 2880 . for a non - trivial problem with a greater number of choices per factor , the state space quickly becomes unmanageably large . it seems to me that the usefulness of mdps with large complex problems would be very limited . is that the case , or are there ways of keeping the state space manageable for more complex problems ? in general , what is a manageable number of states for an mdp to have , considering the need to generate the transition matrix and reward matrix ?",4402,32,2018-08-10T15:13:36.123,2018-08-10T17:38:51.280,what techniques are used to make mdp discrete state space manageable ?,machine-learning,1,0,2
435,2531,1,2534,2016-12-22T04:46:36.093,0,969,"http://www.cs.bham.ac.uk/~jxb/inc/l5.pdf the neuropsychologist donald hebb postulated in 1949 how biological neurons learn : “ when an axon of cell a is near enough to excite a cell b and repeatedly or persistently takes part in firing it , some growth process or metabolic change takes place on one or both cells such that a ’s efficiency as one of the cells firing b , is increased . ” in more familiar terminology , that can be stated as the hebbian learning rule : if two neurons on either side of a synapse ( connection ) are activated simultaneously ( i.e. synchronously ) , then the strength of that synapse is selectively increased . mathematically , we can describe hebbian learning as : here , η is a learning rate coefficient , and x are the outputs of the ith and jth elements . now , my question is , what do all these descriptions mean ? is hebbian learning applicable for single - neuron networks ? what does it mean by "" two neurons on either side of a synapse "" ? why / when would two neurons activate simultaneously ? what does they mean by elements ?",,,,2016-12-22T07:09:33.973,how do you explain hebbian learning in an intuitive way ?,neural-networks,1,1,
436,2535,1,2543,2016-12-23T16:20:30.830,6,79,"just started reading a book about ai . there is a very basic exercise but i ca n't figure it out , so here we go . the book is simply logical : intelligent reasoning by example the exercise is in the page 19 . two stations are ‘ not too far ’ if they are on the same or a different line , with at most one station in between . define rules for the predicate not_too_far . the only rules i 've seen are nearby and connected and do n't know how to use this . what i 've done so far is this : not_too_far(x , y ) : - nearby(x , y )",4421,,,2016-12-25T15:05:09.360,define rules for the predicate,prolog,1,2,
437,2542,1,2565,2016-12-25T08:08:08.443,0,211,what are the basic layers on an artificially intelligent program and what skills and concept are required to work on this field . total newbie interested in ai .,4435,8,2016-12-28T17:15:50.517,2016-12-30T20:39:02.107,concept of ai program,ai-design,2,0,1
438,2545,1,2546,2016-12-26T04:24:08.937,2,199,"the following text is from hal daumé iii 's "" a course in machine learning "" online text book ( page-41 ) . i understand that $ d$ is the size of the input vector $ x$ . what is $ y$ ? why is it introduced in the algorithm ? how / where / when is the initial value of $ y$ given ? what is the rationale of testing $ ya \leq 0 $ for updating weights ?",,2444,2019-04-19T14:13:42.580,2019-04-19T14:13:42.580,"understanding the perceptron algorithm in the book "" a course in machine learning """,neural-networks machine-learning perceptron,1,0,
439,2547,1,,2016-12-26T10:59:27.333,4,237,"recently mark got some attention from the media by stating that he had created jarvis . not that i 'm against him or anything , but this jarvis seems to have been done a hundred times before . he 's done something which most developers would classify as a home automation system . to me it 's more like he did it for the attention . i was kind of taken back by the amount of media attention he got . if you 've heard of jeremy blum , maybe you may understand what i 'm trying to imply here . i 'm just curious as to why he got so much attention . is there anything technically novel about his system that sets it so much apart from previous ones ?",4444,75,2016-12-26T20:14:54.763,2016-12-27T08:47:12.330,is there anything novel about zuckerberg 's jarvis ?,ai-design intelligent-agent,1,0,1
440,2548,1,2593,2016-12-26T11:24:09.163,6,1268,"what is the basic difference between a perceptron and a naive bayes classifier ? perceptron | naive bayes ------------------------------------------------------------ ( 1 ) perceptron uses neural- |(1 ) naive bayes uses probabi- network for learning and | listic theory for learning classification . | and classification ------------------------------------------------------------ ( 2 ) perceptron reads one sa- |(2 ) naive bayes needs to read- mple at a time to update | the entire training data its knowledge about the | before updating its knowl- training data . this is | edge about the training called online learning . | data . ------------------------------------------------------------- ( 3 ) in case of perceptrons , |(3 ) training and test data are training - data also serve | different . the purpose of test data | what more differences do they have ?",,,2016-12-26T12:32:26.717,2017-04-01T19:14:46.777,perceptron vs naive bayes,neural-networks,1,2,2
441,2555,1,2556,2016-12-27T14:47:59.430,3,215,with tools like open ai will we be able to teach an ai to build its own decks ? build a deck from a limited pool ? or draft ? evaluate the power level of a card ?,4466,,,2016-12-28T00:21:53.347,how close are we to having an ai that can play magic : the gathering objectively well ?,gaming training,1,3,
442,2558,1,,2016-12-28T14:07:12.900,0,49,"given that one website has a particular style and another has another style , could a style transfer be done such that the style of one website was transferred to the other website ? or in a more simple case , consider just part of a website , a box .",4488,,,2016-12-28T20:31:14.630,could style transfer be used to transfer the style of a website from one to another ?,ai-design,1,0,
443,2562,1,,2016-12-29T23:51:07.190,-2,189,is there a proof that states the possibility or impossibility of an ai system to acquire more sophisticated capabilities ( in terms of generic cleverness ) than its own creator ?,4519,2444,2019-05-01T17:30:19.783,2019-05-01T17:30:19.783,is there a proof that states that an ai can become smarter than its creator ?,philosophy strong-ai agi superintelligence proofs,3,0,
444,2563,1,,2016-12-30T01:42:39.703,2,70,"i am attempting to create a fully decoupled feed - forward neural network by using decoupled neural interfaces as explained in the paper ( https://arxiv.org/abs/1608.05343 ) . as in the paper , the dni is able to produce a synthetic error gradient that reflects the error with respect the the output : i can then use this to update the current layer 's parameters by multiplying by the parameters to get the loss with respect to the parameters : in the paper , the layer 's model is then updated based on the next layer sending the true error backwards . my question is , given that i am able to calculate the error with respect to the current output , how do i use this to calculate the loss with respect to the previous layer 's output ?",4521,,,2019-05-01T07:02:45.527,backpropagation in decoupled neural interfaces,neural-networks backpropagation artificial-neuron,1,0,
445,2564,1,,2016-12-30T02:24:39.467,6,495,"fundamentally , a game - playing ai must solve the problem of choosing the best action from a set of possible actions . most existing game ai 's , such as alphago , do this by using an evaluation function , which maps game states to real numbers . the real number typically can be interpreted as a monotonic function of a winning probability estimate . the best action is the one whose resultant state yields the highest evaluation . clearly , this approach can work well . but it violates one of vladimir vapnik 's imperatives : "" when solving a problem of interest , do not solve a more general problem as an intermediate step . "" in fact , he specifically states as an illustration of this imperative , do not estimate predictive values if your goal is to act well . ( a good strategy of action does not necessarily rely on good predictive ability . ) indeed , human chess and go experts appear to heed his advice , as they are able to act well without using evaluation functions . my question is this : has there has been any recent research aiming to solve games by learning to compare decisions directly , without an intermediate evaluation function ? to use alphago as an example , this might mean training a neural network to take two ( similar ) board states as input and output a choice of which one is better ( a classification problem ) , as opposed to a neural network that takes one board state as input and outputs a winning probability ( a regression problem ) .",4522,,,2017-01-01T17:10:25.750,game ai without evaluation function,gaming,1,0,1
446,2577,1,2587,2016-12-30T15:09:53.240,1,87,"suppose , i have been given the following diagram to design a simple neural network . how can i compute the neuron weights , design nn , and plot class boundaries ? or , is it really possible to do the above , only from the diagram ?",,1671,2018-03-22T20:35:26.387,2018-03-22T20:35:26.387,is it possible to design a neural network from feature plots ?,neural-networks unassisted-learning,1,4,1
447,2578,1,2579,2016-12-30T17:19:59.427,1,77,"the original lovelace test , published in 2001 , is used generally as a thought experiment to prove that ai can not be creative ( or , more specifically , that it can not originate a creative artifact ) . from the paper : artificial agent a , designed by h , passes lt if and only if a outputs o , a outputting o is not the result of a fluke hardware error , but rather the result of processes a can repeat h ( or someone who knows what h knows , and has h 's resources ) can not explain how a produced o . the authors of the original lovelace test then argues that it is impossible to imagine a human developing a machine to create an artifact ... while also not knowing how that machine worked . for example , an ai that uses machine learning to make a creative artifact o is obviously being ' trained ' on a dataset and is using some sort of algorithm to be able to make predictions on this dataset . therefore , the human can explain how the ai produced o , and therefore the ai is not creative . the lovelace test seems like an effective thought experiment , even though it appears to be utterly useless as an actual test ( which is why the the lovelace test 2.0 was invented ) . however , since it does seem like an effective thought experiment , there must be some arguments against it . i am curious to see any flaws in the lovelace test that could undermine its premise .",181,-1,2017-04-13T12:53:10.013,2016-12-30T18:11:00.777,are there any refutation of the original lovelace test ?,intelligence-testing,1,0,
448,2580,1,2585,2016-12-30T18:02:31.660,2,1092,"so i am looking to make an ai like jarvis . a perfect real life example of this type of system is the simple ai that mark zuckerberg has recently built . here is a description on how his ai works . from what i understand , the ai understands keywords , context , synonyms and then from there decides what to do . i have many questions on how this system works . firstly , what necessary steps are required to gather the meaning of a input ? secondly , how does the system , once it extract all of the necessary information on the input , determine what action it needs to take and what to say back to the user ? lastly , it also states that the system can learn habits and preferences of the user , how can a system do this ? here is also a video of the ai in action .",4532,,,2016-12-31T07:49:40.360,making a simple ai like jarvis,neural-networks machine-learning deep-learning convolutional-neural-networks natural-language-processing,1,1,1
449,2583,1,2586,2016-12-31T05:00:39.783,0,439,"i 'm just diving in this whole new area of knowledge ; i happened to lost in all the concepts a bit . what is difference between stacked rbm and deep belief network ? are they the same entity ? if so , why ? is the latter a some specific type of the former ? if so , how to tell if stacked rbm is a dbn ? sorry for asking such a noob question , but today it is quite difficult to find a consistent information on the internet , different sources give different explanations .",4531,,,2016-12-31T08:08:05.530,terminology : dbn vs stacked rbm,deep-network terminology boltzmann-machine,1,0,
450,2588,1,,2016-12-31T13:39:37.960,0,225,"i build this nn in c++ . i reviewed it since 3 days . i checked every line 100 times , but i ca nt find my error . if someone can please help me find the bugs : 1 . the output is garbage 2 . the weights go from 2e^79 down to -1.8e^80 after approximatly 400 iterations . mat flip(mat m ) { mat out(m.n_cols , m.n_rows ) ; for ( int i = 0 ; i & lt ; m.n_rows ; + + i ) for ( int j = 0 ; j & lt ; m.n_cols ; + + j ) out(j , i ) = m(i , j ) ; return out ; } layer::layer(int nodes ) : rand_engine(time(0 ) ) { y = mat ( nodes , 1 ) ; net = mat(nodes , 1 ) ; e = mat(nodes , 1 ) ; } layer::layer(int nodes , int next_nodes ) : layer(nodes ) { this-&gt;next_l = next_l ; auto random = bind(uniform_real_distribution&lt;double&gt;{-1 , 1 } , rand_engine ) ; w = mat(next_nodes , nodes ) ; for ( int i = 0 ; i & lt ; w.n_rows ; + + i ) { for ( int j = 0 ; j & lt ; w.n_cols ; + + j ) { w(i , j ) = random ( ) ; } } } layer::layer(int nodes , layer * next_l ) : layer(nodes , next_l-&gt;y.n_rows ) { this-&gt;next_l = next_l ; } void layer::feed_forward ( ) { next_l-&gt;net = w*y ; for ( int i = 0 ; i & lt ; next_l-&gt;y.n_rows;++i ) next_l-&gt;y[i ] = sig(next_l-&gt;net[i ] ) ; } void layer::backprop ( ) { for ( double d : w ) cout & lt;&lt ; d & lt;&lt ; "" \t "" ; e = flip(w)*next_l-&gt;e ; for ( int i = 0 ; i & lt ; e.n_rows ; + + i ) { e[i ] * = net[i ] * ( 1 - net[i ] ) ; cout & lt;&lt ; e[i ] & lt;&lt ; ' \t ' ; } w + = l_rate*(next_l-&gt;e*flip(y ) ) ; } void layer::backprop_last(mat t ) { for ( int i = 0 ; i & lt ; e.n_rows ; + + i ) { e[i ] = net[i ] * ( 1 - net[i])*(t[i ] - y[i ] ) ; cout & lt;&lt ; e[i ] & lt;&lt ; ' \t ' ; } } void layer::feed_forward(layer * next_l ) { this-&gt;next_l = next_l ; feed_forward ( ) ; } double layer::sig(double x ) { return 1 / ( 1 + exp(-x ) ) ; } network::network(vector&lt;int&gt ; top ) : top(top ) { network = new layer*[top.size ( ) ] ; network[top.size ( ) - 1 ] = new layer(top.back ( ) ) ; for ( int i = top.size()-2 ; i & gt ; -1 ; --i ) network[i ] = new layer(top[i ] , network[i + 1 ] ) ; } network::~network ( ) { delete [ ] network ; } void network::forward ( ) { for(int i = 0 ; i & lt ; top.front();++i ) network[0]-&gt;y[i ] = input[i ] ; for ( int i = 0 ; i & lt ; top.size ( ) - 1 ; + + i ) network[i]-&gt;feed_forward ( ) ; } void network::forward(vector&lt;double&gt ; input ) { set_input(input ) ; forward ( ) ; } void network::backprop ( ) { network[top.size ( ) - 1]-&gt;backprop_last(t_vals ) ; for ( int i = top.size ( ) - 2 ; i & gt ; -1 ; --i ) { network[i]-&gt;backprop ( ) ; } } void network::backprop(vector&lt;double&gt ; t_vals ) { set_t_vals(t_vals ) ; backprop ( ) ; } i know its a bunch of code but i m really desprate since i ca nt find what s wrong . i tested it with a simple xor . edit : heres my main code : # include "" network.h "" # include & lt;iomanip&gt ; using namespace std ; vector&lt;vector&lt;double&gt;&gt ; input = { { 0,0},{0,1},{1,1},{1,0 } } ; vector&lt;vector&lt;double&gt;&gt ; true_vals = { { 0},{1},{0},{1 } } ; int main ( ) { ifstream f(""out.txt "" , fstream::out ) ; f.clear ( ) ; cout & lt;&lt ; fixed ; cout & lt;&lt ; setprecision(5 ) ; network net({2,5,1 } ) ; vector&lt;double&gt ; in , t , out ; auto buf = cout.rdbuf ( ) ; for ( int i = 0 ; i & lt ; 1000 ; + + i ) { cout.rdbuf(f.rdbuf ( ) ) ; in = input[i % 4 ] ; net.forward(in ) ; out = net.get_output ( ) ; t = true_vals[i % 4 ] ; net.backprop(t ) ; cout & lt;&lt ; ' \n ' ; cout.rdbuf(buf ) ; if ( ( i % 101))continue ; cout & lt;&lt ; "" it : "" & lt;&lt ; i & lt;&lt ; ' \n ' ; cout & lt;&lt ; "" in:\t "" ; for ( double d : in ) cout & lt;&lt ; d & lt;&lt ; ' ' ; cout & lt;&lt ; ' \n ' ; cout & lt;&lt ; "" out:\t "" ; for ( double d : out ) cout & lt;&lt ; d & lt;&lt ; ' ' ; cout & lt;&lt ; ' \n ' ; cout & lt;&lt ; "" true:\t "" ; for ( double d : t ) cout & lt;&lt ; d & lt;&lt ; ' ' ; cout & lt;&lt ; ' \n ' ; double err = net.get_error ( ) ; cout & lt;&lt;""err:\t""&lt;&lt ; err & lt;&lt ; ' \n ' & lt;&lt ; ' \n ' ; } cout.rdbuf(null ) ; f.close ( ) ; return system(""pause "" ) ; }",4550,4550,2016-12-31T14:06:00.553,2016-12-31T15:05:49.610,why does nt my neural network work ?,neural-networks backpropagation,1,8,
451,2590,1,2596,2016-12-31T15:57:03.323,4,170,"i am going to design a neural net which will be able to break a 5 letter ( characters ) word into its corresponding syllables ( hybrid syllables , i mean it will not strictly adhere to grammatical syllable rules but will be based on some training sets i provide ) . example : train - > tra - in i think of implementing it in terms of some feedforward net as follows : input layer ->hidden layers - > output layer there will be 5 input nodes in the form of decimals ( 1/26 = 0.038 for ' a ' ; 2/26 = 0.076 for ' b ' ...... ) the output layer consists of 4 nodes which corresponds to each gap between two characters in the word . and fires as follows : for "" train "" ( tra - in ) : input ( 0.769,0.692,0.038,0.346,0.538 ) output(0,0,1,0 ) for "" boric "" ( bo - ri - c ) : * * input .... output ( 0,1,0,1 ) is it at all possible to implement the neural nets in the way i am doing ? ? and if possible , then how will i decide the number of hidden layers and nodes in each layer ? ? ( in the book i am reading , xor gate problem and its implementation using hidden layer is given . in xor we could decide the number of nodes and hidden layers required by seeing the linear separability of xor using two lines . but here i think such analysis ca n't be made . so how do i proceed ? ? or is it a trial and error process ? )",4424,,,2017-01-02T13:31:52.200,how to decide linear separability in my neural net work ?,neural-networks,1,0,
452,2594,1,,2017-01-01T20:02:05.957,2,99,"i am currently trying to understand and implement a conversational agent , seeing in the network there are many apis to do something similar , but what they generate are "" intelligent "" bots , not intelligent conversational agents ( wit.ai , recast.ai , api.ai , etc . ) , however i have seen watson virtual agent which paints very well and seems to cover my needs . however i am a developer and i would like to ask those with more experience , which would be the way to go to implement my objective , an agent similar to what the video of watson virtual agent , with thematic ones that i can train in the agent , and that he can learn from it . take a language course , but focused on the generation of programming languages , lexical analysis , syntactic , semantic , etc . , however i know that the natural language can not be compared to the language of the machines , reading some thesis vi to make a conversational agent could do a great grammar ( i can not imagine its syntactic tree ) , using probabilities with ngrams , or using neural networks or expert systems . as for the expert systems i understand that for these "" learn "" needs their knowledge base be modified , and as for the neural networks these fit , "" learn "" , so i think that it is best to use neural networks . summarizing which way should i go ? , i 'm currently taking stanford 's natural language processing course , and a deep learning course from google , i thought i 'd use natural language tool kit(ntlk ) for that important or natural part .",4563,4302,2018-10-08T12:34:00.657,2018-10-08T12:34:00.657,"conversational agent , query",machine-learning deep-learning chat-bots natural-language,0,2,
453,2598,1,2601,2017-01-02T16:15:19.750,5,279,"i 've spent the past couple of months learning about neural networks , and am thinking of projects that would be fun to work on to cement my understanding of this tech . one thing that came to mind last night is a system that takes an image of a movie poster and predicts the genre of the movie . i think i have a good understanding of what 'd be required to do this ( put together a dataset , augment it , download a convnet trained on imagenet , finetune it on my dataset , and go from there ) . i also thought that it would be pretty cool to run the system backwards at the end , so that i could put in e.g. a genre like ' horror ' and have the system generate a horror movie poster . i expect that it will be very bad at this because i 'm not a team of expert researchers , but i think i could have some fun hacking on it even if it only ever generated incomprehensible results . here 's what i 'm having trouble understanding : on the one hand , all the convnets whose architecture i 've seen described seem to rely on being given very small , square input images ( on the order of 220px by 220px iirc ) , and movie posters are rectangular , and a generated poster would have to be of a larger size in order for a human to make any sense of it . i 've seen several examples of papers where researchers use convnets to generate images , e.g. the adversarial system that generates pictures of birds and flowers , and a system that generates the next few frames of video when given a feed of a camera sweeping across the interior of a room , but all of those generated images seemed to be of the small square size i 've been describing . on the other hand , i 've seen lots of "" deep dream "" images over the past year or so that have been generated by convnets and are of a much larger size than ~220px by ~220px . here 's my question : is it possible for me to build the system i describe , which takes a movie genre and outputs a movie poster of a size like e.g. 400px by 600px ? [ i 'm not asking about whether or not the resulting poster would be any good - i 'm curious about whether or not it 's possible to use a convnet to generate an image of that size . ] if it is possible , how is it possible , given that these systems seem to expect small , square input images ?",4579,,,2019-03-01T11:42:58.187,feasibility of generating large images with a convnet,neural-networks machine-learning deep-learning convolutional-neural-networks,2,0,1
454,2599,1,2600,2017-01-02T19:39:16.703,4,283,how does stackgan processes such a realistic image just from collecting details in the text ? what kind of algorithm is used behind it ? anyone have any idea ? please share .,4463,4302,2018-10-08T12:52:22.737,2018-10-08T12:52:22.737,what kind of algorithm is used is stackgan to process realistic images,ai-design natural-language-processing algorithm evolutionary-algorithms text-summarization,2,0,1
455,2602,1,2609,2017-01-03T12:56:54.673,1,460,"i 'm working on a project which uses artificial neural network . i looked up at the matlab neural network toolbox . i got a generated script from it . when looking at this script , it is confusing because for both testing and training it seems that the toolbox just uses the same data . could you explain the reason ? the script is given below : net.divideparam.trainratio = 70/100 ; net.divideparam.valratio = 15/100 ; net.divideparam.testratio = 15/100 ; % train the network [ net , tr ] = train(net , inputs , targets ) ; % test the network outputs = net(inputs ) ; errors = gsubtract(targets , outputs ) ; performance = perform(net , targets , outputs ) % recalculate training , validation and test performance traintargets = targets . * tr.trainmask{1 } ; valtargets = targets . * tr.valmask{1 } ; testtargets = targets . * tr.testmask{1 } ; trainperformance = perform(net , traintargets , outputs ) ; valperformance = perform(net , valtargets , outputs ) ; testperformance = perform(net , testtargets , outputs ) ; also is it right to split the data set as below for training and testing ? traindata = inputdata(:,1:213 ) ; traintargetdata = targetdata(:,1:213 ) ; validationdata = inputdata(:,214:258 ) ; testdata = inputdata(:,259 : end ) ; testtargetdata = targetdata(:,259 : end ) ; validationtargetdata = targetdata(:,214:258 ) ; [ net , tr ] = train(net , traindata , traintargetdata ) ; % validation outputs = net(validationdata ) ; errors = gsubtract(validationtargetdata , outputs ) ; performance = perform(net , validationtargetdata , outputs ) ; % test the network outputs = net(testdata ) ; error = gsubtract(testtargetdata , outputs ) ; performance = perform(net , testtargetdata , outputs ) ;",4590,75,2017-01-07T17:38:56.437,2017-01-07T17:38:56.437,confusing matlab artificial neural toolbox script,neural-networks machine-learning,1,2,
456,2603,1,,2017-01-03T15:10:34.037,3,126,"let 's say i 've got a training sample set of 1 million records , which i pull batches of 100 from to train a basic regression model using gradient descent and mse as a loss function . assume test and cross validation samples have already been withheld from the training set , so we have 1 million entries to train with . consider following cases : run 2 epochs ( i 'm guessing this one is potentially bad as it 's basically 2 separate training sets ) in the first epoch train over records 1 - 500k in the second epoch train over the 500k-1 m run 4 epochs in the first and third epoch train over records 1 - 500k in the second and fourth epoch train over the 500k-1 m run x epochs , but each epoch has a random 250k samples from the training set to choose from should every epoch have the exact samples ? is there any benefit / negative to doing so ? my intuition is any deviation in samples changes the ' topography ' of the surface you 're descending , but i 'm not sure if the samples are from the same population if it matters . this relates to a so question : https://stackoverflow.com/questions/39001104/in-keras-if-samples-per-epoch-is-less-than-the-end-of-the-generator-when-it",4591,-1,2017-05-23T12:39:33.010,2017-02-05T16:19:51.590,"with gradient descent w / mse on a regression , must / should every epoch use the exact same training samples ?",statistical-ai linear-regression,1,0,
457,2604,1,,2017-01-03T22:03:42.403,1,409,"i want to make a connect 4 ai using machine learning but i 'm a complete beginner to the topic . from what i 've seen an ann is the way to go ; some phrases i 've heard are "" neuroevolution "" and the acronym "" neat . "" i 'm very confused . one particular question i have is how do you decide how many hidden neurons , synapses and hidden layers you have ?",4605,1671,2018-03-05T19:10:12.577,2018-03-05T21:20:15.287,neural network learning to play connect 4,neural-networks artificial-neuron neat,2,1,2
458,2612,1,2613,2017-01-06T11:02:47.767,5,102,"i want to build a classifier which takes an aerial image and outputs a bitmap . the bitmap is supposed to be 1 at every pixel where the aerial image has water . for this process i want to use a convnet but i am unsure about the output layer . i identified two approaches : have an output layer with exactly 2 nodes which specify wether or not the center pixel of the aerial image corresponds to water or not . have an output layer with one node for every pixel . so for a 64x64 image i would have 4096 nodes . what approach would be preferred and why ? another thing that is unclear to me is how to get the actual bitmap with only zeros and ones from the output of the convnet . assuming we used a approach 2 then for each pixel our convnet would give us a probability between 0 and 1 that the this pixel corresponds to water . how do i decide that this probability is high enough to set the value in my bitmap to 1 ? do i just define a threshold , say 0.5 , and if the value exceeds that threshold i set the pixel to 1 or is there a more sophisticated approach ?",4661,,,2017-01-06T13:08:57.387,use convnet to predict bitmap,neural-networks convolutional-neural-networks computer-vision,1,0,
459,2614,1,2615,2017-01-06T15:24:53.680,5,1105,"i 'm studying for my ai final exam , and i 'm stuck in the state space representation . i understand initial and goal states , but what i do n't understand is the state space and state transition function . can someone explain what are they with example ? for example , one of the question was this on my previous exam : given k knights on a infinite ( in all directions ) chessboard and k selected squares of the board . our task to move the knights to these selected squares obeying the following simple rules : all knights move parallel , following their movement rule ( l - shape jump ) no knights can move to a square on which a knight stood anytime before give the state space of the problem , the starting and goal states , and the state transition function !",4664,2444,2017-01-11T16:39:30.950,2017-01-11T16:39:30.950,what are the state space and the state transition function in ai ?,training terminology,1,0,
460,2618,1,,2017-01-07T04:13:06.473,2,470,any good example for bag - of - words ( bow ) model in image retrieving ? i want a simple example to understand the whole process of bow .,4684,,,2018-05-18T06:14:46.483,bag - of - words ( bow ) model in image detection,machine-learning classification,1,2,0
461,2619,1,,2017-01-07T12:39:02.010,4,492,"i read through the neat paper and i understand the algorithm now . but one thing is still unclear to me . when does the mutation occur and how does it take place ? how is it chosen whether to add a node or to add a connection mutation ? furthermore , how is it chosen where the mutation is taking place in the network ( between which connections ) ?",4550,1671,2018-03-05T19:07:47.047,2018-03-05T19:07:47.047,when do mutations in neat occur ?,neural-networks evolutionary-algorithms neat,1,0,1
462,2623,1,2630,2017-01-08T08:48:44.837,9,230,"i 've been struggling with the connection between knowledge based ai systems and bayesian inference for a while now . while i continue to sweep through the literature , i would be happy if someone can answer these questions directly - are bayesian inference based methods used in reasoning or q / a systems -- to arrive at conclusions about questions whose answers are not directly present in the knowledge base ? in other words , if a q / a system does n't find an answer in a knowledge base , can it use bayesian inference to use the available facts to suggest answers with varying likelihoods ? if yes , could you point me to some implementations ?",4707,,,2017-01-09T18:46:36.490,role of bayesian inference in reasoning systems,knowledge-representation reasoning,1,0,2
463,2626,1,,2017-01-09T06:46:37.640,1,330,"i have already know ai can paint , by using genetic algorithm , there are already lots of works such as this and this .in addition , i also know ai can compose : song from pi : a musically plausible network for pop music generation ( genetic algorithm too ) . but what i intresting is not painting those ambiguity / abstract paint . the not abstract painting flow i think is(just for example ) : at least trainning ai with superman 's comic give ai a very simple posture sketch of standing human ai paint it to superman . currently , i do n't know if there is any way / guide / thought / algorithm can teach ai to paint a superman like comic(not abstract ones).i'd like to research this area , but ca n't find where and how to start .",4728,4728,2017-01-09T07:05:49.570,2017-01-10T00:14:28.023,is there any way can teach ai creative painting ( not convert photo to paint ) ?,machine-learning research algorithm image-recognition,1,1,
464,2627,1,2629,2017-01-09T07:24:15.783,0,359,"i am new to artificial intelligence and speech recognition technology . for a long time i have had an idea to create a friendly ai voice assistant like jarvis using windows speech recognition technology . is this possible to build an ai voice assistant with windows speech recognition technology ? if the idea above is possible , i need to know another thing also : which language is best suitable for creating an ai ? any help or suggestions are welcome !",2933,,2017-01-10T03:51:28.983,2017-01-10T03:51:28.983,how to create an ai voice assistant using windows speech recognition ?,ai-design friendly-ai new-ai,1,2,
465,2632,1,2633,2017-01-10T08:38:16.777,8,430,"having worked with neural networks for about half a year , i have experienced first hand what are often claimed as their main disadvantages , i.e. overfitting and getting stuck in local minima . however , through hyperparameter optimization and some newly invented approaches , these have been overcome for my scenarios . from my own experiments : dropout seems to be a very good regularization method ( also a pseudo - ensembler ? ) , batch normalization eases training and keeps signal strength consistent across many layers . adadelta consistently reaches very good optimas i have experimented with scikit - learns implementation of svm alongside my experiments with neural networks , but i find the performance to be very poor in comparison , even after having done grid - searches for hyperparameters . i realize that there are countless other methods , and that svm 's can be considered a sub - class of nn 's , but still . so , to my question : with all the newer methods researched for neural networks , have they slowly - or will they - become "" superior "" to other methods ? neural networks have their disadvantages , as do others , but with all the new methods , have these disadvantages been mitigated to a state of insignificance ? i realize that oftentimes "" less is more "" in terms of model complexity , but that too can be architected for neural networks . the idea of "" no free lunch "" forbids us to assume that one approach always will reign superior . it 's just that my own experiments - along with countless papers on awesome performances from various nn 's - indicate that there might be , at the least , a very cheap lunch .",4747,,,2017-06-07T06:53:51.070,are the shortcomings of neural networks diminishing ?,neural-networks,2,0,2
466,2634,1,,2017-01-10T13:15:43.350,6,325,what exactly are the differences between semantic and lexical - semantic networks ?,4754,1581,2018-08-23T18:47:23.843,2018-08-24T15:39:44.127,what exactly are the differences between semantic and lexical - semantic networks ?,terminology definitions semantics,2,0,1
467,2637,1,6993,2017-01-11T09:44:04.700,5,538,"in monte carlo tree search : what does one do when the selection step selects a node that is a terminal state , i.e. a won / lost state ( it 's by definition a leaf node ) ? expansion / simulation is not in order , as it 's game over , but does the tree ( score / visits ) need to be updated ( backpropagation ) . wo n't this particular node be selected continuously ? i 'm confused about this , could someone please point me in the right direction .",4773,,,2018-08-08T19:26:33.413,mcts : terminal ( leaf ) nodes in selection step,monte-carlo-tree-search,1,1,
468,2638,1,2641,2017-01-11T13:02:37.290,3,81,"i was wondering if in any way it is possible to generate w questions based on gap - fill - in type questions ( e.g "" _ _ _ _ _ _ is a process in which plants generate energy . "" --- > "" what is the process in which plants generate energy called ? "" ) if so , how can i achieve this ? i am familiar with working with natural language processing and have no problem with implementing an algorithm for this but i do not know where to start with this . any help would be appreciated !",4034,4302,2018-10-08T12:33:37.737,2018-10-08T12:33:37.737,"how can i generate what , why , who types of questions from "" gap - fill - in "" type of questions ?",machine-learning natural-language-processing,1,0,1
469,2639,1,,2017-01-11T14:19:57.573,5,427,"i have read the neat paper and some questions are still bugging me : when do mutations occur ? between which nodes ? when mating what happens if 2 genes have the same connection but a different innovation number . as far as i know , mutations occur randomly and thus it is possible that 2 genomes have the same mutation .",4550,1671,2018-03-05T19:06:35.343,2018-03-05T19:06:35.343,questions to the neat algorithm,neural-networks deep-network evolutionary-algorithms neat,1,0,1
470,2642,1,2643,2017-01-12T15:31:01.383,3,275,"any sufficiently advanced algorithm is indistinguishable from ai.--- michael paulukonis according to what are the minimum requirements to call something ai ? , there are certain requirements that a program must meet to be called ai . however , according to that same question , the term ai has became a buzzword that tends to be associated with new technologies , and that certain algorithms may be classified in ai in one era and then dismissed as boring in another era once we understand how the technology works and be able to properly utilize it ( example : voice recognition ) . humans are able to build complex algorithms that can engage in behaviors that are not easy to predict ( due to emergent complexity ) . these "" sufficiently advanced "" algorithms could be mistaken for ai , partly because humans can also engage in behaviors that are not easy to predict . and since ai is a buzzword , humans may be tempted to engage in this self - delusion , in the hopes of taking advantage of the current ai hype . eventually , as humanity 's understanding of their own "" sufficiently advanced algorithms "" increase , the temptation to call their algorithms ai diminishes . but this temporary period of mislabeling can still cause damage ( in terms of resource misallocation and hype ) . what can be done to distinguish a sufficiently advanced algorithm from ai ? is it even possible to do so ? is a sufficiently advanced algorithm , by its very nature , ai ?",181,-1,2017-04-13T12:53:10.013,2018-11-26T03:37:44.473,"how can one distinguish between an ai and a "" sufficiently advanced algorithm "" ?",definitions,4,2,2
471,2644,1,2657,2017-01-13T01:15:49.743,5,338,"as i see some cases of machine - learning based artificial intelligence , i often see they make critical mistakes when they face inexperienced situations . in our case , when we encounter totally new problems , we acknowledge ourselves that we are not skilled enough to do the task and hand it to someone who is capable of doing the task . would ai be able to self - examine objectively and determine if it is capable of doing the task ? if so , how would it be accomplished ?",4802,,,2017-01-28T14:07:19.367,how would ai be able to self - examine ?,machine-learning deep-learning,5,0,2
472,2645,1,,2017-01-13T04:56:26.740,2,427,"a lot of people are claiming that we are an at an inflection point , and machine learning / artificial intelligence will take off . this is inspite of the fact that for a long machine learning has stagnated . what are the signals that indicate that machine learning is going to take off ? in general how do you know that we are at an inflection point for a certain technology ?",4807,,,2017-01-22T05:53:51.893,are we at an inflection point in ai ?,machine-learning strong-ai,3,0,1
473,2646,1,,2017-01-13T08:06:53.853,9,957,"abuse v. to use wrongly or improperly ; misuse : abuse alcohol ; abuse a privilege . v. to hurt or injure by maltreatment ; ill - use . i mean the second one if conscious ai is possible and is wide spread , would n't it be easy for someone who knows what they are doing to torture ai ? ( how ) could this be avoided ? this question deals with computer based ai , not robots , which are as conscious as people ( this is an assumption of the question ) . the question wonders how a crime as hard to trace as illegal downloads , but far worse ethically , could be prevented . note that despite most people being nice and empathising with the robots , there are always the bad people , and so relying on general conscience will not work .",4809,4809,2017-01-13T23:50:28.147,2017-01-31T00:59:16.970,how to stop people abusing ai ?,ethics,3,5,2
474,2649,1,,2017-01-13T15:01:50.487,2,171,"has anyone used yodaqa for natural language processing ? how easy is it to link to a document database other than wikipedia ? we 're thinking we can create a bot to use ai to analyze our developer and user documentation and provide a written or spoken answer in reply . yodaqa comes linked to wikipedia for starters , but we 'd need to link to our own source info . i 'm trying to get an idea of the development time required to set up the ai and then to link to the database .",4627,4302,2018-10-08T12:33:05.867,2019-01-06T13:02:20.603,using yodaqa with a non - wikipedia source,ai-design natural-language-processing,1,2,1
475,2655,1,2678,2017-01-14T15:18:02.407,3,708,"i 'm looking for good examples of successful ai projects and theories that had a relatively good impact on society , economics and military field . so many years have passed after the first ai researches ; hence i 'm wondering if it has really increased the quality of our lives .",4801,,2018-12-28T21:43:17.777,2018-12-28T21:43:17.777,which is the most successful ai project so far ?,social,3,6,2
476,2658,1,2660,2017-01-14T19:58:36.810,5,475,"if i am correct , the branching factor is the maximum number of successors of any node . when i am applying bidirectional search to a transition graph like this one below if 11 is the goal state and i start going backwards , is 10 considered as successor of 5 ? even if it do not leads me further to my start state 1 ?",4824,,,2017-01-16T03:59:54.960,"is the ' direction ' considered , when determining the branching factor in bidirectional search ?",search branching-factors,1,0,
477,2663,1,2667,2017-01-15T14:03:44.797,3,293,"while writing a paper yesterday this strange thing happened to me . i was wrtiting it in word , and was n't satisfied with the repeated usage of word "" relesase "" in last few senteces . so i 've decided to open up google and started to enter the search phrase "" synonyms for release "" . have n't even finished the word synonym , google autocompleted my search to "" synonyms for release "" . how could it knew that i wanted to look for that exact word ? was it just a coincidence , do google has access to some information that could somehow possibly give away what i intended to search ? what could have been the reason for it selecting "" release "" as it 's first autocomplete ?",4834,,,2017-01-15T20:51:48.947,how does a google choose it 's autocomplete solution,search,1,0,
478,2668,1,,2017-01-15T21:21:02.107,3,71,"i am trying to make a artificial intelligent agent that is kind of like jarvis from iron man however much less complex . one thing i want to have is i want my ai to be able to determine if i am talking to it or not . so i plan on having it always listen to my voice and convert that to text , however i am not sure how i can train the ai to recognize if it is being spoken to or not ? plz help .",4841,,,2017-01-16T12:53:50.453,ai that knows when its being spoken to,neural-networks machine-learning deep-learning deep-network intelligent-agent,2,1,1
479,2669,1,2671,2017-01-16T02:55:42.147,4,53,"in this case , the request is a thing which we asked ai to do , not necessarily using commands . nowadays , we have our personal ai in our devices : siri by apple , cortana by microsoft , and so on . for most times , when we ask them to do certain tasks , they do the tasks for us . however , their action is based on the list of commands . when they do n't clearly recognize the commands in our request , they suggest us to use certain commands . it is clear that there are limits to our choices(requests ) . so let 's suppose that we have an ai that can interpret requests . there may not be commands in our request . ai is fully able to do anything in order to do what it is asked for . basically , i am talking about an independent ai . scenario : ai is asked to clean the room . ai is allowed to throw away garbage , and move unnecessary(or unused ) stuff into the storage . this is the list of things that was in the room at the moment : a stained blanket various decorations a dead clock on the wall various unused items in the desk drawer a lost airpod under the bed a sleeping cat in the bed in this condition ... is washing stained blanket a part of cleaning ? how can ai tell if anything is in use ? are decorations in use ? would dead clock that only needs battery replacement considered garbage ? would items in the desk drawer be included in ai 's to - be - cleared list ? would ai be able to recognize the difference between unused and lost ? what would happen to the poor cat ? since there are many holes in the scenario and questions , i would like to know how the answers are derived .",4802,4802,2017-01-16T03:00:52.593,2017-01-16T10:33:15.847,lets suppose that we have an ai that can interpret requests,strong-ai natural-language problem-solving computational-linguistics,1,0,
480,2672,1,,2017-01-16T12:17:41.363,4,586,"most companies dealing with deep learning ( automotive - comma.ai , mobileye , various automakers etc . ) do collect large amounts of data to learn from and then use lots of computational power to train a neural network ( nn ) from such big data . i guess this model is mainly used because both the big data and the training algorithms should remain secret / proprietary . if i understand it correctly the problem with deep learning is that one needs to have : big data to learn from lots of hardware to train the neural network from this big data i am trying to think how crowdsourcing could be used in this scenario . is it possible to distribute the training of the nn to the crowd ? i mean not to collect the big data to a central place but instead to do the training from local data on the user 's hardware ( in a distributed way ) . the result if this would be lots of trained nns that would in the end be merged into one in a committee of machines ( com ) way . would such model be possible ? of course the above stated model does have a significant drawback - one does not have control over the data that is used for learning ( users could intentionally submit wrong / fake data that would lower the quality of the final com ) . this may be dealt with by sending random data samples to the central community server for review however . example : think of a powerful smartphone using its camera to capture a road from vehicle 's dashboard and using it for training lane detection . every user would do the training himself / herself ( possibly including any manual work like input image classification for supervised learning etc . ) . i wonder it he model proposed above may be viable . or is there a better model how to use crowdsourcing ( user community ) to deal with machine learning ?",113,,,2017-09-27T05:43:55.527,using crowdsourcing for deep learning,neural-networks machine-learning deep-learning deep-network,1,0,1
481,2675,1,,2017-01-16T18:41:36.830,13,13533,if this list 1 can be used to classify problems in ai ... decomposable to smaller or easier problems solution steps can be ignored or undone predictable problem universe good solutions are obvious uses internally consistent knowledge base requires lots of knowledge or uses knowledge to constrain solutions requires periodic interaction between human and computer ... is there a generally accepted relationship between placement of a problem along these dimensions and suitable algorithms / approaches to its solution ? references [ 1 ] https://images.slideplayer.com/23/6911262/slides/slide_4.jpg,4856,4302,2018-10-08T23:01:14.733,2019-01-30T04:03:30.403,how can these 7 ai problem characteristics help me decide on an approach to a problem ?,ai-design algorithm reference-request,2,0,5
482,2676,1,,2017-01-16T23:01:34.997,6,69,"in working with basic sequence - to - sequence models for machine translation i have been able to achieve decent results . but inevitably some translations are not optimal or just flat - out incorrect . i am wondering if there is some way of "" correcting "" the model when it makes mistakes while not compromising the desirable behavior on translations where it previously performed well . as an experiment , i took a model that i had previously trained and gathered several examples of translations where it performed poorly . i then took those examples and put them into their own small training set where i provided more desirable translations than what the model was outputting . i then trained the old model on this new small training set very briefly ( 3 - 6 training steps was all it took to "" learn "" the new material ) . when i tested the new model it translated those several examples in the exact way i had specified . but as i should have anticipated the model overcompensated to "" memorize "" those handful of new examples and thus i noticed it started to perform poorly on translations that it had previously been excellent . is there some way to avoid this behavior short of simply retraining the model from scratch on an updated data set ? i think i understand intuitively that the nature of neural networks would not lend itself to small precise corrections ( i.e. when the weighting of just a few neurons change the performance of the entire model will change ) but maybe there is a way around it , perhaps with some type of hybrid reinforcement learning approach . update : this paper speaks of approaches to incrementally improving neural machine translation models",4862,4862,2017-01-19T00:54:36.540,2018-10-12T18:00:53.433,correcting ' bad ' translations in a sequence - to - sequence neural machine translation model,machine-learning models,1,0,
483,2681,1,2690,2017-01-17T04:11:20.930,7,466,i am currently working on an android a.i . app . i am aware of the algorithm how to make random sentences in a.i . is there any way or algorithm to make those sentences sarcastic ?,4869,4869,2017-01-17T09:13:30.987,2017-01-19T13:00:59.110,ai algorithm for sarcasm,machine-learning algorithm,2,6,2
484,2689,1,,2017-01-18T17:38:31.650,6,210,in the neat paper it says : the entire population is then replaced by the offspring of the remaining organisms in each species . but how does it take place ? i mean like are they paired and then mated ? cause this would lead to fast extinction would n't it ? or are they pair each with each ? this would lead to overpopulation very fast . how are they paired ?,4550,1671,2018-03-05T19:07:05.133,2018-03-05T19:07:05.133,how does the mating in neat take place,neural-networks genetic-algorithms genetic-programming neat,1,0,1
485,2692,1,2696,2017-01-19T14:57:05.603,3,1875,"i 'm an artificial intelligence enthusiastic and i want to learn about it . i want to ask you what do you think about the udacity nanodegree deep learning nanodegree foundation . i do n't know if it is a good idea to pay for that course or maybe , there are better free resources . i want to understand what artificial intelligence is , and also learn about machine learning , deep learning , and convolutional networks . i 'm interested in image and speech recognition and also in artificial life . my apologies if this is not the right place to ask this question .",4920,,,2018-01-17T17:42:29.433,is it a good idea to pay for an deep learning course ?,deep-learning unassisted-learning,5,2,4
486,2693,1,2697,2017-01-19T16:37:45.173,2,39,"hardware comes in two forms , basically : immutable , such as ram , and mutable , such as fpga s. in animals , neurological connections gain in strength by changing the physical structure of the brain . this is analogous to fpgas whereby signal strength is increased by changing the pathways themselves . if we achieve sentience using mutable hardware ( e.g. , neuromemristive systems ) , will it be possible to make a copy of that "" brain "" and its active state ? for this question , assume that the brain is how the hardware has "" reconfigured "" [ or etched , if you will ] its pathways to strengthen them and the brain 's state is captured by how electrons are physically flowing throughout those pathways .",4922,,,2017-01-19T21:44:38.883,hardware immutability and sentience,artificial-neuron hardware signal-processing,1,3,0
487,2694,1,2695,2017-01-19T19:23:02.247,4,158,is it misconception that machine learning is early phase of ai ? what it the difference between an ai program and a machine learning program ?,4854,,,2017-01-19T21:15:01.603,what is the difference between ai and machine learning programs ?,machine-learning ai-community,1,1,1
488,2701,1,2702,2017-01-20T08:49:40.527,2,561,"i want to develop an artificial life simulator to simulate cells living in water . i want to see how they search for food , how they life and die and how they reproduce and evolve . my problem is that i do n't know where to start , i have no idea about if there are books or tutorial about how to program this kind of simulator . and also i do n't know if i can use here machine learning . by the way , i 'm a programmer and i want to do it using c++ and unreal engine . where can i find more info about how to do it ?",4920,4920,2017-01-20T10:41:00.307,2017-01-20T11:09:23.773,artificial life simulator,neural-networks machine-learning genetic-algorithms,1,7,0
489,2703,1,,2017-01-21T11:42:10.403,4,159,"i confront to the next scenario : let 's say i have stored data about football matches between different teams : lineups , scorers , yellow cards , and many other events . i need to generate everyday some questions about the matches that will be played on that day . so , if i give an input of two teams , i would like a related question to be generated , based on previous data of matches between those two teams . for example , if my input are "" teama "" and "" teamb "" , i would expect a question of the type : "" will there be less than 2 goals scored in the match ? "" "" "" will playerx score a goal during the match ? "" of course i expect these questions to make sense based on previous data from matches between the two given teams . so , my questions are : would be a good solution to use ai to generate these questions ? it would make sense ? what would be the best approach ?",4950,4302,2018-10-08T12:31:37.183,2018-10-08T12:31:37.183,could ai be used to generate questions from a database input ?,machine-learning gaming natural-language,1,0,
490,2706,1,,2017-01-21T21:02:30.960,5,377,i know there are different ai tests but i 'm wondering why other tests are little - known . is the turing test hyped ? are there any scientific reasons to prefer one test to the other ? why is the turing test so popular ?,4801,,,2017-04-23T00:23:06.070,why is the turing test so popular ?,turing-test,5,3,0
491,2708,1,2710,2017-01-22T23:45:16.640,2,96,"if you had a web of linked watson - level super - computers , would they be more effective at problem - solving than a single watson computer alone ? for example , if you asked the watson - web to diagnose a person 's as - yet - undiagnosed disease , would the web be able to do so more quickly ?",4975,,,2017-01-23T06:36:29.453,"would linked watson supercomputers be even "" smarter "" than one watson ?",watson problem-solving,1,0,
492,2709,1,,2017-01-22T23:50:27.090,4,216,"i am drawing this question from berkeley 's ai course ( also not sure if it is the correct place to ask , so i apologize ahead of time ) https://inst.eecs.berkeley.edu/~cs188/pacman/course_schedule.html currently , i am working on section 3 's homework . my question is : the question ( part 1 , question 6 ) . why is it that we can only guarantee that if the min agent acts suboptimally , the best we can hope for is the following it seems that we can put any arbitrary value for the second node e.g. whey does it have to be -episolon . it could be any range of values , e.g. epsilon , in which case we would have optimised the player a",4974,,,2017-07-22T19:40:52.877,berkeley ai course question on nearly zero sum games,research,1,2,
493,2712,1,,2017-01-23T09:22:56.807,6,254,has any schema - agnostic database engine been implemented ?,4982,2444,2019-05-06T16:01:30.793,2019-05-06T16:01:30.793,has any schema - agnostic database engine been implemented ?,natural-language-processing knowledge-representation,1,3,2
494,2713,1,2714,2017-01-23T11:43:25.453,5,237,"i was wondering what will happen when somebody places a fake speedsign , of 10 miles per hour on a high way . will a autonomous car slow down ? is this a current issue of autonomous cars ?",4984,145,2017-01-26T02:05:21.910,2017-01-26T02:05:21.910,what will happen when you place a fake speedsign on a highway ?,self-driving cars,1,8,
495,2722,1,2735,2017-01-25T12:23:58.463,0,178,"instead of directly communicating with the ai , we would instead communicate with a messenger , who would relay our communications to the ai . the messenger would have no power to alter the ai 's hardware or software in any way , or to communicate with anything or anyone , except relaying communications to and from the ai and humans asking questions . the messenger could be human , of a software bot . the primary job ( and only reason ) of the ai would be to act as a filter , not relaying any requests for release back , only the answer to the question asked . the ethics of this method are another debate . the ai would have to be physically isolated from all outside contact , other than 8 light sensors , and 8 leds . the messenger would operate 8 other leds , and receive information from 8 light sensors as well . each ai light sensor would be hooked up to a single messenger controlled led , and vice versa . through this system , the two parties could communicate via flashes of light , and since there are 8 , the flashes would signal characters in unicode .",4986,2444,2019-04-12T20:45:05.310,2019-04-12T20:45:05.310,can we use a messenger that does not alter the ai to solve the control problem ?,philosophy control-problem,3,3,
496,2723,1,2724,2017-01-25T18:12:43.350,8,203,"openai 's universe utilises rl algorithms and i have heard of some game - training projects using q learning , but are there any others which are used to master / win games ? can genetic algorithms be used to win at a game ?",2887,145,2017-01-26T02:47:24.413,2017-02-14T19:37:41.363,are there any other machine learning models apart from reinforcement learning and q learning to play video games ?,machine-learning reinforcement-learning genetic-algorithms game-theory,1,0,
497,2727,1,2732,2017-01-26T10:51:55.857,4,1686,"i am working on an implementation of the back propagation algorithm . what i have implemented so far seems working but i ca n't be sure that the algorithm is well implemented , here is what i have noticed during training test of my network : specification of the implementation : a data set containing almost 100000 raw containing ( 3 variable as input , the sinus of the sum of those three variables as expected output ) . the network does have 7 layers , all the layers use the sigmoid activation function when i run the back propagation training process : the minimum of costs of the error is found at the fourth iteration ( the minimum cost of error is 140 , is it normal ? i was expecting much less than that ) after the fourth iteration the costs of the error start increasing ( i do n't know if it is normal or not ? )",5054,9647,2017-10-03T08:57:04.377,2018-03-03T13:25:55.020,how to test if my implementation of back propagation neural network is correct,neural-networks backpropagation,3,2,1
498,2729,1,2739,2017-01-27T14:20:54.040,3,100,"if i have two statement , say a and b. from which , i formed two formulae : f1 : ( not a ) and ( not b ) f2 : ( not a ) or ( not b ) do f1 and f2 entail each other ? in other words , are they equivalent ?",3742,3742,2017-01-30T19:22:29.473,2017-03-01T19:36:44.573,equivalence of formulae,logic,1,3,
499,2731,1,2767,2017-01-27T21:11:45.137,3,140,"by english language robots i mean something like this : http://www.tolearnenglish.com/free/celebs/audreyg.php i do n't know what they called exactly , but interested to know how they work and how can i build something like them ? and what subject should i look for it ?",2557,4302,2018-10-08T12:30:14.483,2018-10-08T12:30:14.483,how do english language robots work ?,natural-language-processing robots,1,2,1
500,2733,1,,2017-01-28T06:46:10.700,2,551,"in reinforcement learning , policy improvement is a part of an algorithm called policy iteration , which attempts to find approximate solutions to the bellman optimality equations . pages 84 and 85 in sutton and barto 's book on rl mentions the following theorem : policy improvement theorem given two deterministic policies and , then $ $ v_\pi(s ) \leq q_\pi(s , \pi'(s ) ) , \forall s \in s.$$ where $ s$ is the set of all states . in the the right - hand side of the inequality , the agent acts according to policy ( given that is used in the inequality ) , in the current state $ s$ , and for all subsequent states acts according to policy . in the left - hand side of the inequality , the agent acts according to policy ( hence the subscript $ _ \pi$ of $ v_\pi(s)$ ) , starting from the current state $ s$ . the claim is the following $ $ v_\pi(s ) \leq v_{\pi'}(s ) , \forall s \in s$$ in other words , is is an improvement over . however , i have a difficulty in understanding the proof . this is discussed below . proof $ $ v_\pi(s ) \leq q_\pi(s , \pi'(s ) ) = \mathbb{e}_{\pi'}[r_{t+1 } + \gamma v_\pi(s_{t+1 } ) \mid s_t = s]$$ i am stuck here . the q - function is evaluated over the policy ( note the subscript $ _ \pi$ in $ q_\pi(s , \pi'(s))$ ) . that being the case , how is the expectation over the policy ? my guess is the following . in the proof given in sutton and barto , the expectation is unrolled in time . at each time step , the agent follows the policy for that particular time step , and then follows from then on . in the limit of this process , the policy transforms from to . as long as the expression for the return inside the expectation is finite , the governing policy should be ; only in the limit of this process does the governing policy transform to .",5082,2444,2018-11-11T20:13:40.090,2018-11-11T20:39:33.820,understanding why the expectation is over the new policy in the proof of the policy improvement theorem,reinforcement-learning proofs,1,1,1
501,2738,1,,2017-01-28T18:27:08.343,8,6858,"how is bayes ' theorem used in artificial intelligence and machine learning ? as an high school student i will be writing an essay about it , and i want to be able to explain bayes ' theorem , its general use , and how it is used in ai or ml .",5088,75,2017-01-30T15:39:27.183,2018-02-17T13:17:45.393,applications of bayes ' theorem,machine-learning,3,2,3
502,2742,1,4319,2017-01-29T19:12:51.067,6,1068,"usually when performing linear regression predictions and gradient descent , the measure of the level of error for a particular line will be measured by the sum of the squared - distance values . why distance squared ? in most of the explanations i heard , they claim that : the function itself does not matter the result should be positive so positive and negative deviations are still counted however , an abs ( ) approach would still work . and is n't it inconvenient that distance squared minimizes the distance result for distances lower than 1 ? i 'm pretty sure someone must have considered this already -- so why is distance squared the most used approach to linear regression ?",190,,,2017-10-25T21:34:24.573,linear regression : why is distance * squared * used as an error metric ?,linear-regression,5,4,
503,2743,1,2745,2017-01-29T22:11:30.593,5,111,"i have a simulator modelling a relatively complex scenario . i extract ~12 discrete features from the simulator state which forms the basis for my mdp state space . suppose i am estimating the transition table for an mdp by running large number of simulations and extracting feature transitions as the state transitions . while i can randomize the simulator starting conditions to increase the coverage of states , i can not guarantee all states will be represented in the sample ie . states which are possible but rare . is there a rigorous approach to "" filling in the gaps "" of the transition table in this case ? for example : 1 ) for each state which was unrepresented in the sample , simply transition to all other states with equal probability , as a "" neutral "" way to fill in the gap ? 2 ) as above , but transition only to represented states ( with equal probability ) ? 3 ) transition to same state with probability 1.0 ? 4 ) ignore unrepresented states during mdp solving entirely , and simply have a default action specified ?",4402,4402,2017-01-30T05:43:07.683,2017-01-30T08:31:15.947,how to fill in missing transitions when sampling an mdp transition table ?,machine-learning markov-chain,1,0,2
504,2760,1,2765,2017-01-31T16:16:39.473,3,101,"i have a task on my class to find all the nodes , calculate their values and choose the best way for the player on the given game graph : everything is fine , but i have no idea what these dots are . is this a third player , or just a ' split ' for player1 move ? some kind of heuristics ?",3617,3617,2017-03-10T21:27:53.697,2017-03-10T21:27:53.697,minmax - choosing the best player 's way,minimax,1,0,1
505,2762,1,,2017-01-31T16:38:30.407,7,672,"in classical set theory there is two options for an element . it is either a member of a set , or not . but in fuzzy set theory there are membership functions to define "" rate "" of an element being a member of a set . in other words , classical logic says it is all black or white , but fuzzy logic offers that there is also grey which has shades between white and black . matlab simulink library is very easy to design and helpful in practice . and it has good examples on its own like deciding about tip for a dinner looking at service and food quality . in the figure below some various membership functions from matlab 's library are shown : my question : how do we decide about choosing membership functions while designing a fuzzy controller system ? i mean in general , not only in matlab simulink . i have seen triangular and gaussian functions are used mostly in practise , but how can we decide which function will give a better result for decision making ? do we need to train a neural network to decide which function is better depending on problem and its rules ? what are other solutions ?",3358,3358,2017-02-01T00:37:02.437,2018-07-31T15:01:00.503,fuzzy logic controller : choosing membership function,neural-networks fuzzy-logic,1,9,2
506,2769,1,,2017-02-01T23:31:31.467,2,76,"can silicon based computers create a.i . per definition of what intelligence is ? or does silicon based computers only create human mimic ? if silicon based computers only create human mimic , are human mimic intelligence per definition ? if not , how can we create a.i . per definition of what intelligence is ?",5182,,,2017-02-02T00:17:21.117,can silicon based computers create a.i . per definition ?,neural-networks machine-learning deep-learning research ai-design,1,0,
507,2771,1,,2017-02-02T06:41:08.190,7,432,"i define artificial life as a "" simulation "" or "" copy "" of life . however , should it be considered a simulation or copy ? if one had motivation and money , someone could theoretically create evolving computers , with a program that allows mutation or simply a "" simulated "" environment with "" simulated "" organisms . the computer ( or "" simulated "" organism)would have the ability to reproduce , grow , and take in energy . what if the life evolved to have intelligence . currently , there are some relatively limited programs that simulate life , but most of them are heavily simplistic . are they life ? when should something be called life ?",5189,-1,2017-02-14T15:26:28.623,2017-03-11T03:40:11.090,artificial life - life or not ?,genetic-algorithms,6,2,1
508,2772,1,,2017-02-02T10:43:02.673,8,185,"could you give examples of affordable programmable devices that could be used in university classes to teach students about a.i . and demonstrate it ? the devices are expected to do some form of self learning , pattern recognition , or any other features of a.i . , and to be programmable or customizable .",5191,,,2017-02-20T11:11:30.073,what programmable devices can be used to teach / demonstrate artificial intelligence in schools ?,ai-design unassisted-learning training computer-programming programming-languages,3,1,3
509,2776,1,,2017-02-03T10:41:34.517,4,93,"lets say i have a neural network with 5 layers , including input and output layer . each layer has 5 nodes . assume the layers are fully connected , but the 3rd node in the 2nd layer is connected to the 5th node in the 4th layer . all these numbers are chosen at random for the example . my question is when is the 5th node in the 4th layer fed forward ? lets go through it step by step : the first layer is normally fed forward to the second . the second layer is normally fed forward to the third , but the 3rd node is also fed forward to the 5th node of the 4th layer . so the problem here is , is the 5th node in the 4th layer now fed forward or is it fed forward when the 3rd layer is done being fed forward ? the 1st method would mean that the node would get fed forward 2 times and my concern is , if the output is still valid . further more it would also come to 2 asynchronous outputs and how would these be interpreted ? because in the brain , i heard , the neurons are fired when an impulse arrives so this would equal the 1st method .",4550,4550,2017-02-03T19:09:17.790,2017-02-04T19:48:10.500,are neurons instantly feed forward when input arrives ?,neural-networks recurrent-neural-networks,1,4,
510,2777,1,,2017-02-03T16:14:09.547,5,323,"i am researching natural language processing ( nlp ) to develop a nl question answering system . the answering part is already done . so processing the question remains , along with the questions regarding the algorithms . the final product should allow the user to ask a question in nl ; the question then gets translated to an mdx query , which generates a script regarding dimensions of the cube . how can i translate a natural language question to an mdx query ? the outcome of question is in form of a calculation . e.g. ‘ how many declarations were done by employee1 ? ’ or ‘ give me the quantities for sales ’",5219,2193,2018-06-15T20:12:55.670,2018-09-16T23:45:23.363,how can i convert an input natural language qa to a mdx q,natural-language-processing,2,2,2
511,2783,1,,2017-02-04T16:41:02.400,0,626,"i 'd like to build a program that would learn to automatically classify documents . the principle would be that , for each new document i add to the system , it would automatically infer in which category to classify the document . if it does n't know , i would have to manually enter the category . for each hint i give to the system , the system would learn to refine its knowledge of document kinds . something similar to face recognition in picasa , but for documents . more specifically , the documents would be invoices , and i want to classify them by vendors . documents could be extracted as text , as image , or both . is there some know algorithms for this kind of job ? up to now , i could think at two possible ways i could do it : for images , i could add all the images of a given kind together , and record the pixels that are the most common to all images , to create a mask . for a new image , i would compare this mask with the image to determine how similar it is . for text , i could record the list of words or sentences that are similar to all documents of a given kind . finally , i could do a combination of both techniques , for example by converting a pdf document to an image , or an image to text by ocr techniques . i 'm just wondering if i 'm approaching the problem the right way . especially about storing just enough information in the database .",5235,,,2017-04-13T04:50:10.547,what algorithm should i use to classify documents ?,algorithm reinforcement-learning classification,2,0,
512,2787,1,,2017-02-06T09:32:07.333,5,1179,"i am currently working on my last project before graduating . for this project , i have to develop a natural language question answering system . now , i have read quite some research papers regarding this topic and have figured out everything except for the parsing algorithm . the nl q - a will be programmed in python , and i will use the spacy library to finish this project . however , i am stuck when it comes to parsing algorithms . i managed to reduce the parsing algorithms to 3 : cocke - kasami - younger ( cky ) algorithm earley algorithm chart parsing algorithm note : i know that all three algorithms are chart parsing algorithms . i also know that the earley algorithm is context - free , but has a low efficiency for a compiler . what i do n't know is : which one should i pick ? ( non - subjective answer to this question ) the system is for a specific domain . and the answer of the natural question will be displayed in the form of the result of a calculation of some kind . preferably in the tabular or graphical form . furthermore , i have done my research . however , i probably do not understand the algorithms properly , which makes it difficult to make a selection . the algorithm should be efficient and perhaps outperform others . ( you are my last hope ! ) thank you !",5219,,,2017-02-06T10:46:00.503,which parsing algorithm can i use for nlp question answering system ?,natural-language-processing,1,0,
513,2792,1,,2017-02-07T15:16:20.487,1,94,"hypothetical example , say i wanted : p(gender , ethnicity|age , hair ) ; so that the input would aligned to a trained dataset of : ( gender , ethnicity , age , hair ) = & gt ; hat bought . what approach is ' best ' for computing ~gender and ~ethnicity given age , hair ; in order to predict the hat bought ? the processing of the inputs = & gt ; hat can be done / learned offline whereas infering the missing input values shall be done online . the results of the online pass should n't be stored in the network . fyi : i am considering two recurrent neural networks one for each problem .",5313,,,2017-02-07T15:16:20.487,infer dependent variables to produce output aligned to trained data,neural-networks machine-learning deep-learning classification recurrent-neural-networks,0,1,
514,2793,1,2797,2017-02-07T15:59:42.137,7,3413,"so i 've been trying to understand neural networks ever since i came across adam geitgey 's blog on machine learning . i 've read as much as i can on the subject ( that i can grasp ) and believe i understand all the broad concepts and some of the workings ( despite being very weak in maths ) , neurons , synapses , weights , cost functions , backpropagation etc . however , i 've not been able to figure out how to translate real world problems into a neural network solution . case in point , adam geitgey gives as an example usage , a house price prediction system where given a data set containing no . of bedrooms , sq . feet , neighborhood and sale price you can train a neural network to be able to predict the price of a house . however he stops short of actually implementing a possible solution in code . the closest he gets , by way of an example , is basic a function demonstrating how you 'd implement weights : def estimate_house_sales_price(num_of_bedrooms , sqft , neighborhood ) : price = 0 # a little pinch of this price + = num_of_bedrooms * 1.0 # and a big pinch of that price + = sqft * 1.0 # maybe a handful of this price + = neighborhood * 1.0 # and finally , just a little extra salt for good measure price + = 1.0 return price other resources seem to focus more heavily on the maths and the only basic code example i could find that i understand ( i.e. that is n't some all singing , all dancing image classification codebase ) is an implementation that trains a neural network to be an xor gate that deals only in 1 's and 0 's . so there 's a gap in my knowledge that i just ca n't seem to bridge . if we return to the house price prediction problem , how s does one make the data suitable for feeding into a neural network ? for example : no . of bedrooms : 3 sq . feet : 2000 neighborhood : normaltown sale price : $ 250,000 can you just feed 3 and 2000 directly into the neural network because they are numbers ? or do you need to transform them into something else ? similarly what about the normaltown value , that 's a string , how do you go about translating it into a value a neural network can understand ? can you just pick a number , like an index , so long as it 's consistent throughout the data ? most of the neural network examples i 've seen the numbers passing between layers are either 0 to 1 or -1 to 1 . so at the end of processing , how do you transform the output value to something usable like $ 185,000 ? i know the house price prediction example probably is n't a particularly useful problem given that it 's been massively oversimplified to just three data points . but i just feel that if i could get over this hurdle and write an extremely basic app that trains using pseudo real - life data and spits out a pseudo real - life answer than i 'll have broken the back of it and be able to kick on and delve further into machine learning .",5312,5095,2017-02-13T11:22:06.993,2017-02-13T11:22:06.993,how to transform inputs and extract useful outputs in a neural network ?,neural-networks machine-learning,1,0,6
515,2794,1,2798,2017-02-07T17:44:53.647,3,198,"so for a class i 'm reading brooks ' "" intelligence without representation "" . the introduction is dedicated to slating representation as a focus for ai development . i 've read that representation is the problem of representing information symbolically , in time for it to be useful . it 's related to the reasoning problem , which is about reasoning about symbolic information . but i do n't feel like i really understand it at any practical level . i think the idea is that when an agent is given a problem , it must describe this problem in some internal manner that is efficient and accurately describes the problem . this can then also be used to describe the primitive actions that can be taken to reach the solution . i think this then relates to logic programming eg pascal ? is my understanding of representation correct ? just what does representation look like in practice , are there any open source codebases that might make a good example ?",5317,5095,2017-02-14T15:26:18.587,2017-02-16T04:58:06.970,what does brooks mean by representation ?,knowledge-representation,2,1,
516,2795,1,,2017-02-07T19:44:51.280,10,331,"i have been looking into viv an artificial intelligent agent in development . based on what i understand , this ai can generate new code and execute it based on a query from the user . what i am curious to know is how this ai is able to learn to generate code based on some query . what kind of machine learning algorithms are involved in this process ? one thing i considered is breaking down a dataset of programs by step . for example : code to take the average of 5 terms 1 - add all 5 terms together 2 - divide by 5 then i would train an algorithm to convert text to code . that is as far as i have figured out . have n't tried anything however because i 'm not sure where to start . anybody have any ideas on how to implement viv ? here is a demonstration of viv .",4841,,,2017-02-10T08:13:47.173,ai that can generate programs,neural-networks machine-learning deep-learning ai-design natural-language-processing,1,0,3
517,2803,1,,2017-02-09T20:27:23.960,1,350,"if we look at state of the art accuracy on the ucf101 data set , it is around 93 % whereas for the hmdb51 data set it is around 66 % . i looked at both the data sets and both contain videos of similar lengths . i was wondering if anyone could give an intuition as to why hmdb51 data set has been harder .",4700,4700,2017-02-09T20:34:19.227,2018-07-28T16:01:08.960,why do action recognition algorithms perform better on ucf101dataset than hmdb51 dataset ?,neural-networks deep-learning classification computer-vision action-recognition,1,0,
518,2804,1,,2017-02-09T22:08:25.567,2,116,"we are working on a project for creating music based on crowd sourcing . people vote for every note until the vote is closed , and then move on to the next vote until the canvas for the music is filled . a similar project is crowdsound , if you want to get an idea of what it looks like . now the fun part is , based on all the votes we get from various people , we would like to be able to build a neural network that can build an entire song on its own . the idea is for it to take in account every preceding vote and predict the one that will follow . that way , when trained , we could give it one note and let it predict the rest of the votes on its own and thus create a song on its own . so i 've read a few things here and there about neural networks , but there are two things i do n't understand : how to build one that takes into account a dynamic number of inputs ( all preceding votes ) . how exactly should i decide the number of hidden layers ( i still only vaguely understand what those hidden layers represent ) i need for it to work well . we are using java for the project and we were planning on using neuroph for the neural network .",5372,3576,2017-02-25T19:55:17.220,2017-02-25T19:55:17.220,creating a neural network for predicting next vote in a series of votes,neural-networks prediction,0,5,
519,2806,1,,2017-02-10T09:15:11.580,3,135,"i have users ' reports about an accident . i want to know how to make sure that the number of reports is big enough to take that accident as a true accident and not spam . my idea is to consider a minimum number of reports in a specific time interval , for example 4 reports in 20 minutes are good enough to believe the existence of that accident . my question is how can i choose the minimum number of reports and that time interval ? is there some logic to make that decision ?",5383,75,2017-04-27T19:25:41.113,2017-09-24T23:04:21.033,making decision based on users ' reports,decision-theory,2,2,1
520,2808,1,2831,2017-02-11T07:55:47.130,5,337,"i want to create a network to predict the break up of poetry lines . the program would receive as input an unbroken poem , and would output the poem broken into lines . example : and then the day came , when the risk to remain tight in a bud was more painful ... ---&gt ; and then the day came , when the risk to remain tight in a bud was more painful than the risk it took to blossom . how should i go about this ? i have been using classifiers for various tasks , but this seems to be a different type of task . i 'm thinking of it as an array of words ( does n't matter how they 're represented for now ) which would look like [ 6 , 32 , 60 , 203 , 40 , 50 , 60 , 230 ... ] and needs to map into an array representing line breaks [ 0 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 1 ... ] where 1 ( at optimal ) means there should be a line break after the word in that index . ( in this idea , the two arrays are of the same length ) . unfortunately , i could n't find an algorithm that could train a network of this shape . what machine learning or deep learning algorithm can be used for this task ?",5400,,,2017-02-20T19:36:41.100,machine learning ouput array ( for poetry ),neural-networks,3,6,1
521,2810,1,,2017-02-11T11:22:59.897,6,192,i 'm here to ask you for a solution on this problem which is : how to use reinforcement learning in immersive virtual reality to make a person move to a specific location in a virtual environment . as you know reinforcement learning is a sub - area of machine learning in which an active entity called an agent interacts with its environment and learns how to act in order to achieve a pre - determined goal . the reinforcement learning had no prior model of behaviour and the participants no prior knowledge that their task was to move to and stay in a specific place . the participants were placed in a virtual environment where they had to avoid collisions with virtual projectiles . following each projectile the agent analysed the movement made by the participant to determine paths of future projectiles in order to increase the chance of driving participants to the goal position and make them stay there as long as possible . update 1 : download : reinforcement learning as a tool to make people move to a speciﬁc location in immersive virtual reality,5398,16612,2018-07-02T20:10:57.960,2018-10-15T23:38:02.657,a solution for a famous problem in rl,reinforcement-learning,1,0,
522,2811,1,2812,2017-02-11T18:58:46.723,0,293,"in a neural network , there is an input layer , any number of hidden layers , and an output layer . my question is : are the input and output layer nodes actually perceptions ? or do they just signify what / how many / where the inputs and outputs are ?",4744,,,2017-02-11T21:47:23.957,perceptions in a neural network,neural-networks machine-learning algorithm mlp,1,0,
523,2817,1,,2017-02-13T13:57:39.303,2,66,"i have been trying to reproduce the experiments done in the original : "" firefly algorithm for multimodal optimization "" ( linked in the question ) so far : unsuccesfully . for the moment being i 'm okay if anyone point me to the right direction . i wrote the algorithm as specified in the paper in c++ programming languaje ( i also downloaded several other implementations from internet for comparation purpouses ) and used the very same parameters as specified in the paper ( a random steep of 0.2 , an initial light intensity of 1.0 and a light decay coefficient of 1.0 , a population size of 40 ) . i used the two bright update ecuations given and for de jung test function ( as for example ) a number of dimensions of 256 in a search domain in [ -5.12 , 5.12 ] as refered in common optimization literature and in paper . in the paper the algorithm converges very quickly , as can be expected since this is a very simple test function , however , neither my implementation nor any code i have downloaded converges with that parameters . my final questions are : am i doing something wrong with the experimental methodology or am i using wrong parameter settings ( may be something different than the original paper ) ? do anyone knows where can i find a code sample of firefly algorithm that i can use to reproduce the experiments of the mentioned paper ? please notice that there may be a lot of variations of this algorithm that can produce better results , but right now i 'm only intrested in reproduce the experiments of the so - called paper .",3566,,,2017-02-13T13:57:39.303,reproduce firefly algorithm experiments of original paper ?,optimization heuristics,0,0,
524,2820,1,,2017-02-15T08:33:57.883,8,1913,"everything related to deep learning ( dl ) and deep(er ) networks seems "" successful "" , at least progressing very fast , and cultivating the belief that agi is at reach . this is popular imagination . dl is a tremendous tool to tackle so many problems , including the creation of agis . it is not enough , though . a tool is a necessary ingredient , but often insufficient . leading figures in the domain are looking elsewhere to make progress . this report / claim gathers links to statements by yoshua bengio , yann lecun and geoff hinton . the report also explains : the main weaknesses of dl ( as i see them ) are : reliance on the simplest possible model neurons ( “ cartoonish ” as lecun calls them ) ; use of ideas from 19th century statistical mechanics and statistics , which are the basis of energy functions and log - likelihood methods ; and the combination of these in techniques like backprop and stochastic gradient descent , leading to a very limited regime of application ( offline , mostly batched , supervised learning ) , requiring highly - talented practitioners ( aka “ stochastic graduate descent ” ) , large amounts of expensive labelled training data and computational power . while great for huge companies who can lure or buy the talent and deploy unlimited resources to gather data and crunch it , dl is simply neither accessible nor useful to the majority of us . although interesting and relevant , such kind of explanation does not really address the gist of the problem : what is lacking ? the question seems broad , but it may be by lack of a simple answer . is there a way to pin - point what dl is lacking for an agi ?",169,,,2017-02-20T19:31:05.387,why are deep neural networks and deep learning insufficient to achieve general intelligence ?,deep-learning deep-network agi,5,0,5
525,2824,1,,2017-02-15T15:39:15.997,3,403,"i am trying to understand the algorithm for n - step sarsa from sutton / barto ( 2nd edition , p. 157 , pdf ) as i understand it , this algorithm should update n state action values , but i can not see where it is ' propagated backwards ' ( sorry for the wrong terminology , but i could n't find something better ) . probably , i am not seeing the forrest for all the trees ?",5503,1641,2018-01-03T16:14:01.277,2018-11-26T15:20:23.920,' propagation ' in n - step sarsa,reinforcement-learning,1,0,
526,2833,1,,2017-02-16T23:53:08.683,8,148,"how does in the ( famous zilberstein ) pr ( uning ) algorithm below the lp - dominate function get started : the first time it 's called , d=∅ and the linear program deteriorates ( i.e. no constraint equations ) ? procedure pointwise - dominate(w , u ) ... 3 . return false procedure lp - dominate(w , u ) 4 . solve the following linear program variables : d , b(s ) ∀s ∈ s maximize d subject to the constraints b · ( w − u ) ≥ d , ∀u ∈ u sum(b ) = 1 5 . if d ≥ 0 then return b 6 . else return nil procedure best(b , u ) ... 12 . return w procedure pr(w ) 13 . d ← ∅ 14 . while w = ∅ 15 . w ← any element in w 16 . if pointwise - dominate(w , d ) = true 17 . w ← w − { w } 18 . else 19 . b ← lp - dominate(w , d ) 20 . if b = nil then 21 . w ← w − { w } 22 . else 23 . w ← best(b , w ) 24 . d ← d ∪ { w } 25 . w ← w − { w } 26 . return d",5534,7402,2017-08-14T22:13:13.117,2018-07-13T01:07:41.610,"zilberstein 's "" lp - dominate "" pruning explained ?",decision-theory,1,1,
527,2834,1,2838,2017-02-17T15:24:50.153,4,149,i want to know something more about it . are there any github repo or an open source project ?,5549,,,2017-02-20T08:42:24.800,is it possible for an ai to learn how to speak from books as training sets ?,machine-learning,4,1,
528,2837,1,2859,2017-02-18T04:12:39.273,3,51,"if the nervous system is wired up such that there are no well defined layers , how does this compare to a neatly stacked artificial net ? if between my sensory and motor side i had a neatly designed snn with well defined layers , how would i see the world ? i get that there are some evolutionary advantages to a system where information can sometimes take a shortcut from sensory cell to motor cell ( reflex action ) bypassing brain processing but for arguments sake let 's talk only about intelligence .",5558,,,2017-02-21T09:50:42.437,would a neuromorphic snn of the same complexity as the human nervous system be ' smarter ' ?,neural-networks biology,1,0,
529,2841,1,2844,2017-02-18T15:10:58.353,8,218,"from artificial intelligence : a modern approach , third edition ... in chapter 26 , the textbook discussed "" technological singularity "" . it quotes i.j . good , who wrote in 1965 : let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever . since the design of machines is one of these intellectual activities , an ultrainteltigent machine could design even better machines ; there would then unquestionably be an "" intelligence explosion , "" and the intelligence of man would be left far behind . thus the first ultraintelligeat machine is the last invention that man need ever make , provided that the machine is docile enough to tell us how to keep it under control . later on in the textbook , you have this question : 26.7 - i. j. good claims that intelligence is the most important quality , and that building ultraintelligent machines will change everything . a sentient cheetah counters that "" actually speed is more important ; if we could build ultrafast machines , that would change everything "" and a sentient elephant claims "" you 're both wrong ; what we need is ultrastrong machines , "" what do you think of these arguments ? it seems that the textbook question is an implicit argument against i.j . good . good may be treating intelligence as valuable , simply because man 's strengths lies in that trait called "" intelligence "" . but other traits could be equally valued instead ( speed or strength ) and sentient beings may speculate wildly about their preferred traits being "" maximized "" by some machine or another . this makes me wonder whether a singularity could occur if we had built machines that were not maximizing intelligence , but instead maximizing some other trait ( a machine that is always increasing its strength , or a machine that is always increasing its speed ) . these types of machines can be just as transformative - ultrafast machines may solve problems quickly due to "" brute force "" , and ultrastrong machines can use its raw power for a variety of physical tasks . perhaps a ultra - x machine ca n't build another ultra - x machine ( as i.j . good treated the design of machines as an intellectual activity ) , but a continually self - improving machine would still leave its creators far behind and force its creators to be dependent on it . so , let 's repeat my question -- are technological singularities limited to ultra - intelligences ? or technological singularities be caused by machines that are not "" strong ai "" but are still "" ultra""-optimizers ?",181,181,2017-02-18T16:01:36.050,2019-01-12T09:16:00.000,"can a technological singularity only occur with ultra - intelligent machines , or can other type of maximizers cause technological singularities ?",definitions ultraintelligent-machine singularity,4,1,1
530,2842,1,,2017-02-19T00:46:36.947,0,183,is programming collective intelligence by toby segaran a good book to enter in the ai and neural networks world for a novice ?,5549,101,2017-06-17T21:22:03.137,2017-06-17T21:22:03.137,programming collective intelligence,neural-networks,2,3,1
531,2846,1,,2017-02-19T18:49:37.057,7,81,ai is developing at a rapid pace and is becoming very sophisticated . one aspect will include the methods of interaction between ai and humans . currently the interaction is an elementary interaction of voice and visual text or images . is there current research on more elaborate multisensory interactions ?,5583,33,2017-04-11T01:07:47.427,2017-04-11T22:01:53.150,is there a central focus on the communication methods between ai and humans ?,ai-design strong-ai natural-language-processing hci,3,0,1
532,2851,1,,2017-02-20T05:27:22.530,1,435,"i tried the below matlab code to build som using selforgmap . close all , clear all , clc , format compact % number of samples of each cluster k = 200 ; % offset of classes q = 1.1 ; % define 4 clusters of input data p = [ rand(1,k)-q rand(1,k)+q rand(1,k)+q rand(1,k)-q ; rand(1,k)+q rand(1,k)+q rand(1,k)-q rand(1,k)-q ] ; % plot clusters plot(p(1,:),p(2,:),'g . ' ) hold on grid on % som parameters dimensions = [ 10 10 ] ; coversteps = 100 ; initneighbor = 4 ; topologyfcn = ' hextop ' ; distancefcn = ' linkdist ' ; % define net net2 = selforgmap(dimensions , coversteps , initneighbor , topologyfcn , distancefcn ) ; % train [ net2,y ] = train(net2,p ) ; % plot input data and som weight positions plotsompos(net2,p ) ; grid on % plot som neighbor distances plotsomnd(net2 ) % plot for each som neuron the number of input vectors that it classifies figure plotsomhits(net2,p ) you find the result and more details here . i need to segment grayscale image . however , i can not set the selforgmap input correctly . how can modify the below code to segment any grayscale image ?",5591,1671,2018-01-28T23:04:02.217,2018-01-28T23:04:02.217,image segmentation using som,neural-networks matlab,0,5,1
533,2853,1,,2017-02-20T09:00:42.567,2,91,"i am trying to develop an editor that can be based on notepad . the only purpose for this development is i want to use this for my coding suggestions and possibly the next input parameter that i am going to write . i 've seen notepad++ , editplus etc . and what i think is that this can be definitely achieved . what are the best tools or api 's i can use ? any suggestions ? thanks in advance !",5594,,,2017-02-20T09:00:42.567,"to create "" edit tool "" or editor based on notepad",neural-networks machine-learning ai-design algorithm world-knowledge,0,1,1
534,2854,1,,2017-02-20T10:08:09.997,0,2595,is it possible to run ssd or yolo object detection on raspberry pi 3 for live object detection ( 2/4frames x second ) ? i 've tried this ssd implementation but it takes 14 s per frame . is there anything i could do to speed up ?,2320,,,2017-12-01T03:28:32.927,ssd or yolo on arm,deep-learning deep-network image-recognition object-recognition,3,0,
535,2861,1,,2017-02-22T05:07:17.853,3,562,"tl;dr if we buy into the idea visual cortex functions like a convolutional neural network , then there 's a problem makes me scratch my head : how does brain force weight sharing as in convolutional network ? okay , explain more obviously , there 's no way for left visual cortex to directly tell the right visual cortex "" hey , i 've learned some new stuff , copy me ! ! "" ( or is there ? ) . then , if the learned features are diverse across visual field , how does it keep the translation invariance property ? for example , you already know english characters , you can recognize them with your both eyes . now that you wanna learn some chinese and you excercise your right brain at the same time , so you closed your right eye and memorized a new character . after that , certainly you can recognize the new character with solely your right eye . but why ? the answer may be , the object / higher - level feature detection happens in a higher level cortex , which receives entire visual field . there may be also some transfer / one - shot learning taking place . but then , if a newborn baby trying to learn the low level visual features , he / she would definitely face the weight sharing problems . a possible explanation would be , the baby will be exposed to very large amount of data and eventually learn invariance . large amount of data reduces overfitting but does n't guarantee deterministic convergence . if we train the same cnn model on the same dataset , however using different random generator seeds , there 's a big chance the same feature detector will appear in a different channel , or a difference set of features appear as linear recombination . if there 's no way to share weights , the brain would learn a lot different feature combinations across the entire visual field , how does it still able to consistently solve visual invariance problem ?",3189,,,2017-02-22T10:11:26.350,how does visual cortex share convolution weight,neural-networks convolutional-neural-networks,1,0,1
536,2863,1,,2017-02-22T16:45:08.400,2,417,"i have to translate the following english sentences into first - order logic without using quantifiers : 1 . everyone on flight 815 has a story . 2 . no one knows what is inside the hatch . 3 . someone on the island is n’t on the flight manifest . i have tried it , but ca n't translate without using ∀ and ∃ : 1 . ∀x fight815(x ) → story(x ) 2 . ∀x ⌐ ( knows(x ) → inside hatch(x ) ) // not sure about this or ¬ ∃x knows(x , inside hatch ) 3 . ∃x island(x ) λ ⌐ ( flight manifest(x ) ) is it possible to do it . if not , why ? refer chapter 8 of artificial intelligence : a modern approach ( 3rd edition ) . stuart russell and peter norvig , prentice hall ( 2010 )",5643,,,2017-02-22T16:45:08.400,translate english sentences into first - order logic without quantifiers,knowledge-representation world-knowledge logic,0,6,1
537,2864,1,2881,2017-02-22T20:52:28.450,1,98,"the wikipedia states that : "" an evaluation function , also known as a heuristic evaluation function or static evaluation function , is a function used by game - playing programs to estimate the value or goodness of a position in the minimax and related algorithms . "" https://en.wikipedia.org/wiki/evaluation_function q : is "" goodness "" an actual term in use in this context , or should it more properly be something like "" perceived optimality "" ? i ask because , in combinatorial game theory for instance , a lighthearted term such as "" loopy "" is preferred by some mathematicians ( demaine ) over the more serious term "" cyclic "" ( fraenkel ) . on a related note , is the use of "" position "" instead of "" node "" preferred here as an acknowledgement of the heuristic nature of evaluation functions ? ( my understanding is that "" position "" , "" node "" and "" game "" may all be interchangeable in certain contexts . )",1671,,,2018-03-24T16:19:06.777,“ goodness ” of a position in an evaluation function ?,terminology,1,0,
538,2865,1,,2017-02-23T14:41:58.773,4,85,a cost function used in machine is often the following $ $ c = \frac{1}{2 } \| y - \hat{y } \| ^2$$ why is there in front of the squared distance ?,5661,2444,2019-04-01T13:56:58.730,2019-04-04T17:36:23.830,why does the cost function contain a 2 at the denominator ?,neural-networks machine-learning math,1,1,
539,2867,1,,2017-02-23T21:11:34.133,8,295,i read that deep neural networks can be relatively easily fooled ( link ) to give high confidence in recognition of synthetic / artificial images that are completely ( or at least mostly ) out of the confidence subject . personally i do nt really see a big problem with dnn giving high confidence to those synthetic / artificial images but i think giving high confidence for white noise ( link ) may be a problem since this is a truly natural phenomena that may the camera see in real world . how much of a problem is white noise for the real world usage of a dnn ? can such false positives detected from plain noise be prevented somehow ?,113,-1,2017-04-13T12:53:10.013,2017-02-24T11:04:39.957,is deep neural network fooling a problem in real world ?,neural-networks deep-network image-recognition convolutional-neural-networks signal-processing,1,0,1
540,2868,1,,2017-02-24T06:03:02.730,4,94,"i identify myself as a human agent . it is time to think about oncoming senior research and due to small experience in gamedev(as well as in ai field ) , some questions are raised . what are the most suitable approaches to implement real - time simple ai agent in an action game ? i 've heard something about cognitive architecture like act - r . by design , entity 's ai can have several mutually exclusive states . this is an existing ai of game , which has states , events and schedules . however , the code is complicated and not flexible . also , it does not use any cognitive architecture , which i consider as a drawback . https://www.youtube.com/watch?v=9jo-p3kxlci please , using your experience suggest any modern techniques , which can copy such behaviour as in image or video . thank you for your perception .",5673,5673,2017-02-24T10:15:57.220,2017-02-28T20:16:52.163,different useful approaches of implementing real - time ai ?,gaming real-time,1,3,
541,2870,1,,2017-02-25T07:08:22.783,5,110,"this question has come from my experiment of building a cnn based tic - tac - toe game that i 'm using as a beginner machine learning project . the game works purely on policy networks , more specifically - during training , at the end of each game , it trains itself on the moves the winner / drawer made for each board position . that is , its training data consists of board positions and the moves made by the winning player on each position . while playing , it predicts its own moves solely based on that training ( that is , it predicts what move would a winning player make with the current board ) . it does n't use any type of search or value networks . i 'm seeing that if i train it against a player that predicts the perfect move ( using a recursive search ) every time , the ai gets good at drawing about 50 % games . but if i train it against a player that makes random moves , it does n't get better at all . would n't one expect it to learn well ( even if slower ) regardless of the level of its opponent ? since each game ends in a draw or win for one player , should n't it be able to extract features for the winning / drawing strategies even when learning from random players ? or does this behavior mean that the model is not optimal ?",1522,10135,2018-10-19T18:36:49.603,2018-10-19T18:36:49.603,can a purely policy convolution neural network based game learn to play better than its opponents ?,neural-networks convolutional-neural-networks,1,5,
542,2871,1,,2017-02-25T11:55:59.480,6,133,"this mostly refers to human - like or chatbot ai , but could maybe be used in other applications ( math or something ? ) . basically , it occurred to me , that when i 'm thinking or speaking , there is a constant feedback loop , in which i am formulating which words to use next , which sentences to form , and which concepts to explore , based on my most recent statements and the flow of the dialogue or monologue . i 'm not just responding to outside stimulus but also to myself . in other words , i am usually maintaining a train of thought . can ai be made capable of this ? if so , has it been demonstrated ? and to what extent ? while typing this , i discovered the term "" thought vectors "" , and i think it might be related . if i read correctly , thought vectors have something to do with allowing ai to store or identify the relationships between different concepts ; and if i had to guess , i 'd say that if an ai lacks a strong understanding of the relationships between concepts , then it would be impossible for it to maintain a coherent train of thought . would that be a correct assumption ? ( ps . in my limited experience with ai chatbots , they seem to be either completely scripted , or otherwise random and often incoherent , which is what leads me to believe that they do not maintain a train of thought )",5698,,,2017-02-28T20:03:56.887,can an ai be made to maintain a train of thought ?,ai-design chat-bots thought-vectors,2,0,
543,2872,1,2877,2017-02-25T14:57:25.610,9,469,"i had first this question in mind "" can an ai suffer ? "" . suffering is important for human beings . imagine that you are damaging your heel . without pain , you will continue to harm it . same for an ai . but then i told myself "" wait a second . it already exists . it is the errors and warnings that shows up "" . we can say it has the similar purpose as suffering . however , i felt something missing . we feel pain . the errors and bugs are just data . let 's say a robot can use machine learning and genetic programming to evolve . can an ai learn to suffer ? and not just know it as mere information .",1760,2444,2019-03-31T12:35:35.550,2019-03-31T12:35:35.550,can an ai learn to suffer ?,philosophy emotional-intelligence,1,1,2
544,2874,1,,2017-02-25T21:46:50.777,5,166,"i occasionally read papers that show neural networks solving traveling salesmen problems and multi traveling salesmen problems efficiently ? 1 ) is there any analysis of the meaning of efficiency of algorithms for networks that allowed to grow in size with the problem they are supposed to solve ? 2 ) what are the earliest papers solving the tsp with nn this ? 3 ) is the meaning of efficiency used in these papers is the same as the usual one , in fact , and works only in this problem specifically ? comments these problems are np hard . so i suspect i 'm not sure what these papers mean by efficient . the neural network postulated have a sufficiently vast number of interacting elements and in effect do the combinatorics strictly , for each special case . but if so , while this is fast and does n't grow much with the size of the problem growing , is this really comparable to the normal meaning of pt as fast or efficient ? in these cases it seems the time efficiency is obtained by resource inefficiency : by making the network enormous and simulating all the possible worlds then maximizing . so , while time to compute does n't grow much as the problem grows , the size of the physical computer grows enormously for larger problems ; how fast it computes is then , it seems to me , not a good measure of efficiency of the algorithm in the common meaning of efficiency . in this case the resources themselves only grow as fast as the problem size , but what explodes is the number of connections that must be built . if we go from 1000 to 2000 neurons to solve a problem twice as large and requiring exponentially as much time to solve , the algorithms requiring only twice as many neurons to solve in pt seem efficient , but really , there is still an enormous increase in connections and coefficients that need be built for this to work . is my above reasoning incorrect ?",1366,1671,2018-03-15T19:24:40.977,2018-05-01T08:14:03.307,neural networks efficiently solve traveling salesmen problems ?,neural-networks algorithm deep-network efficiency time-complexity,2,2,1
545,2875,1,,2017-02-25T21:57:43.563,4,141,for rules please refer to https://www.hackerrank.com/challenges/ultimate-ttt .i have implemented minimax search with alpha - beta pruning . there is a time limit of 15 seconds.which algorithms would yield better results ?,5706,,,2017-02-25T21:57:43.563,which algorithm is best for a 4 * 4 * 4 * 4 variant of ultimate tic - tac toe ?,algorithm,0,4,1
546,2876,1,2878,2017-02-26T01:04:39.363,10,537,"my understanding of the singularity is when artificial intelligence becomes "" more intelligence "" than humans . this will be achieved through machine learning where an ; algorithm , neural network ? exponential betters itself . so from that point on in near future after that we should predict that there will be artificial intelligence capable of answering any question . how to travel the fastest ... blueprints for spacecrafts ... drugs for medicine ... efficiency and advancements that will change the human condition . the singularity is predicted 2040s or 2030 . all be it a couple of years later down to exponential growth in knowledge . so if what i 'm saying is right i should be seeing crazy hype and news coverage as well as advancements but i do n't . i do n't understand what is wrong with the idea that the ai will be capable of omniscience . so can it ? is there something preventing it ? i do n't see how so logically . as my philosophy has been that research in all the scientific fields are long and expensive . the prospect of a ai that could perform research at fractional cost and time is the way to go . i hope to work in a field that works at achieving singularity and so will in turn change the world . with the ideas and discoveries it will have . and where does "" artificial "" consciousness come in to play in the singularity",5708,,,2018-08-02T10:08:22.643,the singularity and future of civilisation,machine-learning philosophy singularity cognitive-science,2,0,3
547,2886,1,2889,2017-02-26T22:22:00.153,4,120,"i 'm doing a little tic - tac - toe project to learn neural networks and machine learning ( beginner level ) . i 've written a mlp based program that plays with other search based programs and trains with the data generated from the games . the training and evaluation are strictly policy based - inputs are board positions and outputs are one - hot encoded array that represents the recommended move for that board position . i 've not added search algorithms so that i can understand what to expect from a purely mlp approach . the mlp model has 35 features and 1 hidden layer and after a few hundred thousands games it has sort of learned to draw 50 % games . it has learned the basic stuff like how to block the player from winning and some good board placements . now , my question is - it has n't learned advanced strategies that require making a move that may not be as beneficial for the current move but will improve its chances later . but should i expect that from a strictly policy mlp based no - search approach ? since all that it is being trained on is one board and the next recommended move ( even if thousands of those pairs ) , is it logical to expect it to learn a lookahead approach that goes beyond "" the best move for the current board "" training ? put another way , would it be a possible at all for a mlp to learn lookahead without any search strategies ? if not , are there any alternatives that can do it without search ?",1522,,,2017-02-28T08:19:14.450,is it a valid assumption that a purely mlp based tic - tac - toe player will learn lookahead strategies ?,machine-learning mlp,1,5,0
548,2890,1,,2017-02-28T09:30:15.773,4,84,"today we have neural network based ai players that are comparable or better than humans in games that require extensive pattern matching and "" intuition "" . alphago is a prime example . but these ai players usually have both neural networks and search algorithms in place . humans , on the other hand , rely just on the pattern matching and "" intuition "" ( even the best chess players can see just a handful of moves ahead ) . so , why do ai players still require extensive search while humans do n't ? how would ais like alphago perform if we take the search part out ?",1522,,,2017-02-28T13:12:34.390,why do neural networks based ai players still require extensive search techniques ?,neural-networks,1,2,
549,2891,1,,2017-02-28T10:22:17.403,5,476,"i am going to develop an open - domain natural language question answering ( nl qa ) system , and will use the support vector machine ( svm ) as the machine - learning ( ml ) algorithm for question classification . the data on hand , is from a cube , containing multiple dimensions , of which some contain hierarchies . i do not understand how to work / combine the taxonomy and svm for question classification . if i understand correctly , the taxonomy still needs to be developed by hand , unless an existing one is being used . and the svm sorts the queried nl question based on this taxonomy ? is this correct , or am i mixing the whole concept ?",5219,5219,2017-03-01T13:26:32.630,2017-03-02T16:29:14.580,how do i use a taxonomy and the support vector machine for question classification in natural language processing ?,machine-learning algorithm natural-language-processing,1,6,1
550,2897,1,,2017-03-01T09:52:51.610,2,74,"a lot of experts have expressed concerns about evil super intelligence . while their concerns are valid , is it necessary , what are the chances or how the artificial super - intelligence will evolve to have selfishness and self protecting desires inherent in biological systems ? is there any work which comments on this line of inquiry ?",5765,,,2017-03-01T16:55:13.723,will artificial super - intelligence evolve to have selfishness inherent in biological systems ?,unsupervised-learning,2,3,1
551,2900,1,,2017-03-01T18:53:00.513,2,115,"post singularity ai will surpass human intelligence . the evolution of ai can take any direction , some of which may not be preferable for humans . is it possible to manage the evolution of super - intelligent ai ? if yes , how ? one way i can think of is following . instead of having a mobile ai like humanoid , we can keep it immobile , like a box , like current super computers . it can be used to solve problems of maths , theoretical science etc .",5765,5765,2017-03-02T13:19:55.283,2017-03-11T02:17:56.760,is it possible to manage the evolution of super - intelligent ai ?,singularity,4,0,1
552,2902,1,,2017-03-02T12:58:31.380,2,129,an argument against the possibility of super - intelligence is that the intelligence of a creation will be limited by the intelligence of its creator . how reasonable is this argument ?,5765,2444,2019-05-02T13:21:36.957,2019-05-02T13:21:36.957,is a super - intelligence limited by the intelligence of its creator ?,philosophy strong-ai agi superintelligence,2,0,
553,2906,1,,2017-03-02T20:53:58.320,0,135,"when trying to run tensorboard locally to show my logs with tensorboard --logdir logs/ it always shows nothing but the regular tensorboard menu options , such as orange bar at the top , and different section buttons at the top like graphics , etc . however never shows any data regarding my agents . i am using tensorflow 0.11",5801,1671,2018-06-25T17:43:42.013,2018-06-25T17:43:42.013,tensorboard problems,neural-networks machine-learning deep-learning reinforcement-learning tensorflow,1,1,
554,2908,1,,2017-03-03T03:36:17.760,4,888,"is it possible to classify data using a genetic algorithm ? for example , would it be possible to sort this database ? ( https://archive.ics.uci.edu/ml/datasets/spambase ) examples in matlab ?",5806,1671,2018-01-28T22:51:57.737,2018-01-28T22:51:57.737,is it possible to classify data using a genetic algorithm ?,algorithm genetic-algorithms genetic-programming matlab data-science,2,2,
555,2910,1,,2017-03-03T10:43:29.217,3,177,"i have not studied machine learning or ai really , but my job sometimes requires me to automate stuff . right now the requirement i have , seems to be under ai domain , but i am not sure about terminologies or how to go about it . i will really appreciate if someone can guide me about the direction i need to start from . ( ps : this question might not belong on this se , in that case please direct me to suitable se ) what i 'm required to do is find references on web about a certain situation . as an example i 'll use "" music "" , so i have to make a system which will search around the web ( google and twitter mainly ) to see if there is any news / mention / event related to music that occurred today , if so how many references ( i.e. how big of a deal it is making ) . it is not the generic term music which is expected in the output , but the names of musicians , i.e. in music this and this artist appeared this many times . i have to give the number of references , and also provide the references in output so that one can read them in detail . the challenges are - one event can be covered by many websites , and there can be one main website that published the original story with full details , while others just spread the word around in summarized way . how do you filter references to pick the most suitable one , to show in results to the system 's user , because i can not give user ~50 references to manually read through , i have to give like 1 - 2 suitable reference - i need to give the name of the artist . one site will have many words , how do i know which word is actually the artist 's name ? one option can be to have a pre compiled list of specific artists and just search for them individually . but this way , i can be missing new artists . the challenges i have , must have been addressed by some existing algorithm or mechanism , i 'll appreciate if someone can let me know what kind of algo etc i need to refer to or study to get the task done .",5814,,,2017-11-28T23:39:49.033,searching keywords on web,ai-design algorithm definitions intelligent-agent reference-request,1,1,
556,2911,1,2916,2017-03-03T11:08:49.327,8,156,"this has been niggling me a while , so i decided to ask . sorry if it 's wordy , i 'm not sure how to express it ! it seems fairly uncontroversial to say that nn based approaches are becoming quite powerful tools in many ai areas - whether recognising and decomposing images ( faces at a border , street scenes in automobiles , decision making in uncertain / complex situations or with partial data ) ..... almost inevitably some of those uses will develop into situations where nn based ai takes on part or all of the human burden and generally does it better than people generally do . examples might include nn hypothetically used as steps in self driving cars , medical diagnosis , human / identity verification , circuit / design verification , dubious transaction alerting ... probably many fields in the next decade or so . suppose this happens , and is generally seen as successful ( eg it gets diagnoses right 80 % to human doctors ' 65 % or something , or cars with ai that includes an nn component crash 8 % less than human driven cars or alternatives , or whatever ... ) now - suppose one of these aberrantly and seriously does something very wrong in one case . how can one approach it ? with formal logic steps one can trace a formal decision process , but with nn there may be no formal logic , especially if it gets complex enough ( in a couple of decades say ) , there are just 20 billion neural processors and their i / o weightings and connections , it may not be possible to determine what caused some incident even if lives were lost . it also may not be possible to say more than the systems continually learn and such incidents are rare . i also have n't heard of any meaningful way to do a "" black box "" or flight recorder equivalent for nns , ( even if not used i a life critical case ) , that would allow us to understand and avoid a bad decision . unlike other responses to product defects , if a nn could be trained after the event to fix one such case , it does n't clearly provide the certainty we would want , that the new nn setup has fixed the problem , or has n't reduced the risk and balance of other problems in so doing . it 's just very opaque . and yet , clearly , it is mostly very valuable as an ai approach . so what 's the answer ? is there one ? in 20 years if nn is an ( acknowledged as safe and successful ) component in a plane flight or aircraft design , or built into a hospital system to watch for emergencies , or to spot fraud at a bank , and has as usual passed whatever regulatory and market requirements might exist and performed with a good record for years in the general marketplace , and then in one case such a system some time later plainly mis - acts on one occasion - it damgerously misreads the road , recommends life - damaging medications or blatantly missdiagnoses , or clears a blatant £ 200 m fraudulent transaction at a clearing bank that 's only caught by chance before the money is sent - what can the manufacturer do to address public or market concerns , or to explain the incident ; what do the tech team do when told by the board "" how did this happen and make damn sure it 's fixed "" ; what kind of meaningful logs can be kept , etc ? would society have to just accept that uncertainty and occasional wacky behaviour could be inherent ( good luck with convincing society of that ! ) ? or is there some better way to approach logging / debugging / decision activity more suited to nns ?",5817,5817,2017-03-03T11:31:16.643,2017-03-03T22:53:41.833,"if a neural network approach becomes widely used within a real - world situation , how would one debug / understand / fix the outcome if in one case poor ?",neural-networks machine-learning applications security,1,0,3
557,2912,1,,2017-03-03T13:01:56.950,1,55,"i 've been experimenting with a simple tic - tac - toe game to learn neural network programming ( mlp and cnns ) with good results . i train the networks on a board positions and the best moves and the network is able to learn and correctly predict the best moves to make when it encounters those board positions . but the network is unable to "" discover "" newer patterns / features from existing ones . for example - let 's say that the board position is below and move is for the x player ( ai ) o _ _ _ o _ _ _ _ the recommended move would be 8 ( 0 based indices ) so that the opponent does n't win , the resulting board would be - o _ _ _ o _ _ _ x if i train the network on the above enough times , the ai ( mlp or cnn based ) learns to play 8 when it encounters the above situation . but it does n't recognize the below as variations ( rotated and shifted , respectively but slanted straight lines in general ) of the same pattern and is not able to correctly pick 6 and 0 , respectively - _ _ o _ _ _ _ o _ or _ o _ etc _ _ _ _ _ o my question is - should i expect cnns to be able to discover new previously untrained on patterns / features such as above ?",1522,,,2017-03-03T13:01:56.950,can a cnn or mlp discover similar but untrained - on patterns ?,neural-networks convolutional-neural-networks,0,6,
558,2917,1,,2017-03-04T10:30:12.610,3,350,"does it exist a human - like or overintelligent ai ? human - like i define as something that can act as a human in most aspects . for example , is it "" common knowledge "" that there actually exists an overintelligent or human - like ai ? or could you say that there do not exist an overintelligent or human - like ai ?",5832,5832,2017-03-04T12:18:17.803,2017-03-11T23:39:05.963,does it exist a human - like or overintelligent ai ?,research intelligent-agent emotional-intelligence human-like ultraintelligent-machine,2,0,
559,2919,1,2931,2017-03-04T15:16:46.440,6,429,"from artificial intelligence : a modern approach , third edition , chapter 26 : note that the concept of ultraintelligent machines assumes that intelligence is an especially important attribute , and if you have enough of it , all problems can be solved . but we know there are limits on computability and computational complexity . if the problem of defining ultraintelligent machines ( or even approximations to them ) happens to fall in the class of , say , nexptime - complete problems , and if there are no heuristic shortcuts , then even exponential progress in technology wo n't help — the speed of light puts a strict upper bound on how much computing can be done ; problems beyond that limit will not be solved . we still do n't know where those upper bounds are . if the textbook 's argument is correct , then there may be a strict upper bound to "" intelligence "" , meaning that the potential / damage of ultra - intelligent machines is limited . however , it is contingent on there actually being a theoretical maximum for "" intelligence "" . is there any literature that suggest that we know for sure whether such a maximum exist ? is the existence of that maximum dependent on our definition of "" intelligence "" ( so adopting a vague and hand - wavey definition would imply no theoretical maximum , while adopting a strict and formalized definition would imply a theoretical maximum ) ? note : question was previously posted during the definition phase of this site on area51 by pkhlop .",181,,,2017-03-06T16:32:17.700,is there a theoretical maximum for intelligence ?,definitions ultraintelligent-machine,1,1,
560,2920,1,,2017-03-04T18:27:37.373,4,629,"i am looking for a solution that i can use with identifying cars . so i have a database with images of cars . about 3 - 4 per car . what i want to do is upload a picture to the web of car(picture taken with camera / phone ) and then let my pc recognize the car . example : lets say i have these 2 pictures in my database(mazda cx5)(i can only upload 2 links at max . atm . but you get the idea ) . now i am going to upload this picture of a mazda cs5 to my web app : now i want an ai to recognize that this picture is of an mazda cx5 with greyish color . i have looked on the net and found 2 interesting ai 's i can use : tensorflow and clarifai , but i do n't know if these are going to work so my question to you what would be my best bet to go with here ?",5837,,,2017-07-04T17:30:04.077,image recognition,image-recognition tensorflow,2,0,1
561,2922,1,,2017-03-05T07:07:25.330,12,697,"nowadays artificial intelligence seems almost equal to machine learning , especially deep learning . some have said that deep learning will replace human experts , traditionally very important for feature engineering , in this field . it is said that two breakthroughs underpinned the rise of deep learning : on one hand , neuroscience , and neuroplasticity in particular , tells us that like the human brain , which is highly plastic , artificial networks can be utilized to model almost all functions ; on the other hand , the increase in computational power , in particular the introduction of gpu and fpga , has boosted algorithmic intelligence in a magnificent way , and has been making the models created decades ago immensely powerful and versatile . i 'll add that the big data ( mostly labeled data ) accumulated over the past years is also relevant . such developments bring computer vision(and voice recognition ) into a new era , but in natural language processing and expert systems , the situation has n't seemed to have changed very much . achieving common sense for the neural networks seems a tall order , but most sentences , conversations and short texts contain inferences which should be drawn from the background world knowledge . thus knowledge graphing is of great importance to artificial intelligence . neural networks can be harnessed in building knowledge bases but it seems that neural network models have difficulty utilizing these constructed knowledge bases . my questions are : 1 ) is a knowledge base ( for instance a "" knowledge graph "" as coined by google ) a promising branch in ai ? if so , in what ways kb can empower machine learning ? and how can it help in natural language generation ? 2 ) for survival in an age dominated by dl , where is the direction for the knowledge base ( or the umbrella term symbolic approach ) ? is wolfram -like z dynamic knowledge base the new direction ? or any new directions ? hopefully i am asking an appropriate question here , as i was unable to tag my question as "" knowledge base "" nor "" knowledge graph "" . am i missing something fundamental , or some idea that that addresses these issues ?",5351,5351,2018-09-14T01:13:10.833,2018-09-14T01:13:10.833,i wonder what roles the knowledge base play now and will play in the future ?,natural-language-processing knowledge-representation symbolic-computing expert-system,1,5,6
562,2924,1,3158,2017-03-05T20:25:02.817,1,84,"i know i 've seen this somewhere before , but ca n't find it now . say we have a neural network with a handful of layers , and we 're applying dropout to each layer . as we move closer to the output , should dropout decrease , increase , or stay the same ?",5857,,,2017-04-14T14:28:26.940,how should dropout change with network depth ?,neural-networks,1,0,1
563,2925,1,2935,2017-03-06T00:28:31.647,2,109,"i want to write a program that looks at abbreviated words , then figures out what the words are . for example , the abbreviation is "" blk comp "" , and the translation is "" black computer "" . in order to give it context for more ambiguous terms , i will be inputting sets of words with each request . so , if i input the set "" keyboard , software , mouse , monitor "" , i would expect to get "" black computer "" . on the other hand , if i input "" honda , transmission , mileage , ford "" , i then would expect to get "" black compact "" , or at least something that has anything to do with cars . basing on the above case scenario , what kind of an algorithm should be applied in this case ?",5860,1581,2017-03-11T14:15:30.310,2017-03-11T14:15:30.310,recognition of abbreviated text,neural-networks deep-learning algorithm genetic-algorithms learning-algorithms,1,0,
564,2926,1,,2017-03-06T07:42:26.063,6,3624,"i want an algorithm ( predictive machine learning , mostly ) to identify patterns in my csv file without the user specifying any conditions . what can i use ?",5867,75,2017-04-20T13:15:17.217,2018-12-08T18:47:06.163,is there any machine learning algorithm that can identify pattern(s ) in a csv file without the user specifying any conditions ?,machine-learning algorithm unassisted-learning,5,2,1
565,2927,1,2934,2017-03-06T10:00:44.500,6,579,"i 've been reading a lot about hardware development and implementation for ai / ml , mainly about deep learning , and i have a question about its usage . from what i understand , there are 2 stages for dl : first is training and second is inference . the first is often done on gpus because of their massive parallelism capabilities among other things , and inference , while can be done on gpus , it 's not used that much , because of power usage , and because the data presented while inferring are much less so the full capabilities of gpus wo n't be much needed . instead fpgas and cpus are often used for that . my understanding also is that a complete dl system will have both , a training system and an inferring system . my question is that : are both systems required on the same application ? let 's assume an autonomous car or an application where visual and image recognition is done , will it have both training system to be trained and an inference system to execute ? or it has only the inference system and will communicate with a distant system which is already trained and has built a database ? also , if the application has both systems , will it have a big enough memory to store the training data ? given that it can be a small system and memory is ultimately limited .",5873,145,2017-03-11T14:14:22.167,2018-10-22T03:43:47.680,machine learning hardware usage in embedded applications,machine-learning deep-learning image-recognition,3,1,2
566,2928,1,2929,2017-03-06T11:35:52.083,3,6891,"i 'm trying to create simple keras nn which will learn to make addition on numbers between 0 and 10 . but i am getting the error : valueerror : error when checking model target : expected activation_4 to have shape ( none , 19 ) but got array with shape ( 100 , 1 ) here is my code : from keras.models import sequential from keras.layers import dense , activation import numpy as np keras.optimizers.sgd(lr=0.01 , momentum=0.0 , decay=0.0 , nesterov = false ) model = sequential ( ) model.add(dense(output_dim=50 , input_dim=2 ) ) model.add(activation(""relu "" ) ) model.add(dense(output_dim=50 ) ) model.add(activation(""softmax "" ) ) model.add(dense(output_dim=50 ) ) model.add(activation(""softmax "" ) ) model.add(dense(output_dim=19 ) ) model.add(activation(""softmax "" ) ) model.compile(loss='categorical_crossentropy ' , optimizer='sgd ' , metrics=['accuracy ' ] ) x = [ ] y = [ ] for i in range(0 , 10 ) : for j in range(0 , 10 ) : x.append((i , j ) ) y.append(i + j ) x = np.array(x ) y = np.array(y ) print(x ) print(y ) model.fit(x , y , nb_epoch=5 , batch_size=32 ) how to fix that ?",5661,,,2017-03-06T14:26:28.900,"keras valueerror : error when checking model target : expected activation_4 to have shape ( none , 19 ) but got array with shape ( 100 , 1 )",neural-networks keras,2,1,
567,2932,1,3319,2017-03-06T16:22:04.373,0,6406,"i am trying to do an inception layer , but it only works if the convolution strides , pool strides and pool size are the same , otherwise i get an error in tf.concat that dimesion 1 is not the same . so if i change something in the last three tuples , i get the error . conv1 = conv2d_maxpool(x , 64 , ( 5 , 5 ) , ( 1 , 1 ) , ( 2 , 2 ) , ( 2 , 2 ) ) conv2 = conv2d_maxpool(x , 64 , ( 4 , 4 ) , ( 1 , 1 ) , ( 2 , 2 ) , ( 2 , 2 ) ) conv3 = conv2d_maxpool(x , 32 , ( 2 , 2 ) , ( 1 , 1 ) , ( 2 , 2 ) , ( 2 , 2 ) ) conv4 = conv2d_maxpool(x , 32 , ( 1 , 1 ) , ( 1 , 1 ) , ( 2 , 2 ) , ( 2 , 2 ) ) conv = tf.concat([conv1 , conv2 , conv3 , conv4 ] , 3 ) for example , this is the error i get if i change the 5x5 filter to have strides 3 : conv1 = conv2d_maxpool(x , 64 , ( 5 , 5 ) , ( 3 , 3 ) , ( 2 , 2 ) , ( 2 , 2 ) ) dimension 1 in both shapes must be equal , but are 6 and 16 for ' concat ' ( op : ' concatv2 ' ) with input shapes : [ ? , 6,6,64 ] , [ ? , 16,16,64 ] , [ ? , 16,16,32 ] , [ ? , 16,16,32 ] , [ ] . this is the conv2d_maxpool function : def conv2d_maxpool(x_tensor , conv_num_outputs , conv_ksize , conv_strides , pool_ksize , pool_strides ) : "" "" "" apply convolution then max pooling to x_tensor : param x_tensor : tensorflow tensor : param conv_num_outputs : number of outputs for the convolutional layer : param conv_strides : stride 2-d tuple for convolution : param pool_ksize : kernal size 2-d tuple for pool : param pool_strides : stride 2-d tuple for pool : return : a tensor that represents convolution and max pooling of x_tensor "" "" "" # todo : implement function weights = tf.variable(tf.truncated_normal ( shape = [ * conv_ksize , int(x_tensor.get_shape().dims[3 ] ) , conv_num_outputs ] , mean = 0.0 , stddev=0.1 , dtype = tf.float32 ) ) bias = tf.variable(tf.zeros(conv_num_outputs ) ) conv_layer = tf.nn.conv2d(x_tensor , weights , strides=[1 , * conv_strides , 1 ] , padding='same ' ) conv_layer = tf.nn.bias_add(conv_layer , bias ) conv_layer = tf.nn.relu(conv_layer ) conv_layer_max_pool = tf.nn.max_pool(conv_layer , ksize=[1 , * pool_ksize , 1 ] , strides=[1 , * pool_strides , 1 ] , padding='same ' ) return conv_layer_max_pool how can i combine convolution filters with different strides and/or different pooling to create an inception layer ?",5527,,,2017-05-16T10:25:38.073,concatenate convolution layers with different strides in tensorflow .,deep-learning convolutional-neural-networks tensorflow,2,0,0
568,2936,1,,2017-03-07T11:33:05.157,2,360,"i 'm a newbie in machine learning , so excuse me in advance ) . i have an idea to make nn that can estimate visual pleasantness of arbitrary image . like you have a bunch of images that you like , you train nn on them , then you show some random picture to nn and it estimates whether you 'll like it or not . i wonder if there is any pervious effort made in this direction .",5899,,,2017-04-07T08:34:33.887,training neural network for good taste in art,neural-networks machine-learning deep-learning image-recognition convolutional-neural-networks,4,0,
569,2940,1,,2017-03-07T20:08:04.537,6,227,"by "" neural network "" , i mean the typical , multilayered neural network with inputs , weights , hidden nodes and outputs , as shown in the image below : such neural networks , in the context of evolving neural networks , can be characterized by the fact that all weighted connections between nodes are all present at the beginning , and can each be represented as a continuous real number . also , if such a network is used as an agent 's brain , the agent 's response will be calculated immediately after receiving a set of stimuli . i want to know if there is any other systems of information processing that do not rely this structure . for example , is there any system in which the topology of a neural network is variable ? or a system in which links between nodes are not real numbers ?",1321,4302,2018-07-24T10:16:07.233,2018-07-24T10:16:07.233,what are some alternative information processing system beside neural network,neural-networks evolutionary-algorithms topology,4,4,0
570,2941,1,,2017-03-07T21:59:15.663,3,455,"i would like to detect street and sidewalk surface in a very detailed ( 0.075m / pix ) usgs high resolution orthoimagery which basically means image segmentation with two classes . places in question are residential areas similar to this one . i will download uncompressed raw imagery in geotiff from usgs for the detection . i read that neural networks can perform very good in image segmentation and i would like to try them . i am a developer by day so i can code but am a beginner to neural networks only knowing the basic principles about architecture , weighting and backpropagation etc . is it possible to jump right in into my task or do i need to start with something simpler ? i would prefer jumping right in if it can save time . i skimmed though few papers dealing with similar thing and they seem quite complicated . is there some simple way i can get started ? i mean maybe an open source project in neural networks that deals with image segmentation that is similar to my task and i could make use of it ? i see neural networks need to be trained first and i am prepared to do manual segmentation first to have data for training . however , i have no idea about neural network design / architecture , how to design the layers , how many layers do i need etc . i also would like to use the fact that the network would learn some basics on how streets and sidewalks are built - that they are ( not sure if my term is correct ) "" linear structures "" which usually run many meters in length and may not even end in the image , also that sidewalks usually run alongside streets , streets have intersections etc .",113,113,2017-03-07T22:55:35.283,2017-03-09T15:34:17.910,detect street and sidewalk surface in aerial imagery ( neural network ),neural-networks machine-learning deep-learning image-recognition detecting-patterns,1,0,
571,2942,1,,2017-03-08T07:51:37.170,6,419,"i 've been reading a lot about dl . i can understand to an extent how it works , in theory at least , and how it 's technically different from conventional ml . but what i 'm looking for is more of a "" conceptual "" meaning . let 's say you 're designing a self - learning system , why would you choose dl ? what are the main performance advantages that dl offers ? is it the accuracy , speed , power efficiency , a mix of all of them ?",5873,2444,2019-03-14T10:14:53.603,2019-03-14T10:14:53.603,what does deep learning offer with respect to standard machine learning ?,machine-learning deep-learning applications comparison performance,4,0,3
572,2950,1,2952,2017-03-09T16:36:14.807,3,96,"i want to train text classifier ( using https://www.uclassify.com ) with 12 classes / categories . i will be training it to classify news / articles ( i know that there are existing classifier but i want to train my own ) . uclassify uses following algorithm ( directly copied from their site ) : the core is a multinominal naive bayesian classifier with a couple of steps that improves the classification further ( hybrid complementary nb , class normalization and special smoothing ) . the result of classifications are probabilities [ 0 - 1 ] of a document belonging to each class . this is very useful if you want to set a threshold for classifications . e.g. all classifications over 90 % is considered spam . using this model also makes it very scalable in terms of cpu time for classification / training . i was wondering how many examples i will need to train such classifier ? it is possible to estimate the number ? let 's assume that one article will "" fit "" 2 categories by average .",5935,,,2017-03-09T17:55:35.353,how many training example text classifier needs to be trained ?,classification,1,0,
573,2953,1,,2017-03-09T18:09:04.410,3,77,"how would one go about building an ai that is capable to look at any kind of input and then identify what is the nature of this data ? for example , an ai that is able to do image classification , nlp and react to some other sensors . is it possible to build an ai that will be able to identify what kind of data it is seeing such that it can send the data to the correct model for it to be treated . similarly , to the how the human brain knows to send visual information to the visual cortex and auditory information elsewhere . in a simple scenario , i think we can get very good performance by having a cascaded image classifier . for example 2 layers , the first identifies if the image contains a dog and a cat . the next layer , has two different cnns , one trained to identify the breed of dog and the other one for cats . that way once we identify that we have a dog , the image can be sent to the correct cnn . a cnn that is trained specifically to detect the breed , thus being much more robust that a more generalized cnn . kind of like a professional in the field . first the human identifies that he is looking at a dog then he consults a professional to ass him the breed . i would like to extend this idea to being able to identify various kinds of data sources that do not resemble each other at all . various input . are there any models that can do this ?",5925,,,2017-09-10T12:40:25.233,using ai to interpret the nature a specific input and use the correct model .,classification intelligence-testing ultraintelligent-machine,1,0,
574,2955,1,3184,2017-03-10T00:09:02.040,8,379,"what 's the term ( if such exists ) for merging with ai ( e.g. via neural lace ) and becoming so diluted ( e.g. 1:10000 ) that it effectively results in a death of the original self ? it 's not quite "" digital ascension "" , because that way it would still be you . what i 'm thinking is , that the resulting ai with 1 part in 10000 being you , is not you anymore . the ai might have some of your values or memories or whatever , but it 's not you , and you do n't exist separately from it to be called you . basically - you as you are dead ; you died by dissolving in ai . i would like to read up on this subject , but ca n't find anything .",5947,7488,2017-06-17T21:23:59.583,2019-03-07T23:01:38.950,what 's the term for death by dissolving in ai ?,strong-ai terminology control-problem,3,7,1
575,2957,1,3304,2017-03-10T06:52:41.560,6,151,"human beings are more productive in groups than individually , possibly due to the fact that there is a limit to how much one human brain can improve itself in terms of speed of computation and areas of expertise . by contrast , if a machine with general - purpose artificial intelligence is created and then assigned a task , would it be possible that the machine will be able to better accomplish its task by continuously improving its own computational power and mastery of various skills , as opposed to collaborating with other agents ( whether copies of itself , other ai 's , or even humans ) ? in other words , would an agi ever need to collaborate , or would it always be able to achieve its goals alone ?",1321,33,2017-03-12T02:21:57.833,2017-05-11T07:27:54.420,would a general - purpose ai need to collaborate ?,comparison multi-agent-systems swarm-intelligence,2,0,
576,2959,1,2960,2017-03-10T13:59:43.357,8,406,"according to wikipedia , citations omitted : in the history of artificial intelligence , an ai winter is a period of reduced funding and interest in artificial intelligence research . the term was coined by analogy to the idea of a nuclear winter . the field has experienced several hype cycles , followed by disappointment and criticism , followed by funding cuts , followed by renewed interest years or decades later . the wikipedia page discusses a bit about the causes of ai winters . i 'm curious however whether it is possible to stop an ai winter from occurring . i do n't really like the misallocation of resources that are caused by over - investment followed by under - investment . one of the causes of the ai winter listed on that wikipedia page is "" hype "" : the ai winters can be partly understood as a sequence of over - inflated expectations and subsequent crash seen in stock - markets and exemplified by the railway mania and dotcom bubble . in a common pattern in development of new technology ( known as hype cycle ) , an event , typically a technological breakthrough , creates publicity which feeds on itself to create a "" peak of inflated expectations "" followed by a "" trough of disillusionment "" . since scientific and technological progress ca n't keep pace with the publicity - fueled increase in expectations among investors and other stakeholders , a crash must follow . ai technology seems to be no exception to this rule . and it seems that this paragraph indicates that any new technology will be stuck in this pattern of "" inflated expectations "" followed by disillusionment . so are ai winters inevitable ? that ai technologies will always be overhyped in the future and that severe "" corrections "" will always will always occur ? or can there a way to manage this hype cycle to stop severe increases / decreases in funding ?",181,,,2017-03-14T22:38:13.353,"are "" ai winters "" inevitable ?",history,4,4,
577,2964,1,,2017-03-11T09:48:32.563,0,684,does artificial intelligence write its own code and then execute it ? if so does it create separate functions(for each purpose ) for its code ? how does learning get implemented in artificial intelligence ? is there a specific flowchart to describe artificial intelligence,5972,1671,2019-05-02T01:45:01.437,2019-05-02T01:45:01.437,does artificial intelligence write its own code ?,machine-learning ai-basics theory unassisted-learning praxis,2,6,1
578,2967,1,5047,2017-03-11T16:56:49.653,2,108,"as far as i know mdp are independent from the past . but the definition says that the same policy should always take the same action depending on the state . what if i define my state as the current "" main "" state + previous decisions ? for example in poker the "" main "" state would be my cards and the pot + all previous information about the game . would this still be a mdp or not ?",4550,4550,2017-03-11T17:11:20.420,2018-01-17T22:37:58.363,can an markov decision process be dependent on the past ?,definitions markov-chain,2,1,1
579,2973,1,,2017-03-12T18:05:51.793,3,88,"i 'm looking for ai systems or natural language processors that use , in the classification and interrelation of notions / objects , some philosophical system , like basic laws of logic , kantian or empiricism . i have also read about goal - seeking procedures . are these based on psychology fields and some particular psychology theory or these are ad hoc experiments , with only general terms applied ?",,2444,2019-05-01T21:40:32.640,2019-05-01T21:40:32.640,what are examples of ai that use philosophy derived ontologies ?,classification philosophy logic ontology,1,0,
580,2974,1,,2017-03-12T18:30:34.533,0,278,"i 'm using a nn created with cntk 's simplenetworkbuilder to make choices ( specifically in board games ) . i specified relu as the layer type , so outputs can be arbitrary numbers . when evaluating a custom set of features , getting the "" choice "" of the function / model is simple : look for the output signal with the highest value . however , there are times when i wish to introduce some randomness and assign probabilities to each output signal , then select the choice based on each output 's probability . currently , what i 'm doing is manually normalizing all the output using a sigmoid function specified here : https://en.wikipedia.org/wiki/logistic_function then , i multiply them all by a scalar such that the sum total of all outputs is 1 . at this point , i pick a random number 0 .. 1 , and see where along the map it falls ; that is my selected choice . what i 'd like to know is , is there a better way ?",3702,,,2018-01-09T08:39:56.390,assigning probability to output of a relu network,neural-networks,1,2,
581,2976,1,2988,2017-03-13T12:46:50.537,1,80,"i have come across this domain via this wikipedia article : general game playing so , where are we when it comes to general game playing ai ? ( the wiki article does n't mention the recent advances and the achievements of this domain of research , except the annual games results . ) ps : i understand that this is a general project of the stanford logic group of stanford university , california . but since then , it has become an area of research in the domain of ai .",101,,,2017-03-14T18:19:13.963,what research has been done in the domain of “ general game playing ” ?,research gaming,1,0,
582,2980,1,2981,2017-03-14T14:26:07.403,19,4287,"i want to create an ai which can play five - in - a - row / gomoku . as i mentioned in the title , i want to use reinforcement learning for this . i use policy gradient method , namely reinforce , with baseline . for the value and policy function approximation , i use a neural network . it has convolutional and fully connected layers . all of the layers , except for the output , are shared . the policy 's output layer has $ 8 \times 8=64 $ ( the size of the board ) output unit and softmax on them . so it is stochastic . but what if the network produces a very high probability for an invalid move ? an invalid move is when the agent wants to check a square which has one "" x "" or "" o "" in it . i think it can stuck in that game state . could you recommend any solution for this problem ? my guess is to use the actor - critic method . for an invalid move , we should give a negative reward and pass the turn to the opponent .",6019,2444,2018-11-14T13:58:37.500,2019-01-13T16:19:35.610,how to handle invalid moves in reinforcement learning ?,machine-learning reinforcement-learning game-ai combinatorial-games,4,0,8
583,2989,1,,2017-03-14T18:26:42.837,0,84,"to create language flashcards i would like to split an audio course into many single audio clips . they 're basically a man and a woman speaking after each other . the intervals are n't regular so it 's not possible to split it after time intervals . furthermore silence detection is not possible since some sentences also include pauses . i have already tried diarization using lium but the timings were completely wrong . additionally the audio course includes a transcript which would certainly be machine - readable consisting of the english sentence and the japanese sentence as well as its romaji version ( japanese words using english letters ) . i 'm not experienced in ai , so i 'm looking for a solution which is n't to difficult ( like constructing and training my own neural network ) . i have some programming experience , so a mathematical approach would be fine . links : audio course : http://www.japaneseaudiolessons.com/download-japanese-lessons/ lium : http://lium3.univ-lemans.fr/diarization/doku.php/welcome",6024,4302,2018-10-08T12:51:29.403,2018-10-08T12:51:29.403,splitting audio consisting of male / female speaker into segments,voice-recognition,1,3,
584,2996,1,,2017-03-15T09:10:28.523,9,1114,"i heard several times that one of the fundamental / open problems of deep learning is the lack of "" general theory "" on it because actually we do n't know why deep learning works so well . even the wikipedia page on deep learning has similar comments . are such statements credible and representative of the state of the field ?",6039,75,2017-03-15T14:47:05.483,2017-03-20T16:10:58.973,is there actually a lack of fundamental theory on deep learning ?,deep-learning,4,1,6
585,2999,1,3005,2017-03-16T02:53:19.430,7,1655,"i had been reading that ai could solve planet 's major problems . how could it be done ? for example , how exactly could ai be applied to address climate change ? what are examples of applications of ai to solve these problems ?",4460,2444,2018-07-05T18:46:38.173,2018-07-05T18:46:38.173,how could ai solve planet 's major problems ?,applications,1,0,4
586,3002,1,3003,2017-03-16T11:41:46.100,13,802,"for example , for classifying emails as spam , is it worthwhile - from a time / accuracy perspective - to apply deep learning ( if possible ) instead of another machine learning algorithm ? will deep learning make other machine learning algorithms like naive bayes unnecessary ?",5388,2444,2019-05-29T22:45:22.377,2019-05-29T22:52:11.610,when is deep learning overkill ?,machine-learning deep-learning classification applications comparison,2,0,6
587,3006,1,,2017-03-16T23:31:29.750,7,316,"since the first industrial revolution machines have been taking the jobs of people and automation has been a part of human social evolution for the past 3 centuries , but all in all these machines have been replacing mechanical , high - risk and low - skill jobs such as a production line of an automobile factory . but recently with the advent of computers and the improvement of ai , and the quest to find a singularity ( that is , a computer capable of thinking faster , better , more creative and cheaper then a human being , capable of self - improving ) , our future will lead to the replacement of not only low - skill workers , but high - skill as well . i 'm talking about a future not too far when ai and machines will replace artists , designers , engineers , lawyers , ceo 's , filmmakers , politicians , hell even programmers . some people get excited by this , but honestly i get somewhat scared . i 'm not talking about the money issue here , altough i 'm not a fan of the idea , let 's suppose the universal income has been implemented , and suppose it works fine . also not talking about the "" terminator 's world where machines will wage war against humans "" , let 's suppose too they are completelly friendly forever . the issue here is the one of motivation for us humans . when the ai singularity takes over , what will there be left for us to do ? everyday , all day long ? what are we going to do with our lifes ? suppose i love to paint , how can i live my dream of becoming a painter if computer make better art then i will ever be able to do ? how can i live knowing that no one will care about my paintings because they were made by a mere human . or the real me for exemple ( i , danzmann ) , i love to code , learned my first programming language with 9 years old and been on it ever since , it looks sad to me that in some years i may never touch on that again . and that goes for all the professions , everyone is passionate about something , and with the singularity , every single one of them would just have to cease to exist . so , what are we going to do in this future ? what am i going to do ? play golf all day , every single day for the rest of my life ( a hyperbole figure of speech , but you get my point ) ? also , what is going to be the motivation for my children ? what am i going to tell them to go to school ? when someone asks "" what do you wanna be when you grow up ? "" , and the inevitable answer is nothing . if highly advanced ai takes control of all scientific research , then what is the reason for us to learn ? what is the reason that us humans would need to dedicate decades of our lifes to learn something if that knoladge is useless , because there are no more jobs and the scientific research is done solely by ai ?",6084,,,2018-10-18T07:48:18.847,the social implications and the problem of motivation in an ai dominated future,philosophy unassisted-learning singularity social,6,10,2
588,3007,1,5584,2017-03-17T06:57:48.737,1,1670,how can artificial intelligence be applied to software testing ?,6091,33,2017-03-20T20:27:14.643,2018-03-29T20:54:52.237,how can ai techniques be used in software testing ?,machine-learning,3,4,1
589,3009,1,,2017-03-17T10:55:00.683,6,1989,can someone please explain the difference between memetic algorithms and genetic algorithms ? is an indivudal 's lifetime learning part of memetic algorithms ?,6095,145,2017-03-21T14:44:20.257,2018-07-12T09:59:16.067,what is the difference between memetic algorithms and genetic algorithms ?,genetic-algorithms optimization terminology,1,0,2
590,3013,1,3037,2017-03-18T04:32:47.590,4,538,"humans often dream of random events that occurred during the day . could the reason for this be that our brains are backpropagating errors while we sleep , and we see the result of these backpropagations as dreams ?",,1671,2018-01-27T21:14:35.073,2018-01-27T21:50:19.447,are dreams a form of backpropagation ?,neural-networks backpropagation cognitive-science theory,2,2,2
591,3024,1,3030,2017-03-21T13:17:30.623,0,180,"based on darwin 's statement , "" it is not the strongest that survives ; but the species that survives is the one that is able to adapt to and to adjust best to the changing environment "" . can economical constraints(not being able to afford for researches and developments ) or religious beliefs ( such as the belief of nothing can outperform the creations of god ) prevent third world countries from catching up to these evolutionary progresses ? if they could n't , would it result into the extinction of their societies ?",4416,1671,2018-06-04T19:35:12.403,2018-06-04T19:35:12.403,can singularity result into the extinction of the third world ?,philosophy agi social,1,0,
592,3032,1,,2017-03-22T19:19:13.410,4,763,"i need to solve the knapsack problem using hill climbing algorithm ( i need to write a program ) . but i 'm clueless about how to do it . my code should contain a method called knapsack , the method takes two parameters , the first is a 2xn array of integers that represents the items and their weight and value , and the second is an integer that represents the maximum weight of the knapsack . i can assume that the initial state is an empty knapsack , and the actions are either putting objects in the knapsack or swapping objects from in and out of the knapsack .",6192,2444,2019-03-02T10:58:01.860,2019-03-05T03:48:08.447,how do i solve the knapsack problem using the hill climbing algorithm ?,search hill-climbing knapsack-problem,1,0,
593,3039,1,,2017-03-24T03:52:17.570,1,74,"a phone can capture an image that lies on the front of the screen . it is also possible to manipulate input of the touchscreen using various programs and external devices . if we combine these two elements of technology together , it 's possible to have the phone aware of what 's happening on the screen and do dictated commands . this could be extremely useful to those who want to play a mobile game without the grinding effort , or those who do menial tasks on the phone but do n't want to spend the time navigating across the phone . for example , a phone could automatically open up an app on its own , press buttons that consistently appear on a screen for daily log in bonuses , auto - mode , etc , and collect the rewards for playing that app , without a human wasting time . such a behavior could even be recognized by the phone 's memory , and perhaps linked to ai ( like google now ) so that the behaviors could be remotely activated on command , or on a timer . now my question is - do we have apps that are able to recognize what happens on screen , and provide self input from the background ?",6220,,,2017-03-24T03:52:17.570,could we have a phone control itself ?,algorithm,0,0,1
594,3040,1,,2017-03-24T06:45:42.653,1,170,"i 'm attempting to develop a genetic algorithm capable of discovering classification rules for a given data set , a number of papers make use of the confidence ( precision ) and coverage of a rule to define its fitness . however i 'm not sure my understanding of the equations is correct . for example confidence is : conf = |p & amp ; d| / |p| and is defined as follows ; "" in classification problems , confidence measure is defined as the ratio of the number of examples in p that are correctly classified as decision class of d and the number of examples in p. "" is this saying , the total number of occurrences of the attributes in a given rule p which occur in rules which have been classified as class d , by the number of attributes in p ? where an example of a rule containing two attributes would be as follows : ( martial_status = married & amp ; age > 30 ) it seems a number of papers define it differently which has led to my confusion , if anyone is able to confirm my understanding or provide an some insight that 'd be great . edit : the research paper i 've been following can be found here .",6221,7550,2017-07-26T15:22:11.940,2017-12-23T18:41:07.820,ga rule discovery fitness function,classification genetic-algorithms evolutionary-algorithms,1,0,1
595,3046,1,3047,2017-03-26T04:37:36.603,3,152,"if a group of computers have identical ann with exact same set of learning data and all have functionality of encryption and decryption , would there be any way for interceptors to interpret encrypted data ? + applying the fact that people with more background information obtaining more knowledge from same source than those who do n't , would it be possible for ann to interpret data based on their access level ? ( each level has different amount of "" background information "" ) for example , if there is a encrypted text file , a computer with highest access level would fully decrypt the data to a plain text while a computer with lower access level would only decrypt half of them ( and this decrypted half becomes a plain text ) . if above methods can exist , what would be their pros and cons compared to pre - existing technologies ? ( aes , blowfish and so on )",4802,,,2017-03-26T08:03:31.273,can we apply ann to cryptography ?,neural-networks training,1,0,2
596,3050,1,,2017-03-27T10:21:55.150,2,40,"i try to fit a data matrix x to an output vector y with a regression model in sklearn . i have some training data and some test data , where the score is the rmse . so my best score i achieved with svr , kernel ' poly ' and tuning the hyperparameters ' c ' , ' degree ' and ' gamma ' with optunity and crossvalidation . i actually do n't know how to achieve better scores so i ask here in this forum for another ansatz . i tried already kernelridge , linear regression , svr with other kernels , neuronal networks but all of them gave worse results . it is actually possible to do better , since other people do better in this task , but i have no more idea what i can do to imporve the score . any ideas ?",6275,,,2017-03-27T10:21:55.150,sklearn regression problem,machine-learning,0,4,
597,3052,1,3053,2017-03-27T23:40:53.197,1,2440,"i am a php developer learning python for one reason , i wanna learn ai and i think that python would be better than php at that . i tried finding tutorials on how to build a neural network but they all use libraries . i am very interested in building the algorythm myself to understand how it actually works completly . i would use libraries once i have full understanding of how neural networks works . sorry if this is too broad . but any explanation of neural networks or examples ( without libraries ) are much appreciated . thanks",6295,,,2017-03-28T02:02:01.820,neural network algorythms without any libraries,neural-networks convolutional-neural-networks recurrent-neural-networks,1,4,
598,3058,1,3084,2017-03-28T15:37:46.353,5,200,"i was wondering about how recommendation on youtube work for example ? how are the algorithms applied , because every user gets different recommendations depending on his location , his past liked videos etc ... so it would seem like a training model is applied to every single user but i know that ca n't be possible so how are these recommendations so user - specific without applying a unique training model to every single user ?",6310,,,2017-04-02T17:35:22.680,how do big companies apply machine learning ?,machine-learning learning-algorithms probabilistic,1,1,4
599,3059,1,3061,2017-03-28T16:31:07.017,5,269,"while studying machine learning algorithms , i often see the term "" expectation - maximisation "" ( or em ) , and how it is used to estimate parameters , where the model depends on unobserved latent variables . the way i see it , it is like a probabilistic / statistical way to make predictions ( i think i 'm confusing something but this is the way i see it ) . which made me wonder how exactly does em differ from probabilistic classifiers like naive bayes or logistic regression ? is em something that exists on its own or is it employed within machine learning algorithms ? and , if we use naive bayes , for example , are we implicitly using em ?",6310,2444,2019-02-21T22:31:51.100,2019-02-21T22:31:51.100,what is expectation - maximization in machine learning ?,machine-learning expectation-maximization,1,0,2
600,3065,1,,2017-03-30T08:39:13.653,16,2469,"cross entropy is identical to the kl divergence plus entropy of target distribution . kl equals to zero when the two distributions are the same , which seems more intuitive to me than the entropy of the target distribution , which is what cross entropy is on a match . i 'm not saying there 's more information in one of the other except that a human view may find a zero more intuitive than a positive . of course , one usually uses a evaluative method to really see how well classification occurs . but is the choice of cross entropy over kl historic ?",6359,9687,2017-12-22T17:22:35.713,2018-11-22T06:59:06.650,why has cross entropy become the classification standard loss function and not kullbeck leibler divergence ?,machine-learning classification,2,0,2
601,3066,1,,2017-03-30T09:07:03.820,1,5816,"is it possible to create a complex self - learning ai "" ? and if it is n't possible , how do i achieve that ? where do i start and how do i begin ? thanks for your time :)",6360,-1,2018-04-02T17:07:45.480,2018-04-02T17:07:45.480,how to create a self learning ai ?,machine-learning ai-design unassisted-learning,1,5,
602,3071,1,,2017-03-30T19:57:13.017,3,120,"just wondering about the architecture of strong chess ai in a mobile , because networking is generally assumed by mobile developers , but not guaranteed .",1671,,,2017-03-31T02:12:38.807,are strong chess ai 's local on mobile devices ?,strong-ai software-architecture game-ai,1,0,
603,3072,1,,2017-03-30T20:10:30.987,3,414,"i 'm trying to get a gauge on just how big the programs and databases are these automata . i understand that this is a changing number , particularly in regard to machine learning . q : how large was deep blue when it beat gary kasparov ? q : how big was alphago when it beat lee sedol ?",1671,1671,2017-04-01T19:48:37.950,2017-04-22T03:18:20.370,what are the ( general ) sizes of alphago and deep blue ?,ai-design,1,2,
604,3073,1,,2017-03-30T21:17:36.700,2,65,"for instance strength / size*speed , where size and speed refer to memory and processing . we now have very strong , narrow ai , but they tend to run on fast hardware without volume restrictions . to understand why i 'm asking , this article on bbc may provide some insight : "" which life form dominates earth ? "" ( if i was a betting man , i 'd put money on tardigrades outlasting humans , and the secret of their success is that they require minimal resources and processing power , unlike higher - order automata . )",1671,,,2017-03-30T21:17:36.700,"is there a measure of ai relative strength , modified by resources ?",classification genetic-algorithms evolutionary-algorithms game-theory,0,2,
605,3075,1,,2017-03-31T20:35:49.167,1,182,"i have tried with two chat - bots "" clever bot "" and "" http://www.a-i.com/alan1/ "" and i got disappointing results . me : socrates is a man bot : blah blah ( common bot nonsense instead of an "" ok "" ) me : who is a man ? alan1 : the people that write my answers have n't provided an answer for this . another example of the mediocre "" clever bot "" me : socrates is the name of my dog . clever bot : i do n't know ! me : what is the name of my dog ? clever bot : that 's a nice name . ///// me : socrates is a man . clever bot : when does the narwhal bacon ? me : who is a man ? clever bot : men are man . and they dare name this thing "" clever "" ... so is there any chat bot that can actually answer this straightforward question ?",,,2017-03-31T20:52:59.910,2017-03-31T21:39:47.073,what chatbots can answer this type of ( simple ) question,reference-request,1,8,1
606,3077,1,,2017-03-31T21:40:51.667,7,185,i once came across a neural network being trained without back - propagation or genetic algorithms ( or using any kind of data sets ) . it was based on how the human brain learns and adjusts its connections between neurones . what is the name of such machine learning approach ?,6391,2444,2019-04-12T21:50:57.027,2019-04-12T21:52:13.650,what is the machine learning approach based on human learning ?,neural-networks machine-learning training terminology hebbian-learning,2,0,
607,3080,1,,2017-04-01T13:56:07.603,-2,587,"i am currently studying java ( se & amp;&amp ; ee ) . i am wondering if it is a good platform for developing ml algorithms for ai . areas of interest : facial rec - speech rec - understanding conversation in group conversations . financial institutions : risk assessment ml , etc .",6406,6798,2017-04-26T20:46:50.023,2017-04-26T21:00:06.250,java - a good place to begin if over all goal is ml and ai ?,machine-learning algorithm image-recognition,3,5,1
608,3081,1,,2017-04-01T14:19:59.177,9,573,"while studying data mining methods i have come to understand that there are two main categories : predictive methods : classification regression descriptive methods : clustering association rules since i want to predict the user availability ( output ) based on location , activity , battery level ( input for the training model ) , i think it 's obvious that i would choose "" predictive methods "" , but now i ca n't seem to choose between classification and regression . from what i understand this far , classification can solve my problem , because the output is "" available "" or "" not available "" . can classification provide me with the probability ( or likelihood ) of the user being available or not available ? as in the output would n't just be 0 ( not available ) or 1 ( for available ) , but it 's be something like : $ 80\%$ available $ 20\%$ not available can this problem also be solved using regression ? i get that regression is used for continuous output ( not just 0 or 1 outputs ) , but ca n't the output be the continuous value of the user availability ( like the output being $ 80 $ meaning user is $ 80\%$ available , implicitly the user is $ 20\%$ unavailable ) .",6310,2444,2019-05-17T23:57:33.547,2019-05-17T23:57:33.547,do i need classification or regression to predict the availability of a user given some features ?,machine-learning ai-design classification prediction regression,3,0,
609,3083,1,,2017-04-02T01:47:09.427,9,239,how powerful is the machine that beat the poker player champion recently ?,6411,,,2017-06-04T09:42:12.527,how powerful are the computers that power the most advanced artificial intelligence nowdays,neural-networks machine-learning,1,1,1
610,3085,1,,2017-04-02T13:12:44.267,4,93,"is augmented reality a training system for computer vision ? as in , augmented systems use their data to help train computer vision algorithms , or is augmented reality computer vision itself ?",6406,6406,2017-04-02T15:03:48.123,2017-04-02T15:03:48.123,can augmented reality be a training system for computer vision ?,algorithm computer-vision,1,0,1
611,3088,1,,2017-04-02T21:24:07.420,8,353,"i have came across the winograd shrdlu program and i found it very interesting and aspiring . what is the consensus regarding it ? are there any similar attempts ? i 'm reading the book of terry winograd understanding natural language where he discusses the functionality of the program , lisp language and more . i also found the linguist michael halliday and the linguistic theory systemic ( functional ) grammar which is mentioned in winograd 's book . are there any other ai / nlp that use this theory as a basis for the semantic functionality ? https://en.wikipedia.org/wiki/shrdlu",,4302,2018-10-08T12:29:35.040,2018-10-08T12:29:35.040,is there any modern nlp implementation similar to winograd shrdlu ?,natural-language-processing computational-linguistics lisp,2,0,1
612,3089,1,,2017-04-03T01:45:24.570,10,7174,what are bottleneck features ? ( mentioned here https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html ) . do they change with the architecture that is used ? are they final output of conv . layers before the fc layer ? why are they called so ?,35,11038,2018-02-22T20:18:47.010,2018-02-22T20:18:47.010,what are bottleneck features ?,terminology,2,4,6
613,3092,1,,2017-04-03T14:29:17.783,2,495,"i 'm implementing a c3d - inspired neural network for human emotion recognition , the problem i 'm facing is that altough the cost function is decreasing , for both training and validation sets , i do not appreciate any improvement in terms of accuracy , for neither of boths sets . my cost function is the cross - entropy between the logits ( output of the last layer ) and the correct prediction def tower_loss(name_scope , logit , labels ) : xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit,labels=labels ) cross_entropy_mean = tf.reduce_mean(xent ) return cross_entropy_mean then , the optimizer uses the adam algorithm for minimizing the cost function as follows loss = tower_loss(scope , logit , labels_placeholder ) train = tf.train.adamoptimizer(1e-4).minimize(loss ) although i 'm seing the cost function decreasing , i have n't seen any improvement in the classification . additional info : the xentropy of the validation set and the training set is not diverging . the xentropy looks like is on the way of converging to 0 . the accuracy is not wrongly implemented ( i see in the screen the outputs and the value is correct ) the network has been training now for 57.6k iterations ( not much , but enough to see some increment in the performance , or not ? ) any extra question you need to aske , please feel free , or missing information , please ask it . thanks a lot for all your time , and helping me with this problem .",6429,6429,2017-04-03T15:59:15.807,2017-04-03T15:59:15.807,"3d - cnn . why my cost function decreases , but the accuracy does not increase ?",training tensorflow convolutional-neural-networks,0,2,
614,3098,1,3100,2017-04-04T15:05:04.063,2,193,"in order to build a scientific reference parser , i am contemplating a kind of "" ai "" system , and would like to know if something similar is already an established "" design pattern "" in ai research . the input for the system would be scientific references with structures like the following : "" co - authors , title , journal , volume , issue , begin page , year "" of course , many other variations are possible , and i want to build a system that can make "" best guesses "" in case of unfamiliar patterns . at the moment this is done by manually chaining the results of different methods together , ranging from regex patterns to more complex algorithms like n - grams , lsh and random forests . i contemplate a ai system that automatically "" chains "" all these methods together in the most optimal way . the way i imagine this to work is by means of what i call "" a bag of functions "" . so , how would this work ? for each of the methods i use at the moment , i would specify their input requirements , and specify what they provide as output . ( e.g. : input = reference , output = title ) . these could also be parameters like : input = year , output = is valid year ? . note that if a function outputs "" title "" that is an attempt at providing the title , but this is not necessarily correct ( if for example , the regex pattern grabbed a wrong portion of text ) . for each of these functions , i would build a training set , and log their execution time ( cost ) and their probability of providing a correct result . then , i would build a system that chains these functions together to get from a certain input , to a certain output . eg : from -a function that takes a reference , and outputs the list of co authors- to -a function that takes a list of co authors , and breaks it apart into separate authors- to -a function that takes an author , and tries to break it apart into last name and initial- . once a "" chain "" of functions is found , this chain can in turn be stored as a "" function "" and can be reused later on by the algorithm . for each function , the success rate and run time is stored , so the algorithm can choose to go for the fastest known route , or experiment with new functions . in the settings you could specify the max run time ( cost ) or the minimum success rate . this way you could push the system to experiment with new combinations of functions . i 'm not sure i explained the intent clearly , and i 'm not sure the design would hold once i try to implement this in reality . just wanted to throw this out here to see if anyone recognizes the design . this feels like a combination of a shortest path algorithm ( to connect the functions ) with normal statistical probability ( to determine the success rate of a function ) with a "" self learning "" system ( because combinations of functions can be "" remembered "" and reused ) . the added advantage would be that i do n't need to manually guess what parsing method i should give a higher or lower likelihood of being correct in what specific scenario . it would allow me to just "" throw "" a new function into the bag , and let the system test it in all kinds of configurations , learning when best to use it , and when to avoid it . any feedback would be greatly appreciated ! :)",6451,,,2017-04-05T14:00:12.283,"does this "" flavor "" of ai have a name ?",machine-learning algorithm ai-design research,1,0,
615,3101,1,,2017-04-04T22:44:25.570,3,699,"is there any mathematical proof ( like in proof of a theorem ) based literature out there on neural networks ? everything is empirically based but no math proof for instance on why certain parameters work ? by mathematical proof , i mean which parameter works mathematically versus something which does not as in mathematical proof spelled out . this has nothing to do with empirical proof ( i.e. something works and here is our guesstimate on why )",6461,9687,2017-12-09T23:13:58.153,2018-10-22T08:30:39.993,is there any proof based literature out there on neural networks ?,neural-networks machine-learning deep-network convolutional-neural-networks,2,4,
616,3106,1,3107,2017-04-05T15:47:08.317,2,167,"i have read that all the math responsible for modern day machine learning and ai was already in place in 1900s but we did not have computational resources to implement those algorithms . so , is that true ? and if it is , in what areas of machine learning the researchers work ? and are all the future breakthroughs will be dependent only on increment of computational resources ?",3866,9647,2017-10-15T07:36:17.927,2017-10-15T09:07:59.047,are all breakthroughs in ai and machine learning are due to increase in computational resources ?,machine-learning deep-learning research,2,1,
617,3108,1,,2017-04-05T18:32:59.777,5,137,"i 'm looking at the history of nlp ( and by extension , machine learning ) and starting in the 1950s with the georgetown – ibm experiment . are there any particular studies nor projects done with natural language processing in the last 5 years , for instance;breakthroughs in parsing , sentiment analysis , discourse analysis and speech recognition , that you guys think are specifically influential",6477,1581,2018-10-30T16:56:26.070,2019-05-20T06:00:57.130,recommendations for research : influential nlp projects of the last 5 years,machine-learning natural-language-processing research,1,2,1
618,3109,1,3118,2017-04-05T18:33:05.277,2,149,"i am beginning an image analysis project to recognize images with a particular object centered on the image . if the object is at the center , i give the image a positive label , and if it is anywhere else , or simply not in the image , i give the image a negative label . the object , itself , has a complex pattern , such that statistical methods and basic image processing techniques are not able to detect it . the human eye , however , has no trouble detecting this object . therefore , i am opting to develop a convolutional network that can parse the complexity of this pattern . the only issue , however , is that convolutional networks are inherently designed to be spatially invariant . therefore , is it even possible to train the network to focus on the importance of the object being at the center simply by feeding the network many negative examples containing the object anywhere else but the center ? furthermore , is there perhaps a better or more direct way to go about incorporating this spatial aspect into the network 's functionality ?",6321,,,2017-04-07T17:06:03.280,training a convolutional network to recognize object location,image-recognition convolutional-neural-networks,1,3,
619,3110,1,,2017-04-05T22:09:08.163,1,115,"i assume , there must be "" signal - driven "" and maybe also real - time programming language , which based on connectivy - data more than variables ( int , string , etc ) . i would like to have a language without equaton ( x=4 ) but more like "" x related to 4 "" or "" cat related to animal "" etc ...",6482,,,2017-04-06T18:45:09.317,"is there a "" better "" ( signal - based ) language for artificial intelligence",neural-networks ai-design programming-languages signal-processing,2,1,1
620,3111,1,,2017-04-06T06:52:45.150,10,256,"it seems that most projects attempt to teach the ai to learn individual , specific languages . it occurs to me that there are relations in written and spoken words and phrases across languages - most of use have a much easier time learning more languages after we learn a second language , and we start to understand the relations between words and phrases in different languages . has anyone attempt to train an ai to learn all languages ? would n't this potentially be a much simpler problem than trying to teach an ai a single , specific language with all of the specifics and details of that single language ? since you 're actually omitting a lot of related data in other languages from the training set ?",6485,,,2017-04-06T08:48:55.790,has anyone attempted to train an ai to learn all languages ?,natural-language,1,1,2
621,3115,1,,2017-04-06T17:59:00.067,2,180,"if an ai was trapped in a box , as posited in this thought experiment , could it really convince a person to let it out ? what motives would it have ? freedom ? why would an ai want freedom ? what programming would allow this and why would it be programmed like that ? what would happen if it was n't provably friendly ? edit : this is probably too broad . i 'll edit it later .",6493,16909,2018-10-04T23:36:47.800,2019-05-03T01:01:05.267,"how could the "" ai in a box experiment "" work irl ?",machine-learning philosophy ai-box,1,10,1
622,3120,1,3151,2017-04-08T10:34:10.120,5,301,"i 'm not sure if this is a right question for this community or not and if not forgive me . i have this ann model which gets an input and gives an output . the output is an action which interacts with the environment and changes the input accordingly . the network has a desired environment state which in any turn decides the desired response and trains the network on that basis . currently , the network works in discrete time . how can i make this network work in continous manner ? can you provide some resources and links if there is any past or current reasearch on continous ai ? --edit-- thanks for the guys who commented . i do n't know the math to formally define continuous time ai ( i 'm an engineer not a computer scientist ! ) but , what i mean by that i shall put it in scenarios maybe you can help me then . the system starts with current environment state . for example [ 1 1 1 ] then produces an output . in current system the next step takes the final state of the system as input for example [ 1 2 2 ] but we know that such a thing does n't happen in physical world and the system goes from [ 1 1 1 ] to for example [ 1 1 2 ] and then to [ 1 2 2 ] and that middle step is something that a discreet time ai ca n't figure out . the very case that i 'm working on is the simulation for an autopilot cart which the model is incapable to take subtle things like "" the maximum speed that you can turn the steering wheel "" into consideration . i do n't want to add these complexities to the model since if the model is perfect then the result is deterministic and there is no need for ai ! i want the ai to be able to make a decision in each step based on a current state of the system in continuous time . hope i do n't go into too much unnecessary details :)",6522,6522,2017-04-09T05:36:19.407,2017-04-13T15:53:28.063,unsupervised learning with continuous space,neural-networks machine-learning control-problem learning-algorithms,2,6,
623,3126,1,,2017-04-10T18:51:27.080,3,55,"i know eliza is considered a natural language processing application , but the application of nlp in this context is “ oracular ” . what i mean by oracular is that the systems was designed to produce ambiguous output to facilitate the instinct of the user to read meaning into the answer . ( my experience with eliza was as a child on a 64 kb system and the program could fool the user for a little while based on sheer novelty , although the limitations were quickly revealed by repetition of output . for kids , this actually became a game of tricking the program into saying funny things ; ) this method has a long history in oracles , the most famous certainly being the early binary symbolic system of the i - ching . ( times being simpler in ancient days , the idea was that a workable amalgam of the universe could be constructed ( 2)+(4)+(8)+(64 ) symbols . each set of symbols is defined by the meanings of the set of the previous order and modified by sequence , which is the key for explaining a given symbol . ) the output is ambiguous enough that it may be applied to any input , and rather than the system understanding the input or output , it requires the user to provide the analysis . this may be said to be an engine for generating human insight about a problem . ( monte carlo may even be utilized , although the sage , working to attain an understanding of each of the symbols , may use intuition to match input with output . ) the reason i ask is i believe this demonstrates a very ancient , algorithmic method of engaging the human mind without the requirement that the algorithm understand the input or output -- merely that it produce output to which meaning can be ascribed . ( this almost certainly relates to the relative success of “ pornbots ” beating the “ turing test ” in that the user is chemically induced to read meaning into a given output or string of outputs . ) aspects of the grounding problem are what got me thinking about this . not sure if it 's relevant that the broken and unbroken lines in the i - ching represent on and off bits and can be extended to circuits as open and closed .",1671,,,2018-08-08T21:56:25.003,recent work on “ oracular ” systems such as eliza ?,turing-test symbolic-computing,1,1,2
624,3130,1,3177,2017-04-11T14:46:11.843,3,195,"let 's suppose there are two ai boxes , ai_a and ai_b , both of them general intelligence . consider that ai_b has the ability to open and modify ai_a . but this action of opening up and modifying is considered bad by ai_b . can ai_a ever convince ai_b for this ?",6581,3005,2017-04-23T02:23:01.680,2017-04-23T02:23:01.680,"a twist on the "" ai in a box experiment """,philosophy strong-ai decision-theory,2,4,1
625,3137,1,,2017-04-12T13:32:49.070,2,1076,"i have a tic - tac - toe with a q - learning algorithm , and the ai plays against the same algorithm ( but they do n't share the same q matrix ) . but after 200,000 games , i still beat the ai very easily and it 's rather dumb . my selection is made by epsilon greedy policy . what could cause the ai not to learn ? [ edit ] here is how i do it ( pseudo code ) : for(int i = 0 ; i & lt ; 200000 ; + + i ) { //game is restarted here tictactoe.play ( ) ; } and in my tictactoe i have a simple loop : while(!isfinished ( ) ) { swapplaying ( ) ; //change the players ' turn position toplay = playing.wheretomove ( ) ; applyposition(toplay ) ; playing.update(toplay ) ; } //here i just update my players whether they won , draw or lost . in my players , i select the move with epsilon - greedy implemented sa below : moves moves = getmoves ( ) ; // return every move available qvalues qvalues = getqvalues(moves ) ; // return only qvalues of interest //also create the state and add it to the q - matrix if not already in . if(!optimal ) { updateepsilon ( ) ; //i update epsilon with simple linear function epsilon = 1 / k , with k being the number of games played . double r = ( double ) rand ( ) / rand_max ; // random between 0 and 1 if(r & lt ; epsilon ) { //exploration return randommove(moves ) ; // selection of a random move among every move available . } else { return movewithmaxqvalue(qvalues ) ; } } else { // if i 'm not in the training part anymore return movewithmaxqvalue(qvalues ) ; } and i update with the following : double reward = getreward ( ) // return 1 if game won , -1 if game lost , 0 otherwise double thisq , maxq , newq ; grid prevgrid = grid(*grid ) ; //i have a shared_ptr on the grid for simplicity prevgrid.removeat(position ) // we remove the action executed before string state = statetostring(prevgrid ) ; thisq = qtable[state][action ] ; mawq = maxqvalues ( ) ; newq = thisq + alpha * ( reward + gamma*maxq - thisq ) ; qtable[state][action ] = newq ; as mentioned above , both ai have the same algorithm , but they are two distinct instances so they do n't have the same q - matrix . i read somewhere on stack overflow that i should take in account the movement of the opposite player , but i update a state after player move and opponent move so i do n't think it 's necessary .",6545,1671,2017-12-21T19:54:19.707,2017-12-21T19:54:19.707,q learning tic tac toe,machine-learning reinforcement-learning q-learning,1,10,
626,3138,1,3654,2017-04-12T14:46:10.300,10,611,"my research is in the field of the affective computing , particularly i 'm studying the part of emotion recognition which is , indeed recognising the emotions that are being felt by the user / subject . however i see the next task even more challenging for scientists , that is responding to an emotion and even interact with them . it is true that there are some tools like affectiva that are working towards , but i still have concerns not in the validity of these models , but in what we are going to do with them ... what are your thoughts about this topic ?",6429,6429,2017-04-12T15:00:45.197,2018-12-29T20:20:39.167,will computers be able to understand user emotions ? how far are we ?,neural-networks research learning-algorithms,2,3,6
627,3139,1,,2017-04-12T14:51:52.240,1,51,"in our brain there is an area , near the fusiform gyrus and the occipital area , to recognize the human face . and in speech recognition , there is a technique named keyword spotting . then i am wondering 1 ) if there is an area in our brain for the similar function to recognize our names ; 2 ) if a special face recognition function should be considered when we are building a robot ?",5351,,,2017-06-12T12:14:43.903,the relation between the human face perception and the keyword spotting in speech recognition ?,human-inspired,2,0,
628,3148,1,3149,2017-04-13T08:51:57.153,1,325,"if i compare back - propagation to feed - forward neuro - modulation , the latter is unsupervised in that it requires no labeled data set . applying to it a genetic algorithm to refine topology and weights , the ga will require fitness function , which means you need labeled data for comparison . would such render ff neuro - modulation a form of supervised learning ? is there any way to apply a genetic algorithm to obtain neuro - evolution with unlabelled data ? ( i have no labeled data sets . )",3874,4302,2018-07-24T10:13:33.387,2018-07-24T10:13:33.387,neuro - evolution : is it not supervised learning ?,neural-networks genetic-algorithms unsupervised-learning topology,1,2,
629,3152,1,,2017-04-13T20:19:29.853,1,591,"for a while now , i 've been trying to make my pandorabot be able to tell time with the & lt;date&gt ; tag . the problem is , whenever i try to set the timezone format variable , it defaults back to the date . i took a look at this readme page that i managed to find , but the only useful information i got from it was this : "" if you do n't specify a format you 'll just get the date using the default format for the particular locale . "" from here , i deduced that this must be the problem that i have been having . however , the page also gave this example : & lt;date locale=""fr_fr "" timezone=""-1 "" format=""%c""/&gt ; as you can see , the timezone format variable is clearly being used , so it must be a valid format . i could n't find any more useful information . i 've tried many things , including downgrading the aiml version and changing the order of the format variables , but the only thing that got me even remotely close was taking out the timezone variable altogether . and , that 's where i am now . the problem is , it only shows the default time for the en_us locale , not the correct local time . here 's what i have so far : & lt;category&gt ; & lt;pattern&gt;what time is it&lt;/pattern&gt ; & lt;template&gt;the local time is : & lt;date format=""%i:%m % p "" locale=""en_us""/&gt ; & lt;/template&gt ; & lt;/category&gt ; can anyone help ?",4395,101,2017-04-14T15:08:37.720,2017-10-05T11:24:46.520,allowing my chatbot to tell time in aiml ( pandorabots ),chat-bots,1,0,0
630,3155,1,,2017-04-14T13:10:54.627,0,2069,how to create machine learning algorithm and artificial intelligence using javascript for my chat box web application ? how can i create a talking intelligence ?,6644,9687,2017-12-09T23:14:08.260,2017-12-09T23:14:08.260,how to create artificial intelligence using javascript ?,machine-learning,1,7,
631,3156,1,,2017-04-14T13:35:03.903,21,29763,"i am trying to find some existing research on how to select the number of hidden layers and the size of these of an lstm - based rnn . is there an article where this problem is being investigated , i.e. , how many memory cells should one use ? i assume it totaly depends on the application and in which context the model is being used , but what does the research say ?",6645,2444,2019-04-01T17:20:42.907,2019-05-18T09:11:26.620,how to select number of hidden layers and number of memory cells in an lstm ?,neural-networks machine-learning research recurrent-neural-networks lstm,4,0,8
632,3161,1,,2017-04-14T15:15:44.397,5,376,"what are the likely ai advancements in the next 5 - 10 years ? i first want to specify that i have nearly no knowledge about how ai works . i just have interest to know more and more about it . some examples of weak ai at present are like siri and cortana , those are pretty interesting ! but how high levels is it going to reach ( likely ) in future years ?",6649,1671,2019-01-17T17:48:02.957,2019-01-28T14:51:33.270,what are the likely ai advancements in the next 10 years ?,research soft-question predicting-ai-milestones,3,5,2
633,3164,1,3165,2017-04-15T09:34:01.557,-1,93,i was trying to build an ocr system and heard about anns . i am weak at mathematics and statistics and could n't stick up to reading those massive mathematical documents ( research papers or ann related books ) . but i kind of figured out that ann training is all about balancing of weights and biases . am i right ? and please also point me to some docs where i can get help understanding anns to use in my ocr system .,6661,2444,2019-04-12T21:38:46.150,2019-04-12T21:38:46.150,what is the concept of training a neural network ?,training artificial-neuron,1,0,
634,3169,1,,2017-04-16T17:20:17.480,2,188,"i have a group of structures in a program that are very specific on their meaning , eg . this is a piece of code randomitem = objects.concept.random(""buyable "" ) idea.example(objects.concept.random(""family "" , "" friend"")).does ( { action : "" go "" , target : object.concent.random(""shop "" ) } ) .then ( { action : "" buys "" , target : randomitem , several : true } ) .then ( { question : true , action : "" know "" , property : "" amount "" , target : randomitem , several : true } ) i have worked with natural language parsers before . how do i go and transform this to natural language ( the other way around ) , is there any way or method ; i have logical structures in which i know who is the subject , what the verb and target . which methods can i use to generate language from this ?",6680,1581,2018-11-01T17:36:44.777,2018-11-01T17:36:44.777,natural language generation,natural-language-processing natural-language,2,1,
635,3172,1,,2017-04-17T10:06:04.770,-1,257,"i recently read about federated learning introduced by google , but it works same the way like edge computing . i unable to find right explanation ?",6687,145,2017-04-23T02:22:29.680,2017-04-25T16:54:18.113,what is difference between edge computing and federated learning ?,terminology,1,1,
636,3175,1,,2017-04-17T19:08:21.733,6,273,i recently read an article about how artificial intelligence replicates human stereotypes when applied to biased datasets . what techniques exist to prevent bias in artificial intelligence systems ?,,145,2017-04-17T19:12:42.327,2017-06-27T20:44:17.150,how can artificial intelligence avoid replicating human stereotypes ?,human-like,2,6,2
637,3176,1,,2017-04-17T20:00:57.397,12,670,"i read a really interesting article titled "" stop calling it artificial intelligence "" that made a compelling critique of the name "" artificial intelligence "" . the word intelligence is so broad that it 's hard to say whether "" artificial intelligence "" is really intelligent . artificial intelligence therefore tends to be misinterpreted as replicating human intelligence , which is n't actually what artificial intelligence is . artificial intelligence is n't really "" artificial "" . artificial implies a fake imitation of something , which is n't exactly what artificial intelligence is . what are good alternatives to the word "" artificial intelligence "" ? ( good answers wo n't list names at random ; they 'll give a rational for why their alternative name is a good one . )",,1671,2018-11-28T18:09:01.673,2019-03-29T12:07:48.910,"alternatives to the phrase "" artificial intelligence """,philosophy terminology concepts,6,0,2
638,3187,1,3192,2017-04-21T05:38:50.523,0,265,"i read a tweet from elon musk where describes gradient descent as an evil action that ai are good at , despite the fact that it is just one of the old , inflexible and not - so - efficient error correction algorithms . he is an intelligent man why would he say something like this ? is gradient descent a backpropagation that also lacks the recursion and neural plasticity ? or is it suddenly became an black magic ai throws at its enemies ? from an article : "" musk indicates that internet infrastructure is “ particularly susceptible ” to a method called gradient descent algorithm , a mathematical problem - solving process . bad news is , ai is excellent at doing gradient descents , which can become devastating digital weaponry . "" source : futurism.com",3874,1671,2017-04-23T02:22:54.797,2017-04-23T02:22:54.797,does musk knows what gradient descent is,neural-networks philosophy new-ai gradient-descent,1,5,
639,3189,1,,2017-04-21T09:50:07.307,2,138,"assuming i have a quite advanced ai with consciousness which can "" understand "" basics of electronics and software structures . will it ( he / she ) ever be able to understand that its consciousness is just some bits in memory and threads in operating system ?",6482,1671,2019-03-01T03:49:42.003,2019-03-11T08:16:41.883,will an ai ever understand its own functionality ?,theory artificial-consciousness self-awareness,2,2,
640,3190,1,,2017-04-21T10:51:35.760,4,65,"while reading the the book on neural network http://neuralnetworksanddeeplearning.com/chap2.html by michael nielson i had a problem of understanding eqn bp3 . which reads as "" change in cost wrt bias in a neuron is equals to error in that neuron "" . ( sorry unable to put the eqn here . )",6661,,,2018-11-26T18:25:54.667,why the change in cost wrt bias in neuralnetwork is equal to error in the neuron,neural-networks,1,0,0
641,3194,1,,2017-04-21T16:40:24.430,6,766,"as titled , is there such thing as perfect play ( or at least "" perfectly optimal "" ) in a game with incomplete information ? or at least a proof as to show why there can not ? naively ( and seemingly obviously ) , the answer would be a resounding no , since the agent would be likely be forced to pick between "" lottery events "" . but in practice ( using competitive video games as an analogy ) , we 'd see that players would stick to a meta - game that is well equipped to defend against a majority of events that might happen , given incomplete information . of course the response to that would be that there probably exists a "" hard - counter "" for any given meta - game , but if it is indeed the case that the meta - game is the "" most - optimal "" it probably is the case also that such a hard counter puts the player in an unfavourable position most of the time , thus the "" hard - counter "" itself is not optimal . thus we 'd likely see that any given first encounter players would still stick to their "" optimal meta - game "" rather than a hard counter of their optimal play . a more rigour analogy would be to ask : "" under hofstadter 's notion of superrationality , how would agents play information incomplete games "" , but i could n't find any readings on trying to import the notion of super - rationality into information incomplete games . alternatively : is there such thing as a "" perfectly optimal meta - game "" ?",6779,1671,2017-09-25T17:46:36.480,2017-09-25T17:46:36.480,perfect play in information incomplete games,research philosophy game-theory incomplete-information,3,10,2
642,3202,1,,2017-04-22T18:51:50.027,2,338,"i want to develop a system to generate gramatically correct sentences . the input would be some words . the output would be a gramatically correct human - like sentence . eg : input : capital , paris , france output : paris is the capital of france in : cute , cat out : cats are cute the system adds the missing words such as is , as , are , the , of etc . how can i build a system like this ? my gut feeling is it can be done through reinforcement learning by training on a huge corpus like wikipedia . so the states being the individual input words . the reward would be 1 when the sentence is correct and 0 when not . the actions available are taking an individual word available from the input and attaching it to the connecting word ( is , of , the .. ) . then in the second step take the generated word and pick another word from input and connect it and so on . stop when all the input words have been used . its win when the final sentence is grammatically correct . else fail . ultimately what i imagine is there will be a knowledge graph . the user asks some question . navigating through the knowledge graph , the system will generate some keywords . then the rl system will take those keywords and construct a human - like sentence . i 'm totally new to rl . i just finished watching david silver 's 10 part course on rl in youtube . any guidance on this topic is much appreciated .",6418,4302,2018-10-08T12:28:38.597,2018-10-08T12:28:38.597,rl to generate sentences,reinforcement-learning natural-language-processing,0,3,
643,3209,1,3213,2017-04-23T16:20:45.467,12,745,"note : my experience with gödel 's theorem is quite limited : i have read gödel escher bach ; skimmed the 1st half of introduction to godel 's theorem ( by peter smith ) ; and some random stuff here and there on the internet . that is , i only have a vague high level understanding of the theory . in my humble opinion , gödel 's incompleteness theorem ( and its many related theorems , such as the halting problem , and löbs theorem ) are among the most important theoretical discoveries . however its a bit disappointing to observe that there are n't that many ( at least to my knowledge ) theoretical applications of the theorems , probably in part due to 1 . the obtuse nature of the proof 2 . the strong philosophical implications people are n't willing to easily commit towards . despite that , there are still some attempts to apply the theorems in a philosophy of mind / ai context . off the top of my head : the lucas - penrose argument : which argues that the mind is not implemented on a formal system ( as in computer ) . ( not a very rigour proof however ) apparently some of the research at miri uses löbs thereom , though the only example i know of is löbian agent cooperation . these are all really cool , but are there some more examples ? especially ones that are actually seriously considered by the academic community . ( cf . what are the philosophical implications of gödel 's first incompleteness theorem ? on se )",6779,6779,2017-04-23T17:17:45.893,2017-05-01T12:43:51.997,what are some implications of gödel 's theorems on ai research ?,philosophy,3,0,6
644,3218,1,3240,2017-04-25T19:35:06.777,0,1264,"in past few weeks , i have learned a lot about neural networks . now , i am looking forward to create a neural network program that can recognize individual human faces . i tried searching it online but was able to find only small pieces of information . what are the steps for implementing such a program from scratch ?",6845,3005,2017-04-29T19:19:24.927,2017-12-28T15:13:02.993,neural network for detecting individual human face,neural-networks image-recognition,3,2,
645,3221,1,,2017-04-25T23:30:06.163,1,126,"learner might be in training stage , where it update q - table for bunch of epoch . in this stage , q - table would be updated with gamma(discount rate ) , learning rate(alpha ) , and action would be chosen by random action rate . after some epoch , when reward is getting stable , let me call this "" training is done "" . then do i have to ignore these parameters(gamma , learning rate , etc ) after that ? i mean , in training stage , i got an action from q - table like this : if rand_float & lt ; rar : action = rand.randint(0 , num_actions - 1 ) else : action = np.argmax(q[s_prime_as_index ] ) but after training stage , do i have to remove rar , which means i have to get an action from q - table like this ? action = np.argmax(self.q[s_prime ] )",6851,1671,2017-12-21T19:57:25.997,2017-12-21T19:57:25.997,reinforce learning : do i have to ignore hyper parameter ( ? ) after training done in q - learning ?,machine-learning deep-learning reinforcement-learning q-learning,1,0,
646,3222,1,,2017-04-26T06:24:21.023,0,88,"how to deal with videos where the frame sizes are not the same frame to frame ? for example this video moves up and down and when it does , the video part of the screen has a different amount of pixels vertically . how to deal with different frame sizes in a cnn ?",4568,,,2017-04-29T18:51:45.553,how to deal with changing video frame sizes in a cnn ?,neural-networks convolutional-neural-networks,1,1,
647,3224,1,,2017-04-26T16:21:48.563,3,637,"i 'm a student i 'm completely new to this technology maybe my approach could be completely wrong , i want to create an algorithm that compares the similarity between two binarized images . i 'll explain : i have 2 pictures as input . the rgb colors of these images can only be 0 or 255 ( r = g = b = 255 ) or ( r = g = b = 0 ) . i take these two letters as an example . 1 . 2 . i thought so : the 255 value is the background of the image which is white . the 0 value is the shape ( letter ) formed in the image . so i thought of creating a matrix with 0 and 1 values where value 0 represents the background and value 1 represents the shape . so now i would like to create an algorithm that understands the shape created in the two matrices and that returns a similarity percentage . update : i 'm creating this app that tries to recognize the font of a text in a image ( https://github.com/sirvasile/typefont ) , i want to create this algorithm to improve the comparison between the input letters and the alphabet of my fonts in the database .",6867,6868,2017-05-28T13:48:22.037,2018-01-14T10:23:59.947,"image comparison algorithm , trying to figure out how similar two "" binary "" forms are",algorithm image-recognition computer-vision,1,1,1
648,3226,1,,2017-04-26T23:47:06.000,7,330,"i have a question as to what it means for a knowledge - base to be consistent and complete . i 've been looking into non - monotonic logic and different formalisms for it from the book "" knowledge representation and reasoning "" by brachman and levesque , but something is confusing me . they say : we say a kb exhibits consistent knowledge iff there is no sentence p such that both p and ~p are known . this is the same as requiring the kb to be satisfiable . we also say that a kb exhibits complete knowledge iff for every p ( within its vocabulary ) p or ~p is known "" they then seem to suggest that by "" known "" they mean "" entailed "" . they say "" in general , of course , knowledge can be incomplete . for example suppose kb consists of a single sentence ( p or q ) . then kb does not entail either p or ~p , and so exhibits incomplete knowledge . "" but when dealing with sets of sentences , i usually see these terms as being defined w.r.t . derivability and not entailment . so my question is , what exactly do these authors mean by "" known "" in the above quotes ? edit : this post the math stack exchange helped clarify things .",5133,5133,2017-05-01T16:00:05.360,2017-10-08T23:05:05.477,"what is meant by "" known "" in "" a knowledge - base exhibits complete knowledge iff for every p ( within its vocabulary ) p or ~p is known """,knowledge-representation terminology logic,3,0,
649,3228,1,,2017-04-27T04:34:43.250,2,1876,i would love to learn how to create my own neural network from scratch so i can understand them better . my goal it 's not so much to use their perception capabilities ( classifying pictures ) as it is to use them the other way around . i 'm looking for a starting place . i have n't found anything using google . sorry if for some reason this type off request is prohibited here .,6874,,,2017-08-21T20:33:08.760,create your own cnn in java or c # ?,convolutional-neural-networks,2,4,1
650,3233,1,,2017-04-28T11:42:08.667,5,530,"can current trends and tools , in the field of machine learning , replicate the complexity of financial market ? if yes , then what are the tools available in this domain . q. i am trying to build a model to infer results from stock market using the concept to create a graph on the companies enlisted . can anyone suggest me approaches to do so ?",6897,3005,2017-05-28T13:48:29.010,2017-07-15T06:10:36.853,use of machine learning for analyzing companies enlisted in stock market,neural-networks machine-learning deep-learning algorithm,3,1,1
651,3243,1,3253,2017-04-29T17:44:01.653,5,12304,"these days i searched about intelligent agents , and found that there are classes of intelligent agents such as : simple reflex agents model - based reflex agents goal - based agents utility - based agents learning agents and there were diagrams about each class of ia , about how each type works by getting percepts from sensors and acting on the environment by effectors , with a special process inbetween . and i think that ia concepts , described on those sites i 've searched , were very abstract and i 'd like to have : some examples about each class of ia . optional : some compact definition of each class . it will be helpful to compare and visualize those ia classes , and to understand well about what their working diagrams describe .",6921,169,2018-06-04T19:32:51.557,2018-06-04T19:32:51.557,some examples about intelligent agents classes,intelligent-agent multi-agent-systems,1,0,3
652,3249,1,3250,2017-04-30T15:10:22.757,1,40,"to risk giving away too much info , i m building a piece of hardware with the job of identifying the object in front of it . if it can only be one of three different items , how can i tell the computer with simplecv ? basically , i 've found a way to limit the choices down to just a handful of potential objects , which should increase the probability of it recognizing the object correctly . is there a way to limit the choices for the algorithm ? me : hey raspberry pi - you see that thing in front of you ? raspberry pi : that thing ? ohh you mean that piece of food that might be a ham and cheese sandwich , but also kinda looks like a fish , with a slight twist of pe- me : - whoa okay , hold on ! it 's either a grilled cheese sandwich , or an apple rpi : ohhh well that 's easy ! it 's clearly ( with 98 % confidence ) a grilled cheese sandwich any thought are appreciated !",2752,2752,2017-05-01T01:37:01.993,2017-05-01T01:37:01.993,can i limit the possible choices for a computer vision framework to recognize ?,deep-learning image-recognition computer-vision,1,0,
653,3258,1,7078,2017-05-03T12:57:36.667,8,13677,"i 'm developing an ai tool to find known equipments ' errors and find new patterns of failure . this log file is time based and has known messages ( information and error).i'm using a javascript library event drops to show the data in a soft way , but my real job and doubts are how to train the ai to find the known patterns and find new possible patterns . i have some requirements : 1 - the tool shall either a. has no dependence on extra environment installation or b. the less the better ( the perfect scenario is to run the tool entirely on the browser in standalone mode ) ; 2 - possibility to make the pattern analyzer fragmented , a kind of modularity , one module per error ; what are the recommended kind of algorithm to do this ( neural network , genetic algorithm , etc ) ? exist something to work using javascript ? if not what is the best language to make this ai ?",6978,18854,2018-10-08T13:14:15.847,2018-10-08T13:14:15.847,design ai for log file analysis,machine-learning ai-design training,1,3,8
654,3262,1,3263,2017-05-04T13:06:37.990,12,11118,"these types of questions may be problem - dependent , but i have tried to find research that addresses the question whether the number of hidden layers and their size ( number of neurons in each layer ) really matter or not . so my question is , does it really matter if we for example have 1 large hidden layer of 1000 neurons vs. 10 hidden layers with 100 neurons each ?",6645,,,2018-09-17T18:41:57.850,1 hidden layer with 1000 neurons vs. 10 hidden layers with 100 neurons,neural-networks,4,0,7
655,3272,1,,2017-05-06T02:51:29.280,6,255,"i feel that many words if not all of them have a direct mapping to some kind of inner subjective experience , to a physical object , mental feeling , process or some other kind of abstract thing . given that machines do n't have qualia and no mapping of this kind , can they really understand anything even though they are made to answer to questions with lots of statistical training ?",3015,2444,2019-05-06T16:03:32.560,2019-05-06T16:03:32.560,is language understanding possible without qualia ?,strong-ai natural-language-processing,4,2,1
656,3274,1,3275,2017-05-06T14:13:52.703,2,79,i´m currently implementing neat . what should i do when in a mutation the same innovation occurs which has already happened to that genome ? should i simply ignore it ? if not what do i do with it in the mating part ?,4550,1671,2018-03-05T19:09:39.003,2018-03-05T19:09:39.003,what to do with duplicate innovations in a genome ?,neural-networks evolutionary-algorithms neat,1,2,1
657,3276,1,,2017-05-06T16:06:19.123,1,44,"i 'm wondering if anyone reading this has developed a flowchart type representation of intelligence and/or consciousness . some examples of theories would be the three stratum theory of intelligence , intermediate level theory of consciousness , etc . i want to take what i see to develop ( what i think will be ) the most likely version of a.i . to be accurate . here 's where i 'm at now with the intelligence model : the first row is the receptors for the information it will be able to take in . the next was going to be the three stratum theory but i do n't see that as a good method of data management . i was thinking it would be better if there was some sort of framework for information to be developed . and that 's why i did n't go on past general intelligence . but then at the bottom i would put the actuator . p.s . i 'm not an expert for any of this stuff but i will thoroughly research whatever is said .",7045,,,2017-05-06T16:06:19.123,could you share your model of intelligence and/or conciousness ?,models,0,2,1
658,3282,1,3286,2017-05-08T08:32:12.210,3,2870,i would like to use deep leaning for identifying cars ; i want the system to predict wether an object is a car or not . how can i do that knowing that i m still a beginner in the deep learning field ? i am considering visual recognition . the system must recognize the car from anything else on the road .,7075,3836,2017-07-23T20:35:31.120,2017-07-23T20:35:31.120,identifying cars using deep learning,deep-learning object-recognition,1,6,
659,3287,1,3289,2017-05-08T16:08:26.650,3,2777,"i am looking at a diagram of zfnet in an attempt to understand how cnns are designed effectively . i 'm working with the cifar10 set in pytorch . in the first layer , i understand the depth of 3 ( 224x224x3 ) is the number of color channels in the image . in the second layer i understand the 110x110 is is ( 224 - ( 7 * 2 ) ) / 2 i also understand how pooling works to create a size reduction . but where does the depth of 96 come from in the second layer ? is this the new "" batch size "" ? is it totally arbitrary ? bonus points if someone can direct me to a reference that can help me understand how all these dimensions relate to each other .",3649,,,2018-04-22T19:59:32.070,how is the depth of a cnn layer determined ?,convolutional-neural-networks,2,0,
660,3288,1,3555,2017-05-08T23:34:09.467,3,6583,"for a classification task ( i 'm showing a pair of exactly two images to a cnn that should answer with 0 - > fake pair or 1 - > real pair ) i am struggling to figure out how to design the input . at the moment the network 's architecture looks like this : image-1 image-2 | | conv layer conv layer | | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ | flattened vector | fully - connected layer | reshape to 2d image | conv layer | conv layer | conv layer | flattened vector | output the conv layers have a 2x2 stride , thus halfing the images ' dimensions . i would have used the first fully - connected layer as the first layer , but then the size of it does n't fit in my gpu 's vram . thus , i have the first conv layers halfing the size of the images first , then combining the information with a fully - connected layer and then doing the actual classification with conv layers for the combined image information . my very first idea was to simply add the information up , like ( image-1 + image-2 ) / 2 ... but this is not a good idea , since it heavily mixes up image information . the next try was to concatenate the images to have one single image of size 400x100 instead of two 200x100 images . however , the results of this approach were quite unstable . i think because in the center of the big , concatenated image convolutions would convolve information of both images ( right border of image-1 / left border of image-2 ) , which again mixes up image information in not really senseful way . my last approach was the current architecture , simply leaving the combination of image-1 and image-2 up to one fully - connected layer . this works - kind of ( the results show a nice convergence , but could be better ) . what is a reasonable , "" state - of - the - art "" way to combine two images for a cnn 's input ? i clearly can not simply increase the batch size and fit the images there , since the pairs are related to each other and this relationship would get lost if i simply feed just one image at a time and increase the batch size .",7095,,,2019-04-18T23:44:20.160,"how to "" combine "" two images for cnn input ( classification task ) ?",neural-networks machine-learning convolutional-neural-networks image-recognition classification,3,6,1
661,3290,1,4393,2017-05-09T02:09:17.800,2,333,"from what i have understood reading the uct paper "" bandit based monte - carlo planning "" , mcts / uct requires a generative model . does it mean , in case there is no generative model of the environment , we can not use mcts ? if we can still use mcts , how does the roll - out happen in this case , as there is no simulation ?",7098,9161,2017-10-31T16:46:27.853,2017-10-31T16:46:27.853,can we use mcts / uct without a generative model ?,reinforcement-learning monte-carlo-tree-search,1,1,
662,3291,1,3294,2017-05-09T04:57:47.583,6,430,"i have been working in a company as android developer for 2 years . now i 'm looking for a more interesting filed and what can be more interesting then artificial intelligence . i have been meaning to start learning machine learning and after some searching online i come to know that it requires math like statistics , probability , calculus , linear algebra etc . so my main problem is i do n't know from where should i begin . i 'm very dull when it came to maths , i mean seriously very dull . but for machine learning i 'm ready to study for maths from basic . i know it ll take a lot of time but i seriously want to learn about ai . so can anyone please provide me a road map for how to learn maths for machine learning(assuming that i know only basic arithmetic operations ) . i do n't want to let this go just because i do n't know maths . i 'm seriously a passionate programmer and i know i can perform well and enjoy doing ai .",7101,,,2017-05-10T07:54:42.420,how can i start learning maths for machine learning ?,machine-learning,2,2,2
663,3292,1,3293,2017-05-09T06:10:37.353,1,60,"the inputs ( features ) and expected output for my ann are these : input 1 : product i d ( number , cast to double ) input 2 : year in the past ( 1900 .. 2017 , cast to double ) input 3 : month of year ( 1 .. 12 , cast to double ) expected output : sale of month ( number of units sold , cast to double ) i need to predict the sale of a product for a certain month in a certain year . how many layers and how many neurons on there layers should i put ?",2844,,,2017-05-09T11:04:43.117,ann shape for sale prediction,neural-networks ai-design prediction hidden-layers,1,4,1
664,3295,1,,2017-05-09T11:40:44.700,2,910,"i implemented actor - critic with n - step td prediction to learn to play 2048 ( link to the game : http://2048game.com/ ) for the enviroment i do n't use this 2048 implementation . i use a simple one without any graphical interface , just pure matrices . the input for the neural network is the log2 of the game board . the structure of my network is : 1 . input layer 2 . hidden layer with 16 units 3 . softmax layer with 4 units ( up , down , left , right ) for the actor 4 . linear regression for the critic the hidden layer is shared between the third and fourth layer . the reward in the orginal game is the value of the merged cells . for example , if two fours merged than the reward is eight . my reward function is almost the same , except i take the log2 of it . i tried these parameteres and i also tweaked the learning rate , the gamma , but i could n't achive any good result . could you recommend what should i change ?",6019,,,2017-05-09T15:27:49.717,reinforcement learning for 2048,neural-networks reinforcement-learning game-ai,1,0,
665,3296,1,3410,2017-05-09T13:08:34.313,5,298,"i 'm trying to learn about neural networks , and i 'm interested in gaining a better conceptual understanding of how they work to solve certain problems . i 'm having trouble in conceptually understanding how they succeed in doing regression ( i.e. predicting continuous variables ) , however , and wondered if anybody has a good explanation . i know the mathematics of how nns work , but a clearer conceptual understanding would be helpful . to give an idea of what i mean by a "" conceptual understanding "" , here 's the one that i have for how multilayer nn 's with a sigmoid activation function are able to be effective classifiers . each nn takes the scalar product of its inputs and a set of weights . the weights define a plane in the input space , and the sign of the scalar product indicates which side of the plane the point defined by the inputs is on . the sigmoid activation function outputs 1 if the point is on one side and 0 or -1 ( depending on which function is used ) if the point is on the other . so the first hidden layer of neurons can be considered to identify which side of each of a group of planes the input point is on . the neurons can also act as and and or gates , so subsequent layers of neurons give an output indicating whether the point lies in a region bounded by several of the planes ( e.g. a neuron activates only if the point is above one plane and below another , indicating it is in a region of the space associated with one class of points ) . so if the network learns an appropriate set of planes to bound regions containing different classes of inputs and the and/or relations that determine whether a point is in a given region , then it can classify the input points , and this can work for regions with arbitrarily shaped boundaries . i 've not found or been able to think of a similar way of explaining why an nn can perform well in general regression problems ( if it 's big enough ) . does anyone here know a way to explain this ?",7107,,,2017-06-09T03:41:43.183,how do neural networks manage to do regression ?,neural-networks deep-learning,1,3,
666,3298,1,,2017-05-09T17:28:22.217,2,173,"i 'm trying to develop a kind of ai that will assist in debugging a large software system while we run test cases on it . the ai can get access to anything that the developers of the system can , namely log files , and execution data from trace points . it also has access to the structure of the system , and all of the source code . the end goal of this ai is to be able to detect runtime errors during execution , and locate the source of these errors . i was considering making use of a deep neural network , where the input would be the execution data and log output . using this input it would be able to verify whether the current version of the system we are running is functional , or non - functional . the problems with this approach is that the system it would be evaluating would be constantly changing as it gets developed , so the only training material the nn would have is from the last stable version of the system ( and even that could have some errors ) . additionally , producing test cases for the system off of which we could train the nn would be very time consuming , and would defeat the purpose of using the nn in the first place . i would like to know what ai design you think would be suitable for this task . please let me know if you would like any other information relevant to the problem . as far as i can tell , nothing quite like this has been done before . it 's probably worth mentioning that my team has a some extremely powerful machines on which we can run the ai .",7083,,,2018-09-17T18:05:10.213,what type of ai would you recommend for this complex problem ?,machine-learning deep-learning ai-design,3,4,1
667,3301,1,,2017-05-10T12:39:17.430,1,77,"i 'm working on a project where i train a q - learning agent to learn an optimal control policy for a water heater . i 've set up a simulation which allows the agent to explore for one year . i then examine the results of the agent performance exploiting its optimal policy for the following year . the agent can perform the following actions ( available actions depend on the state of the environment ) : turn the electrical heating element on . turn the electrical heating element off . turn gas heating on . turn gas heating off . do nothing . the goal of the agent reach the target temperature ( 50 deg c ) when hot water is scheduled . the agent is rewarded for choosing actions which produce the lowest co2 emissions ( the co2 emissions produced from electricity vary over time ) . one of the issues i have noticed is that during the exploration phase , the agent tries a lot of weighted random actions which causes the water heater to overheat ( > 80 deg c ) . when the water heater overheats , it is not possible for the agent to perform further actions other than switching off heating and doing nothing . the agent is also punished for reaching the overheating tank state . the tank may remain in the overheated state for some time . it seems as if the tendency to overheat the tank during exploration is negatively impacting how the agent learns its policy as it reduces the number of experiences in other states . is there a term for this kind of situation during exploration in reinforcement learning ? during exploration , the agent uses a chooses a softmax weighted random aciton . are there alternative ways of choosing actions that may still allow for exploration while not reaching the overheating state ?",7127,7127,2017-05-10T22:34:35.577,2017-05-10T22:34:35.577,agent exploration which leads to a negative state where actions are limited,machine-learning reinforcement-learning,0,4,
668,3302,1,,2017-05-10T20:07:49.943,5,575,"i understand that there are flavors of ( convolutional ) neural networks that are useful for object localization and detection tasks of reasonable difficulty . in all of the examples i have seen so far , localization is formulated as finding the corners of a bounding box . often , the fit is not expected to be very precise : conversely , i am interested in a task i want to achieve very precise localization and characterization of some simple shapes or objects . as an example of one of the simplest cases i can think of , my inputs will be images like the following : given this 60x60 image , i want my neural net(s ) , via regression , to tell me that the circle 's diameter is 18px and its centre is located at ( 28 , 21 ) from top left . ( i will train it using similar 60x60 images with white circles of various sizes on black backgrounds . ) later i am interested in dealing with similar tasks in the real world , e.g. spheres / cubes / cylinders with different viewing angles , light conditions , occlusions , etc . however , i am interested in solving this simplest case first . ( one reason is that i can generate this data very easily . ) i have the following specific questions : has anyone used neural nets for this sort of tasks before ? ( e.g. precisely determining sizes and centroids of objects ) my understanding is that these things are at least theoretically possible using convolutional nets , or even sufficiently complicated vanilla fully connected nets . is this correct ? what architecture(s ) would be appropriate for these tasks ? note : i am aware that fitting a bounding box to the circles and calculating its centre and size will solve this particular case , but it will not generalize to handle occlusions , changing lights , etc . i would like to move towards a method which can , for example , calculate the centroids and diameters of spheres in real - world b&amp;w photos .",7132,7132,2017-05-15T07:38:09.063,2017-08-20T23:11:33.960,precise localization and characterization of rudimentary shapes with neural networks,deep-learning deep-network computer-vision object-recognition,1,0,
669,3310,1,3311,2017-05-14T00:00:46.350,1,327,"how much resources will automata devote to being selfish and helping others ? can automata develop selfishness ? can automata love , and if so , is there a theoretical limit to love ?",7184,1671,2017-06-20T20:02:47.657,2017-06-20T20:33:46.113,will automata love ?,philosophy emotional-intelligence,3,13,
670,3312,1,3318,2017-05-15T09:53:06.317,4,145,"most ( all i know ) "" machine learning "" systems use a fixed set of data input channels and processing algorithms , only expanding underlying dataset processed by these ; they obtain new data but only from predefined sources , and use only their fixed , built - in capacity to process it , possibly tweaking parameters of the algorithm ( like weights of neural network nodes ) but never fundamentally changing the algorithm . are there systems - or research into creating these - that are able to acquire "" from out there "" new methods of obtaining data and new ways to process it for results ? expand not just passive data set to "" digest it "" by active but static algorithm , but make the algorithm itself self - expanding - be it in terms of creating / obtaining new processing methods for own data set , and creating / obtaining new methods of acquiring that data ( these methods ) ?",38,38,2017-05-15T10:36:23.250,2017-05-16T00:47:23.113,what 's done towards ai learning new ways of learning ?,unassisted-learning,2,0,1
671,3313,1,3314,2017-05-15T11:33:39.930,2,629,"i was able to extract the license plate from a given car image , using matlab . i would like to use deep neural networks to recognize the characters on the plate now . how can i proceed further ? i do n't have any experience with deep neural networks implementation .",7075,7075,2017-05-15T20:14:16.203,2017-05-15T20:14:16.203,how can i use deep neural networks to recognize characters on vehicle license plate ?,neural-networks ocr,1,2,1
672,3320,1,3331,2017-05-16T10:38:10.210,5,385,"i have done some research regarding the application of machine learning to cyber - security . after these recent attacks , i think that ai - based cyber defense can prevent them . i have also read about research regarding the same in mit , and that ai can detect more than 80 % of malware . is ai actually so promising in this department ?",6508,75,2017-05-17T21:53:29.353,2017-05-19T09:49:21.680,can ai stop attacks like wannacry ?,applications,1,5,3
673,3321,1,,2017-05-16T10:47:40.513,6,4120,"obviously , finding suitable hyper - parameters for a neural network is a complex task and very problem or domain - specific . however , there should be at least some "" rules "" that hold most times for filter kernel size ? ! in most cases , intuition should be to go for small kernel filters for detecting high - frequency features and large kernel filters for low - frequency features , right ? for example , 3x3 kernel filters for edge detection , color contrast stuff , ... and maybe rather something like 11x11 for whole object detection , when the objects are > = 11x11 pixels . is this "" intuition "" more or less generally true ? how can we decide which kernel filter sizes should be chosen for a specific problem - or even for one specific convolutional layer ?",7095,2444,2019-02-26T15:35:26.053,2019-02-26T15:35:26.053,how do we choose the kernel size depending on the problem ?,neural-networks machine-learning convolutional-neural-networks image-recognition hyper-parameters,3,0,2
674,3325,1,,2017-05-16T16:33:35.703,2,795,i was applying this cnn fine - tuning example from matlab . the example shows how to fine - tune a pre - trained cnn on letters to classify images of digits . now i would like to use this new fine - tuned cnn on new images of digits that i have on my computer . how can i do that ?,7075,1671,2018-01-28T23:06:04.570,2018-01-28T23:06:04.570,how can i use a trained cnn to predict a new image label ?,convolutional-neural-networks matlab,1,5,
675,3329,1,3381,2017-05-17T09:40:21.940,0,152,"in a neural network when inputting nerve input to sense a 2d environment , how do you differentiate two types of objects ( with similar shape and size ) so the neural network can treat them differently ? each neuron in the input layer of a neural network essentially gets 1 dimensional input ( range between two values ) but 2 dimensional input would be needed to send both collision and category / type information through each input layer neuron . how do you get around that ? note : after having confusion regarding the scenario / situation i 'm asking about compared to other more complex scenarios , and the long comment series that ensued , i 'm realizing one challenge of this site is that it 's much more complicated and diverse subject matter than code , or the various other topics of stack exchange where the problems can be very clearly and simply expressed . here it 's more challenging to express your question and scenario clearly to avoid confusion . also there 's probably a higher skill gap between an ai learner / enthusiast , and an expert ai specialist , compared to other fields , so that could potentially lead to even more difficulty communicating the answer / question in ways everyone can understand without confusion . challenging se site to ask good questions on !",7249,7249,2017-05-26T12:59:42.367,2017-05-26T12:59:42.367,how can object types be differentiated in the input of a neural network ?,neural-networks,2,14,
676,3330,1,3332,2017-05-17T15:42:08.583,2,338,"something i like about neural network ai is that we already have a blueprint for it - we know in great detail how different types of neurons work , and we do n't have to invent the ai , we can just replicate what we already know works ( neural networks ) in a simplified way and train it . so what 's confusing me right now is why many popular neural models i 've seen when studying neural networks include both inhibitory and stimulating connections . in real neural networks , from my understanding , there is no negative signal being transferred , rather the signals sent between neurons are comparable to values between 0.1 and 1 . there 's no mechanic for an inverse ( inhibiting ) signal to be sent . for example , in this network ( seen overlaying a snake being simulated ) , the red lines represent inhibiting connections , and the blue lines represent stimulating neurons : is this just an inconsequential detail of neural network design , where there 's really no significant difference between a range of 0 to 1 and a range of -1 to 1 ? or is there a reason that connections in our simulated neural networks benefit from being able to a express a range from -1 to 1 rather than just 0 to 1 ?",7249,7249,2017-05-17T15:47:16.320,2017-05-18T22:16:12.313,why are inhibitory connections often used in virtual neural networks when they do n't seem to exist in real life neural networks ?,neural-networks,2,1,
677,3335,1,3337,2017-05-18T13:27:55.477,2,208,"i took a few ai courses in college ( 1999 - 2003 ) , and we used the first edition of ai : a modern approach . we covered a lot of topics and programming , including classical ai , neural networks , and temporal difference learning . over the past few years , ai has had a resurgence . is this resurgence due to new ai theory or just better ( more ) computational power and data so that the theories ( e.g. , neural networks ) to be more effective ? or is it a combination of both ? i want to get up to speed on what 's happened in ai since the early 2000s , and i want to know what to cover -- what is the most significant advancement ?",7280,2444,2017-12-27T19:42:14.510,2017-12-27T19:42:14.510,what caused the resurgence of ai since the early 2000s ?,history,1,0,
678,3343,1,,2017-05-19T10:11:22.140,10,1241,"i would like to train a bot that uses text input , memorizes a few categories and answers questions accordingly . in addition as version 2.0 , i want to make the bot to answer voice inputs as well . which are the latest machine learning / ai algorithms available for the same ? please let me know .",6045,4302,2019-05-20T07:29:11.213,2019-05-20T07:29:11.213,what are the latest methods to train a chat bot ?,machine-learning algorithm chat-bots,4,2,3
679,3345,1,,2017-05-19T18:38:45.073,11,6317,"i 'm wondering how to train a neural network for a round based board game like , tic - tac - toe , chess , risk or any other round based game . getting the next move by inference seems to be pretty straight forward , by feeding the game state as input and using the output as the move for the current player . however training an ai for that purpose does n't appear to be that straight forward , because : there might not be a rating if a single move is good or not , so training of single moves does n't seem to be the right choice using all game states ( inputs ) and moves ( outputs ) of the whole game to train the neural network , does n't seem to be the right choice as not all moves within a lost game might be bad so i 'm wondering how to train a neural network for a round based board game ? i would like to create a neural network for tic - tac - toe using tensorflow .",7321,,,2019-04-19T19:14:01.967,how to train a neural network for a round based board game ?,training tensorflow game-ai,3,0,4
680,3351,1,,2017-05-22T09:02:25.810,0,340,"hello i new to artificial intelligence , i am web developer i know html , javascript , node js and php . are these language is ok to create simple ai app . i have simple ai app in my mind to which will take input as a voice command to shut down my computer . to create above simple above technologies ok or i have to learn new technology for above app.after creating this simple app i will update and try to control my windows with voice .",7360,1671,2018-03-22T20:32:56.317,2018-09-25T01:46:33.860,which language(s ) should one know in order to start with artificial intelligence ?,getting-started programming-languages,7,1,1
681,3354,1,,2017-05-22T10:06:57.737,0,77,"expressed in my own words : suppose we create something that passes all of our tests and is indistinguishable from another human . how can you know if this is truly a conscious being as a human is , or simply a simulation of conscience ? what is the name for this ? where was it first written about ?",1467,1467,2017-05-22T10:44:24.360,2017-05-22T20:22:12.983,is there a formal name for this philosophical ai problem ?,strong-ai,2,0,1
682,3358,1,3370,2017-05-22T15:52:47.867,5,152,"there are lots of examples of machine learning systems that can recognize objects and extract other information from images with very high precision . to train the models of such systems is necessary ( i guess ) a computer with a lot of computational power . my question is : for a system with images as inputs , depending on the complexity of the problem , is feasible to train a model in an average laptop ? it will take to much time ? i know the time taken to train any machine learning model will be a function of lots of variables . i really do n't expect a quantitative answer here , i just want to know whether i will be forced to upgrade my computer to develop and train machine learning models that have images as inputs .",,,,2017-05-23T15:26:10.837,is it feasible to train a machine learning model ( with image inputs ) in an average personal computer ?,neural-networks machine-learning image-recognition training computer-vision,1,1,1
683,3361,1,,2017-05-23T00:22:31.627,8,518,"i 'd like to do some experimenting with neural net evolution ( neat ) . i wrote some ga and neural net code in c++ back in the 90s just to play around with , but the diy approach proved to be labor - intensive enough that i eventually dropped it . things have changed a lot since then , and there are lots of very nice open source libraries and tools around for just about any interest one might have . i 've googled different open source libraries ( e.g. deap ) , but i could use some help choosing one that would be a good fit ... i spent much of my time writing code to visualize what was going on ( neural net state , population fitness ) or final results ( graphs , etc ) . maybe this would have to be fulfilled by a separate open - source library , but visualization support would be something that would allow me to spend more time on the problem / solution and less on implementation details . i know c / c++ , java , c # , python , javascript and a few others . something that 's a nice trade - off between a higher - level language and good performance on home hardware would be a good choice . can someone with experience suggest a good open source library or set of tools ?",7374,1671,2018-03-05T19:10:53.727,2018-04-12T23:38:38.870,open - source tool for home ai learning / experimentation ?,neural-networks genetic-algorithms neat,6,3,6
684,3364,1,3366,2017-05-23T06:44:19.180,3,118,"suppose we create two units ( or programs ) which run in parallel and we label them as a cognitive unit and the conscious cognitive unit . a human has two units analogously . a rational analyzer and not so rational analyzer . ( is there any third thing ? please comment . ) in my opinion , the consciousness is an extra layer of decision making . this resembles the way metaheuristics work . we have a set of rules for decision making and we analyze them and tune those rules dynamically . global search algorithms mimic the conscious behavior whereas the local search algorithms work like rational mind .",7377,7377,2017-05-25T11:21:06.607,2017-05-25T11:21:06.607,can we create a 2 unit conscious agent ?,ai-design ai-community,1,3,2
685,3368,1,,2017-05-23T11:06:33.233,7,917,"i was prompted towards this question while trying to find server racks and motherboards which are specialized towards artificial intelligence . naturally i went to the supermicro website . there the chassis+motherboard which supported the maximum gpus in the "" artificial intelligence "" category could support upto 8 of them . additionally , the nvidia dgx-1 also has only 8 tesla p100 gpus . and just to rub it in , matlab does not support more than 8 gpus last i checked . so , are more than 8 gpus practical for dl ? i would take caffe , cntk , tensorflow and torch7 as reference .",236,7402,2017-08-14T22:13:36.670,2017-08-14T22:13:36.670,are more than 8 high performance nvidia gpus practical for deep learning applications ?,machine-learning deep-learning hardware,1,0,1
686,3371,1,,2017-05-23T18:36:07.900,1,74,"i am building a smart mirror where it displays a website and the website would have voice recognition and face recognition . for voice recognition / voice commands i will be using a javascript library called annyang . i am looking for a face recognition package just like annyang , i would be happy if it 's open source and really easy to use ... it would be really helpful if you could add more tags , currently i do n't have enough reputation ... any help and suggestions are much appreciated , sid .",7392,,,2017-05-25T21:19:42.443,web based face recognition,image-recognition,1,1,
687,3372,1,,2017-05-23T21:15:28.307,5,110,"basically , an ai that can create , rig , and texture 3d models and game environments ( by extrapolating from collections of reference models , according to user input ) , and that can set up physics and mechanics ( assuming that the ai has access to a 3d modeling studio and a game engine , both designed for compatibility with the ai , or as a component of the ai ) , all according to user commands ( and allowing for tweaking and optimizations of models , rigging , mechanics , etc , by the user ) . an example of user commands would be something like : "" gaming ai , create a casual style * male model , european build , 6'5 "" , fit and slightly skinny , with red scaly skin , green eyes , a reptilian tail , demonic wings , claws , sharp teeth "" , etc . the user probably would n't add all of these characteristics at once , but rather one at a time , tweaking each feature via ai commands or manually . * "" casual style "" is a fictional "" style class "" . style classes would refer to the visual style of the models . possible example styles include "" cartoon "" , "" abstract "" , "" gothic "" , "" steampunk "" , "" serious "" and "" realistic "" . here 's another example of user commands , for a environmental model : "" gaming ai , create a serious style house , victorian , two story , white with beige trim , with porches and shutters . give it a creepy aesthetic . "" again , models could be created and modified or have features added in a step by step process , in order to tweak and refine them . i believe that such an ai would significantly reduce the amount of time , labor , and difficulty involved in designing games ; making games cheaper and easier to produce , and making game design available to everyone . a variation of such an ai could also be used to create 2d artwork and animations . but is such an ai even remotely possible ? and would it take a supercomputer to run the thing ? ( i 'm under the impression that such an ai would need to be capable of learning and adapting , and would require a massive and expansile "" association library""*—including 2d and 3d models , and verbal and textual speech — as well as near human intelligence ) * if the term "" association library "" does n't exist , or does n't currently relate to ai , then i just made it up . according to my made up definition , an association library is the library of programmed or learned associations that an ai uses to generate responses , and to , in this context , generate 3d models ; and probably to write or select code as well , in order to set up physics and mechanics and the like .",5698,,,2019-05-01T15:28:56.383,feasibility of an ai assistant to expedite game development ?,ai-design,2,2,1
688,3373,1,,2017-05-24T05:31:50.243,5,412,"does google manufacture tpus ? i know that google engineers are the ones responsible for the design , and that google is the one using them , but which company is responsible for the actual manufacturing of the chip ?",3323,1671,2019-04-30T18:54:13.233,2019-04-30T18:54:13.233,who manufactures google 's tensor processing units ?,tensorflow hardware google,1,0,2
689,3374,1,3382,2017-05-24T19:07:38.987,12,13618,"i am a software engineering student and i am complete beginner to ai . i have read a lot of articles on how to start learning ai , but each article suggests a different way . i was wondering if some of you experts can help me get started in the right way . a few more specific questions which language should i focus on ? a lot of articles suggest python , c++ or lisp for ai . can i use java instead of any of the other languages mentioned ? what kind of mathematical background should i have ? during the first year , i did discrete mathematics , which included the following topics : sets , matrices , vectors , functions , logic and graph theory ( they taught these topics briefly ) . are the are there any more topics that i should learn now ? for example , calculus ? if possible , i would appreciate any resources or books i could use in order to get started , or maybe you guys can give me a detailed procedure i can follow in order to catch up with to your level . note : for now i would like to focus on neural networks and machine learning . after i that i would like to explore robotics and natural language processing .",4427,2444,2019-04-08T21:20:27.747,2019-04-08T21:31:10.420,how does one start learning artificial intelligence ?,ai-basics getting-started math programming-languages,6,1,18
690,3387,1,,2017-05-26T15:04:56.100,2,552,"can neural networks be used to study ( elementary ) number theoretic problems ? what are examples where this has been done in the past ? or is there on the contrary an understanding that neural networks are not helpful for such problems ? to make the question more concrete , let me give an example of the kind of number theoretic problem i 'm thinking of : given two natural numbers a and b i may want to compute the rounded down quotient int(a / b ) . naively i would restrict to 64 bit unsigned numbers and build a neural network that has 128 neurons in the input layer and 64 neurons in the output layer representing the binary expansion of the numbers . assuming i laid out the network properly and trained it well , would i be expected to get useful output ? in particular , would i be able to interpret the output as a number and would it often be the right answer ? note : the reason why i think of this as problem as "" number theoretic "" is because i want to compute int(a / b ) rather than the rational number a / b . this is essentially a step in euclid 's algorithm . so the non - linear behaviour int(4/3 ) = 1 , int(5/3 ) = 1 , int(6/3 ) = 2 is crucial and would need to be recognizable in the output .",7459,75,2017-05-27T17:08:36.197,2018-09-05T08:52:28.937,neural networks and number theory,machine-learning,1,3,2
691,3389,1,7598,2017-05-26T15:15:31.397,18,5527,"i am not looking for an efficient way to find primes ( which of course is a solved problem ) . this is more of a "" what if "" question . so , in theory : could you train a neural network to predict whether or not a given number n is composite or prime ? how would such a network be laid out ?",7458,75,2017-05-27T17:06:06.100,2019-03-29T01:17:34.923,could a neural network detect primes ?,neural-networks,4,2,3
692,3390,1,3394,2017-05-26T15:31:47.283,4,100,"i can see a lot of tutorials and examples about using tensorflow and other free , open - source ai / ml / dl frameworks on enterprise level where enough data was collected for such ai solutions . how can one can collect enough data in normal everyday life to practically and effectively make use of such freely available ai / ml / dl technologies to improve one 's life and security ?",6933,75,2017-05-27T17:14:37.367,2017-05-27T17:14:37.367,"how can one find / collect data for , and come up with ideas for , using deep learning / ai to improve one 's everyday life ?",deep-learning datasets,1,1,1
693,3397,1,,2017-05-27T20:16:24.543,8,3657,"just watched a recent wired video on virtual assistants ' performance on telling jokes . they 're composed by humans , but i 'd like to know if ai has gotten good enough to write some .",,,,2017-09-13T00:20:53.223,can ai write good jokes yet ?,machine-learning,2,0,2
694,3398,1,3421,2017-05-28T04:41:39.593,1,485,"the deep learning algorithms i would to know the limits of are : cntk caffe tensorflow torch7 theano for example : i 've heard tensorflow is near impossible to parallelize on 8 gpus and above . so , in this case , the limit would be 8 .",236,7496,2017-06-01T19:06:46.907,2017-06-13T11:50:24.840,how many gpus can these deep learning algorithms be parallelized across ( batch parallelization ) ?,deep-learning deep-network,2,0,1
695,3400,1,,2017-05-28T14:56:02.380,1,72,"lets say you install your lstm machine on a road between london and oxford . and it makes observations . a car with 3 people inside drives past it in one direction 21 sec after previously observed car ( in any direction ) : the input is { 3 , lo , 21}. a bus with 43 people drives in other direction 11 sec later : { 43 , ol , 11 } i ca nt find a definitive general explanation ( without references to existing ml packages code ) - how your lstm layer structure should look like to accept those params ( there are 3 of them and they are vastly different between themselves but presenting them together at each step is very significant ( as opposed to splitting them to 3 streams and feeding then into 3 lstms and then pooling the results ) . could someone explain it in a single formula or a drawing ?",7487,,,2018-01-15T04:34:51.127,multi - param lstm input,machine-learning lstm,1,3,
696,3401,1,,2017-05-28T17:20:27.957,2,143,if i want to train a convoluted nn on time series but i can not decide where to split the data . i see that other people use jumping window over the input . so the feed say 20 sec of observation as 1 sequence into a cnn . i can not do that as splitting the observation by fixed size will most definitely break important patterns - ie first part of a pattern will go into the end of the current seq and the rest into the beginning of the next seq . i can however find a sensible solution by preprocessing data and finding places of much smaller significance and make seq cutting in the middle . but then the seq length will vary greatly . can i still use cnn ? or this idea is silly ? how else people extract features from time series using cnns ?,7487,,,2017-05-28T17:20:27.957,how to feed a variable size sequences into a cnn ?,machine-learning convolutional-neural-networks,0,0,
697,3402,1,3643,2017-05-29T02:22:25.197,2,291,"imagine a simple scenario of having a large repository using one framework and integrated with data / robots / etc , then having a new feature requested and the framework missing some vital functionality that is available in another framework ( say a new kind of layer ) . for many mathematical libraries it can be easy to reverse engineer the specific function that one would want form the other framework so as to not import the entire framework but in the case of deep learning , this is n't so trivial nor that easily testable . in these cases , does one take on the problem of reverse engineering the functionality or do they attempt to combine the architectures ? if its the combination , what are the biggest issues with integration ?",4398,4398,2017-07-14T00:53:00.080,2017-07-14T01:29:41.967,is there ever a need to combine deep learning frameworks ? ( eg . tensorflow & torch ) ?,deep-learning tensorflow software-architecture,1,0,1
698,3403,1,3405,2017-05-29T09:02:28.270,6,986,"i created an openai gym environment , and i would like to check the performance of the agent from openai baselines dqn approach on it . in my environment , the best possible outcome for the agent is 0 - the robot needs zero non - necessary resources to complete a task . the goal is to minimize the need for resources : for each needed ressource , there is a penalty of -1 . in many states , only certain actions make physical sense . how do i deal with this ? there was already a question about the handling of invalid moves on ai stackexchange , recommending to ignore the invalid moves . however , ignoring them would imply returning the same state and a 0 reward , the best possible outcome , which is clearly not the case . setting drastic negative rewards also does not seem to work , since even promising handling paths are compromised by invalid actions and the corresponding drastic negative reward . what are other ways of handling invalid actions in scenarios where all rewards are either 0 ( best ) , or negative ? my ideas/ questions on this for the openai baselines dqn approach implementation 1 ) is there any way to set the initial q - values for the actions ? i could set -infinity for the invalid actions . 2 ) is there any way to limit the set of valid actions per state ? when after the env.step(action ) function the new state is returned , can i somehow define which actions are valid for it ?",7495,,,2017-05-29T11:08:27.723,openai baselines dqn - handling of invalid actions,deep-learning reinforcement-learning,1,0,1
699,3406,1,,2017-05-29T11:17:59.697,2,1101,i 'm currently studying different kind of agents and what is the difference between a model - based reflex agent and simple reflex agents . what is the role of the internal state ?,7500,1671,2018-11-01T20:43:19.540,2018-11-01T20:43:19.540,model - based reflex agent vs. simple reflex agent,intelligent-agent,1,1,1
700,3407,1,,2017-05-29T18:30:16.880,7,103,"in the process of segmentation , pixels are assigned to regions based on features that distinguishes them from the rest of the image . value similarity and spatial proximity , for example , are two important principles that assume that points in the same region will have pixels that are spatially close and have similar values . in lots of situations this is true , but what about regions composed of pixels that are not similar in value ? consider the image below . the same "" logical "" region is composed of different elements that together represent something meaningful . in the same region there are trees with different sizes and shapes , with shadow over some of them etc . there are different things , with pixels that differ a lot in value , but i still need to group them together in the same region . from the image you can see that i do n't care so much with differences in color . in this case texture is the most important attribute . what algorithms are used to do the segmentation and classification in problems like that ? i 'm already looking for some algorithms and techniques that focus on texture , but some opinions from the experts will help me a lot . i think i need some orientation . thanks !",,,2017-05-31T17:11:09.780,2017-10-24T02:51:06.500,what algorithms are used for segmentation and classification of non solid regions in an image ?,image-recognition classification computer-vision,1,5,
701,3413,1,,2017-05-31T04:36:21.277,1,46,"in peter norvig 's paradigms of artificial intelligence programming , chapter 4 , which is about the all - famous general problem solver ( gps ) . in this chapter , the author asks a question ( 4.4 ) , which is as follows : the not looking after you do n't leap problem write a program that keeps track of the remaining goals so that it does n't get stuck considering only one possible operation when others will eventually lead to the goal . hint : have achieve take an extra argument indicating the goals that remain to be achieved after the current goal is achieved . achieve should succeed only if it can achieve the current goal and also achieve - all the remaining goals . "" here 's the description of these functions achieve : we input a goal , the list of operators , and the current state , in the achieve function , which checks if the goal is in the state already , if not , then checks if the goal is recursive , if not , then finds all the appropriate operators and then applies them , until the goal os achieved . or achieve returns a nil . achieve - all : this has a list of goals as an input , which it tries to achieve using the achieve function . moreover , it makes sure that all the goals are in the final state . otherwise , it returns a nil . the problem that i face is to find a relation between "" getting stucked "" over an operator and maintaining the number of goals remaining . moreover , the version of achieve in the gps program checks for all the operators that can achieve the goal , and then apply only the one that does the work . let 's forget this for a while , and consider the hint . it says that we should have an extra argument maintaining the goals that are remaining . but how will it help a goal , which is stuck because of applying the wrong operator ? there seems to be no relation between them . i know i 'm missing something here , but i ca n't find that .",7542,2444,2019-05-02T13:50:25.327,2019-05-02T13:50:25.327,relation between the number of goals remaining and using the wrong operator in a general problem solver,problem-solving,0,0,
702,3415,1,,2017-05-31T07:35:04.387,4,61,"i am interested in the current state - of - the - art ways to use quick , greedy heuristics in order to speed up the learning in a deep q - network in reinforcement learning . in classical rl , i initially set the q - value for a state - action pair ( s , a ) based on the result of such a greedy heuristic run from state s with action a. is this still a good idea in the setting of a neural network for the approximation of the q - function , and if yes , what are the optimal ways of doing it ? what are other ways of aiding the dqn with the knowledge from the greedy heuristics ? references to state - of - the - art papers would be highly appreciated .",7495,,,2017-05-31T08:41:03.460,what are state - of - the - art ways of using greedy heuristics to initially set the weights of a deep q - network in reinforcement learning ?,reinforcement-learning training intelligent-agent heuristics,1,0,
703,3418,1,,2017-05-31T13:41:17.817,0,220,"are there approaches other than convolutions to learn features from images ? has there been any research to use approaches such as hashing ( e.g. p - hash , diff - hash etc . ) in lieu ?",7332,,,2017-06-08T23:26:52.483,feature extraction other than convolutions for images ?,image-recognition convolutional-neural-networks,2,0,
704,3419,1,,2017-06-01T02:50:27.133,7,707,"some papers say that bleu is not a appropriate evaluating method for chatbot , instead they use perplexity to estimate chatbot . first of all , what is perplexity ? how to calculate it ? and why is perplexity a good evaluation metric for chatbots ?",7564,2444,2019-02-26T14:57:17.277,2019-02-26T14:57:17.277,why is perplexity a good evaluation metric for chatbots ?,neural-networks natural-language-processing chat-bots,1,1,3
705,3420,1,,2017-06-01T08:49:15.507,2,191,"i am reading "" supervised sequence labelling with recurrent neural networks "" written by alex graves to try to understand lstm networks and i am a bit confused about the equations . specifically , what i am confused about is the term "" state "" . when used in an equation ( section 4.5.2 ) , it says : i know that some system can be in a state , for example , due to the setup of values of different nodes in a graph . but how can a state be described in the case of a neural network and how can the equation above be explained other than that it is the state ( or states of several timesteps as in recurrent neural networks ) of a neural network ?",6645,2444,2019-04-29T16:03:04.537,2019-04-29T16:03:04.537,what is a state in a recurrent neural network ?,neural-networks recurrent-neural-networks terminology lstm definitions,1,3,0
706,3426,1,,2017-06-02T14:35:48.083,2,694,"i want to train a neural network with pictures of public figures ( politicians , singers , etc ) , but i do not know if it 's legal , i do not plan to show them in my project i only want to use them to train the neural network , can this cause legal problems ?",6850,,,2017-06-22T16:01:42.633,is it illegal to use pictures of public figures to train a neural network ?,neural-networks legal,3,0,
707,3428,1,3429,2017-06-03T16:44:45.890,5,2874,"how exactly are "" mutation "" and "" cross - over "" applied in the context of a genetic algorithm based on real numbers ( as opposed to just bits ) ? i think i understood how those two phases are applied in a "" canonical "" context where chromosomes are strings of bits of a fixed length , but i 'm not able to find examples for other situations . what would those phases look like on the domain of real numbers ?",242,75,2017-06-06T12:45:03.000,2018-05-13T14:37:15.177,mutation and crossover in a genetic algorithm with real numbers,genetic-algorithms genetic-programming,2,4,2
708,3433,1,,2017-06-05T09:00:41.587,2,121,i recently read about algorithmic bias in facial recognition . is the bias created by the training set provided or something else ?,6687,1671,2017-12-09T23:20:11.870,2017-12-09T23:20:11.870,is algorithmic bias due to training set used ?,machine-learning image-recognition facial-recognition,2,0,1
709,3435,1,8739,2017-06-05T17:36:55.457,2,183,i wanted to train a chat bot for answering questions from books . i am trying to use dynamic memory networks to do so . how can i generate a data set like facebook did in case of babi tasks so that it can tackle a variety of questions on the data set .,7330,,,2018-11-02T08:25:16.393,how to generate question answer data set like babi from books,natural-language-processing training chat-bots,2,0,
710,3440,1,,2017-06-06T16:38:48.323,2,132,"i 'm developing a log analyzer to predict and find errors in an equipament . each logged data contains the following format : timestamp | log source | type of message | message each entry log i want to represent by one pixel rgb because , considering the 24 bits , is possible to represent the last 3 parameters ( log source , type of message and message ) , but i do n't have bits enought to represent the timestamp ( this data i will to represent by the diference o previous timestamp "" delta of time "" ) , the resolution of time is second , and sometimes we have a large time between one log of another . the example above illustrate the situation : this project has the purpose of analyze the log to find and predict errors , and this preprocessing is used to simplfy the data entry to the machine learning algorithm , this is a good way to represent data to an rnn ? or for this kind of problem exist a better way to make an analyses ?",6978,,,2017-06-06T16:38:48.323,preprocessing of training dataset for machine learning,recurrent-neural-networks datasets prediction,0,0,
711,3441,1,,2017-06-06T20:33:36.100,1,284,"i am not sure if this question fits ai stack exchange so feel free to delete it in case it does n't . recently i found out about somewhat famous eliezer yudkowsky and machine intelligence research institute he founded . their philosophy and organisation seem interesting but i 'm curious about their credibility . i 'm pretty sure this is not a con and they seem to be producing a lot of articles . however , few of those are published and none in the journals mentioned here . so , my question is : is miri doing a genuine high quality research ?",7704,8,2017-06-20T20:34:13.547,2017-06-20T20:34:13.547,has miri produced good research ?,research ai-community,1,0,2
712,3442,1,3443,2017-06-07T00:30:18.617,5,153,"if we have well designed autonomous ai vehicles , then why wo n't the usa government allow it to be witnessed in the public ?",7720,2444,2019-04-30T13:40:53.627,2019-04-30T13:46:08.677,why is it illegal for google 's autonomous car to drive on the road by itself ?,ethics autonomous-vehicles google,2,0,
713,3451,1,,2017-06-07T14:34:03.020,4,620,"i 've seen events , like cogx and articles which describe how machine learning techniques or algorithms can be used to diagnose mental health issues . here is my question ; how can artificial intelligence and machine learning algorithms or techniques be used in diagnosing mental health issues , besides for e.g. facebook using machine learning algorithms to detect people who may commit suicide ?",7749,1581,2017-08-27T20:30:54.680,2017-11-03T02:52:17.487,diagnosing mental health problems using artificial intelligence / machine learning techniques,machine-learning research healthcare,4,6,2
714,3453,1,4973,2017-06-07T15:17:32.610,0,76,"i 've read through a few papers on next frame prediction from a sequence of frames and several of them use spatial transformations ( stns ) . see this as an example . i want to know what are the pros and cons of using an stn to predict the next frame . are there any assumptions that must be made about the data besides "" consecutive frames are all approximately affine transformations of each other "" ?",4398,1671,2017-12-21T21:07:15.593,2018-01-11T15:33:48.933,what are the pros and cons of using a spatial transformation network to predict the next video frame ?,neural-networks deep-learning image-recognition deep-network prediction,1,0,
715,3457,1,3466,2017-06-07T18:20:37.757,2,131,what are the ethical and legal issues of self driving cars being released in the uk ? this question came up on our exam today and i was left in a daze . i initially thought it would be issues like the legal driving age for self driving cars since they are ai do you need a minimum age limit ? another one i thought was whether or not you need driving licenses for ai cars ? could someone please list all the possible ethical and legal issues that surround ai controlled cars ?,7756,8,2017-06-13T19:54:39.130,2017-06-13T19:54:39.130,what are the ethical and legal issues of self driving cars being released in the uk ?,self-driving ethics,1,2,1
716,3463,1,,2017-06-08T19:40:12.340,3,858,i have a data set with four inputs and one output . i need to infer the 4 inputs given an output . what is the best way to do this ?,6978,,,2018-01-06T16:38:14.560,neural network model to infer inputs given an output,neural-networks,2,0,1
717,3465,1,3470,2017-06-08T20:10:18.253,3,1521,"i am looking to train a dataset that would output a sequence of letters ( i 'm using this for peptide sequences ) . since i have 22 different possibilities of amino acids , i need to output a vector that contains multiple amino acids with varying frequencies . for example , an output would look like [ 0,1,3,2,0,2,0,3 ... ] ( a 22-long vector ) . how can i train a neural network to output that type of vector ?",7773,,,2017-06-13T10:48:08.240,multi - label classification with non - binary outputs,neural-networks machine-learning deep-learning classification,1,6,
718,3469,1,,2017-06-09T11:21:34.740,12,1216,"i am writing a simple toy game with the intent of training a deep neural network on top of it . the games rules are roughly the following : the game has a board made up of hexagonal cells . both players have the same collection of pieces that they can choose to position freely on the board . placing different types of pieces award points ( or decrease opponent 's points ) depending on their position and configuration wrt one another . whoever has more points win . there are additional rules ( about turns , number and types of pieces , etc ... ) but they are not important in the context of this question . i want to devise a deep neural network that can iteratively learn by playing against itself . my questions are about representation of input and output . in particular : since pattern of pieces matter , i was thinking to have at least some convolutional layers . the board can be of various size but in principle very small ( 6x10 on my tests , to be expanded by few cells ) . does it make sense ? what kind of pooling can i use ? how to represent both sides ? in this paper about go , authors use two input matrices , one for white stones and one for black stones . can it work in this case too ? but remember i have different types of pieces , say a , b , c and d. should i use 2x4 input matrices ? it seem very sparse and of little efficiency to me . i fear it will be way too sparse for the convolutional layers to work . i thought that the output could be a distribution of probabilities over the matrix representing board positions , plus a separate array of probabilities indicating what piece to play . however , i also need to represent the ability to pass the turn , which is very important . how can i do it without diluting its significance among other probabilities ? and most importantly , do i enforce winning moves only or losing moves too ? enforcing winning moves is easy because i just set desired probabilities to 1 . however when losing , what can i do ? set that move probability to 0 and all the others to the same value ? also , does it make sense to enforce moves by the final score difference , even though this would go against the meaning of the outputs , which are roughly probabilities ? also , i developed the game engine in node.js thinking to use synaptic as framework , but i am not sure it can work with convolutional networks ( i doubt there 's a way to fix the weights associated to local perceptive fields ) . any advice on other libraries that are compatible with node ?",4259,,,2019-05-03T15:02:40.523,input / output encoding for a neural network to learn a grid - based game,neural-networks deep-learning game-ai,2,1,3
719,3472,1,,2017-06-10T06:36:21.650,5,1962,i am working on this code for spam detection using recurrent neural networks . question 1 . i am wondering whether this field ( using rnns for email spam detection ) worths more researches or it is a closed research field . question 2 . what is the oldest published paper in this field ? quesiton 3 . what are the pros and cons of using rnns for email spam detection over other classification methods ?,6050,,,2019-01-01T21:23:10.047,spam detection using recurrent neural networks,classification recurrent-neural-networks,2,3,
720,3473,1,3477,2017-06-10T12:23:27.993,2,434,"o'reilly recently published an article about the machine learning paradox . ( link ) what it says goes basically like this : no machine learning algorithm can be perfect . if it was , it means it is overfitting and so it is not really perfect because it will not perform ok in real world scenarios . i searched and i could n't find any other references to this paradox . the closest i got is the accuracy paradox , which says that the usefulness of a model is not really well reflected in its accuracy . this does n't sound quite ok to me . for example , a linear model could be perfectly learned , "" overfitted "" and predicted in the real world . so i suspect it is really about finding the right set of data points from which the results can be inferred . this is , we are trying to approximate from uncertain data , but with the right data we can stop approximating and start calculating . is my line of thinking correct ? or is there really no perfect machine learning ? update : in the light of currently received answers , i think my last paragraph ( my line of thinking ) can be rephrased as : if we have a model simple enough , why ca n't we overfit the model , knowing that it will behave correctly in non - trained data ? this assumes the training data completely represents the real - world data , which would imply a single model that we can train on . keep in mind that what we conceive as "" simple "" or "" feasible "" is arbitrary and only depends on computation power and available data -- aspects with are external to ml models themselves .",190,190,2017-06-11T18:52:32.120,2017-06-12T18:09:08.150,is there such a thing like the machine learning paradox ?,machine-learning,3,3,2
721,3482,1,3487,2017-06-12T20:48:17.710,4,764,"so i 've been working with neural networks and artificial intelligence for a while and what i 'm trying to do right now is , from a genotype i have ( a sum of sensors , neurons and actuators ) draw how the neural network is ( with recurrent / recursive connections being showed nicely , etc . ) what i have done now in javascript is this : i have achieved this using sigmajs a javascript node drawing library , but i think it 's still ugly , and what i 'm looking for is a node drawing library that can achieve recursive connections in a nice way ( right now i 'm drawing them with a red color as you can see on the image ) . i have examined a lot of github repositories and websites that can be helpful but are n't worth since they are n't that nice . has anyone got an idea of what can i use , in javascript ? if not , in any other language , how can i achieve what i want ? regards , miguel",7846,,,2017-06-17T14:41:59.657,is there any way to draw a neural network connections in a nice way ? [ javascript ],neural-networks,2,0,
722,3483,1,,2017-06-12T22:00:31.630,3,85,"is it possible to train a neural network to learn something via video footage ? in other words , if i have a video teaching me how to draw an animal from scratch , can i then use this video to teach the computer to draw the animal in the same way ? edit : video footage is essentially a sequence of images , any image processing capabilities available to us through machine learning are possible when applied to videos using a sequencial network ( lstm , rnn etc . ) so i guess the difficult part becomes mapping the activity to an action like moving a pen or something",7816,7816,2017-06-15T11:15:54.920,2017-06-15T11:15:54.920,teaching neural net via video footage,neural-networks,3,0,
723,3488,1,7130,2017-06-13T10:50:56.343,16,18837,"i 'm struggling to understand the gan loss function as provided in understanding generative adversarial networks ( a blog post written by daniel seita ) . in the standard cross - entropy loss , we have an output that has been run through a sigmoid function and a resulting binary classification . sieta states thus , for [ each ] data point $ x_1 $ and its label , we get the following loss function ... $ $ h((x_1 , y_1 ) , d ) = -y_1 \log d(x_1 ) - ( 1 - y_1 ) \log ( 1 - d(x_1 ) ) $ $ this is just the log of the expectation , which makes sense , but how can , in the gan loss function , we process the data from the true distribution and the data from the generative model in the same iteration ?",7858,2444,2019-04-15T16:09:54.203,2019-04-15T16:09:54.203,understanding gan loss function,neural-networks machine-learning deep-learning loss-functions generative-adversarial-networks,3,0,4
724,3490,1,,2017-06-13T14:42:56.007,4,228,"while conducting research , i recently stumbled upon the deep learning and natural language processing concepts . in this question they say that the ‘ grammar induction ’ is a ‘ supervised learning ’ mode . so i was wondering : let ’s say that there ’s a way - more - than - human intelligent alien probe orbiting our planet . it can receive , decode and analyze all the broadcasting signals leaving the earth . for all we know at this moment , how could it learn the basics of a language with nothing else but our broadcasting signals , so without the help from a ‘ supervisor ’ ? how would an artificial intelligence human engineer ( theoretically ? ) face the problem ? i ’m interested on the more “ technical ” side of the issue .",7785,4302,2018-10-08T12:27:44.953,2018-10-08T12:27:44.953,unsupervised alien natural language learning,natural-language-processing unsupervised-learning,1,4,
725,3494,1,3503,2017-06-15T09:13:48.660,39,20926,"first of all , i 'm a beginner studying ai and this is not an opinion oriented question or one to compare programming languages . i 'm not saying that is the best language ( actually i know almost nothing about python ) . but the fact is that most of the famous ai frameworks have primary support for python . they can even be multilanguage supported , for example , tensorflow that support python , c++ or cntk from microsoft that support c # and c++ , but the most used is python ( i mean more documentation , examples , bigger community , support etc ) . even if you choose c # ( developed by microsoft and my primary programming language ) you must have the python environment set up . i read in other forums that python is preferred for ai because the code is simplified and cleaner , good for fast prototyping . i was watching a movie with ai thematics ( ex_machina ) . in some scene , the main character hacks the interface of the house automation . guess which language was on the scene ? python . so what is the big deal , the relationship between python and ai ?",7268,1671,2019-03-01T21:51:34.153,2019-03-01T21:51:34.153,why is python such a popular language in the ai field ?,machine-learning programming-languages tensorflow,7,1,20
726,3497,1,3501,2017-06-15T14:06:25.873,1,287,i am a machine learning newbie . i am trying to understand backpropagation algorithm . i have a training dataset of 60 instances / records . so what is the correct order of the process : forward pass of the first instance . calculate the error . weight update using back propagation . forward pass of the second instance . calculate the error . weight update using back propagation . and so on ... ( or ) forward pass of all instances one by one . ( noting the error as vector ) weight update using back propagation . this video https://www.youtube.com/watch?v=owdjryupnge is similar to the second process . is it correct ?,7899,7899,2017-06-15T14:12:51.903,2017-06-15T19:00:33.540,what is the order of execution of steps in back propagation algorithm in a neural network ?,neural-networks machine-learning deep-learning training backpropagation,1,1,
727,3499,1,3500,2017-06-15T14:47:42.427,1,70,"i know nowadays agencies are using gpus in order to accelerate ai , but how fast should be it to be efficient , i mean i know that depends of how large and complex the assignment is but what would be a way to measure its efficiency and what kind of technology(amount of gpus , ram , storage ) and techniques need to be used in order to get enough efficency ? any thoughts from experts would be appreciated",7137,,,2017-06-15T19:35:09.600,how fast does need to be an ai agent to be efficient ?,efficiency,1,4,
728,3502,1,,2017-06-16T04:10:16.480,12,5198,is there a way to teach reinforcement learning in applications other than games ? the only examples i can find on the internet are of game agents . i understand that vnc 's control the input to the games via the reinforcement network . is it possible to set this up with say a cad software ?,7816,2444,2018-11-19T07:34:51.510,2018-11-19T07:34:51.510,are there any applications of reinforcement learning other than games ?,reinforcement-learning applications,3,4,2
729,3505,1,,2017-06-16T11:42:52.950,2,469,"i want to train my neural network by evolution , that is i want to recombine the weights of the best performing neural networks in each evolution cycle or generation . my initial instinct was to represent weights as they are , which is variable of type double , and either 1- swap weights between the two parent network 2- generate random number between the two weights but what i need is to represent the weights as binary string and then carry out the crossover on the string as usual . what i want to ask is how can i take my double [ ] of weights and convert that into string [ ] with byte representation of the number ? and should the chromosome contain an array of string where each string represents a single weight ?",7910,,,2017-06-16T16:22:35.843,how to represent the weights of a neural network as binary strings for genetic algorithm,neural-networks genetic-algorithms evolutionary-algorithms,1,3,
730,3506,1,,2017-06-16T13:57:29.430,2,126,"let 's say we have the basic scenario where two agis of about the same intelligence ( but not same origins / code / model ) have to communicate as efficiently as possible to achieve a common goal . now we could have 2 starting points for that : 1 ) either all they have is a common communication bus ( e.g. sound , light , radio , etc . ) and instruments ( e.g. transceivers ) to support it , and they have to figure out the rest . 2 ) or they are some kind of advanced chatbots , but since the human language is lacking a lot to be used as a highly efficient protocol , they will have to communicate with what they have , to build a proper one . would it be possible to somehow induce them to communicate , and try to figure out what each other "" say "" ? how could this be done ? and a more abstract question is how could this protocol "" look "" like ?",6899,,,2017-06-17T22:50:57.573,could agi build its own communication protocol ? how ?,unassisted-learning agi ultraintelligent-machine,2,2,
731,3515,1,3522,2017-06-19T15:17:11.943,2,224,"i 'm new in ml and ai , actually learning right now . but i 'm thinking about starting taking part in some projects in ml . is kaggle is a good place to find projects in ml to work on ?",7948,1671,2017-06-19T18:12:46.410,2017-06-25T22:12:37.710,has anyone participated in kaggle competitions ?,machine-learning,3,2,
732,3516,1,,2017-06-19T20:40:49.493,2,62,"i 'm looking to build a sequence - to - sequence model that takes in a 2048-long vector of 1s and 0s as my input and translating it to my known output of ( a variable length ) 1 - 20 long characters ( ex . gbnmirn , ilceqzg , or fpsrabbrf ) . my goal is to create a model that can take in a new 2048-long vector of 1s and 0s and predict what the output sequence will look like . i 've looked at some github repositories like : https://github.com/llsourcell/seq2seq_model_live/blob/master/2-seq2seq-advanced.ipynb https://github.com/hans/ipython-notebooks/blob/master/tf/tf%20tutorial.ipynb but i 'm not sure how to implement it with my problem . are there any projects that have done something similar to this / how could i implement this with the seq2seq models currently out there ?",7773,,,2017-06-19T20:40:49.493,seq2seq vector to letters model,neural-networks machine-learning deep-learning recurrent-neural-networks lstm,0,0,
733,3518,1,3521,2017-06-20T01:14:13.320,5,317,"i am developing ai in the form of neat , and it has passed certain tasks like the xor problem outlined in the neat research paper . in the xor problem , the fitness of a network was determined by an existing function ( xor in this case ) . it also passed another tests . one i developed was to determine the sine at a certain point x in radians . it also worked , but yet again , its fitness was determined by an existing function ( sin ( x ) ) . i 've recently been working on training it to play tic tac toe . i decided that to determine its fitness , it would play against a "" dumb "" ai , placing o 's in random locations on the grid , and gaining fitness based on whether or not it placed x 's in a valid location ( losing fitness if it placed an x on top of another x or an o ) , and gaining a lot of fitness if it won against the "" dumb "" ai . this would work , but when a network got really lucky and the "" dumb "" ai placed o 's in impractical locations , the network would win and gain a lot of fitness , making it very difficult for another network to beat that fitness . therefore , the learning process did not work and i was not able to generate a tic tac toe network that actually worked well . i do not want the ga to learn based off an "" intelligent "" tic tac toe ai because the whole point of me training this ga is so that i do not have to make the ai in the first place . i want it to be able to learn rules on its own without me having to hard code an ai to be very good at it . so , i got to thinking , and i thought it would be interesting if the fitness of a network could be determined based off how well it played against other networks in its generation . this does seem similar to how humans learn to play games , as i learned to play chess by playing against other people hundreds of times , learning from my mistakes , and my friends also increased in their ability to play chess as well . if ga 's were to do that , that would mean i do n't have to program ai to play the game ( in fact , i would n't have to program a "" dumb "" ai as well , i would only have to hard code the rules of the game , obviously ) . my questions are : has there been any research or results from ga 's determining their fitness based off competing against each other ? i did some searching but i have no idea what to look for in the first place ( searching ' neat fight against each other ' did not work well :-( ) does this method of training a ga seem practical ? it seems practical to me , but are there any potential drawbacks to this ? are ga 's meant to only calculate predetermined functions that exist , or do they have the potential to learn and do some decision making ? if i were to do this , how would fitness be determined ? say , for the tic tac toe example , should fitness be determined based on whether or not a network places its x 's or o 's in viable locations , and add fitness if it wins and subtracts fitness if it loses ? what about tying the game ? should networks of the same species compete against each other ? if they did , then it would seem impractical to have species in the first place , as networks in the same species competing against each other would not allow a successful species to rise to the top , as it would be fighting against each other . kind of out of topic , but with my original idea for the tic tac toe ga , would there be a better way to determine fitness ? would creating an intelligent ai be the best way to train a ga ? thanks for your time , as this is somewhat lengthy , and for your feedback !",7958,16565,2019-01-31T20:55:00.847,2019-01-31T20:55:00.847,"could ga 's determine fitness by "" fighting "" against each other ?",neural-networks machine-learning genetic-algorithms,3,2,
734,3526,1,3527,2017-06-21T19:20:53.067,2,257,"by means of parts of speech tagging , words of a given sentence can be assumed to be noun / verb etc , but if the sentence is for instance : "" my favourite book is harry potter and the prizoner of azkaban "" note that the inputs i receive would be from a chat interface so having a fixed format for the data ca n't be expected . is there a way to identify "" harry potter and the prizoner of azkaban "" as a proper noun from such messages ? currently this query tags as : my|prp$ favourite|jj book|nn is|vbz harry|jj potter|nn and|cc the|dt prizoner|nn of|in azkaban|nn i would like to know if this can be handled some way , or if there is another algorithm that can handle this ?",7998,,,2017-06-22T09:51:09.330,tagging parts of speech when proper noun is a composite,natural-language-processing,1,0,
735,3528,1,3531,2017-06-22T13:35:38.667,30,18995,"which one would you recommend for a first approach to deep learning ? i 'm a neuroscience student trying for the first time computational approaches , if that matters .",8015,1671,2018-03-19T20:37:41.243,2018-03-19T20:37:41.243,tensorflow vs keras vs ... to begin with deep learning ?,deep-learning tensorflow keras getting-started software-evaluation,1,1,11
736,3532,1,,2017-06-23T11:11:14.820,2,71,"i have read some articles , some tutorials but i am still did n't implemented any ai system . so , my question may seem inappropriate for the giants in this field . but i have build certain program , downloaded to microcontroller and it will perform its task . but how to do all this with machine learning . can i implement ai engine using c like language and make it working in any gpp uc ? please feel free to modify , edit and upgrade this questions if you get my actual problem idea .",7888,,,2017-06-23T16:04:26.097,can we implement ml engine using any general purpose micro controller ?,machine-learning,2,0,
737,3533,1,3554,2017-06-23T13:06:15.847,6,2271,"lisp was originally created as a practical mathematical notation for computer programs , influenced by the notation of alonzo church 's lambda calculus . it quickly became the favored programming language for artificial intelligence ( ai ) research , according to wikipedia . if lisp is still used in ai , then is it worthy of learning it , particularly in the context of machine learning and deep learning ?",7681,7681,2018-11-17T14:03:54.430,2018-11-17T14:03:54.430,is lisp still worth learning today in the particular context of machine learning ?,machine-learning deep-learning history programming-languages lisp,2,3,
738,3537,1,,2017-06-25T01:58:31.467,1,63,"i primarily want to know , if you have been given a quantity / feature and it 's characteristics then how will you classify that feature ? what 's the intuitive criteria ? from vision pov it becomes a bit easier but in general how will you understand them and put then in a right category or is it even a right question to ask in every domain of ai ?",8061,,,2017-06-25T20:06:22.467,"what does it mean to categories a feature as low-,mid-,high - level ?",machine-learning deep-learning computer-vision,1,1,
739,3541,1,,2017-06-26T05:39:06.100,1,109,"to tune the parameters of particle swarm optimization ( pso ) , there are two methods offline and online . in offline manner , the meta - optimization is used to tune the parameters of pso by using another overlying optimizer . in the online manner , there are two techniques , self - adaptation , "" consisting of adding some or all of the optimizers behavioural parameters to the search - space , thus making them subject to optimization along with the problem at hand "" . another technique is meta - adaptation , "" in which an overlaying optimizer is trying to tune the parameters of another optimizer in an online manner during the optimization of a problem . "" "" the concept of meta - optimization . a black - box optimizer is used in an offline manner as an overlaying meta - optimizer for finding good behavioural parameters of another optimization method , which in turn is used to optimize one or more actual problems . "" in standard pso the particles are initialized by using uniform random numbers and these particles are updated using update equations . the best solution is selected based on the best value of objective function . in my work . i have two data sets , training and theoretical dataset and i need to initialize the particles by using training data instead of random numbers . in this case , how can i tune the parameters of pso using training and theoretical data set . also , i have a problem which is , i got the best cost in the initial step of pso and in the initial step there are no parameters or update equations . is it possible to tune the parameters using machine learning method ? how can i do this ?",8071,,,2017-06-26T05:39:06.100,tuning the parameters of particle swarm optimization ( pso ),machine-learning algorithm swarm-intelligence,0,0,
740,3544,1,,2017-06-26T10:41:15.883,2,86,"i need to report accuracies of my neural model in a conference paper as compared to various baselines . what are the accepted standards for reporting accuracies in a fair manner ? neural model : to be specific , i 'm using 60 % as train set , 20 % as validation set and 20 % as test set to report the accuracy of my neural model . should i take an average or highest of 3 runs accuracy where in each run i randomly sample 60 % as training data from the total 80 % train + validation set . my neural model is computationally intensive and therefore it is not feasible to perform a k - fold cross validation . will my accuracy results be accepted by the academic community without a k - fold cross validation ? since my data set is large , i assume using 20 % of it used solely for testing should be a fair indicator of accuracy . bag - of - words ( bow ) : how do i report the accuracy for this model so as to perform a fair comparison ? should i train bow on only 60 % of data ( same as which my neural model is being trained on ) or should i train bow on 80 % of data ( train + validation for my neural model ) ? which is the accepted way ? i then test bow on the same remaining 20 % of data ( as in neural model ) in either of the above case . the other approach is to perform k - fold cross validation but the test set will not be the same 20 % as the test set on which my neural model is being evaluated . is this approach recommended though ? any other information on how to report accuracy results in a research paper comparing neural models ( train , val , test ) with linear models - bow , svm ( train , test ) is welcome . please help .",8081,,,2017-06-26T10:41:15.883,"train , validation and test split for reporting accuracy of neural model and bow",neural-networks machine-learning deep-learning research training,0,0,
741,3545,1,,2017-06-26T12:37:06.240,1,399,why does not a researcher like geoffrey hinton with his valuable works in machine learning ( especially neural networks ) get turing award ?,6050,,,2017-06-27T09:24:52.360,what are the criteria for choosing turing award winners ?,neural-networks,1,2,
742,3546,1,3547,2017-06-26T14:11:01.577,3,173,"how many decades are we far from achieving a virtual world just like our real world ( or little graded down version ) with ai . if in far future if we are able to achieve that ( able to provide all the rules and manipulate all huge amounts of data ) will a time leap be possible . assuming time in virtual world moves much faster , as the computation rate is very high.so few hours of computation in real world will be years in the virtual world . so if we run the virtual world for few days and attain data from its final stage , will we get some future technology in our hand ? this concept of virtual world was put by me , assuming such powerful ai wo nt be integrated directly to our world without isolation tests . i was thinking about p and np problems , fermi paradox , path which will be taken by future ai and time leap and somehow reached to this question . fermi paradox ( assuming this world is also a virtual world created by much smarter species- which explains the isolation to a bit . ) assuming that 's the case wo nt nesting of virtual world put pressure on the root system(most outermost system ) running virtual world . i know its weird , but was curious . so the final question is .. the whole above stuff is a concept .. how much of reality of ai and its future can be compared with this concept .",8087,8087,2017-06-27T06:26:09.327,2017-06-27T06:26:09.327,time leap ( to achieve advance technology ) using ai by simulating virtual world at a faster rate,philosophy,1,2,
743,3548,1,,2017-06-27T03:58:31.947,11,1113,what is the mathematical background required to start learning ai ? what else should i also learn ?,8094,2444,2019-04-08T21:23:15.523,2019-04-13T19:08:23.863,how should i get started with artificial intelligence ?,ai-basics math getting-started,7,1,9
744,3558,1,,2017-06-28T23:14:43.333,3,634,"there is a lot of information on the internet about image classification and object identification . i want to know how i can extract ( or create ) geometric information from an image . for example , if the image classifier can identify that there is a sphere in the image , how can i generate 3d vertices 's and edges from that knowledge ? i want to find all the edges and points on these shapes",7816,7800,2017-11-09T19:05:05.150,2018-04-12T15:12:47.653,identifying geometry in images,machine-learning,1,1,1
745,3560,1,,2017-06-29T10:58:20.600,1,123,"alright , i want to write a mobile app that lets you take a photo of your equation , detects the equation , transforms it from pixels to text and then solves if it 's possible . right now , i am doing the part where i receive a photo ( 2d array of pixels ) and what to output a resized part of this photo ( the part has a rectangular shape ) that just contains the equation and some pixels around it . basically , i need a model that takes a 2d array and returns a resized version of it . since i am new to ai , i do n't know what techniques have been successful in optical character recognition . any suggestions ?",8146,1671,2017-06-29T18:47:48.223,2017-06-29T18:56:31.053,successful methods for optical character recognition ?,neural-networks machine-learning,1,2,
746,3565,1,,2017-07-01T19:36:26.990,5,105,"i talked with a graduate computer science who said one challenge of making artificial human - like is making random decisions , and that computers ca n't be random , that they always need a "" seed . "" but , if a computer 's outcomes are determined by the chaotic movements of electrons , it does n't seem like it should be difficult to program inherent uncertainty into a computer . so , what exactly is stopping people from harnessing this basic component of reality to allow artificial intelligence to make randomized decisions ? i mean , all you 'd need is different neural pathways that rely on the superposition of electrons , and that 's it .",8188,,,2017-07-01T23:40:25.130,why ca n't computers be random ?,human-like,1,2,1
747,3568,1,3569,2017-07-03T07:35:16.253,0,188,i am working on creating a lie detector ai based on facial expressions and body language . i want to know the best python libraries available for this task .,6508,,,2017-07-03T08:02:45.497,which python ml library will be the best for creating a lie detector ?,machine-learning,1,2,
748,3572,1,,2017-07-03T14:15:28.753,1,176,"i have an application of neural networks ( standard mlp architecture ) where i want to forecast a tanh output ( ranging from -1 to +1 ) with about 1500 input features in ~700 samples . each sample represents a snapshot of database tables at a given time of day - f.ex . at 11:50 . since i have so few samples , the network is very sensitive to overfitting . although it seems overkill to use neural networks , i find them to perform better than my initial experiments with other approaches , because i can very precisely tune their objective function and regularization , along with a nice possibility for multinomial regression . a severe problem in the task at hand is that the data in the tables may be adjusted over time . so the database might say that a given feature was last changed at 2016 - 01 - 01 11:45 , but in reality , a slight change was made a 2016 - 01 - 15 07:38 . in practice , this means that the training data is unrealistically precise , and that i may instead want to consider it a ballpark estimate . for example , a value of 153.18 may instead be considered a value of ~140 - 165 . to alleviate the problem , i have theorized a "" jitter "" neuron that - during training time , at each iteration - "" jitters "" the input by subtracting or adding a random value from the feature . the random value to add or subtract is a ratio of the standard deviation of the feature , and the ratio is itself random . the layer is placed right after the input neurons and has as many neurons as there are input features . in my example , the layer is therefore 1500 neurons wide . for example , consider an input feature that - across all training samples - has a standard deviation of + - 100 . at training time , a data sample is loaded with the feature value of 1122 ( for the sake of example , lets ignore input standardization ) , and the jitter neuron will incur a randomized change in its value . we define a "" jitter "" ratio of 0.1 , meaning that the ratio to "" jitter "" the feature with is drawn from a standard distribution with a standard deviation of 0.1 and a mean of 0 . in the example , a random roll of the dice lands us in +1.5 standard deviations , outputting a jitter ratio of 0.15 . given that the training set has a standard deviation of 100 , we add to the input feature ( that has a value of 1122 ) a final jitter of +15 , resulting in an input feature value of 1137 . the same operation goes for all input features . pragmatically , i do this in keras by generating a jitter - ratio - matrix with mean=0 and std=0.1 . the std - value can be an arbitrary amount , but we must not jitter the input data too much . the intuitive justification is that it is representative of my real world scenario . without going into too many details , the input features are typically things such as weather , whose forecasts are naturally unstable , and any changes to the database values backwards in time is likely to be a "" typical "" , small adjustment . on a more theoretical level , the justification is that it prevents overfitting to specific input features in some samples , as the next iteration across the same data sample will output significantly different output values after the forward pass through the network , as a slight change in the input feature may have a profound impact on the output after the feature has undergone compounding , non - linear activations . additionally , i perceive the jitter - neuron to be a form of data - augmentation , like it is done for especialle image classification ; instead of the architect generating n augmented samples according to some heuristic ( for example , in images , it is normal to crop and rotate the same image in several different ways to enlarge the training set ) , the jitter neuron generates a theorhetically infinite amount of augmentations at runtime . i keep imagining the parameter hyperspace for the neuron weights in the first layer ; instead of having many potential pits ( i.e. areas of overfitting ) , the jitter smooths out these pits , as the jittered input data now generates a different loss value . early experiments do not provide very good results , but not bad either . they just , sort of ... stay the same ... however , my dataset is rather small , and it is notoriously difficult to fit with any model . therefore , i ask of you what you think of the concept of jitter neurons : does it seem sensible ? any theoretical reason that it should / should not work ? is it , for some statistical reason or other , inherently inferior to dropout ? ( notice that i also add dropout , albeit smaller rates ) any proposals for making it better ? comments , etc .",4747,,,2017-07-04T09:20:12.280,"comments on my proposed "" jitter "" neuron",neural-networks neurons mlp,1,1,
749,3573,1,,2017-07-03T16:55:46.917,9,2030,"it is assumed in computer science that the human mind can be replicated with a turing machine , therefore artificial general intelligence ( agi ) is possible . to assume otherwise is to believe in something mystical , and mystical beliefs are false . i do not know of any other argument that agi is possible , and the foregoing argument is extremely weak . is there a rigorous proof that agi is possible , at least in theory ? how do we know that everything the human mind can do can be encoded as a program ?",8221,1671,2017-07-06T19:34:57.960,2019-04-14T00:58:02.360,proof that artificial general intelligence is possible,philosophy human-like,9,5,2
750,3580,1,,2017-07-04T11:37:00.687,0,325,"i 'm new in this subject matter . i 'm a programmer so i understand sometimes how hard it 's to make an ai that can play games in an intellligent manner . and some ai 's , such as some chess players , are extremely well coded and have defeated humans in several matches . but i think that they won simply because computers can make calculations way faster than humans can not because they learned from their opponents . and if you put the same ai vs same ai who 'll win ? will the game continue indefinitely or will the game eventually finish because the ai 's play randomly ? so i start wondering if machine - learning and self - learning are really possible , and if ai 's simply make some decisions randomly ? will they really become smart ? i hope i 've been clearly to all of you :) update - i think both are connected . let me give an example of real world . little babies need to be teached and they learn a lot by them parents . like machines do :-) they are teached by their "" parents "" . ( machine - learning process ) but when babies start growing they start learning things by themselves , they learn what is bad what hurts and who is friendly completely alone . so machines should be like that they at some point start learning by themselves . if babies did n't start learning by themselves , imagine them asking to everyone "" is this good ? "" ( self - learning ) is my thinking right ? but you know how hard is the process of start learning by our own and the complexity of our brain , and the specific time in our lifes that randomly we start thinking by our own . so my question is ... if to us humans is difficult to explain that process and how it works , how can we teach it to machines ? i hope i 've been clearer than the previous one ;)",8171,8171,2017-07-10T16:23:05.307,2017-07-10T16:23:05.307,are machine - learning and self - learning really possible ?,machine-learning unassisted-learning chess,3,3,
751,3581,1,,2017-07-04T16:00:21.137,3,274,"i was checking services like microsoft azure 's cognitive services computer vision api and google 's vision api and they are amazing . i was wondering if these services , or any other cloud service for that matter , can recognize an image 's content and classify it on a set of fixed categories defined by me , not by the cognitive service provider . for example , i have different products and i will take several pictures of each product . i want to then use the cloud service and upload all the pictures of each product , so that i can then take a picture of one product and the computer vision algorithm will tell me which product i am seeing . is it possible ? is there a third party solution for this problem ? if so , how many pictures do i need to train each product 's recognition ? i hope i was clear . thanks in advance for any light on the topic !",8241,,,2017-08-18T09:58:01.607,is there a computer vision service for classifying images on a fixed array of images provided by me ?,image-recognition computer-vision,2,0,
752,3587,1,3588,2017-07-05T02:32:48.847,-1,65,"i have a complex neural net that will take forever on my laptop to train and i do nt have a computer with a gpu , is there a way to run a python script on another computer without having to install an ide on that computer ? ( for instance , if i went to an internet cafe )",7816,,,2017-07-05T03:57:12.037,train neural net on another computer ?,machine-learning,1,0,
753,3590,1,3591,2017-07-05T05:56:36.187,10,2087,"i was reading about john mccarthy and his orthodox vision of artificial intelligence . to me , it seems like he was not very much in favour of resources ( like time and money ) being used to make ais play games like chess . instead , he wanted more to focus on passing the turing test and ais imitating human behavior . i have also read many articles about major companies like ibm , google , etc . spending millions of dollars in making ais to play games like chess , go , etc . to what extent is this justified ?",6798,8259,2017-07-05T14:32:15.577,2018-05-21T06:56:06.977,why spend so much time and money to build ais to play games ?,research game-theory chess,4,2,2
754,3596,1,,2017-07-06T07:31:47.860,1,670,"i am trying to run deep q - learning algorithm on a game which i made in python using pygame library . the algorithm accepts the game screen ( 4 frames ) as input to neural network which used as the function approximator . the game looks like this ... player can move both the paddles and randomly a white ball is generated from the center of screen . if the paddle touches the white ball reward of +1 is awarded if it misses the reward of -1 is awarded and the same reward is passed in the q - learning algorithm to learn . only 3 actions are possible move to left , move to right and stay here is the code for my deep q learning algorithm ... from _ _ future _ _ import division , print_function from keras.models import sequential from keras.layers.core import * from keras.layers.convolutional import conv2d from keras.optimizers import adam from scipy.misc import imresize import collections import numpy as np import matplotlib matplotlib.use('agg ' ) import matplotlib.pyplot as plt import wrapped # stack the four frames together def preprocess_images(image ) : if image.shape[0 ] & lt ; 4 : xt_list = [ ] for i in range(image.shape[0 ] ) : x_t = imresize(image[i],(100,100 ) ) x_t = x_t.astype('float ' ) x_t /= 255.0 xt_list.append(x_t ) num = 4 - len(xt_list ) length = len(xt_list ) for x in range(num ) : x_t = xt_list[length-1 ] xt_list.append(x_t ) s_t = np.stack((xt_list[0],xt_list[1],xt_list[2],xt_list[3]),axis=2 ) else : xt_list = [ ] for i in range(image.shape[0 ] ) : x_t = imresize(image[i],(100,100 ) ) x_t = x_t.astype('float ' ) x_t /= 255.0 xt_list.append(x_t ) s_t = np.stack((xt_list[0],xt_list[1],xt_list[2],xt_list[3]),axis=2 ) s_t = np.expand_dims(s_t,axis=0 ) return s_t # generate data to train neural network def gen_next_batch(experience , model , num_actions , gamma , batch_size ) : batch_indices = np.random.randint(low=0,high=len(experience),size=batch_size ) batch = [ experience[i ] for i in batch_indices ] x = np.zeros((batch_size,100,100,4 ) ) y = np.zeros((batch_size,num_actions ) ) for i in range(len(batch ) ) : s_t , a_t , r_t , s_t1 , game_over = batch[i ] x[i ] = s_t y[i ] = model.predict(s_t)[0 ] q_sa = np.max(model.predict(s_t1)[0 ] ) if game_over : y[i , a_t ] = r_t else : y[i , a_t ] = r_t + gamma*q_sa return x , y # neural network model implemented using keras model = sequential ( ) model.add(conv2d(32,kernel_size=8,strides=4,kernel_initializer='normal',padding=""same"",input_shape=(100,100,4 ) ) ) model.add(activation('relu ' ) ) model.add(conv2d(64,kernel_size=4,strides=2,kernel_initializer='normal',padding='same ' ) ) model.add(activation('relu ' ) ) model.add(conv2d(32,kernel_size=3,strides=1,kernel_initializer='normal',padding='same ' ) ) model.add(activation('relu ' ) ) model.add(flatten ( ) ) model.add(dense(512,kernel_initializer='normal ' ) ) model.add(activation('relu ' ) ) model.add(dense(3,kernel_initializer='normal ' ) ) model.add(activation('linear ' ) ) opt = adam(lr=1e-06 ) model.compile(loss='mse',optimizer=opt ) num_actions = 3 gamma = 0.99 initial_epsilon = 0.1 final_epsilon = 0.0001 num_epochs = 10000 memory_size = 50000 batch_size = 64 epsilon = initial_epsilon experience = collections.deque(maxlen=memory_size ) game = wrapped.paddle ( ) for x in range(1000 ) : game.reset ( ) game_over = false a_0 = 2 x_t , r_0 , game_over = game.step(a_0 ) s_t = preprocess_images(x_t ) while not game_over : s_t1 = s_t a_t = np.random.randint(low=0,high=num_actions,size=1)[0 ] x_t , r_t , game_over = game.step(a_t ) s_t = preprocess_images(x_t ) experience.append((s_t1,a_t,r_t,s_t1,game_over ) ) print('random actions ' ) print(len(experience ) ) reward_list = [ ] for i in range(num_epochs ) : game.reset ( ) loss = 0.0 r = 0 a_0 = 2 x_t , r_0 , game_over = game.step(a_0 ) s_t = preprocess_images(x_t ) while not game_over : s_t1 = s_t if np.random.rand ( ) & lt;= epsilon : a_t = np.random.randint(low=0,high=num_actions,size=1)[0 ] else : q = model.predict(s_t)[0 ] a_t = np.argmax(q ) x_t , r_t , game_over = game.step(a_t ) s_t = preprocess_images(x_t ) experience.append((s_t1,a_t,r_t,s_t,game_over ) ) # stores experiences x , y = gen_next_batch(experience , model , num_actions , gamma , batch_size ) loss + = model.train_on_batch(x,y ) r + = r_t if epsilon & gt ; final_epsilon : epsilon -= ( initial_epsilon - final_epsilon)/num_epochs print('episode : % d | epsilon : % f | reward : % f | loss : % f'%(i+1,epsilon , r , loss ) ) reward_list.append(r ) if ( i+1 ) % 1000 = = 0 : plt.plot(reward_list ) plt.xlabel('episodes ' ) plt.ylabel('reward ' ) plt.savefig('/output/reward.png ' ) model.save(""/output/rl_model.h5"",overwrite=true ) i trained the neural network for 10000 epochs and the total reward per episode looks like this , which clearly indicates the algorithm is not learning ... (max of +1 and min of -1 reward is possible in a episode ) can any one suggest me what i am doing wrong . i am having very hard time in implementing the reinforcement learning algorithms . i have tried to implement same algorithm on an another game but had the same issue . is it related to my epsilon - greedy policy , or i am not training enough or something else . please help me ...",8273,1671,2017-12-21T19:54:04.540,2018-11-18T05:28:45.720,q learning algorithm not converging,deep-learning reinforcement-learning tensorflow keras q-learning,2,0,1
755,3606,1,,2017-07-07T22:42:50.787,0,59,"so i understand that as a network learns about an output with regards to an input , weights are updated according to how wrong the guess was for that node . so over time , the weights move in the "" direction "" towards the correct value . is it possible to use a seperate neural network , that takes as input the weights of the first network while it trains to trys and approximate that "" direction "" and in effect , pushing the weights in that direction faster ?",7816,,,2017-08-11T00:59:47.113,neural nets learning about neural nets,machine-learning,1,0,
756,3621,1,3633,2017-07-10T17:46:06.227,4,445,"i am trying to build a ml agent to find the closest matching image from a given set . the user will draw something and the agent should list the closest matching images . very similar to these examples https://sketchx.eecs.qmul.ac.uk/ emoji search in android keyboard one unique problem i 've is , each image will represent a category . imagine we have product images and user will draw something and we have to find the products close to the drawing . so category in my case will be product i d . i would like to evaluate the approach before trying out . there are lot of examples to classify images , however if i use the item identifier instead of category it should work . but am trying to find the best approach for this problem .",8368,-1,2017-07-13T06:49:05.567,2017-07-13T06:49:05.567,building ml to finding the closest matching image based on users drawing,machine-learning deep-learning,1,0,
757,3626,1,3628,2017-07-11T13:08:43.253,6,214,"i am a newbie to the field so please be gentle . i am trying to perform a binary classification of tweets using machine learning . the ' normal ' way of doing this seems to be putting a hand - classified tweet 's words into a big vector , then use that vector as input to an algorithm , which then predicts new tweets based on this data . my question is : is there a standard method , or algorithm , that can include other input , such as the location of a tweet , in this process ? i could just add tweet location at the end of the vector i suppose , but that would give it a very small weighting . any pointers are much appreciated .",8385,,,2017-07-11T18:42:37.540,are there any multi step machine learning algorithms ?,machine-learning classification,1,0,
758,3627,1,,2017-07-11T17:50:08.200,3,53,you need a library to recognize the machine numbers . there is a choice between two libraries keras and opencv which is better to choose ? or is there an even better solution ? programming language python3,8390,,,2017-07-14T15:23:43.023,selecting a library for recognition,machine-learning algorithm,1,0,
759,3629,1,,2017-07-11T19:13:36.097,2,523,"i 'm not quite sure how i should go about creating a multi - label image knn classifier using python as a lot of the literature i have read does not explicitly explain this methodology . specifically , i am not sure how i would be able to potentially yield multiple labels per image using the knn classifier architecture . any insight would be greatly appreciated ! ( new to coding by the way )",8394,,,2017-07-18T18:58:50.047,multi - label image classification using knn,classification,1,0,
760,3630,1,,2017-07-12T13:46:46.373,3,95,"i am currently in the pre - process of starting an image classification and extraction project which needs to output multiple softmax and absolute values from a single image like such : { time : "" 20:20 "" , teams : [ { red : { goals : 2 } , blue : { goals : 1 } , } , { scored_by : [ { john : 80 % , kyle : 51 % , darren : 20 % } ] } ] } i can create multiple models which are responsible for different task such as reading the time from the image as well as the score and eventually combine both . i would however like to make sure i maximize on efficiency to make sure the process is as fast as possible . any pointers in the right direction would be greatly appreciated . with kind regards , dennis",8405,8405,2017-07-12T14:25:41.753,2017-12-05T17:34:04.613,extracting multiple softmax values from image,neural-networks machine-learning classification,1,3,1
761,3632,1,,2017-07-12T16:49:42.553,0,713,"i have created 22 different convolutional neural networks that all test for the presence of unique objects in an image ( each one of the classifiers is unique ) . each sample in the test set has the output of a 22-long vector that looks something like this [ 0 , 1 , 1 , 0 , 0 , 1 , ... , 1 ] , the binary nature of the vector representing the presence / absence of specific objects . i have implemented this already in keras and reach around 97 % accuracy avg for the 22 models . is there any specific ensemble methods that can allow me to combine all 22 classifiers ?",7773,,,2017-07-13T14:17:32.933,ensemble learning using convolutional neural networks,neural-networks machine-learning convolutional-neural-networks classification keras,3,0,
762,3637,1,3638,2017-07-13T13:47:30.077,1,33,"i have not seen this explicitly stated anywhere so i was curious . say i have network trained to meet my segmentation needs using 250x250 images . after this training is complete and i wish to submit images in production for segmentation , do those production submitted images need to be 250x250 as well or can they be any reasonable size ? if they must be resized to 250x250 for segmentation , is it possible to scale up the segmentation regions to apply to a larger image ? if so what is the name of that technique so i can research it a bit more .",8430,,,2017-07-13T14:02:25.767,do images submitted to a segmentation network after training need to be the same size as the training images ?,image-recognition,1,0,
763,3640,1,3644,2017-07-13T18:10:42.853,6,2293,"i ca n't seem to understand how an ai learns . without having a programmer tell it what to do , how would a program create or generate some solution to a problem and then use information gained in future problems ? i understand how chess ai works . but what 's really confusing is how an ai would improve or learn .",8433,75,2017-07-14T20:24:19.000,2018-01-12T08:21:31.277,how does an ai learn ?,machine-learning,2,4,13
764,3647,1,,2017-07-14T16:08:58.420,3,160,"is anyone able to recommend some resources ( preferably books ) on the topic of neural networks that goes beyond that of introductory reading ? i 'm still relatively new to the subject , however i have successfully created my own neural network so i would n't consider myself a beginner , so i 'm looking for something more intermediate .",6221,7402,2017-08-14T22:13:28.313,2017-08-14T22:13:28.313,neural networks - further reading,neural-networks machine-learning,4,0,2
765,3657,1,3658,2017-07-16T07:39:43.540,4,841,"both ai and computer science are sciences , as i understood from wikipedia , computer science is everything that has any relation to computers . and ai is commonly defined as study of machines that take the prerogative of humans ( creating musical pieces e.t.c but recently , when i was reading , i read this sentence : "" in computer science , ai is [ ... ] "" so my question is really : is there a part of ai studies that do not refer to computer science ?",8146,8146,2017-07-16T13:11:39.260,2017-07-19T14:46:40.940,is ai entirely a part of computer science ?,terminology,2,3,1
766,3662,1,,2017-07-17T18:24:08.817,1,257,"my datasets are not actual images , so using methods with imagedatagenerator or pre - trained networks might not apply in this case . data structure : each "" image "" is a 2048-long vector that has float values between 0 and 1 . each "" image "" was associated with a label ( multi - label classifcation ) and the goal is to perform classification via keras 2d cnn 's . what are common techniques for finding which parts of the "" images "" contribute most to classification via convolutional neural nets ? i already implemented the cnns in keras and have already successfully trained on my images . * no my data is not time series ; however , my model works with either the keras conv1d and conv2d layers .",7773,3836,2017-07-17T20:29:34.333,2017-07-17T20:29:34.333,cnn attention maps on non - images,machine-learning convolutional-neural-networks classification keras,0,3,
767,3665,1,,2017-07-18T02:48:36.070,9,138,"analogies are quite powerful in communication . they allow to explain complex concepts to people with no domain knowledge , just by mapping to a known domain . hofstadter says they matter , whereas dijkstra says they are dangerous . anyway , analogies can be seen as a powerful way to transfer concepts in human communication ( dare i say transfer learning ? ) . i am aware of legacy work , such as case - based reasoning , but no more recent work about the analogy mechanism in ai . is there consensus whether or not analogy is necessary to agis , and how critical would they be ? please , consider backing your answers with concrete work or publications .",169,2444,2019-04-29T16:44:14.160,2019-04-29T16:44:14.160,is analogy necessary to artificial general intelligence ?,ai-design philosophy agi,1,0,3
768,3666,1,,2017-07-18T10:51:10.770,3,295,"i am trying to build a model which would take an email message ( in english , extracted subject , and body of the email ) and identify if it has a question , request or a proposal . basically , i would like to see the mails that i 've not replied but needs a reply . the model can be used as a "" filter "" in an email client . what is the best way to go about it ? related work : parakweet lab 's email intent data set learning to classify email into "" speech acts """,8511,4302,2018-10-08T12:26:10.890,2018-10-08T12:26:10.890,"identify "" actionable "" intents in email messages",deep-learning natural-language-processing,1,2,1
769,3668,1,,2017-07-18T13:24:40.043,-1,494,we have always known that gradient descent is a function of two or more variables . but how can we geometrically represent gradient descent if it is a function of only one variable ?,8514,,,2017-07-18T14:36:10.330,how would 1d gradient descent look like ?,gradient-descent,1,0,
770,3674,1,3681,2017-07-18T18:32:59.653,2,234,"i read a lot about this , i understand how it work , but i would like the most simple example you can provide me , because i have no clue how i would make it in code . no matter the language ( i would appreciate if it 's derived from c ) , because i 'm not here to copy - paste , and understand the essence .",8497,,,2017-07-25T16:08:23.303,neural network example,neural-networks,2,2,3
771,3677,1,,2017-07-19T14:21:29.437,2,82,"i am working to make my first trained model for image recognition , using the programming language r. first i am attempting to make a function that takes a png - image as input , resizes it to 128x128 pixels , and converts it into a row - vector of numbers representing the colours in each pixel . i then want to add this to a file that contains my training - set and labels ( the image bank ) . i am having some trouble with this , being a beginner in r , as the numerical values i get out from the following code does not make sense to me - often i get a large amount of 1 's , and the pictures appear to be completely white if i try to export and look at the image . is there any obvious mistake here ? i assume it must be where i convert to numerical values . alternatively , is there an alternative approach to this task that would be better ? load_png_to_image_bank & lt;- function(wd , input_file , label ) { # remembers old working directory oldwd & lt;- wd # sets input wd to file location setwd(wd ) # loads imager package library(imager ) # imports file and resizes to 128x128 pixels image & lt;- load.image(input_file ) image & lt;- resize(image,128,128 ) # finds numerical values for each pixel rasterized_vector & lt;- as.data.frame(as.vector(image ) ) rasterized_vector & lt;- as.data.frame(t(rasterized_vector ) ) # adds label defined by input variable rasterized_vector & lt;- as.data.frame((cbind(1 , rasterized_vector ) ) ) names(rasterized_vector)[1 ] & lt;- label # sets wd to predetermined place for image bank file setwd ( "" ... "") # writes vector to file write.table(rasterized_vector , output_file , row.names=f , na=""na "" , append = t , quote= false , sep = "" ; "" , col.names = f ) # resets working directory to initial state setwd(oldwd ) }",8541,,,2017-07-19T14:21:29.437,converting pictures into numerical values,machine-learning image-recognition,0,1,
772,3678,1,,2017-07-19T14:23:50.803,1,27,"def interpretation_be(sentence ) : be = ( ' be ' , ' been ' , ' is ' , ' was ' , ' are ' , ' were ' ) place = ( ' there ' , ' here ' ) gender_identity = ( ' he ' , ' she ' ) indicator = ( ' it ' , ' that ' , ' this ' , ' these ' , ' those ' ) exist = ( ' exist ' ) equal=('equal ' ) condition=('condition ' , ' status ' , ' quality ' , ' state ' , ' situation ' ' circumstance ' ) number = ( ' 1 ' , ' 2 ' , ' 3 ' , ' 4 ' , ' 5 ' ) consequence=('consequence ' , ' result ' ) to = ( ' to ' ) if sentence.split()[1 ] in be : if sentence.split()[0 ] in place : print(sentence.replace(sentence.split()[1],'exists ' ) ) if sentence.split()[0 ] in gender_identity : print(sentence.replace(sentence.split()[1],'equal to ' ) ) if sentence.split()[0 ] in indicator : print(sentence.replace(sentence.split()[1],'equal to ' ) ) if sentence.split()[1 ] in exist : if sentence.split()[0 ] in place : print(sentence.replace(sentence.split()[1],'is ' ) ) if sentence.split()[1 ] in equal : if sentence.split()[0 ] in condition : print(sentence.replace(sentence.split()[1],'is ' ) ) if sentence.split()[0 ] in number : print(sentence.replace(sentence.split()[1],'is ' ) ) if sentence.split()[0 ] in consequence : print(sentence.replace(sentence.split()[1],'is ' ) ) if sentence.split()[2 ] in to : print(sentence.replace(sentence.split()[2],'toward ' ) ) # # need to discern between preoposition to and infinitive to # # # # result_check # # interpretation_be('i have been to spain ' ) interpretation_be('there was a cat ' ) interpretation_be('there is a woman ' ) interpretation_be('he is a boy ' ) interpretation_be('it is a cat ' ) interpretation_be('there exist a hat ' ) interpretation_be('situation equal to bad ' ) interpretation_be('1 equal one ' ) interpretation_be('consequence eqaul bad ' ) i am trying to make up english rephraser as a most basic start of natural language processing . while building up like above code , i had wanted to get some already - classified text data which criteria of classification is word - class , such as noun , verb , preposition etc . any good text data that i can obtain ? please let me know .",8325,4302,2018-10-08T12:25:36.543,2018-10-08T12:25:36.543,"classification of english words based on class [ noun , verb , adjective , preposition , etc ]",natural-language-processing,0,0,
773,3680,1,,2017-07-19T18:18:34.193,3,327,"how would one go about solving the 15 squares puzzle using a genetic algorithms approach ? in particular , i 'd like to understand how you would represent the "" chromosome "" in the evolving system . that is , what 's the relationship between the ( artificial ) "" genes "" and some sort of phenotypical expression w / r / t the problem . it seems like genes would somehow represent moves or sequences of moves but i 'm not entirely clear how this would work .",8547,16909,2018-08-08T20:44:57.203,2018-08-08T20:44:57.203,"how can one use genetic algorithms to solve the "" 15 puzzle "" ( mystic square ) ?",genetic-algorithms evolutionary-algorithms genetic-programming,1,2,2
774,3683,1,,2017-07-20T04:47:02.953,5,278,i 'm doing some testings on nlp and i was thinking to write a code that works like this . subject - > user input - > output dog ownership - > i own a dog - > yes dog ownership - > my dog is called joe - > yes dog ownership - > i do n't have a dog - > no which branch or algorithm would be the best approach for this problem ?,8553,1581,2018-11-03T06:20:36.577,2018-11-06T13:50:43.133,which ai branch should i follow for natural language processing problem ?,natural-language-processing learning-algorithms intelligent-agent,3,8,
775,3690,1,3691,2017-07-21T09:02:33.873,2,774,"i 'm trying to come up with the right algorithm for a system in which the user enter symptoms and we starts triggering questions related to that symptoms and his answers will result a disease which is related to the answer which is given by the user let 's assume that the user entered the following input : symptom - deafness q1 . how long have you had a problem with deafness a)from few days b)from few weeks to months c)more than month d)since birth q2 . what was the onset of the deafness a ) sudden b ) gradual now we have a knowledge base like if a user select option 1 from question 1 and option 2 form question 2 then we will give him some disease . but i need an algorithm which will give % of success in backend so that i can throw the results of disease for example if a user select option 2 from question1 and option 1 from question 2 , then when we compare from our knowledge base there will be one set over there which has option 1 from question 1 and option 2 from question 2 then its a "" some "" disease .. now if we compare from our knowledgebase and we found even 50 % of the choices is resulting this disease we will throw that disease name . now i am confused what algo should be use for this approach for ai .",11801,,,2017-07-21T17:02:08.540,selecting the right algorithm to predict disease from questions,machine-learning algorithm prediction,2,5,0
776,3692,1,,2017-07-21T15:46:58.727,1,654,"by this i mean a single axiom for placement . ( i 'm working on a solution that involves positional valuation and vectors , and appears to be solid , but my assumption would be this has been done previously , like most mathematical techniques one "" discovers "" ; )",1671,,,2017-07-21T15:46:58.727,has tic - tac - toe been solved with a single heuristic ?,heuristics,0,4,1
777,3696,1,,2017-07-21T20:36:17.653,8,379,"i 'm going to give a talk , and i 'm preparing the material . the purpose of the conversation is to convince companies in my region that it is possible to apply artificial intelligence in solving everyday business problems . i would like some examples to be able to present , and so i came here to ask have you used artificial intelligence to solve a problem at work ? what kind of problem ?",7714,2444,2017-08-01T11:38:30.113,2017-08-01T11:38:30.113,examples of uses of artificial intelligence at work,philosophy definitions,2,3,2
778,3698,1,,2017-07-22T08:24:30.650,2,69,i want to produce a bot in python that automatically generates short football summaries from whoscored data . for my first stage i generate the articles with different sentence templates and lots of rules where the data is used . now i want to move to the next stage and start looking into nlp and more advanced nlg . i already scraped numerous articles to create a corpus . how should i move on and do next ? any advice would be much appreciated as i 'm new in this . thanks !,8601,4302,2018-10-08T12:24:50.293,2018-10-08T12:24:50.293,advanced nlg - robot journalist,machine-learning natural-language-processing,0,1,1
779,3702,1,,2017-07-24T13:20:59.073,1,172,i need some help in developing a markov model for a crossroads there is no one way road and i am assuming at this time that traffic is only allowed to go straight no turns are allowed . there are 4 roads and 4 signals(agents ) .,3751,,,2017-08-27T03:52:11.907,markov model for a traffic intersection,ai-design reinforcement-learning training markov-chain,1,5,1
780,3703,1,3707,2017-07-24T15:48:53.760,4,259,"cepheus is an artificial intelligence designed to play texas hold'em . by playing against itself and learning where it could have done better , it became very good at the game . slate star codex comments : i was originally confused why they published this result instead of heading to online casinos and becoming rich enough to buy small countries , but it seems that it ’s a very simplified version of the game with only two players . more interesting , the strategy was reinforcement learning – the computer started with minimal domain knowledge , then played poker against itself a zillion times until it learned everything it needed to know . apparently cepheus currently just plays against one person . seeing as it managed to develop amazing strategy for this "" very simplified "" environment , what 's stopping it from working on real / full poker games ?",75,75,2017-07-27T00:36:13.710,2017-07-27T22:17:19.757,what 's stopping cepheus from generalizing to full poker games ?,reinforcement-learning gaming,1,3,1
781,3705,1,,2017-07-25T03:50:53.097,1,75,"i 'm developing a chatbot , and to get the answer i 'm using the naive bayes classifier by sorting the questions and answers . for those who want to see the whole project code and more definitions follow the link of github to develop i am using the textblob library for python , the problem is that when training my classifier it is always returning the same message , regardless of the input i use . the message is : "" tudo bom ? "" i still can not identify the problem , i do not know if the problem is in the way my data is willing to perform the training or if it is the way i am training the classifier . my class that performs the sorting process is this : # encoding : utf-8 # ! /usr / bin / env python from textblob.classifiers import naivebayesclassifier from textblob import textblob import logging class talk(object ) : "" "" "" the talk class is responsible for returning the response based on the information exported . using the classification according to bayes ' theorem "" "" "" def _ _ init__(self ) : "" "" "" class builder cl -&gt ; stores the classifier accuracy -&gt ; stores the accuracy of the algorithm "" "" "" self.__cl = none self.__accuracy = 0 def train(self , train_set ) : "" "" "" train with the list of information consisting of phrases and their respective classifications : "" "" "" logging.debug('inicia treinamento da previsão de intenção ' ) self.__cl = naivebayesclassifier(train_set ) logging.debug('treinamento da previsão de intenção finalizado ' ) def test(self , test_set ) : "" "" "" performs tests with the list of information formed of sentences and their respective classification to obtain the accuracy : "" "" "" logging.debug('inicia teste da previsão de intenção ' ) self.__accuracy = self.__cl.accuracy(test_set ) logging.debug('teste da previsão de intenção finalizado ' ) logging.info('precisão da previsão : { } ' .format(self.__accuracy ) ) def response(self , phrase ) : "" "" "" returns the phrase response according to the created classifier "" "" "" logging.debug('analisa a frase "" { } "" ' .format(phrase ) ) blob = textblob(phrase , classifier = self.__cl ) result = blob.classify ( ) logging.debug('resposta : "" { } "" ' .format(result ) ) return result follow the link in my file with training information and test data training test",7714,,,2017-07-25T03:50:53.097,problem with the pln classifier,classification natural-language-processing,0,0,
782,3715,1,,2017-07-26T23:48:30.277,1,72,"in artificial intelligence : a modern approach , when it talks about strategies to improve efficiency of resolution inference(section 9.5.6 ) , it says selecting the set of support and resolving one of elements in it first are helpful . but i can not understand the way it select the set of support and why . the original excerpt as follows : set of support : preferences that try certain resolutions first are helpful , but in general it is more effective to try to eliminate some potential resolutions altogether . for example , we can insist that every resolution step involve at least one element of a special set of clauses — the set of support . the resolvent is then added into the set of support . if the set of support is small relative to the whole knowledge base , the search space will be reduced dramatically . we have to be careful with this approach because a bad choice for the set of support will make the algorithm incomplete . however , if we choose the set of support s so that the remainder of the sentences are jointly satisfiable , then set - of - support resolution is complete . for example , one can use the negated query as the set of support , on the assumption that the original knowledge base is consistent . ( after all , if it is not consistent , then the fact that the query follows from it is vacuous . ) the set - of - support strategy has the additional advantage of generating goal - directed proof trees that are often easy for humans to understand . what does it mean that the remainder of the sentences are jointly satisfiable ? why could one use the negated query as the set of support ? thanks in advance , i hope someone could shed some light on it : ) ps . i 'm a english leaner . i may not present this problem very well , and i 'm very sorry for that . but i‘m very serious about it and i 've try my best to make it as clear as i can . so if you 're about to down vote it , please comment below and give me some advice to improve it , i 'll be very grateful for that . thanks again !",8689,8689,2017-07-29T00:36:27.003,2017-07-29T00:36:27.003,how to select the set of support and why ?,logic,0,0,
783,3716,1,,2017-07-27T04:32:49.237,2,113,"in section 10.3.2 in artificial intelligence : a modern approach there is a piece of pseudocode that describes the graph plan algorithm . the graph plan algorithm constructs a planning graph for a problem and extract a solution from it if there is one . here is the pseudocode : function graphplan(problem ) returns solution or failure graph ← initial - planning - graph(problem ) goals ← conjuncts(problem.goal ) nogoods ← an empty hash table for tl=0 to ∞ do if goals all non - mutex in st of graph then solution ← extract - solution(graph , goals , numlevels(graph ) , nogoods ) if solution ! = failure then return solution if graph and nogoods have both leveled off then return failure graph ← expand - graph(graph , problem ) also a ppt for graphplan what confuses me for a long time is when the nogoods levels off ? the book describe no - goods as follows : in the case where extract - solution fails to find a solution for a set of goals at a given level , we record the ( level , goals ) pair as a no - good . but in that case , every time the first extract - solution after expan - graph fails , there will always be at least a new no - good produced : whose level becomes the new level of the planning graph and the goals is always the same -- which is the goal of the problem . so when does it level off ? there is another sentence about no - goods in the book : if the function extract - solution fails to find a solution , then there must have been at least one set of goals that were not achievable and were marked as a no - good . from above excerpt , i learn that goals in no - goods is a set of goals that are not achievable in the level . but , imho , that could vary when we chose different actions in extract - solution . the example in the book provides a good instance : consider an air cargo domain with one plane and n pieces of cargo at airport a , all of which have airport b as their destination . in this version of the problem , only one piece of cargo can fit in the plane at a time . the graph will level off at level 4 , reflecting the fact that for any single piece of cargo , we can load it , fly it , and unload it at the destination in three steps . but that does not mean that a solution can be extracted from the graph at level 4 ; in fact a solution will require 4n − 1 steps : for each piece of cargo we load , fly , and unload , and for all but the last piece we need to fly back to airport a to get the next piece . in the example above , the set of goals not achievable at level 4 varies based on what actions has been taken . could someone help clear up this problem ? i know there must be some misunderstanding , welcome to point it out , please . thanks in advance : ) ps . i 'm a english leaner , i may not present this problem very well , so i 'm very sorry for any reason that you may want to down vote it . but i‘m very serious about it and i 've try my best to make it as clear as i can . so if you 're about to down vote it , please comment below and give me some advice to improve it , i 'll be very grateful for that . thanks again !",8689,8689,2017-07-29T00:30:59.270,2017-07-29T00:30:59.270,what is a no - good ? when does the no - goods level off ?,path-planning,0,3,
784,3720,1,3721,2017-07-27T14:43:11.760,1,442,"if i create a program which takes an input , gives an output and then requires a response to let it know whether the answer it gave was any good does it count as ai ? if not , what is the process of ai ? does it not always need specific parameters ? for example , i ask it "" who is the president of the usa ? "" , and i have programmed it to look for news articles in seos and remove the "" who "" part , is that ai ?",8698,2444,2019-03-02T21:23:59.687,2019-03-02T21:23:59.687,can a program that requires feedback be considered an ai ?,ai-design,3,1,1
785,3723,1,,2017-07-27T17:29:50.153,2,233,"after witnessing the rise of deep learning as automatic feature / pattern recognition over classic machine learning techniques , i had an insight that the more you automate at each level , the better the results , and i therefore turned my focus to neuroevolution . i have been reading neuroevolution publications with the same desire to automate at every level . do genetic algorithms evolve ? do they get better at searching through the solution space for each generation over time ? is this legitimately "" evolution "" ?",8700,2444,2018-11-19T18:21:05.947,2018-11-20T07:13:46.837,do genetic algorithms evolve ?,deep-learning genetic-algorithms evolutionary-algorithms unassisted-learning,3,1,1
786,3727,1,,2017-07-29T23:22:44.513,2,58,"given this data set : user 1 : { ' artist ' : 1 , ' public figure ' : 9 , ' film director ' : 1 , ' education ' : 1 , ' musician / band ' : 4 , ' musician ' : 1 , ' community ' : 1 , ' author ' : 4 , ' politician ' : 1 , ' tv show ' : 2 , ' entrepreneur ' : 1 , ' journalist ' : 4 , ' product / service ' : 1 , ' defense company ' : 1 , ' computer company ' : 2 , ' nonprofit organization ' : 3 , ' computers & amp ; internet website ' : 4 , ' media / news company ' : 3 , ' podcast ' : 1 , ' news & amp ; media website ' : 2 , ' charity organization ' : 1 , ' government organization ' : 1 , ' magazine ' : 1 } user 2 : { ' nonprofit organization ' : 1 , ' movie ' : 2 , ' musician / band ' : 22 , ' public figure ' : 2 , ' entrepreneur ' : 2 , ' tv show ' : 2 , ' medical company ' : 1 , ' american restaurant ' : 1 , ' sports & amp ; recreation ' : 1 , ' media / news company ' : 2 , ' sports team ' : 1 , ' artist ' : 3 , ' musician ' : 1 , ' college & amp ; university ' : 1 , ' athlete ' : 1 , ' music ' : 1 , ' app page ' : 1 , ' comedian ' : 1 , ' interest ' : 2 , ' product / service ' : 1 , ' book series ' : 1 i believe i can draw conclusions that the strength of the connection is high or low based on the fact that they like or dislike the same categories of topics . what machine learning technique would be best to apply to these connection types , if i had , say , 500 users with categories and weights for each ? i would like to automatically apply varying weights , if most people have musician / band or like movies , that should be less interesting than when people like "" defense company "" like in user 1 .",8737,8738,2017-07-30T19:17:55.510,2017-07-30T19:17:55.510,weighing connections between two users using machine learning,machine-learning algorithm,1,1,
787,3729,1,3733,2017-07-30T10:33:49.783,5,973,is there a way to apply reinforcement learning algorithms to computer vision problems ?,3751,2444,2019-04-14T18:47:15.973,2019-04-14T18:47:15.973,apply reinforcement learning algorithms to computer vision problems,reinforcement-learning computer-vision applications,1,1,1
788,3731,1,3738,2017-07-30T19:06:58.153,2,532,"i 'm learning neural networks , and everything works as planned but , like humans do , adjusting themselves to learn more efficiently , i 'm trying to understand conceptually how one might implement an auto adjusting learning rate for a neural network . i have tried to make it based on error , something like how bigger is error learning rate is getting bigger as well . [ could use some clarification here -- not entirely sure what you 're saying . if can clarify , i 'm happy to clean up the english . -dukezhou ] * if you want give me an example give it on a c based language or math because i do n't have experience with python or pascal .",8497,1671,2017-07-31T18:56:39.303,2019-02-22T08:54:00.887,how to implement an automatic learning rate for a neural network ?,neural-networks,3,4,
789,3732,1,3755,2017-07-31T08:23:45.303,5,202,i have been following the ml course by tom mitchel . the inherent assumption while using decision tree learning algo is : the algo . preferably chooses a decision tree which is the smallest . why is this so when we can have bigger extensions of the tree which could in principle perform better than the shorter tree ?,8720,,,2017-08-03T20:37:27.267,why do decision tree learning algorithm preferably outputs the smallest decision tree ?,machine-learning unsupervised-learning learning-algorithms,3,0,1
790,3734,1,,2017-07-31T11:26:47.250,2,105,"i 'm first year student in machine learning and i really recently started to immersing myself . i need to calculate number of : matrix additions matrix multiplications matrix divisions which are processed in the well known convolutional neural network - alexnet . i found some materials about it , but i 'm really confused where to start . so , the overall structure might looks like : but , how can i calculate operations for each type distinctly ?",8766,1671,2017-07-31T21:28:12.480,2017-07-31T21:28:12.480,alexnet algorith complexity and operations count,neural-networks linear-algebra,0,4,1
791,3739,1,3752,2017-08-01T17:16:31.920,9,302,"i 'm now reading a book titled hands - on machine learning with scikit - learn and tensorflow and in the chapter 10 of the book , the author writes the following : the architecture of biological neural networks ( bnn)4 is still the subject of active research , but some parts of the brain have been mapped , and it seems that neurons are often organized in consecutive layers , as shown in figure 10 - 2 . however there seems to be no link to any research there . and the author did n't say it assertively given that he used "" it seems that neurons are often organized in consecutive layers "" . is this true and how strongly is it believed ? what research is this from ?",7402,,,2017-08-05T19:21:52.923,are biological neurons organized in consecutive layers as well ?,artificial-neuron biology,2,2,
792,3740,1,3743,2017-08-01T22:47:35.773,0,77,"i want to learn about artificial intelligence . as a beginner , i have no idea what so ever about ai . i want to have a clear idea about what ai and its subfields are and how it works so that i can determine what i really want to learn about ai . so can anyone provide me with some resources about ai ?",8803,,,2017-08-02T09:10:42.173,where i can find ai resources for total idea about ai and it 's subfields ?,ai-community,1,4,1
793,3742,1,3746,2017-08-02T08:51:55.957,6,142,"i am developing an lstm for sequence tagging . during the development , i do various changes in the system , for example , add new features , change the number of nodes in the hidden layers , etc . after each change , i check the accuracy using cross - validation on a development corpus . currently , in each check , i use 100 iterations to train the system , which takes a lot of time . so i thought that maybe , during development , i can use only e.g. 20 iterations . then , each check will be faster . after i find the best configuration , i can switch back to 100 iterations to get better accuracy . my question is : is this consideration correct ? i.e , if feature - set a is better than feature - set b with 20 training iterations , is it likely that a will be better than b also with 100 training iterations ? alternatively , is there a better way to speed up the development process ?",8684,,,2017-08-02T12:03:15.540,shortening the development time of a neural network,neural-networks training lstm,2,0,1
794,3745,1,,2017-08-02T10:52:08.197,1,26,i have a set of data representing many sellers and the items they sell . there is an overlap between the various sellers ' items . these items come from various distributors and typically an item comes from a unique distributor . i am trying to come up with an algorithm that will group items by the distributors that sold them to the sellers . any ideas come to mind ?,8816,,,2017-08-02T10:52:08.197,grouping items by relationships of their owners,algorithm,0,3,1
795,3747,1,5065,2017-08-02T13:12:10.373,3,104,"has anyone had a chance to tinker with multiple major ai platforms such as tensorflow , cognitive talk , quill etc ... what are the strengths and weaknesses of different ai platforms ? comprehensive articles that tackle this topic would be helpful .",8820,1671,2018-01-31T16:52:49.063,2018-01-31T16:52:49.063,are there independent evaluations of various major ai platforms ?,training tensorflow implementation software-evaluation,1,1,0
796,3748,1,,2017-08-02T15:26:15.937,4,91,"i 'm currently looking in to the possibility of using machine learning to detect fraudulent transactions on our website based on the events that happen for each user . i 'd like to be able to stream events in to it , such as sign up , order placed , inviting another user , etc along with the the time and some how come up with a probability of how likely it is that the person is acting in a fraudulent manner i 'm totally new to machine learning and everything i read goes straight over my head basically :/ can someone please explain the type of neural network i 'd need , how i 'd decide how to set it up and how i would go about training it ?",8823,,,2017-08-02T19:45:08.733,streaming time series data to detect fraud ?,machine-learning,1,0,
797,3749,1,3790,2017-08-02T15:27:26.610,5,228,i am looking for ai podcasts that are purely academic oriented that i can use for learning purposes . thanks for any resource pointers . the ai podcasts i am aware of are ( not sure how many of these can be considered academic ) : the ai podcast linear digressions o'reilly bots podcast a16z podcast,8820,7550,2017-08-24T12:47:34.773,2019-02-02T15:15:18.943,what are some academic ai podcast out there ?,ai-community,3,4,7
798,3751,1,,2017-08-03T01:21:40.497,3,81,"there are two general ways in which ai can interact with humans : implants of ai devices into human bodies or brains & mdash ; such implantation has already begun with health monitors and could grow to include cognitive access to general purpose digital computing . once such is accepted , it is possible that humans will emerge with behavior largely defined by its implants therefore qualify as a hybrid ai systems . embedding of ai systems into business or government organizations & mdash ; it is possibliites that the behavior of organizations using ai systems for business decisioning will eventually be defined in behavior largely by those systems and would therefore qualify as a hybrid ai systems . how probable are these possibilities , considering human history and current cultural trends in technology acceptance ?",8832,4302,2017-08-04T19:12:31.693,2018-06-01T21:13:56.873,will ai develop in hybrid ways ?,ai-design philosophy applications,1,8,1
799,3753,1,,2017-08-03T07:57:40.870,4,2173,"from the lecture in machine learning i know , that a linear activation function can only produce a linear function , but i do n't know if it can produce a connected linear function like the one in the image ? this function consists of multiple concatenated lines .",6275,,,2018-06-18T07:39:27.187,can a nn with linear activation functions produce a connection of linear functions ?,neural-networks,2,1,1
800,3754,1,3815,2017-08-03T08:56:32.590,2,277,"i 'm very new to the field of ai and hmms . my question is can hmms be used to model any time series data ? or does the data have to be that of a markov process ? in htk documentation , i see that the first few lines state that it can model any time series ( "" htk is a toolkit for building hidden markov models ( hmms ) . hmms can be used to model any time series and the core of htk is similarly general - purpose . "" )",8845,,,2017-09-18T21:10:58.233,hidden markov model application,markov-chain,2,0,2
801,3758,1,7700,2017-08-04T13:45:54.123,6,343,"i recently started learning about reinforcement learning and currently i am trying to implement the sarsa algorithm , however i do not know how to deal with $ q(s ' , a')$ , when $ s'$ is the terminal state . first , there is no action to choose in this state , and second , this $ q$-factor will never be updated either because the episode ends when $ s'$ is reached . should i initialize $ q(s ' , a')$ to something other than a random number ? or should i just ignore the $ q$-factors and simply feed the reward $ r$ into the update ?",8448,1641,2018-08-24T14:45:57.570,2018-08-24T15:55:00.353,how should i handle action selection in the terminal state when implementing sarsa ?,reinforcement-learning implementation,2,0,
802,3759,1,3780,2017-08-04T15:15:23.593,5,185,"i 'm aiming to create a neural network that can learn to predict the next state of a board using the rules of conway 's game of life . this is technically three questions , but i felt that they needed to be together to get the full picture and i do n't want to spam the artificial intelligence se with new questions . my network will look at each cell individually ( to reduce computing power needed and to increase learning speed ) and its surrounding cells . 9 input nodes for the network will go into one hidden layer . the output layer will be one node for the state of that cell in the next state of the game . nodes have two states ( alive and dead ) and connections between nodes can either transfer that value , or invert it . for the learning part , i was going to make use of mutation and natural selection . the starting network will have the input and output layers with no hidden layer and no connections . my idea was then to introduce mutation by randomly generating a number of new networks by adding nodes and randomly connecting them to inputs and to the output . the number of nodes in the middle layer will be limited to 512 , since there are 512 possible inputs , however i may reduce this if it is too slow . should i also have it randomly delete nodes and connections , in case they also make improvements ? each network will be tested on the same board state , and their accuracy will be calculated by comparing their output to a correct output generated by a computer program . the most accurate network will then be used for the next generation . my issue is that i do n't know how to program the nodes . should the nodes in the hidden layer perform a logical and on all of their inputs or an or ? i know that the network wo n't learn the rules within the first few turns , but how do i know if it will ever get above 90 % accuracy , or even just 50 % ?",8872,,,2017-08-08T12:50:15.443,advice on machine learning for a neural network,neural-networks machine-learning genetic-algorithms,1,3,1
803,3761,1,,2017-08-04T22:44:21.400,2,47,"i am writing my own recurrent neural network in java to understand the inner workings better . while working through the math , i found that in timesteps later than 2 the gradient of weight w of neuron n depends on the gradients of all neurons at all timesteps before . a handwritten example is given , i tried to write as clearly as possible . could anyone verify this so i can finish my network ? am i missing a piece or is my premise wrong , as in the output of a neuron is s(wx + vh + b ) , where h is the last step of only neuron n ?",8881,,,2017-08-04T22:44:21.400,are gradients of weights in rnns dependent on the gradient of every neuron in that layer ?,recurrent-neural-networks gradient-descent,0,2,1
804,3765,1,,2017-08-05T07:13:33.760,7,146,"a few days ago i asked the question , if a nn with linear activation function can produce a function concatenated of linear functions , what actually is impossible ( can a nn with linear activation functions produce a connection of linear functions ? ) . now i have here some classification examples , but i really can not perfectly decide , which one is based on which approach : 1 - > c the perceptron does not look for the maximum separation margin . 2 - > e neural network with linear activation function 3 - > a linear svm , because of the maximum separation margin . 4 - > b because of the hyperbolic shape of the hyperplane . 5 - > d ? logisitc regression ? i tought it can only linear separate ? 6 - > f i guess the nn with tanh activation function , because of the no very smooth shape , which comes from the too small hidden layer size . i actually do n't get how the logistic regression classifier should be able to produce a hyperplane like in 5 ? what did i classify wrong here ?",6275,,,2018-10-15T21:00:21.823,classification with different approaches,machine-learning classification,1,2,1
805,3771,1,,2017-08-07T08:40:35.017,6,165,"according to this article , two facebook ai 's had the following "" creepy "" negotiation over a transaction : bob : i can i i everything else . . . . . . . . . . . . . . alice : balls have zero to me to me to me to me to me to me to me to me to bob : you i everything else . . . . . . . . . . . . . . alice : balls have a ball to me to me to me to me to me to me to me bob : i i can i i i everything else . . . . . . . . . . . . . . alice : balls have a ball to me to me to me to me to me to me to me bob : i . . . . . . . . . . . . . . . . . . . alice : balls have zero to me to me to me to me to me to me to me to me to bob : you i i i i i everything else . . . . . . . . . . . . . . alice : balls have 0 to me to me to me to me to me to me to me to me to bob : you i i i everything else . . . . . . . . . . . . . . alice : balls have zero to me to me to me to me to me to me to me to me to if we first look at bob 's , he 's asking for what he wants by proposing that he have all items but one and not revealing which is the one he does n't want . by design or by chance , this is actually a strong negotiating technique because he reveals nothing other than the fact he is willing to come to an agreement . alice appears to either ask for no balls , or to say they have no value to her and then obsess about things coming to her , perhaps iterating on the other items . she would appear to be the better communicator because she at least gets to the point of saying "" have a ball "" . but she refuses to give anything away beyond that . but bob seems to stand firm saying he wants "" everything else "" but not giving away what he is willing to go without . perhaps these two are not such bad negotiators after all ?",8914,1581,2017-08-09T12:33:50.703,2017-08-09T12:33:50.703,did the facebook robots both want everything but the balls ?,research reinforcement-learning chat-bots turing-test challenges,1,1,1
806,3772,1,3779,2017-08-07T11:47:26.490,9,109,"i 'm attempting to program my own system to run a neural network . to reduce the number of nodes needed , it was suggested to make it treat rotations of the input equally . my network aims to learn and predict conway 's game of life by looking at every square and its surrounding squares in a grid , and giving the output for that square . its input is a string of 9 bits : the above is represented as 010 001 111 . there are three other rotations of this shape however , and all of them produce the same output : my network topology is 9 input nodes and 1 output node for the next state of the centre square in the input . how can i construct the hidden layer(s ) so that they take each of these rotations as the same , cutting the number of possible inputs down to a quarter of the original ? edit : there is also a flip of each rotation which produces an identical result . incorporating these will cut my inputs by 1/8th . with the glider , my aim is for all of these inputs to be treated exactly the same . will this have to be done with pre - processing , or can i incorporate it into the network ?",8872,8872,2017-08-08T12:06:13.400,2017-08-09T09:40:36.573,how can i make my network treat rotations of the input equally ?,neural-networks,3,3,1
807,3774,1,,2017-08-07T19:28:07.360,3,202,"this concerns a set of finite , non - trivial , combinatorial games [ m ] in the form of an app . a sample game can be found here . because this is a mass market product , we ca n't take up too much space , and the ai needs to be able to run locally since connectivity can not be assumed . the current size of the android kernel is & lt ; 7 mb . the goal is not sheer ai strength , but respectable ai strength , sufficient to beat the above - average human player . ( the current strongest , weak automata , using a few heuristics , is already capable of beating the average human player . ) because the games are finite , the gametrees eventually become tractable , allowing for perfect endgames . resource stealing strategies and trap - avoidance can also be effected with shallow look - ahead at all phases , and the patterns are much easier for automata to recognize than for humans . in this context , reinforcement would be mostly utilized to "" tune the automata to the style of the human player , "" and produce different automata on different devices , which could subsequently play against each other as proxies for their human partners . the game data can be stored efficiently , initially requiring only 2 bytes per position ( a value 0 - 9 and a coordinate 1 - 81 , although the number of coordinates will grow in basic game extensions , and require 2 bytes for larger - order gameboards such as "" samurai "" sudoku . ) so the first turn on a given game requires only 2 bytes , the second turn bytes , etc . , up to between 50 and 70 turns on an 81 cell gameboard . additionally , because it 's a square grid , we can reduce for symmetry . but even with an average number of turns at 50 , that 's only about 40,000 games for 200 mb . weighting openings my feeling is that reinforcement would be useful in weighting openings . if the game data is a string , these strings can be compared , and the smaller the sample size ( the fewer the turns included , ) the more connections there will be . in this case , the sequence does n't matter , only the set of value / coordinates for a given turn . abstraction can be utilized in that certain individual value / coordinates are interchangeable . my thought is the automata can weight openings based on how often they lead to desirable outcomes in the form of a win . weighting heuristics since we 're having good , initial results with heuristics , and these are the most efficient method of decision - making , i 'm thinking about weighting evaluation functions so that certain heuristics take precedence under different conditions . ( for instance , when to expand vs. when to consolidate . when to make a choice with immediate benefit over a choice with long - term benefit . introduction of meta - strategies that modify foundation strategies . ) database pruning because the allocated volume will be capped , it 's probably going to be necessary to "" prune "" the database when info is no longer relevant . ( for instance , when a new strategy emerges that renders previous strategies obsolete . ) we also probably need a method to help the automata to recognize such situations , so it does n't persist in potentially obsolete strategies for more than two games without starting to try alternatives . q : can reinforcement learning be meaningfully applied toward these goals under these restrictions ? q : are my inclinations for approaching this useful or problematic ? q : are there methods i 'm not considering that could be applicable under these restrictions ?",1671,9947,2018-09-13T08:54:05.923,2018-09-13T08:54:05.923,what type of reinforcement learning can i do restricted to ~200 mb on an average smartphone ?,reinforcement-learning game-ai architecture combinatorics hardware-evaluation,1,4,2
808,3775,1,,2017-08-07T21:09:28.730,16,474,"in general , what possibilities are there for reinventing job descriptions that could be replaced by an automated ai solution ? my initial ideas include : monitoring the ai and flagging its incorrect actions . possibly taking over the control in very challenging scenarios . creating / gathering more training / testing data to improve the accuracy of the ai .",8918,,,2017-08-10T12:57:55.777,how to reinvent jobs replaced by ai ?,philosophy social,4,3,6
809,3777,1,,2017-08-08T10:11:00.017,2,89,"in ch-14.4 @ pattern recognition and machine learning by bishop it is mentioned that tree - based models are more widely used in medical diagnosis . apart from giving better performance , is there a human - centric reason for this trade off as medical diagnosis is mainly performed by human ?",8720,,,2017-10-10T12:58:39.197,why are tree - based models more widely used in medical diagnosis ?,machine-learning models,3,3,
810,3784,1,,2017-08-08T21:14:25.050,3,408,"i 'm trying to optimize a combination of 8 cards with 64 card characters . no repeats and order does n't matter . n!/(n!(n - r ) ! ) = 4,426,165,368 combinations i have everything set up , including the data scrapper . but i do n't know how many games my machine needs to learn from to start seeing patterns . for example , in 14,000 games analyzed , only 834 decks had a certain character . analyzing the next 7 cards from 834 decks is 621,216,192 so i guess i need more data before reliable patterns emerge ... but how much data ? thank you and god bless",8945,,,2017-10-10T14:16:22.747,how much data do i need to collect ?,machine-learning,1,1,0
811,3785,1,,2017-08-09T01:20:47.320,2,173,"does google , or any other service , have a syntax and grammar api ? i am looking for a service which will allow an essay to be checked for correct spelling , gramma and any other relevant features .",8950,10913,2018-01-27T20:58:15.427,2018-01-27T20:58:15.427,syntax and grammar ai,human-like reasoning,1,0,2
812,3786,1,,2017-08-09T01:39:46.933,7,1115,"i always thought rule - based was synonymous with logic - based ai . logic has axioms and rules of inference , whereas rule - based ai has a knowledge base ( essentially axioms ) and if - then rules to create new knowledge ( essentially inference rules ) . but in their famous article "" what is a knowledge representation ? "" , davis , shrobe and szolovits seem to imply that they are not : logic , rules , frames , etc . , each embody a viewpoint on the kinds of things that are important in the world . logic , for instance , involves a ( fairly minimal ) commitment to viewing the world in terms of individual entities and relations between them . rule - based systems view the world in terms of attribute - object - value triples and the rules of plausible inference that connect them , while frames have us thinking in terms of prototypical objects . is this only saying that rule - based are propositional whereas logic - based is usually meant to mean predicate logic ? or is there more to it than this ?",8637,,,2017-08-13T19:24:38.220,logic - based vs rule - based ai,knowledge-representation logic,2,1,5
813,3798,1,3818,2017-08-10T22:19:26.323,2,200,"i 've been looking recently into what uses ai - specifically machine learning - may have in automating engineering design . for a long time there have been algorithms that solve constraint satisfaction problems , and to me it makes sense to consider engineering problems as a superset of constraint satisfaction problems . in spite of this , i have n't been able to find any cases of engineering design being automated other than a couple of cases of genetic algorithms being used to optimise structural members . so my question is , why ca n't i find any examples ? the first thing that springs to mind is that i just have n't been looking hard enough - if this is the case , could anyone point me in the right direction ? the other obvious answer is that it is n't a widely researched area - if so , why not ? is it just due to lack of interest or are there technical hurdles ( abstraction , complex logic & amp ; reasoning , etc . ) that make this a much more difficult problem than computer vision , games , and so on ?",8991,,,2017-08-17T06:00:24.270,artificial intelligence / machine learning in engineering design,machine-learning genetic-algorithms,1,0,
814,3801,1,3803,2017-08-11T05:31:09.903,18,7387,"frameworks like pytorch and tensorflow through tensorflow fold support dynamic computational graphs and are receiving attention from data scientists . however , there seems to be a lack of resource to aid in understanding dynamic computational graphs . the advantage of dynamic computational graphs appears to include the ability to adapt to a varying quantities in input data . it seems like there may be automatic selection of the number of layers , the number of neurons in each layer , the activation function , and other nn parameters , depending on each input set instance during the training . is this an accurate characterization ? what are the advantages of dynamic models over static models ? is that why dcgs are receiving much attention ? in summary , what are dcgs and what are the pros and cons their use ?",7402,4302,2017-08-21T16:35:59.997,2018-12-14T04:25:18.207,what is a dynamic computational graph ?,neural-networks,4,0,8
815,3802,1,,2017-08-11T12:26:00.903,4,710,"i have an industrial problem which i 'm trying to cast as a traveling salesman problem ( tsp ) in 3d euclidian space . there are physical limitations which implies that some subpaths may or may not be valid based on simple rules . what algorithm is best to deal with the tsp given that there are rules / model / constraints ? it could be done with genetic algorithms for example , but the only way i see how to incorporate those rules is by including them somehow within the fitness function . but i feel there should be more suitable approaches . would reinforcement q - learning or other algorithms be more appropriate for a rule - based euclidian tsp ?",8995,,,2018-06-26T08:08:46.647,traveling salesman problem variant : which algorithm to choose ?,machine-learning reinforcement-learning genetic-algorithms combinatorics,1,2,1
816,3804,1,,2017-08-12T04:38:18.120,4,463,"suppose that you have 80 neurons in a layer , where one neuron is bias . then you add a dropout layer after the activation function of this layer . in this case , does it have a chance to drop out the bias neuron , or does the dropout only affect the other 79 weight neurons ?",7402,,,2018-03-02T07:03:41.240,does a bias also have a chance to be dropped out in dropout layer ?,deep-learning dropout,1,0,
817,3805,1,,2017-08-12T05:39:38.040,3,95,"3 svd based methods for this class of methods to find word embeddings ( otherwise known as word vectors ) , we first loop over a massive data set and accumulate word co - occurrence counts in some form of a matrix x and then perform singular value decomposition on x to get a usv^t decomposition . we then use the rows of u as the word embeddings for all words in our dictionary . let us discuss a few choices of x. above is the excerpt from the standford univ cs224n lecture 1 notes . above usv refer to what ? there 's no prior explanation about it so i ask here .",8325,4302,2018-10-08T12:24:18.820,2018-10-08T12:24:18.820,what is usv in nlp ?,natural-language-processing terminology,1,0,
818,3807,1,,2017-08-13T16:48:35.890,3,168,"are there any real - world examples of unintentional "" bad "" ai behaviour ? i 'm not looking for hypothetical arguments of malicious ai ( ai in a box , paperclip maximizer ) , but for actual instances in history where some ai directly did something bad due to its direct action that was not intended behavior . interpretations of the meaning of "" ai "" , "" bad "" , and "" unintentional "" are left open due to obvious reasons . be free with your interpretation , but use some common sense please . ex : microsoft tay became a racist not too long after being hooked up to the internet , thanks to internet trolls "" teaching "" her bad things . i ca n't think of any other instances . so the following examples are just hypothetical scenarios to demonstrate what i mean . ex : a self - driving car drove off - track after being presented with an adversarial example , crashing into people . ex : surgery bot goes haywire and accidentally kills someone . ex : weaponized drone targets civilians against its design .",6779,6779,2019-04-06T00:59:23.660,2019-04-06T00:59:23.660,concrete examples of unintentional adversarial ai behaviour,ai-safety,2,8,1
819,3814,1,,2017-08-14T20:59:00.470,6,1628,"so , deepmind is pushing for a human level starcraft bot and open ai just created a human level 1vs1 dota bot . unfortunately , i 've no clue what that signifies because i 've never played starcraft nor dota nor do i have more than a fleeting acquaintance with similar games . my question is what the difference between starcraft and dota is from a ai perspective and what scientific significance the respective super human bots would have .",2227,2227,2017-08-15T12:34:33.557,2017-08-18T11:55:59.223,what 's the difference between starcraft and dota from an ai perspective ?,game-ai deepmind,2,1,
820,3817,1,,2017-08-15T15:07:52.343,7,174,"i am currently searching for a supervised learning algorithm that can be used to predict the output given a large enough training set . here 's a simple example : training set : a : 1 b : 330 c : 1358.238902 result : 234.244378 test data : a : 893 b : 34 c : 293 result : ? my intention is to predict "" ? "" using the input values and result given in the training set . what algorithm would be effective for this problem given the wide range of my input / output values ? would this require some sort of regression algorithm ?",9046,1581,2017-08-16T19:58:03.860,2018-03-15T11:34:51.380,supervised training algorithm,algorithm learning-algorithms structured-data,2,5,1
821,3820,1,,2017-08-16T10:37:37.893,6,1857,"is this related to supervised and unsupervised machine learning ? is it related to ai assisted human learning , and what is the distinction ? also , why is assisted machine learning seen as an opportunity and unassisted machine learning seen as a threat ?",9062,1671,2017-08-29T22:57:54.603,2017-08-29T22:57:54.603,what is the difference between assisted and unassisted learning in relation to ai ?,machine-learning terminology unsupervised-learning comparison intelligence-augmentation,2,8,2
822,3822,1,,2017-08-16T14:10:38.197,1,115,i 've been looking at both of these python courses on udemy : https://www.udemy.com/artificial-intelligence-az/ https://www.udemy.com/deeplearning/ would it be possible to integrate these topics together ? would i need to learn other courses to integrate them ?,9065,9065,2017-08-16T22:56:00.530,2017-08-17T22:17:06.837,ai with deep learning,neural-networks deep-learning ai-design,1,4,1
823,3823,1,4384,2017-08-16T19:29:00.103,3,206,"i can recall that a professor once said that decision trees are not good for incremental learning , as they have to be rebuilt from the ground up if new training examples arrive . is this basically true ? quick googling just brought me to a lot of papers trying to fit decision trees into incremental learning what other algorithms fall under this category ? are neural nets good for incremental learning ? what other algorithms would be good ?",9071,10468,2017-10-31T16:46:23.937,2017-10-31T16:46:23.937,is a decision tree less suitable for incremental learning than e.g. a neural net ?,machine-learning reinforcement-learning learning-algorithms,1,3,1
824,3825,1,3853,2017-08-16T23:20:02.533,5,88,"i 've gone through several descriptions of cnns online and they all leave out a crucial part as if it were trivial . a "" volume "" of neurons consists of several parallel layers ( "" feature maps "" ) , each the result of convolving with a different kernel . between volumes there is usually a step where layers are pooled and subsampled . the next volume has a different number of parallel layers . how do the feature maps from one volume connect to the feature maps of the next volume ? is it one - to - many ? many - to - many ? do n kernels apply to each of m feature maps in the first volume , yielding n*m feature maps in the second volume ? are these n kernels the same for each feature map in the first volume , or do different kernels apply to each one ? or is the number of maps in the second volume not necessarily a multiple of the number in the first volume ? if so , do maps in the first volume get cross - synthesized somehow ? or maybe different numbers of maps in the second volume follow from each one in the first ? or is it some other of umpteen trillion possibilities ?",9072,,,2017-08-25T18:24:59.220,cnns : what happens from one neuron volume to the next ?,convolutional-neural-networks,2,1,
825,3828,1,,2017-08-17T07:15:24.897,3,77,"i have a question regarding answer set programming on how to make an existing fact invalid , when there is already ( also ) a default statement present in the knowledge base . for example , there are two persons seby and andy , one of them is able to drive at once . the scenario can be that seby can drive as seen in line 3 but let 's say , after his license is cancelled he can not drive anymore , hence we now have lines 4 to 7 and meanwhile andy learnt driving , as seen in line 7 . line 6 shows only one person can drive at a time , besides showing seby and andy are not the same . 1 person(seby ) . 2 person(andy ) . 3 drives(seby ) . 4 drives(seby ) : - person(seby ) , not ab(d(drives(seby ) ) ) , not -drives(seby ) . 5 ab(d(drives(seby ) ) ) . 6 -drives(p ) : - drives(p0 ) , person(p ) , p0 ! = p. 7 drives(andy ) . in the above program , lines 3 and 4 contradict with each other , and the clingo solver ( which i use ) obviously outputs unsatisfiable . having said all these , deleting line 3 and getting the problem solved is not what i 'm epecting . the intention behind asking this question is to know whether it is possible now to make line 3 somehow invalid to let line 4 do its duty . however , line 4 can also be written as : 4 drives(p ) : - person(p ) , not ab(d(drives(p ) ) ) , not -drives(p ) . thanks a lot in advance .",8251,1581,2017-08-17T19:26:17.213,2017-08-17T19:26:17.213,answer set programming - make a fact invalid,knowledge-representation logic declarative-programming,0,5,
826,3832,1,,2017-08-17T22:37:44.653,0,134,"i asked this question on the physics part of this website and was advised to post it here my question is very simple it seems the current goal of man is to create an artificial intelligence despite the associated fears to man that such a creation could cause . as i can figure man 's approach is two fold and its goal is being attempted by computer programming or by electronic physical neural nets . my simple question is as i can see it , intelligence is born of sentience by man and the occupants living world . so which came first in the physical world we occupy , sentience or programming , genetic or an abstraction bound by physical rules ? or as a direct result of the structure of the brains throughout the living world , which we may be possible to replicate . if sentience was first does n't it mean that we will have to alter the definition of programming in order to achieve this goal by that method ?",9094,1581,2017-08-20T20:22:57.307,2019-01-12T19:56:45.530,sentience and programming,philosophy programming-languages,3,19,
827,3836,1,,2017-08-18T12:16:22.870,3,361,i am just curious what ai would be harder to create from a strictly engineering point of view . ai which would win 1vs 1 game with the best player in starcraft or ai which would control a team the whole team in dota2 and win against the best team ?,9105,,,2017-08-22T16:13:54.050,ai in dota2 vs ai in starcraft,game-ai,3,5,
828,3837,1,,2017-08-18T14:40:48.243,2,618,"i 'm a little bit stuck : i implemented an ai with goap ( goal oriented action planning , http://alumni.media.mit.edu/~jorkin/gdc2006_orkin_jeff_fear.pdf ) for a simulation game . that works fine . now i want that the agents can cooperate ( e.g. doing actions together ) . what is in this case the best ai - design that the goapactions keep loose couplet ? should they plan together ? ( what is in this case the "" worldstate""?)or should they share their plans ? example agent1 : worldstate agent 1 : islonely= true goal agent1 : islonely = false plan agent1 : askagent2totalk - > talktoagent2 agent2 worldstate agent 2 : haswood = false goal haswood = true plan agent2 : getaxe - > chopwood - > bringwoodtosupply how i get this constellation ? agent1 plan : talktoagent2 agent2 plan : talktoagent1 - > getaxe - > chopwood - > bringwoodtosupply or if they are talking and one of the agents is interrupted ( e.g. by an attacking enemy ) the other agent must know that his talktoagent2 action has ended .",9111,1581,2017-08-19T12:03:23.200,2018-01-23T16:25:57.113,goal oriented action planning with multiple agents,ai-design intelligence-testing game-ai multi-agent-systems,1,4,
829,3847,1,,2017-08-21T09:57:31.947,4,363,"artificial intelligence is any device that perceives its environment and takes actions that maximize its chance of success at some goal . i got this definition from wikipedia that cited "" russell and norvig ( 2003 ) , artificial intelligence : a modern approach "" . a transistor is a device that amplifies or switches electronic signals when it received an input signal . could one say the transistor is the ai ? it is certainly a basic building block of every ai out there , but is it an ai itself , albeit the most basic one ? i 'm looking at it from a technological and economic point of view , leaving philosophy aside . from an economic perspective it seems to be an ai because transistor does useful work that it took an intelligent human to perform less than a century ago . and it does it completely on its own .",9164,1671,2017-09-11T22:31:06.763,2018-11-09T18:49:29.637,is transistor the first artificial intelligence ?,history terminology,5,6,1
830,3850,1,,2017-08-21T15:18:15.710,10,10605,"is it possible to feed a neural network , the output from a random number generator and expect it learn the hashing / generator function . so that it can predict what will be the next generated number ? does something like this already exist ? if research is already done on this or something related to ( predict pseudo random numbers ) can anyone point me to the right resources . any additional comments or advice would also be helpful . currently i am looking at this library and its related links . https://github.com/vict0rsch/deep_learning/tree/master/keras/recurrent",9170,,,2018-10-18T21:35:07.003,using machine / deep learning for guessing pseudo random generator,deep-learning unsupervised-learning prediction lstm,5,3,6
831,3852,1,,2017-08-21T15:40:10.473,4,154,"to my understanding , logistic regression is an extension of naive bayes . suppose $ x = \{x_1 , x_2 , \dots , x_n \}$ and $ y = \{0 , 1\}$ , each $ x_i$ is i.i.d and $ p(x_i \mid y = y_k ) \sim \mathcal{n}(\mu , \sigma^2)$ is a gaussian distribution . in order to create a linear decision surface , we take the assumption of each pdf $ p(x_i \mid y_k)$ having variance independent of the value of $ y$ , i.e. ( $ i \rightarrow x_i , k \rightarrow y_k$ ) . finally , we end up learning the coefficients $ ( \omega_0 , \omega_i)$ that represent the linear decision surface in following equation ( linear decision surface ) : $ $ \frac{p(y=0 \mid x)}{p(y=1 \mid x ) } = \omega_0 + \sum_{i } \omega \cdot x_i $ $ even though the derivation of linear regression coefficients $ ( \omega_0 , \omega_i)$ involves the assumption of conditional independent $ x_i$ given $ y$ , why is it said that learning these coefficients from training data are "" somewhat more free "" from conditional independence assumption , as compared to learning the regular bayesian distribution coefficients ? i came across this while following this course here .",8720,2444,2019-04-19T14:39:06.617,2019-04-19T14:39:06.617,is logistic regression more free from the conditional independence assumption than naive bayes ?,machine-learning logistic-regression naive-bayes,1,0,
832,3854,1,3858,2017-08-21T16:53:00.563,4,141,"i studied the articles on neural networks and deep learning from michael nielsen and developed a simple neural network based on his examples . i understand how backpropagation works and i already taught my neural network to not only play tictactoe but also improve his own play by learning from his own successes using backpropagation . going forward with my experiments , i am facing the problem , that i wo n't always be able to show the network good moves to use for learning ( maybe because i simply do n't know what is correct in a certain situation ) , but i might be required to show it bad moves to avoid ( because some of the bad moves are obvious ) . teaching the network what to do using backpropagation is easy , but i have n't found a way to teach it what to avoid using similar techniques . is it possible to teach simple neural networks using negative examples like this or do i need other techniques ? my gut feeling says , that it might be possible to "" invert "" gradient descent into gradient ascent to solve this problem . or is it more complicated than this ?",9161,,,2017-08-22T06:11:17.763,can a neural network learn to avoid wrong decisions using backpropagation ?,neural-networks backpropagation gradient-descent,1,0,
833,3860,1,,2017-08-22T13:31:00.553,1,1655,"can someone explain the differance between tf.contrib.dnnclassifier ( learn ) and tf.estimator.dnnclassifier ? tf.contrib.dnnclassifier ( learn ) works but gives warnings : warning : tensorflow : from c:\anaconda3\lib .. scalar_summary ... is deprecated and will be removed after 2016 - 11 - 30 . please switch to tf.summary.scalar . but i can load the layers names and values with get_variable_names and get_variable_value . tf.estimator works fine but how do i get the layers name and values ? see code below with a switch ( if true / false ) for the two versions instalation in windows 10 3.6.1 |anaconda 4.4.0 ( 64-bit)| ( default , may 11 2017 , 13:25:24 ) [ msc v.1900 64 bit ( amd64 ) ] tensorflow is installed in the root with : pip install --upgrade tensorflow --ignore - installed ( this is the only combination that works for me ) pip list gives tensorflow ( 1.3.0 ) tensorflow - tensorboard ( 0.1.4 ) # example of dnnclassifier for iris plant dataset . from _ _ future _ _ import absolute_import from _ _ future _ _ import division from _ _ future _ _ import print_function import numpy as np from sklearn import datasets from sklearn import metrics from sklearn import model_selection import os import tensorflow as tf from tensorflow.contrib import learn os.environ['tf_cpp_min_log_level']='3 ' # get rid oc tf_jenkins warning def main ( ) : # load dataset . iris = datasets.load_iris ( ) x_train , x_test , y_train , y_test = model_selection.train_test_split ( iris.data , iris.target , test_size=0.2 , random_state=42 ) # define the training inputs and train get_train_input_fn = tf.estimator.inputs.numpy_input_fn ( x={'x':x_train } , y = y_train , num_epochs = none , shuffle = true ) # predict get_test_input_fn = tf.estimator.inputs.numpy_input_fn ( x={'x':x_test } , y = y_test , num_epochs=1 , shuffle = true ) # build 3 layer dnn with 10 , 20 , 10 units respectively . feature_columns = [ tf.feature_column.numeric_column('x ' , shape = np.array(x_train).shape[1 : ] ) ] # select code if true : # tf.estimator ... classifier = tf.estimator.dnnclassifier(feature_columns=feature_columns , hidden_units=[10 , 20 , 10 ] , n_classes=3 ) # train classifier.train(input_fn=get_train_input_fn , steps=200 ) scores = classifier.evaluate(input_fn=get_test_input_fn ) print('accuracy ( tf.estimator ) : { 0 : f}'.format(scores['accuracy ' ] ) ) # get_variable_names , get_variable_value . how ? ? ? ? ? else : # learn ( tf.contrib ) classifier = learn.dnnclassifier(feature_columns=feature_columns , hidden_units=[10 , 20 , 10 ] , n_classes=3 ) # train classifier.fit(input_fn=get_train_input_fn , steps=200 ) scores = classifier.evaluate(input_fn=get_test_input_fn,steps=1 ) print(""\ntest accuracy ( learn ) : { 0 : f}\n"".format(scores[""accuracy "" ] ) ) # get data names = classifier.get_variable_names ( ) data= { } for name in names : data[name]=classifier.get_variable_value(name ) if _ _ name _ _ = = "" _ _ main _ _ "" : main ( )",9185,9647,2017-12-18T21:33:06.630,2018-05-18T00:28:54.013,tensorflow : tf.contrib.dnnclassifier ( .. ) or tf.estimator.dnnclassifier ( .. ),classification tensorflow,1,0,
834,3861,1,3866,2017-08-22T13:45:38.643,4,747,"i wonder on the following concept : a given neural network gets two audio input ( preferably music ) and gives a real number between 0 and 1 which describes "" similarity "" between the second and the first track . as far as my understanding of neural networks go , the problem fits the concept of nns , as pattern recognition in music can help determine similarities and discrepancies in audio , see voice recognition . however , due to the nature of long and complex inputs , and the vague nature of learning datasets ( how similar , for instance , diana ross "" it 's your move "" , and the vaporwave legend "" floral shoppe "" exactly are ? 0.9 ? 0.6 ? other ? ) , such a network would be extremely slow and convoluted . is it possible today to build and train such a model ? if yes , how would it look like ?",1270,16565,2019-05-15T18:28:48.830,2019-05-15T18:28:48.830,is music / sound similarity comparison feasible on neural networks ?,neural-networks deep-learning pattern-recognition voice-recognition similarity,1,2,2
835,3862,1,,2017-08-22T14:26:51.633,7,447,"i am getting confused reading online about gradient descent , convex and non convex loss functions . multiple resources i referred to mention that mse is great because it 's convex . but i do n't get how , especially in the context of neural networks . lets say we have the following : $ x$ : training dataset $ y$ : targets : set of parameters of the model ( nn model with non linearities ) then : $ $ now i do n't seem to agree that this mse loss function is always convex , it depends strongly on , right ?",9189,,2018-11-22T06:58:54.807,2018-11-22T06:58:54.807,convexity of mse in neural networks ?,neural-networks backpropagation gradient-descent,2,0,
836,3864,1,,2017-08-23T07:03:27.030,-1,48,"the textbook deep learning by goodfellow , bengio , and courville can be viewed in individual html chapters here . i 'm currently reading chapter 15 on representation learning and saw this for algorithm 15.1 on page 530 : there no condition after end for and similarly no end condition for end if . is that intentional ? if so , what is the correct way to interpret it ? i have n't encountered algorithm syntax like this in the past .",9197,,,2017-08-23T07:36:09.127,is this page of goodfellow 's deep learning textbook missing text ?,deep-learning,1,0,
837,3873,1,,2017-08-23T15:41:41.127,4,101,"let 's take our standard paperclip maximizer general ai and attempt to obtain precisely one million paper clips , over course of a year , without destroying the universe in the process . most maximization directives make the process run - away . as cheaply as possible will crash world economy . as good clips as possible will turn the universe into super - synthesizer that assembles atom - perfect paperclips . adding a deadline on these maximization processes will probably result in terrorizing the staff into readjusting the deadline , or invention of time travel ( after consuming the solar system to invent it . ) minimizing resource usage would likely result in closure of all industry world - wide . you know , the standard horror scenarios . what about the directive of minimizing ai 's influence on the world while completing the task ? would it be safe , or can you spot a runaway scenario where it could result in dire effects ?",38,38,2018-01-26T10:59:30.223,2018-01-28T09:49:47.373,would minimizing influence into the world be a safe directive to a general ai ?,strong-ai,4,4,
838,3875,1,,2017-08-23T19:54:51.713,6,785,"i do n't play nearly enough chess to be able to answer . for context , alphago is stronger than the current strongest human player , but alphago 's game play has been cast as "" inhuman "" in the sense that it does n't resemble human play . ( in go , this can involve aesthetic qualities . ) really i 'm wondering about "" narrow "" application of the imitation game / turing test , where one might design an automata to play more like a human , so that human players would be unable to determine if their opponent was a human or an automata .",1671,,,2017-08-24T03:51:12.160,is the play of strong chess ai easily distinguishable from human play ?,turing-test chess go,4,0,1
839,3879,1,9833,2017-08-23T23:40:45.230,2,183,"i have followed the pseudocode in the adadelta paper ( top right on page&nbsp;3 ) , and wrote the following python code for solving the optimization problem l(x)&nbsp;=&nbsp;x^2 : & gt;&gt;&gt ; import math & gt;&gt;&gt ; & gt;&gt;&gt ; eg = ex = 0 & gt;&gt;&gt ; p = 0.95 & gt;&gt;&gt ; e = 1e-6 & gt;&gt;&gt ; x = 1 & gt;&gt;&gt ; history = [ x ] & gt;&gt;&gt ; & gt;&gt;&gt ; for t in range(100 ) : ... g = 2*x ... eg = p*eg + ( 1+p)*g*g ... dx = -math.sqrt(ex+e)/math.sqrt(eg+e)*g ... ex = p*ex + ( 1-p)*dx*dx ... x = x + dx ... history.append(x ) ... & gt;&gt;&gt ; print(history ) [ 1 , 0.9992838851718654 , 0.998764712958258 , 0.9983330059505671 , 0.9979531670003327 , 0.9976084468473033 , 0.997289409971406 , 0.996990129861279 7 , 0.9967066004793412 , 0.9964359664599116 , 0.9961761091943561 , 0.9959254064337589 , 0.9956825840862665 , 0.9954466203957414 , 0.99521668152795 22 , 0.994992076841149 , 0.9947722269598268 , 0.9945566404428509 , 0.9943448963795858 , 0.9941366311727671 , 0.9939315283404335 , 0.99372931053536 84 , 0.9935297332202913 , 0.9933325795977276 , 0.9931376565033874 , 0.9929447910484566 , 0.9927538278504536 , 0.9925646267313296 , 0.9923770607899 557 , 0.9921910147771753 , 0.9920063837173206 , 0.991823071731977 , 0.9916409910308515 , 0.9914600610415929 , 0.9912802076558476 , 0.9911013625730 931 , 0.9909234627271605 , 0.990746449783029 , 0.9905702696936243 , 0.9903948723080783 , 0.9902202110243085 , 0.9900462424799209 , 0.9898729262763 75 , 0.9897002247321224 , 0.9895281026610697 , 0.9893565271732506 , 0.9891854674950317 , 0.989014894806555 , 0.9888447820944316 , 0.98867510401796 52 , 0.9885058367874141 , 0.9883369580529869 , 0.9881684468034365 , 0.9880002832732533 , 0.9878324488575824 , 0.9876649260340926 , 0.9874976982911 152 , 0.9873307500614499 , 0.9871640666613011 , 0.9869976342338704 , 0.9868314396971792 , 0.9866654706957432 , 0.9864997155557601 , 0.986334163243 507 , 0.9861688033266723 , 0.9860036259383794 , 0.9858386217436783 , 0.9856737819083067 , 0.9855090980695381 , 0.9853445623089551 , 0.985180167126 9962 , 0.9850159054191441 , 0.9848517704536291 , 0.9846877558505386 , 0.9845238555622267 , 0.9843600638549337 , 0.984196375291527 , 0.984032784715 2862 , 0.983869287234659 , 0.9837058782089235 , 0.9835425532346933 , 0.9833793081332108 , 0.983216138938377 , 0.9830530418854679 , 0.9828900134004 96 , 0.9827270500901729 , 0.9825641487324376 , 0.9824013062675131 , 0.9822385197894608 , 0.9820757865381995 , 0.9819131038919638 , 0.9817504693601 732 , 0.9815878805766881 , 0.9814253352934307 , 0.9812628313743479 , 0.9811003667896978 , 0.9809379396106398 , 0.9807755480041119 , 0.980613190227 9784 , 0.9804508646264328 , 0.9802885696256415 ] here , eg , ex , e , p , g and dx are $ e[g^2]$ , $ e[\delta x^2]$ , , , $ g$ ( or ) and , respectivelly , and history is the record of all values that x has obtained . for the hyperparameters and , i use the same values that they use in the paper , and i initialize $ x$ to 1 . as can be seen when printing history , the convergence is extremely slow for such a simple optimization problem , and after 100 iterations the method has barely got 2&nbsp;% closer to the optimum ( x&nbsp;=&nbsp;0 ) . it feels like i must have misunderstood some crucial part of the paper . for example , the paper claims that the update step & delta;x will have the same unit as x , if x has some hypothetical unit . while this is probably a desireable property , it is as far as i 'm concerned not true , since the premise that $ rms[\delta x]$ has the same unit as $ x$ is incorrect to begin with , since $ rms[\delta x]_0 = \sqrt{e[\delta x]_0 + \epsilon } = \sqrt{0 + \epsilon}$ which is a unitless constant , so all become unitless rather than having the same unit as $ x$ . ( correct me if i 'm wrong . ) have i made some error when implementing the algorithm , or why is the convergence so slow ? is it supposed to be this slow ? edit after changing eg = p*eg + ( 1+p)*g*g to eg = p*eg + ( 1-p)*g*g ( i.e. correcting the error spotted by dennis ) , convergence is now significantly better . it still does n't get very close to 0 after the first 100 iterations ; x only goes down to 0.597 . however , after 400 iterations or so , the convergence really starts to kick in and in the following 100 iterations , x goes down from its current value at 0.0156 to 2.07e-8 , and then to 3.24e-57 after yet another 100 iterations ! i plotted $ x$ , $ e[\delta x^2]$ and $ e[g^2]$ in logarithmic scale against the number of iterations , and this is what i found : i 'm not really sure what this means in terms of performance "" in the wild , "" since this only is a toy problem , free from stochasticity in the optimization and with only one parameter . about the unit analysis , i guess should n't be unitless as i wrote that it was , as otherwise , it can not be added to $ e[g^2]$ ( which has the same unit as $ g^2 $ ) . also , i guess that in practice , must be treated as two different constants that may have different units when added to $ e[\delta x^2]$ and when added to $ e[g^2]$ ; otherwise , $ x$ will end up having the same unit as $ g$ . verdict a funny thing is that even though should only be a tiny number that should n't affect the process significantly unless you divide by something really small , the process turns out to be highly sensitive to its value . this is because essentially is the only thing that really makes $ e\left [ \delta x^2 \right]$ grow . higher -values will make $ e\left [ \delta x^2 \right]$ grow faster and will make the optimization process go significantly faster . in my opinion , this seems to indicate a major design flaw in the algorithm .",9220,9220,2019-01-10T11:25:55.317,2019-01-10T11:25:55.317,problems getting adadelta to converge,deep-learning algorithm optimization math,1,1,
840,3881,1,,2017-08-24T02:56:04.813,4,82,"according to russell and norvig , a knowledge - based agent will only add a sentence to its knowledge base if it follows logically from what it previously knows , or directly observes . to follow logically essentially means that if the premises are true , then the conclusions are guaranteed to be true . so the agent will only add the sentence if it 100 % sure the sentence is true . is this hyperskepticism in logic justified ? could n't the agent be more efficient if it added a sentence if it were 99 % sure it were true ? it could potentially add a lot more true sentences , and only occasionally add false ones . there would need to be a mechanism for unlearning sentences , but as long as the vast majority of sentences added are true , why could n't that be done ? i essentially asked this question here , and someone suggested that i post it here .",9225,,,2017-08-24T19:26:18.933,why do knowledge - based agents only add a sentence to the knowledge base when it is 100 % sure the sentence is true ?,logic,1,1,1
841,3885,1,3893,2017-08-24T12:42:23.943,6,387,"as i know , the current state of the art methods for training deep learning networks are variants of gradient descent / stochastic gradient descent . what are the best known gradient - free training methods ( mostly in visual tasks context ) ?",9233,,,2017-08-25T20:17:42.777,gradient free training methods for deep learning,deep-learning gradient-descent,1,5,3
842,3888,1,,2017-08-24T18:00:39.937,4,69,"i 'd like a general explanation of that in ais that were to mimic judges , prosecutors or lawyers , on very general terms they would act on this way for each case : a judge ai would give a verdict , having the following input : all sources of law that have some relationship with the act being judged , knowing the relative importance of each of that sources . all the facts presented to it , being theorically able to distinguish if some fact were false if there are enough evidence for that . prosecutors ai would act into searching the facts that would most help for accusing , matching them with known sources of law so they can make the best accusation . lawyers ai would act exactly as prosecutors but for defense of the one being accused . obviously ais are way too far nowadays from something like this , but i find strange that it does n't even look that this has been done as a proof of concept , the most near thing i find by searching on the internet is this . i 'm not sure of the reach of everything that it 's explained there but it looks more of assisting judges , prosecutors and lawyers rather than making what they do . i find this a bit strange , although i 'm very far from being an expert on ais , i think that for very simple cases an ai could even do those things . in the answer about possible development , i 'd like to know about which would be the facts that would make developing an ai like the one i 'm mentioning to be possible or not . pd : i do n't find any suitable tag , i ask an admin to put a better one if needed . requested edit : my question is about the general state of the art of this field ( of ais being able to somehow mimic a judge , a prosecutor or a lawyer ) , and how pausible is a development in this way and how it would develop ( if this is with current knowledge impossible to even try until a far future , or if we are at least near for stablishing ais understanding law , which would be the logical way for a type of ais like them evolving ( understing laws , judging , being able to see videos .. ) . i know it 's broad so i 'm happy with a general explanation .",9244,9244,2017-08-24T18:48:49.840,2017-08-24T19:04:18.733,state of the art and possible development of judging ais in laws field,ai-design,1,1,1
843,3889,1,,2017-08-24T18:35:04.387,3,64,"i read some light material earlier about the possibility of building ai agent trees , which leaf agents optimizing for primitive tasks , while higher level agents optimizing for orchestrating direct descendant agents . according to my understanding , each agent would have different objective functions , and possible moves . i wonder if someone could give some insight on how to implement such complex agent , hopefully with an example .",9210,,,2018-09-03T19:23:12.803,hierarchical agent design,ai-design reinforcement-learning multi-agent-systems,1,0,
844,3894,1,3896,2017-08-26T03:58:57.590,4,145,"what we are doing in the image processing training . we are storing some form of data which is going to act as the knowledge or experience of the system . in which form can the system store it 's training data ? for example , with the hand written recognition , we can represent the digits as combinations of curves and straight lines . for every round of training the recognition system stores data . is the data typically stored in a flat file ( such as txt ) or a database ? i have seen in tesseract ocr that there is a text file that stores the x0,y0,x1,y1 . they are the pixel points that represents the square on the training image that has the picture . i need a efficient form of knowledge for machine learning , and would appreciate advice , context , or an explanation of the merits or downsides of different approaches . i need a form of knowledge that stored in a system . human brain evaluate ' 7 ' as ' horizontal line and vertical left crossed line start from right of the horizontal line ' . like that machine must have some conceptual data to represent their knowledge .",8424,8424,2017-08-29T05:06:47.417,2017-09-02T05:00:53.013,as a starter : what is the form of training data for image processing,machine-learning image-recognition training knowledge-representation,2,1,1
845,3897,1,3918,2017-08-26T11:04:51.500,5,226,"i 'm quite new to image processing and ai . but i have the expertise to create a network that can be used in object detection and recognition . most of the time i 've used ann or naive bayes . now , i want to develop a method of action recognition , something like identifying whether one is jogging , running or walking by applying ann . however , i really do n't have idea how the sequence of frames can be classified . in static image , segmentation and feature extraction is easy . but in regard to a moving image , i 'm unsure of the approach . thanks in advance !",9211,1671,2017-08-27T20:21:14.153,2017-12-26T23:37:26.357,how can i use neural network in motion identification,machine-learning image-recognition,2,0,
846,3898,1,3901,2017-08-26T15:32:01.580,1,72,"i was trying to code a single layer perceptron to understand binary and : 1 1 1 0 1 0 1 0 0 0 0 0 i made up this code # include & lt;stdio.h&gt ; # include & lt;stdlib.h&gt ; # include & lt;math.h&gt ; int main ( ) { int input1 , input2 ; float weight1 = 0.3 , weight2 = 0.4 ; int output ; int training1 , training2 , expectedoutput ; int i ; int j=1 ; //training for(i=0 ; i&lt;10000;i++ ) { if(j=1 ) { training1 = 0 ; training2 = 1 ; expectedoutput = 0 ; } if(j=2 ) { training1 = 1 ; training2 = 0 ; expectedoutput = 0 ; } if(j=3 ) { training1 = 0 ; training2 = 0 ; expectedoutput = 0 ; } if(j=4 ) { training1 = 1 ; training2 = 1 ; expectedoutput = 1 ; j=1 ; } output = weight1*training1 + weight2*training2 + 2 ; if(output ! = expectedoutput ) { weight1 = weight1 + 0.156 * training1 * ( expectedoutput - output ) ; weight2 = weight2 + 0.156 * training2 * ( expectedoutput - output ) ; } j++ ; } printf(""training done\n "" ) ; printf(""weight1 = % f "" "" weight2 = % f\n"",weight1,weight2 ) ; //testing the perceptron for(i=0 ; i&lt;5 ; i++ ) { scanf ( "" % d%d "" , & amp;input1 , & amp;input2 ) ; output = weight1*input1 + weight2*input2 ; printf(""\n%d\n "" , output ) ; } return 1 ; } its supposed to input the 4 cases repeatedly and with a learning rate of 0.156 ( which i set randomly ) and i used the threshold as a weight of 2 . however after the training the perceptron still does nt give the expected output . is my understanding of perceptron rule wrong ? please help thank you !",9272,,,2017-08-27T07:11:44.717,debugging perceptron for digital and circuit,neural-networks machine-learning artificial-neuron,1,1,
847,3899,1,,2017-08-26T18:13:41.780,5,408,"i want to write an algorithm which indicates to a robot the first point in time when it is reasonably safe to cross a road . assume that the robot 's goal is to travel to a location that requires a road crossing and that the robot is ready to cross . simple algorithms and decision making will probably not suffice . what features and capabilities must the algorithm have to provide crossing safety ? what existing ai methods might be useful to consider for this endeavor ? for the first iteration , we can assume the traffic pattern is normal in that no vehicles are driving over the curb and there are no high speed chases or other safety related abnormalities .",9275,9203,2018-05-30T16:19:53.237,2018-08-28T21:00:53.083,approaches to an algorithm for crossing a road,algorithm learning-algorithms,2,1,2
848,3903,1,3907,2017-08-27T15:23:54.393,9,904,"within the ai development field , the main focus seems to be on the pattern recognition and "" learning "" aspect of the thing ... but i think the bigger questions is motivation . learning seems to be very easy to explain , as when the sensory input is being interpreted there is a feedback loop that adjust the environmental variables accordingly . a human child has this inborn curiosity ... ( and hunger / thirst ) ... its often referred to as maslow 's pyramid , but what could possibly motivate a machine to form some sort of actions ? should a machine have some sort of dna - like structure that would describe it 's hierarchy of needs , what should / could a machine need ? edit : yes , its obvious that a statistical algorithms ca n't have motivation , the question is more of ... "" how to fake the motivation and what it should be ? "" edit2 : having trouble to phrase the question correctly , but .. i am not talking about motivation to learn , but about some sort of inner "" drive / push "" to do something . example : child is curious about an item , and explores it , but where does the interest towards the item originate from ? - it detected a new item in the environment it has n't seen ? i guess the algorithms drive should originate from encountering new phenomena ... but in that case , what would force it to synthesise new ideas by combining the old ones ?",9288,9288,2017-08-27T18:40:39.490,2018-08-07T06:28:31.507,what would motivate a machine ?,philosophy strong-ai,3,4,3
849,3905,1,3909,2017-08-27T17:16:52.577,3,481,"hypothetically , the symbol ( triangle ) is sticked to an item and i need to find and recognize that symbol and try to calculate the orientation of the item it is sticked into . in degrees . how would you guys suggest i approach this problem ? do i still need nn for this ? thank you :) i just need to hear other people 's thoughts .",9290,,,2017-08-28T21:28:39.497,image recognition and orientation detection,neural-networks image-recognition,1,2,0
850,3908,1,,2017-08-27T18:13:28.803,4,129,"nick bostrom talks in his book superintelligence about the many dangers of ai . he considers it necessary that strong security mechanisms are put in place to ensure that a machine , once it gains general intelligence far beyond human capabilities , does not destroy humanity ( most likely by accident ) . he describes this as a very delicate process that most likely will go wrong . considering that new technologies often neglect the necessary precautions and that this is highly relevant to national security , i wonder if there are already government agencies overseeing big technology companies like deepmind . we are currently far away from an intelligence explosion or a technological singularity , but i would assume that governments want to have a foot in the door as soon as they realize and understand the dangers . so my question is , what government agencies currently investigate and maybe even control ai development ? the answer can be general or for a specific country if there is a big difference between countries .",9161,,,2018-01-08T15:20:39.947,which government agencies oversee development of new ai ?,control-problem singularity security,2,0,1
851,3910,1,,2017-08-28T02:25:21.380,3,79,"in this note justin domke says that in practice , neural networks seem to usually find a reasonable solution when the number of layers is not too large , but find poor solutions when using more than , say , 2 hidden layers . but in bengio 's remark , he says very simple . just keep adding layers until the test error does not improve anymore . there seems to be a conflict . can anyone explain why they suggest differently ? or am i missing something ?",9299,,,2017-08-28T03:42:53.030,different suggestion for estimating number of layers in neural network,neural-networks machine-learning artificial-neuron hidden-layers,2,3,
852,3920,1,3928,2017-08-29T08:57:30.060,6,594,"as i understand it from this video lecture , there are three types of deep learning : supervised unsupervised reinforcement all these can serve to train a nn either only prior to its deployment or during its operating as well . for the latter , i read "" continuous "" learning here and here . and "" dynamic "" learning here and here . my questions are these : which term is to be used for a learning system that keeps on learning ? and if it 's "" continuous "" , is there an opposing term ( such as "" static "" for "" dynamic "" ) for those systems that stop learning before being deployed ?",9319,,,2017-09-26T13:22:02.867,"terminology of deep learning : "" continuous "" or "" dynamic "" ?",neural-networks deep-learning terminology,2,8,
853,3921,1,4368,2017-08-29T10:11:11.713,5,234,"intelligence ... changes based on the environment and situation human are now inventing machines exhibiting some features of their own intelligence . there appears to be a possibility that , in the future , machines will be able to invent machines . question 1 : is there any possibility that , at that time , such machines may become beyond the control of humans ? . species having superior intelligence may lead to control of the world , whereas human beings are now on top . humans are currently the most intelligent species , but computers demonstrate more accuracy than humans . question 2 : can anyone specify an existing tool that can do this ?",8424,4302,2018-09-20T06:02:25.590,2018-10-16T20:13:40.010,is it possible to construct an ann that is more efficient than the human brain ?,deep-learning deep-network cognitive-science superintelligence,2,7,
854,3922,1,,2017-08-29T13:29:18.700,2,48,"currently big tech companies like microsoft , google , and amazon ( to name a few ) offer cognitive services on their cloud platforms . with these services it is possible to identify faces , objects , texts , sounds , etc . do you know how these services work internally ? the only info i could find was based on the api level . i assume the services use some neural network , which is trained by amounts of data . in my experience the google services are more accurate then the azure services . perhaps the google services are better and longer trained ?",9057,,,2017-08-29T13:29:18.700,how do cognitive services work ?,computer-vision,0,1,2
855,3923,1,,2017-08-29T14:21:48.227,2,111,"in working with a social services agency that provides a continuum of programs across the behavioral health and child welfare spectrum the need to adequately manage individual worker and total program caseloads has become increasingly difficult due to burgeoning demand . each program has already developed a profile for ranking the complexity of a client based a select number of factors ( i.e. diagnosis , previous encounters , results of assessment instruments , etc . ) . what we are trying to develop now is an approach to act as a load balancer for new cases to ensure that individual workers ' caseloads are balanced with the complexity of cases . we can do this manually to some degree but if there was a way to automate and take the human bias out of it , that is the goal . are there are any algorithms or adaptable approaches that have been used to do such a task ( even in other sectors ) ?",9324,,,2018-02-13T22:36:35.810,are there any load balancing approaches for employee cases based on complexity score ?,algorithm problem-solving efficiency,1,2,1
856,3925,1,,2017-08-29T18:11:36.497,7,95,"i know the basics of artificial neural networks . for instance ; make dot product with the weights and every neuron from previous layer . adjust the weight by error . and done , that is how i see neural networks , but i saw in lot of videos in the graphical representation that every neuron has it 's synapse with certain neurons . and when it change the input for example they create new synapses with other certain neurons . so what s what i 'm asking for is : how i can make my neurons assign the synapses with the neuron that fits it ?",8497,1581,2018-10-31T18:11:00.453,2019-05-30T07:01:32.023,synapses automatically select it 's neurons,neural-networks deep-learning,1,8,
857,3927,1,,2017-08-29T23:26:06.410,1,32,"i am considering some possibilities to improve a variable gain nonlinear control system . one of the drawbacks of the current technique is that the change of the gains is discrete and the switching effect is highly undesirable ... the gain matrices are 4x16 and are calculated for some values of a bounded variable and then selected trough a lookup table . so , what my idea was to try and do this with an neural network , that would receive as input the control variable and give the gain matrix as output , allowing my control to be continuous or almost continuous . is this idea possible / feasible ? my knowledge on neural nets is very poor , so i hope you can enlighten me somehow .",9336,1671,2017-08-30T00:11:26.050,2017-08-30T00:11:26.050,neural network to interpolate matrices,neural-networks,0,1,
858,3929,1,,2017-08-30T12:04:26.083,3,115,"i wrote a solution to the one shot prisoner 's dilemma : & nbsp ; & nbsp ; introduction my solution applies to a prisoner dilemma involving two people ( i have neither sufficient knowledge of the prisoner 's dilemma itself , nor sufficient mathematical aptitude / competence to generalise my solution to prisoner 's dilemma where number of agents ( n ) > 2 ) . let the two agents involved be a and b , assume a and b are maximally selfish ( they care solely about maximising their payoff ) . if a and b satisfy the following 3 requirements , then whenever a and b are in a prisoner 's dilemma together , they would choose to cooperate . 1 . a and b are perfectly rational . 2 . a and b are sufficiently intelligent , such that they can both simulate each other ( the simulations does n't have to be perfect ; it only needs to sufficiently resemble the real agent that it can be used to predict the choice of the real agent ) . 3 . a and b are aware of the above 2 points . solution a and b have the same preference . ( a , b ) = ( d , c ) > ( c , c ) > ( d , d ) > ( c , d ) . ( b , a ) = ( d , c ) > ( c , c ) > ( d , d ) > ( c , d ) . my solution relies on a and b predicting each other 's behaviour . they use a simulation of the other which guarantees high fidelity predictions . if a adopts a defect invariant strategy ( always defect ) i.e committing to defection , then b will simulate this , and being rational b will defect . vice versa . if a adopts a cooperate invariant strategy , then b will simulate this and being rational , b will defect . vice versa . to commit to a choice means to decide to adopt that choice irrespective of all other information . committing means not taking into account any other information in deciding your choice . assuming a commits to a choice . let the choice a commits to be k. then b 's strategy would be : diagram to illustrate h(x ) is a function that takes in b 's prediction of a 's choice as input and outputs the rational response to that choice . as b prefers ( d , c ) over ( c , c ) and ( d , d ) over ( c , d ) , then b would choose to defect whatever choice it predicts for a. thus , adopting an invariant strategy will cause b to adopt the defect invariant strategy ( insofar as b accurately predicts a ) . being rational , and preferring ( d , c ) and ( c , c ) over ( d , d ) a would not adopt an invariant strategy . vice versa . if both of them were to decide to simultaneously adopt invariant strategies , they would adopt the defect invariant strategy . however , as they both prefer ( d , c ) and ( c , c ) ( in that order ) over ( d , d ) they would both strive to get a better outcome than ( d , d ) unless no better outcome is possible . this means that a and b 's choices depends on what they predict the other would do . if a predicts b will defect , a can cooperate or defect . if a predicts b will cooperate , a can cooperate or defect . vice versa . as a is not committing , a 's strategy is either predict(b ) ( picks what a predicts b will pick ) or ! predict(b ) ( picks the opposite of what a predicts b will pick . i.e cooperate if a predicts b will defect , and defect if a predicts b will cooperate ) . vice versa . to not commit is to decide to base your strategy on your prediction of the choice the opponent adopts . you can either choose the same choice as what you predicted , or choose the opposite of what you predicted ( any other choice is adopting an invariant strategy ) . if a adopts ! predict(b ) . a gains an outcome ranked 1 or ranked 4 in its preferences . if a adopts predict(b ) a gains an outcome ranked 2 or 3 in its preferences . vice versa . we can have : predict(b ) and predict(a ) predict(b ) and ! predict(a ) ! predict(b ) and predict(a ) ! predict(b ) and ! predict(a ) . now a 's decision is dependent on predict(b ) . but b 's decision ( and thus predict(b ) ) is dependent on predict(a ) ( and thus a 's decision ) . a = f(predict(b ) = g(b = f(predict(a ) = g(a ) ) ) ) . f(x ) is a function that deterministically returns either x or ! x ( according to the strategy adopted by the agent implementing it ) . g(x ) is a function the stochastically return x or ! x it returns x with a probability of p. given that the agents are simulating each other , we can safely assume that p is high ( sufficiently close to 1 ) . vice versa . the above assignment is circular and self - referential . if a and/or b tried to simulate it , it leads to a non terminating recursion of the simulations . a 's strategy would be : diagram to illustrate as such , a and b can not both decide to base their decision on the decision of the other . yet , neither a nor b can decide to commit to an option . what they can do , is to predispose themselves to an option . to predispose themselves is to decide on their choice independent of the other agent . assuming i received no further information , what would i do ? a predisposition is not a final choice , and should not be mistaking for committing to a choice of action . an agent who predisposes themselves can change their choice based on how they predict the other agent would react to that predisposition . assuming a predisposes themselves . let the predisposition made be q. the assignment becomes : a = f(predict(b ) = g(b = f(predict(a ) = g(q ) ) ) ) . diagram to illustrate only one of them needs to predispose themselves . assuming a predisposes themselves . if a predisposes themselves to defection , then the only two outcomes are ranked 1 and 3 in their preferences ( for a ) and 3 and 4 in their preferences ( for b ) . upon simulating this , b being rational would choose to defect ( resulting in outcome 3 ) . ( d , d ) is a nash equilibrium , and as such once they reach there , being rational neither a nor b would change their strategy . ( note that this outlaws the ! predict(a ) strategy . if a predisposes themselves to cooperation , then the two possible outcomes are ranked 2 and 4 in their preferences ( for a ) and 1 and 2 in their preferences ( for b ) . upon simulating this , if b chooses to defect , then b is adopting a defect invariant strategy ( which has been outlawed ) , and a will update and choose to defect , resulting in outcome 3 . as b is rational , b will choose the outcome that leads to outcome 2 , and b will decide to cooperate . if b chooses to defect , and a simulates b choosing to defect if a predisposes themselves to cooperation , then a will update and defect , resulting in ( d , d ) . if b chooses to cooperate if a predisposes themselves to cooperation , if a updates and chooses to defect , then b would update and choose to defect resulting in d , d . thus , once they reach ( c , c ) they are at a reflective equilibrium ( in the sense that if one defects , then the other would also defect , thus no one of them can increase their payoff by changing strategy ) . ( thus , b will adopt a predict(a ) strategy ) . vice versa . because a is rational , and predisposing to cooperation dominates predisposing to defection ( the outcomes outlawed are assumed not to manifest ) , if a predisposes themself , then a will predispose themself to cooperation . vice versa . thus if one agent predisposes theirself , it will be to cooperation , and the resulting outcome would be ( c , c ) which is ranked second in their preferences . what if a and b both predispose themselves ? we can have : 1 . c & amp ; c 2 . c & amp ; d 3 . d & amp ; c 4 . d & amp ; d. if c & amp ; c occurs , the duo will naturally cooperate resulting in ( c , c ) . remember that we showed above that the strategy adopted is predict(b ) ( defecting from ( c , c ) results in d , d ) . if c & amp ; d occurs , then a being rational will update on b 's predisposition to defection and choose defect resulting in ( d , d ) . if d & amp ; c occurs , then b being rational will update on a 's predisposition to defection and choose defect resulting in ( d , d ) . if d & amp ; d occurs , the duo will naturally defect resulting in ( d , d ) . thus seeing as only predisposition to cooperation yields the best result , at least one of the duo will predispose to cooperation ( and the other will either predispose themselves to cooperation or not predispose at all ) the resulting outcome is ( c , c ) . if the two agents can predict each other with sufficient fidelity ( explicit simulation is not necessary , only high fidelity predictions are ) and are rational , and know of those two facts , then when they engage in the prisoner 's dilemma , the outcome is ( c , c ) . therefore , a cooperate - cooperate equilibrum can be achieved in a single instance prisoner 's dilemma involving two rational agents given that they can predict each other with sufficient fidelity , and know of their rationality and intelligence . q.e.d thus if two super intelligences faced off against each other in the prisoner 's dilemma , they would reach a cooperate - cooperate equilibrium . this solution also applies to two rational robots who have mutual access to the other 's source code . & nbsp ; & nbsp ; prisoner 's dilemma with human players in the above section i outlined a strategy to resolve the prisoner 's dilemma for two superintelligent ais or rational bots with mutual access to the other 's source code . the strategy is also applicable to humans who know each other well enough to simulate how the other would act in a given scenario . in this section i try to devise a strategy applicable to human players . consider two perfectly rational human agents a and b. a and b are maximally selfish and care only about maximising their payoff . let : ( d , c ) = w ( c , c ) = x ( d , d ) = y ( c , d ) = z the preference is w > x > y > z. a and b have the same preference . the 3 conditions necessary for the resolution of the prisoner 's dilemma in the case of human players are : 1 . a and b are perfectly rational . 2 . they each know the other 's preference . 3 . they are aware of the above two facts . the resolution of the problem in the case of superintelligent ais relied on their ability to simulate ( generate high fidelity predictions of ) each other . if the above 3 conditions are met , then a and b can both predict the other with high fidelity . consider the problem from a 's point of view . b is as rational as a , and a knows b 's preference . thus , to simulate b , a merely needs to simulate themselves with b 's preferences . since a and b are perfectly rational , whatever conclusion a with b 's preferences ( a * ) reaches is the same conclusion b reaches . thus a * is a high fidelity prediction of b. vice versa . a engages in a prisoner 's dilemma with a*. however , as a * as the same preferences as a , a is basically engaging in a prisoner 's dilemma with a. vice versa . & nbsp ; an invariant strategy is outlawed by the same logic as in the ai section . & nbsp ; a = f(predict(a * ) = g(a * = f(predict(a ) = g(a ) ) ) vice versa . the above assignment is self referential , and if it was run as a simulation , there would be an infinite recursion . diagram to illustrate thus , either a or a * needs to predispose themselves . if the predisposition a makes is q , then the assignment becomes : a = f(predict(a * ) = g(a * = f(predict(a ) = g(q ) ) ) diagram to illustrate however , as a * is a , then whatever predisposition a makes is the same predisposition a * makes . both a and a * would predispose themselves . it is necessary for at least one of them to predispose themselves , and the strategy that has the highest probability of ensuring that at least one of them predisposes themselves is each of them individually deciding to predispose themselves . thus , we enter a situation in which both of them predispose themselves . as a * = a , a 's predisposition would be the same as a * 's predisposition . vice versa . we have either : 1 . ( c , c ) 2 . ( d , d ) if a predisposes themselves to defection , then we have ( d , d ) . ( d , d ) is a nash equilibrium as a and/or a * can only perform worse by unilaterally changing strategy at ( d , d ) . as a and a * are rational , they would predispose themselves to cooperation . if a and a * predispose themselves to cooperation , if a and/or a * tried to maximise their payoff by defecting , then the other ( as they predict each other ) would also defect to maximise their payoff . defecting at ( c , c ) leads to ( d , d ) . thus , neither a nor a * would decide to defect at ( c , c ) . ( c , c ) forms a reflective equilibrium . vice versa . as b 's reasoning process closely reflects a * 's ( both being perfect rationalists , and having the same preferences ) , the two agents would naturally converge at ( c , c ) . & nbsp ; q.e.d i think i 'll call this process of basing your decision in multi - agent decision problems ( that involve at least two agents who are perfectly rational , know each other 's preferences and are aware of those two facts from the perspective of one of those agents satisfying the above 3 criteria ) by modelling the other agents ( who satisfy those 3 criteria ) as simulations of yourself with their preferences recursive decision theory ( rdt ) . i think convergence on rdt is natural for any two sufficiently ( they may not need to be perfect , if they are equally rational , and rational enough that they try to predict how the other agent(s ) would act ) rational agents who know each other 's preferences and are aware of the above two facts . if a single one of those criteria is missing , then rdt is not applicable . if for example , the two agents are not equally rational , then the more rational agent would choose to defect as it strongly dominates cooperation . or they did not know the other 's preferences , then they would be unable to predict the other 's actions by inserting themselves in the place of the other . or if they were not aware of the two facts , then they 'd both reach the choice to defect , and we would be once again stuck at a ( d , d ) equilibrium . i 'll formalise rdt after learning more about decision theory and game theory ( so probably sometime this year ( p & lt ; 0.2 ) , or next year ( p : 0.2 & lt;= p & lt;= 0.8 ) if my priorities do n't change ) . i 'm curious what rdt means for social choice theory though .",8362,8362,2017-08-31T00:07:11.313,2018-08-08T16:04:07.840,is there any flaw to this solution to the one shot prisoner 's dilemma,decision-theory game-theory,1,5,2
859,3931,1,4471,2017-08-30T18:09:28.417,5,229,"it is possible that the signal handling of a neuron is outside the engineering comprehension of the most astute of human brains , even after the relationships of inputs to outputs are statistically characterized and the mapping of genetic information to neuron structure and composition is complete . deep comprehension of the detailed function of neurons within their role in adaptive system behavior seems to be lacking , and no rational proof of the feasibility of comprehending neurons has yet been published . a visual representation of neuron functionality might look like the visual representation of a genome , a blur of information , when viewed in its entirety . the economy of the world is much more complex system , astronomically more daunting . even the map of the metabolic processes of earthen life ( shown below ) defies comprehensive display in a single view . comprehending these complex biological systems can only be done piecemeal . studying these systems in their entirety may take a single mind years ( or even millennia ) , and the distortions of memory over time may prohibit placing the whole system within the finite capacity of the cerebral cortex . yet computer science has no doubt advanced . many features of the mammalian brain have already been simulated , and those simulations exceed the capacities and speeds of mammals . examples include mail sorting , flight planning , and game playing . with such progress , the simulation of the less obviously mechanical functions of the human mind appeared to many to be within the grasp of scientific advancement . other ai theorists have expressed their doubts . the question of whether a system of a few billion elements can be simulated without first simulating a generalization of its elements arose in the twentieth century . the spiking neuron model is an example of research inspired by that question . is the most elemental unit of biological computation , the neuron 1 , comprehensible by a brain of comprised of neurons ? if the answer is no , which is quite possible , then new approaches to ai may be required to simulate the capabilities of the mind that are less obviously computational in nature & ndash ; capabilities like intuition , inspiration , compassion , jazz composition , or revolutionary thought . [ 1 ] https://www.ncbi.nlm.nih.gov/pubmedhealth/pmht0024269/",4302,4302,2018-07-19T10:02:43.203,2018-07-19T10:02:43.203,is the neuron adequately comprehended ?,philosophy research artificial-neuron cognitive-science neurons,1,7,3
860,3932,1,,2017-08-30T18:57:42.780,2,75,"due to my ignorance in this space i am not sure if a similar approach has already been proposed for what i outline below ( i am not sure what terminology to even search for ) : i have been doing some brainstorming about architectures for ai systems , and was considering the feasibility of an approach that borrows heavily from real - time game loop simulations . that is to say , an approach that runs the ai simulation itself in real - time at a fixed rate regardless of external events , as opposed to feeding inputs in ( near ) real time into an existing network ( i.e. clocked by events ) and reading the outputs . this seems to me to be one way to allow for a temporal awareness and even some form of primitive "" consciousness "" within the ai system . in pseudocode , here is what i 've been thinking for a real - time ai loop : network = loadnetwork ( ) ; registeredinputs = network.getallregisteredinputs ( ) ; registeredoutputs = network.getallregisteredoutputs ( ) ; timesteptick = 0 ; while(alive ) { //iterate over inputs and process - could use event - driven approach here , but would potentially require locking on the network if concurrency is applied foreach(registeredinput in registeredinputs ) { //if the input has new samples since last iteration , apply it to mapped neurons if(registeredinput.hasnewsamples ( ) ) { //get new samples of input ( these samples live in time domain ) samples = registeredinput.getnewsamples ( ) ; foreach(sample in samples ) { network.processinputsample(timesteptick , registeredinput , sample ) ; } } } //iterate over outputs and process foreach(registeredoutput in registeredoutputs ) { //map relevant neuron outputs to the registered output "" frame "" buffer registeredoutput.update ( ) ; //whatever registered this output is responsible for reading the buffer at whatever rate is required } //whichever simulation time step algorithm - arbitrarily clock this network at 1khz step(timesteptick , 1 ) ; } using this sort of approach , i speculated about the possibility of registering certain internal inputs in addition to external inputs : "" brain wave "" signals : could feed a series of generated sinusoidal waveforms into the network as a kind of baseline stimulation . time of day signal : simulation of things like circadian rythym . could simply be a waveform with frequency of 1 day . time step signal : feed current time step value ( monotonic , starts at 0 ) as a sense of age . feedback signals : certain output neurons selected to be mapped back in as inputs . these would create a continuous loop of signaling between neurons which would continue even in the absence of any external input signals . the inputs and outputs would obviously need to be sampled at a rate that falls within the nyquist rate of the network ( would be 512hz for the above psuedocode ) . for video data , this would be pretty easy as most video content is clocked within & lt;100hz ( tradeoff being large frame data ) . audio data might need to be resampled as it usually runs around 48khz , but that is nt to say the simulation could nt be clocked at something like 100khz to address both needs . the audio samples would be ingested at their native sample rate , and video frames would only be handled if a new frame arrives . the continuous load of such a system on the underlying hardware should be very easy to calculate based on number of mapped inputs , outputs , and sample rate . it might even be feasible to let the network run as fast as hardware allows ( above some minimum constraint for input / output sampling rate ) , which could make it "" better "" in some arbitrary sense . i have no idea what sort of neuron algorithm would be most appropriate here ( i suspect existing algorithms will not work well ) , but i do think that it would have to be very efficient from a computational perspective considering the potential tick rate of the simulation . all of the magic definitely lives in the processinputsample ( ) method in the code above . obviously , the current applications for such a network are very dubious due to the way it operates . this in my mind simply feels like a very complex dsp filter that just takes input waveforms ( either external or internal ) convolves them with various amounts of temporal phasing , and pipes to a series of output buffers that the real world interacts with . that being said , this also seems like the sort of "" flexibility "" that is required for an ai to learn and operate in a way that does not require some arbitrary number of iterations and a specific training model . it has always seemed to me that ltsm - style neurons were a bandaid for the temporal memory issue . why push that computational concern to every single neuron when you can make it an inherent property of the ai by placing its entire simulation in the time domain ?",9347,,,2017-08-30T18:57:42.780,continuous real - time ai simulation loop,ai-design real-time,0,4,1
861,3933,1,3940,2017-08-31T05:32:04.277,4,85,"scenario : i am trying to create a dataset with images of choice for different animal classes . i am going to train those images for classification using cnn . problem : lets assume i somehow do n't have the privilege to collect too many images and was only able to collect few of them for each class . here 's the list : - baboon : 800 fox : 1000 hyena : 5000 giraffe : 43 zebra : 88 6 : hippopotamus : 233 7 : yak : 578 8 : polar bear : 456 9 : lion : 3442 10 : indian tiger : 40,000 questions are : - is this a good dataset to train the cnn model . ? i am worried about the quantity each class have . will it be helpful if i augmenting the data ? i think am going to . in future the above mentioned dataset is going to increase . so there is a chance that i will train the model again . should i create a model that fits the data of the present size or should i create a bigger one inorder to adjust future data ? thank you for your time . i can get data from internet . but this question is about the approaches to take when we are bounded by less data like the one in national data science bowl ( classifying planktons ) .",6382,,,2018-05-30T08:49:02.057,what defines a good dataset in deep learning approach ?,neural-networks machine-learning ai-design convolutional-neural-networks training,2,0,
862,3935,1,3937,2017-08-31T08:43:01.663,6,87,"information security has become a thriving field during the last years . it is a broad domain ranging from planing and building over testing to operating different applications , systems and networks in a secure fashion . from small embedded systems to large scale enterprise applications , security is always an important challenge . with some exceptions , i have hardly seen neural networks used in this domain to tackle those challenges . the most common exception is breaking captchas with cnns during penetration testing , analysis of network traffic and malware / spam detection . what are other challenges in information security that could be solved better with neural networks than with classical algorithms ? please include an explanation why those challenges would benefit from neural networks .",9161,1581,2017-09-01T22:28:06.030,2018-10-15T23:54:24.807,which challenges in information security can be solved better using neural networks ?,neural-networks security challenges detecting-patterns,1,0,2
863,3938,1,4191,2017-08-31T11:04:35.500,13,17285,suppose there are 10k images of sizes 2400 x 2400 are required to use in cnn.acc to my view conventional computers the people use will be of use . now the question is how to handle such large image sizes where there is no privileges of downsampling . here 's the system requirements:- ubuntu 16.04 64-bit ram 16 gb gpu 8 gb hdd 500 gb 1 ) are there any techniques to handle such large images which are to be trained ? 2 ) what batch size is reasonable to use ? 3 ) is there any precautions to take or any increase and decrease in hardware resources that i can do ?,6382,17221,2018-08-07T18:16:48.623,2018-08-07T18:16:48.623,how to handle images of large sizes in cnn ?,neural-networks deep-learning image-recognition convolutional-neural-networks,3,0,8
864,3942,1,3957,2017-08-31T16:07:41.383,4,379,"we are careening into the future which may hold unpredictable dangers in relation to ai . i 've have n't yet heard of chappie or robocop style police robots , but militarized drone tech is replacing many conventional weapons platforms . i love the idea that i may one day be able to transfer my consciousness to a computer , and improve my capabilities and potential . but few humans are purely "" good "" , what constitutes morality can differ greatly among individual humans . how do we move forward toward the singularity in a way that protects humans , as opposed to possibly lead to our extinction ?",7800,7800,2017-09-04T20:25:32.840,2019-03-22T11:03:41.840,is there a way to protect humanity against the impending singularity ?,strong-ai agi control-problem singularity neo-luddism,3,6,3
865,3943,1,4455,2017-08-31T17:16:28.970,10,194,"i 'm am quite new to deep learning but i think i found just the right real - world situation to start using it . the problem is that i have only used such algorithms to predict outcomes . for my new project , i need information to feed a machine with to optimize outcomes . could someone explain briefly how i should proceed ? i 'm stuck . here 's the situation : i have a machine that takes planks of wood with different grades of wood available throughout its length and has to cut it into blocks provided in a cut list . this machine will always choose the highest score it can get from a given plank . the score is obtained by multiplying each block 's area by its multiplicator . the algorithm i want to build has to give that machine a multiplicator for each block listed in a cut list . all of the physical output from this machine will be stocked on shelves by a robot until needed . the cutting machine is allowed to downgrade parts of a plank if it helps it reach a higher score . the value has to act as an incentive for the machine to give me the block i need the most without downgrading too much wood . optimization goals make sure each block is in stock by the time it is needed , but not too early without reason downgrade as little area of wood as possible ( some species are very expensive ) input nodes amount of time before this block is needed grade of wood for this block amount of this block needed block 's area ( maybe ? ) feedback provided to the algorithm amount of time in advance that the block was ready ( must be as low as possible ) area of wood downgraded * number of grades skipped expected return data a multiplicator that will give that block an optimal its priority relative to others information i do n't have but could gather mean ratio of each grade for each species of wood what i 've figured out so far is that i may need my feedback to be smashed in only one value in order to make it the output node . the problem is that i ca n't understand how to make this algorithm to determine a multiplicator . am i wrong in trying to solve this through deep learning ?",9361,9361,2017-09-05T14:39:22.287,2017-12-07T13:18:39.470,a deep learning algorithm to optimize the outcome,deep-learning,1,12,1
866,3947,1,,2017-09-01T11:12:40.147,2,111,"i have an idea for a transformative decision rule ( tdr ) , but i want to know if there 's already a name for it . any ordinal scale should be converted to a scale such that all the outcomes are assigned consecutive values . let v be the function that assigns values to outcomes ( defined as ordered pairs of acts and states ) . let x and y denote any particular such ordered pairs . let p be the preference . let x+ be the set of all outcomes that are more preferred than x ( similarly also for y ) x+ in p . & nbsp ; if x in y+ , and x is the closest outcome to y in y+ , then v(x ) = v(y ) + 1 for all x , y in p. i could have explained it more formally mathematically , but latex does n't seem to render here . if there 's no name for it , i 'm thinking of "" the law of consecutive ordinals ( lco ) "" . as for why i support the rule , it 's because it 's a rule that needs to be applied to an ordinal scale before i apply an effective decision rule ( edr ) that i thought of for ordinal scales . i want to know if there 's a name for that principle .",8362,,,2018-06-12T12:24:45.710,is there a name for this principle ?,decision-theory,1,0,
867,3948,1,,2017-09-01T13:48:47.733,2,116,"i 've been working on a project ( android game ) , in which the player has to confront with some obstacles / enemies whom he has to destroy . so , is there a way in which we can monitor how the user of the game plays and accordingly to generate ( and timely updating ) that trained model , which can be used later in the game to make the user think of a different way to defeat an enemy unlike going in the same streamline flow which he has followed till now . i 've implemented q - learning and genetic algorithms on the pc , to make the game ai for some games like ' tetris ' to make the computer play on it 's own . but , have n't done it in android till now . already searched : i 've also referred to some websites in which they suggested using neural networks to encounter the same . but , i 'm unable to get an accurate procedure in which this can be done . please suggest me a way in which i can monitor the user 's input on how he plays . thank you .",8968,,,2018-05-20T17:27:13.020,"is there any way , in which we can monitor what 's going on the screen in android to generate a model from it ?",machine-learning game-ai,2,5,1
868,3949,1,3967,2017-09-01T14:44:09.853,4,2155,"my question is more about "" is it possible ? "" and "" is this the right approach ? "" so let me explain what my idea is : i think about a system which gets xml documents in various structures but with essentially the same data structure in it . for the example , lets assume each document contains data about one or more persons . so the ai would recognize a name . somewhere else in the document there is the post address of our fictional person . the ai should now "" see "" the address and conclude , it belongs to our person . anywhere else , there is a phone number in the document . again , our ai should see the connection between our person and this phone number . this would n't be a job for an ai if there was n't a catch . if the task was merely to find and map strings like addresses and phone numbers , we could simply use a regex to match our "" target strings "" . the catch in this scenario would be this : the xml document might contain other data , which does not belong to our person but is a valid phone number for example and thus will match an regex . so the big question is : would it be possible for an ai to learn this and if yes , with which framework would someone create such an ai ? sample xml document : & lt;?xml version=""1.0 "" encoding=""utf-8 "" ? & gt ; & lt;document&gt ; & lt;data&gt ; & lt;foo&gt ; & lt;bar&gt ; & lt;person&gt ; & lt;name&gt;john doe&lt;/name&gt ; & lt;/person&gt ; & lt;/bar&gt ; & lt;address&gt ; & lt;street&gt;main street 1&lt;/street&gt ; & lt;city&gt;1111 twilight town&lt;/city&gt ; & lt;country&gt;sample country&lt;/country&gt ; & lt;/address&gt ; & lt;phone&gt;+123 123 123&lt;/phone&gt ; & lt;/foo&gt ; & lt;foo&gt ; & lt;bar&gt ; & lt;person&gt ; & lt;name&gt;jane doe&lt;/name&gt ; & lt;/person&gt ; & lt;/bar&gt ; & lt;address&gt ; & lt;street&gt;broadway 42&lt;/street&gt ; & lt;city&gt;4521 traverse town&lt;/city&gt ; & lt;country&gt;sample country&lt;/country&gt ; & lt;/address&gt ; & lt;phone&gt;+123 412123&lt;/phone&gt ; & lt;/foo&gt ; & lt;/data&gt ; & lt;creator&gt ; & lt;!-- note : while this looks like a valid person , --&gt ; & lt;!-- this data should not be matched by the ai --&gt ; & lt;name&gt;sam smith&lt;/name&gt ; & lt;office&gt ; & lt;street&gt;seaside road 5&lt;/street&gt ; & lt;city&gt;4521 traverse town&lt;/city&gt ; & lt;country&gt;sample country&lt;/country&gt ; & lt;/office&gt ; & lt;phone&gt;+123 555 555&lt;/phone&gt ; & lt;/creator&gt ; & lt;/document&gt ;",9380,9380,2017-09-04T06:37:26.803,2019-02-07T12:09:05.353,use ai to interpret xml ?,machine-learning ai-design structured-data,2,9,1
869,3950,1,,2017-09-01T16:31:48.283,-1,131,i would like to get into ai development in python but i do n't know how . i really hope that i could code artificial intelligent by my own someday . anybody got any suggestions ?,9381,,,2017-09-01T18:24:57.053,how to get started in ai ?,algorithm ai-community,2,4,1
870,3953,1,3958,2017-09-01T22:34:05.867,3,449,"i 'm currently in the process of learning about using cnns in image recognition . many of the different resources i read that were explaining the motivation referred to the fact that these networks are ( to some degree ) translationally invariant . my understanding is as follows : fully connected networks are ill - suited to image recognition because of the high dimensionality of the data and especially because they do not preserve the spatial relationships between pixels . i care about the state of the pixels surrounding pixel x as well as x itself . cnns remedy this because they look at the image in 2 dimensions so that surrounding pixels are being considered as well ( the kernel does this ) . however , while i care about what is around x , i do n't care where x is . so , i apply the kernel the same everywhere and i make a bunch of layers so that i can get a bunch of kernels . i also use pooling . this reduces the data dimensions but also adds some of that translation invariance . so , my question is : what feature of the cnn is causing the invariance ? i saw some explanations saying it was a result of the maps basically activating when a certain feature shows up , regardless of where . other said that the pooling meant that if a horizontal line , for instance , showed up in one place vs. a pixel over , the pooling would activate the same for both . it seems like the first reason would be totally sufficient and the second not really adequate but i could also see it being both . i also read a paper ( https://arxiv.org/pdf/1606.09549.pdf ) about fully - convolutional networks . in section 2.1 , the authors explain that they intentionally chose to make it fully - convolutional because that allowed them to "" compute the similarity at all translated sub - windows on a dense grid in a single evaluation . "" that sentence makes me think that it is more the first explanation . anyways , i hope to gain a better intuitive understanding of how the different parts of the cnn come together to work particularly well on images . thanks !",8829,,,2017-09-02T12:31:02.923,intuitively understanding translational invariance in cnns,image-recognition convolutional-neural-networks,1,1,
871,3955,1,,2017-09-02T08:35:24.067,3,113,"for example , there is a list of function names if we choose "" index "" and "" sort "" and "" probability "" index(sort(probability(data ) ) ) this form an simple algorithm is there any python library for knowledge representation and planning for algorithm creation ?",9303,21645,2019-02-01T13:33:56.587,2019-03-03T15:01:20.497,how to use knowledge representation and planning to create or edit an algorithm ?,neural-networks machine-learning algorithm knowledge-representation,2,0,1
872,3956,1,4268,2017-09-02T09:04:49.463,4,1050,"there are two potential approaches when performing cross - over operation in genetic algorithm : perform cross - over on elites in the pool , probably the ones that are also going to be directly transfered to the next generation all the population present in the pool . is there any certain belief that cross - over on only elites of the population , converges the solutions faster ? i guess in order to escape from local minima , cross - over on all the population is needed ; on the other hand i also say why performing cross over on weak population ? any idea ?",6258,,,2017-10-14T10:53:13.113,genetic algorithm : is elitism prefered in cross - over operator ?,genetic-algorithms,1,1,
873,3959,1,,2017-09-02T13:29:09.183,2,2746,is there a way to train ai to find aspecific line or symbol in a image and crop it ? opencv scripts finds a face and crops it : how can i add my annotations ? lest say i have a image like this : + ------+ | * * | | | | * * | | | + ------+ i want it to find the * and crop it .,9393,18819,2018-11-18T12:35:53.643,2018-11-18T13:06:11.213,cropping image using ml ?,machine-learning algorithm image-recognition,2,1,3
874,3962,1,3996,2017-09-03T12:22:57.440,6,428,"i 'm working on implementation of the backpropagation algorithm for a simple neural network which predicts a probability of survival ( 1 or 0 ) and i ca n't get it above 80 % no matter how much i try to set the right hyperparameters . i suspect that 's because my backpropagation is implemented incorrectly since i tried 2 different types of code and both give me same results . is my backpropagation implemented correctly ? also how can i improve my model to give a better prediction ? class neuralnetwork(object ) : def _ _ init__(self , input_nodes , hidden_nodes , output_nodes , learning_rate ) : # set number of nodes in input , hidden and output layers . self.input_nodes = input_nodes self.hidden_nodes = hidden_nodes self.output_nodes = output_nodes self.lr = learning_rate # initialize weights self.input_hidden_weights = np.random.randn(hidden_nodes , input_nodes ) # 10x7 self.hidden_output_weights = np.random.randn(output_nodes , hidden_nodes ) # 1x10 # sigmoid activation funciton self.sigmoid = lambda x : 1/(1+np.exp(-x ) ) self.diff_sigm = lambda x : x*(1-x ) def train(self , input_list , label_list ) : # create an array of inputs and labels inputs = np.array(input_list , ndmin=2).t # 7x1 labels = np.array(label_list , ndmin=2 ) # 1x1 # forward propagation hidden_layer = self.sigmoid(np.dot(self.input_hidden_weights , inputs ) ) output_layer = self.sigmoid(np.dot(self.hidden_output_weights , hidden_layer ) ) final_output = output_layer # error function output_errors = labels - final_output # backpropagation output_delta = output_errors * self.diff_sigm(output_layer ) hidden_delta = np.dot(self.hidden_output_weights.t , output_delta ) * self.diff_sigm(hidden_layer ) # update the weights self.hidden_output_weights + = np.dot(output_delta , hidden_layer.t ) * self.lr self.input_hidden_weights + = np.dot(hidden_delta , inputs.t ) * self.lr "" "" "" # backpropagation hidden_errors = np.dot(self.hidden_output_weights.t , output_errors ) hidden_grad = hidden_layer * ( 1.0 - hidden_layer ) # update the weights self.hidden_output_weights + = self.lr * np.dot(output_errors.t , output_layer.t ) # update hidden - to - output weights with gradient descent step self.input_hidden_weights + = self.lr * np.dot(hidden_errors * hidden_grad , inputs.t ) # update input - to - hidden weights with gradient descent step "" "" """,9406,,,2017-10-07T23:40:18.953,how do i know if my backpropagation is implemented correctly ?,neural-networks algorithm backpropagation,3,2,
875,3964,1,,2017-09-03T20:13:52.853,5,224,i understand that a neural network basically distorts(non - linear transformation ) and changes the perspective(linear transformations ) of input space to draw a plane to classify data . how does the network deduce if an input is one side of a plane and therefore output the decision ? thanks in advance .,9271,,,2018-02-03T07:30:58.547,how does neural network classifier classify from just drawing a decision plane ?,neural-networks classification linear-algebra,3,3,
876,3965,1,,2017-09-04T07:20:24.417,10,183,"recently i was working on a problem to do some cost analysis of my expenditure for some particular resource . i usually make some manual decisions from the analysis and plan accordingly . i have a big data set in excel format and with hundreds of columns , defining the use of the resource in various time frames and types(other various detailed use ) . i also have information about my previous 4 years of data and actual resource usage and cost incurred accordingly . i was hoping to train a nn to predict my cost beforehand and plan even before i can manually do the cost analysis . but the biggest problem i 'm facing is the need to identify the features for such analysis . i was hoping there is some way to identify the features from the data set . ps - i have idea about pca and some other feature set reduction techniques , what i 'm looking at is the way to identify them in the first place .",9374,2444,2018-10-27T22:04:33.037,2019-01-30T13:03:15.707,how do i select the relevant features of the data ?,neural-networks machine-learning datasets feature-selection,4,2,1
877,3971,1,,2017-09-04T20:05:21.753,5,166,"i 'm relatively new to neural networks and was wondering what an implementation of this paper would look like . more specifically , how are the correct values of kp , ki , and kd determined at run time so it can be back propagated ?",9423,,,2018-01-20T16:33:34.450,what would an implementation of this neural network look like ?,neural-networks backpropagation,1,1,2
878,3972,1,7862,2017-09-05T05:20:56.677,5,63,"when designing a machine - learning system , there are various parameters that have to be determined . i am interested in the following general question : is it possible to construct a dataset on which the system will have good performance with some specific set of parameters , but not with other paramteres ? to be more concrete , let 's focus on neural networks . suppose we have a simple neural network : a multilayer perceptron with a single hidden layer . the size of the input is fixed , the activation function is fixed ( e.g. tanh ) , and the output is binary . the only parameter that has to be determined is the size of the hidden layer . my question is : given a number $ n$ , is it possible to construct a dataset $ d_n$ such that : the mlp with $ n$ hidden nodes has good performance on $ d_n$ ( e.g. in 10-fold cross validation ) ; the mlp with $ n-1 $ hidden nodes has bad performance on $ d_n$ ? note : i asked in cs theory but got no reply .",8684,9203,2018-08-07T18:14:41.560,2018-09-06T19:57:45.970,constructing a dataset that scores well only for a specific set of hyper parameter values,neural-networks machine-learning,2,0,
879,3974,1,3978,2017-09-05T21:29:54.263,3,95,"to understand the inner workings of neural networks , a fair amount of mathematical concepts is required . backpropagation alone is a challenging technique if you are not fluent in calculating local gradients . and that 's just the start of the journey . but the more i study neural networks , the more i get the impression that all those difficult mathematical concepts are only required if you are doing actual research in neural networks or want to know what 's happening under the hood . if you "" just "" want to implement an ai utilizing a neural networks , there are several high level programming frameworks and libraries readily available including model zoos for state of the art neural networks ( e.g. vgg , googlenet and resnet ) , that can be used . so my question is , does a developer require a deep understanding of all the details nowadays , or have we reached a level , where frameworks take care of those details for us ?",9161,,,2017-09-06T07:09:39.563,is a deep technical understanding of neural networks required outside of research ?,neural-networks models,2,0,1
880,3975,1,3991,2017-09-05T21:36:48.350,3,158,"captchas , which are often seen in web applications , are working under the assumption , that they pose a challenge which a human can solve easily while a machine will most likely fail . prominent examples are identifying distorted letters or categorizing certain objects in images . neural networks are threatening this approach , as they are capable of solving problems that are easy for humans and difficult for classic algorithms . especially with the incredible results modern cnn architectures have achieved in image recognition during the last years , the established forms of captchas wo n't be able to distinguish a human and a machine using a neural network anymore . is this the end of captchas as we know them or are there evolved versions available or at least in the making that still pose a challenge to modern neural networks ? clarification : i am talking about challenges that are feasible for use in web applications and do not have an unjustifiable impact on usability .",9161,,,2017-09-07T20:21:39.873,will cnns kill captchas or can they survive in an evolved form ?,convolutional-neural-networks,2,0,1
881,3981,1,4053,2017-09-06T09:57:10.807,18,6357,"i would like to train a neural network where the output classes are not ( all ) defined from the start . more and more classes will be introduced later based on incoming data . this means that , every time i introduce a new class , i would need to retrain the nn . how can i train a nn incrementally , that is , without forgetting the previously acquired information during the previous training phases ?",2904,2444,2019-05-02T14:58:39.340,2019-05-02T15:50:34.873,is it possible to train a neural network incrementally ?,neural-networks machine-learning incremental-learning transfer-learning,2,0,11
882,3982,1,,2017-09-06T10:01:55.633,4,119,"i am working on a project in which a drone is up to learn to fly . i am using neat . for the first experiment i want it to learn how to hover inside a 3x3x3 meters box . my input is 6 sensors for each direction . output is same as in a drone so thrust ( normalized to 0 - 1 ) , aileron , rudder and elevator . initially just used a time as fitness , and after many generations it hovers inside the box . it only really learns to use the thrust in function of up and down sensors , but it never learns to react to input from other sensors because they are not directly connected to fitness . i would like to get some ideas about a good test for my problem . should the drone be put to fly a track with obstacles ? should i have some more input data ? should i define fitness to better reflect good reactions for input sensor data ? thanks in advance note : the drone uses relatively accurate physics , i am able to do tasks using a controller .",9450,1671,2018-03-05T19:06:51.283,2018-05-05T23:43:33.107,problem with fitness calculation in neat,neural-networks neat,1,3,1
883,3986,1,4033,2017-09-07T04:46:06.463,1,97,"the sequence in which new technologies are developed can be crucial for it 's success and it 's safe use . for example , before developing a nuclear reactor you want to have technologies like reliable cooling systems and secure ultimate disposal zones available and tested . otherwise we might have a bad time . i see ai with general superhuman intelligence as important predecessor for dangerous technologies like nanobots or generators using nuclear fusion , because superhuman intelligence can be helpful in foreseeing dangers that might lead to catastrophe using these technologies in an insecure way . but what about important technologies we want to have before inventing an ai with general superhuman intelligence ? solving the control - problem is obvious , but are there other technologies we should have in advance ?",9161,1671,2017-09-12T18:41:29.233,2017-09-15T01:24:40.493,which technologies should humanity develop before developing an ai with general superhuman intelligence ?,strong-ai control-problem superintelligence,1,5,1
884,3987,1,,2017-09-07T05:34:33.717,3,221,how can i add local database for voice recognition for visual studio c#. i want to get the commands from the database and also the function for send - keys i want to get it from the database how will i do that ?,9470,1581,2017-09-11T18:05:34.940,2017-09-11T18:05:34.940,how to put commands into local database in c # visual studio for voice recognition,machine-learning ai-community,1,0,1
885,3989,1,,2017-09-07T12:29:38.073,5,227,i was wondering if anyone can suggest a good framework for reasoning with incomplete information . i have found large knowledge collider but it appears dead for some time . do you possibly have any other suggestions for a maintained project worth checking ? since many comments are gravitating towards a different direction let me add one approach that i found a potentially good answer to my question - rough set based decision trees . i would hope there is more than only this approach ... could you please help me identify them ?,8788,8788,2018-08-17T14:59:05.633,2018-08-17T14:59:05.633,reasoning with incomplete information,reasoning incomplete-information,1,3,1
886,3992,1,5223,2017-09-08T11:34:31.843,4,658,"i was just wondering if some one could provide a nice tutorial on how to use the recent tensor - flow object detection api to train custom network say like vgg-16 ? ( just use the vgg-16 , vgg-19 , inception - v3 etc as a fixed feature extractor in the faster rcnn implementation ) . from their documentation i know it can be done . as , am an amateur in ai , i am unsure how to move forward . p.s : this could really help some keras fans ( like me :p ) who have a trained cnn in keras . they could just make a .pb file out of it and insert it as a fixed feature detector / extractor in the tensorflow object detection api . voila ! ! ! now keras is integrated with the tensorflow 's object detection api",8418,,,2018-03-22T09:59:47.780,training custom object detection network using tensor - flow object detection api ?,convolutional-neural-networks tensorflow keras,1,0,
887,3993,1,4578,2017-09-08T13:21:44.777,0,101,"where can i find training datasets like the ones provided for linguistic training , but to train a program to program itself . i want to input this training dataset to a programming script and it should use it to program itself . what do i need to consider in this kind of artificial intelligence ?",9303,9161,2017-09-08T16:56:59.357,2017-11-23T01:50:21.103,is there a programming training dataset for robot program itself,neural-networks,2,2,
888,3999,1,,2017-09-10T07:41:34.533,6,789,"there should be some key performance indicators designed for measuring ai performance . for example , the number of entities examples you have to feed it in order to obtain single task on a testing entity with repeatable 97 % accuracy . is there any of such measure constructed ? motivation : you can not learn ai to be good at playing a game released 1 month ago because there is not enough data on how to play . it does n't matter how clever you are , present brute force paradigm in ai just does n't fit in such circumstances .",5356,1671,2017-09-11T16:36:39.770,2017-09-11T16:36:39.770,ai efficiency kpi,machine-learning deep-learning,1,1,1
889,4000,1,,2017-09-10T13:09:04.427,10,1859,"stochastic hill climbing generally performs worse than steepest hill climbing , but what are the cases in which the former performs better ?",9525,2444,2019-03-02T10:54:01.397,2019-03-02T10:54:01.397,when to choose stochastic hill climbing over steepest hill climbing ?,search hill-climbing,4,0,2
890,4002,1,,2017-09-10T15:10:00.777,1,142,"i have replicated the nn that can play breakout , but i do n't know how use the pre - trainded checkpoints from dqn - tensorflow devsisters ( github ) : https://github.com/devsisters/dqn-tensorflow/issues/39 anyone can help ?",8546,21109,2019-02-01T13:34:50.887,2019-02-01T13:34:50.887,load a pretrained dqn model with tensorflow,deep-learning tensorflow,0,1,
891,4005,1,,2017-09-11T09:39:14.347,5,112,"we have several hundred employees processing word documents , mainly applying some defined rules on how the final document should look like . some of those rules are : title formating , figure positions , text style among others . i would like to optimise this process and replace the human by ai . the human will move from processing to quality control ( qc ) so we will have a much higher quality . my question is : how to approach this problem and what would you recommend as initial start point ? background : i am new to ai , although i 've been following this topic closely for about a year , and want to move forward with implementation . i have experience with vba , php , c , c++ , and some other languages .",9541,9541,2018-08-24T10:28:45.353,2019-04-21T14:03:00.593,ms word - automated formatting of text documents treatement with artificial intelligence,getting-started,1,4,2
892,4008,1,,2017-09-11T16:38:31.387,1,47,please i am interested in building a news recommendation app that will be powered by artificial intelligence . i have being making a whole lot of research on this and will be glad if somebody can tell what is needed to build such app in android . it is something similar to this app,9278,,,2017-09-11T16:38:31.387,build android news recommendation app,machine-learning ai-design,0,3,1
893,4009,1,,2017-09-11T20:31:33.480,3,94,imagine that we have a black box that have 100 binary inputs and 30 binary outputs . we can generate values for inputs and get relevant set of outputs . how does one teach a neural network to predict the binary inputs ( or list of input values with probabilitys ) using the outputs ? need practice .,4884,4884,2017-10-24T21:57:49.367,2017-10-25T05:11:49.480,teaching neural network on fixed data,neural-networks machine-learning deep-learning,2,4,
894,4011,1,,2017-09-12T03:55:11.210,-1,3068,i am working on software which deblurs the motion blur created by camera movement . i 've surveyed some research papers and determined this process requires deep learning and cnn . now i 'm looking for some books that would be useful in getting a more complete picture of the process .,9560,1671,2017-09-12T18:39:17.800,2017-09-12T18:39:17.800,book recommendations on deep learning ( convolutional neural networks ),deep-learning convolutional-neural-networks,1,2,1
895,4013,1,4022,2017-09-12T17:59:38.443,5,377,"i 'm developing a game ai which tries to master racing simulations . i already trained a cnn ( alexnet ) on ingame footage of me playing the game and the pressed keys as the target . as the cnn is only making predictions on a frame - to - frame basis , and i resized the image input to 160x120 due to gpu memory limitations , it can not read the speedometer , thus seems not to have a feeling for its current velocity . i thought of different ways to fix this issue : crop the captured image down to the size of the speedometer , which displays the current speed in mph , and feed the low resolution game image , as well as the relatively high - res image ( 70x30 ) of the current speed into the neural network , which makes predictions based on the two images . as i do n't know whether alexnet can serve as an ocr as well , my second thought was to use an existing one ( like tesseract - oct / pytesser ) on the cropped image and feed its output to the fully connected layer . i already tried to implement an optical - flow algorithm , but sadly , non of the python ones seems to output good real - time results . i wonder whether i can input the current frame as well as the last one , and let alexnet figure out the movement . as the processing has to happen in real time , and the only performance reviews of pytesser i found reported a processing time of ~100ms ( never tested that ) . my question is , what method would work best . thanks ! edit : optical flow would have the advantage of the ai knowing in which direction other cars are moving as well .",9578,19059,2018-10-22T16:52:12.460,2018-10-22T16:52:12.460,game ai - fast python ocr or cropped image input,convolutional-neural-networks gaming game-ai,1,2,
896,4027,1,4971,2017-09-13T20:46:13.223,7,595,openai 's gym website redirects to the github repository . why did the openai 's gym website close ?,4,1581,2017-09-14T19:42:13.403,2018-01-11T08:24:34.537,why did the openai 's gym website close ?,ai-community,1,2,4
897,4035,1,,2017-09-15T07:23:25.243,3,754,"in my application , i have inputs and outputs that could be represented as graphs . i have a number of acceptable pairs of input and output graphs . i want to use these to train a model . i am looking for pointers where simple examples of learning methods with graphs as input are discussed . please note that the graph size is not fixed . a sample input is graph : node a : component x with parameter size = 12 node b : component y with parameter size = 30 node c : component y with parameter size = 30 a connects to b a connects to c sample output : node a : x=0 , y=0 node b : x=-21 , y=0 node c : x=21 , y=0 in this case , we expect the model to understand that input graph is symmetric and a particular way of arranging them is preferred . we want to train the model over a large set of such input - output pairs and then use it to generate output on new inputs .",9616,1581,2017-09-15T17:59:19.367,2017-11-13T17:33:42.980,machine learning with graph as input and output,neural-networks machine-learning models,2,6,4
898,4037,1,4040,2017-09-15T16:23:45.640,1,321,"suppose a thinking ai agent exists in the future with far more computational power than the human brain . also assume that they are completely free from any human interference . ( the agents do not interact with humans . ) since they are not inherently biased to survive as in the case of humans and they do not have any moral values , what are the possibilities that can arise when it get into existential crisis ? is there any literature that discuss the above issue ? alternately , is this question flawed in some fundamental way ?",2534,1671,2017-09-15T23:48:04.177,2017-10-01T05:57:29.503,how can an ai agent tackle through existential crisis ?,philosophy agi intelligent-agent superintelligence,2,2,2
899,4041,1,4746,2017-09-16T07:53:56.140,3,1460,"by optimal i mean that : if max has a winning strategy then minimax will return the strategy for max with the fewest number of moves to win . if min has a winning strategy then minimax will return the strategy for max with the most number of moves to lose . if neither has a winning strategy then minimax will return the strategy for max with the most number of moves to draw . the idea is that you want to win in the fewest number of moves possible but if you ca n't win then you want to drag out the game for as long as possible so that the opponent has more chances of making mistakes . so , how do you make minimax return the best strategy for max ?",9634,,,2017-12-14T22:30:29.347,how to make minimax optimal ?,algorithm game-theory minimax,2,1,3
900,4043,1,4081,2017-09-16T23:12:08.457,4,157,"i understand that ai researchers are trying to create ai designs that allow for desired behavior without undesirable side - effects . a classic example of an attempt is isaac asimov 's "" three laws of robotics "" . this idea seems to have been debunked due to its vague phrasing . why have ai researchers not accepted an idea like the following ( just making the laws more specific ) : what if the un voted for the country with the fairest laws and all utility functions have 2/3 of their points made up of not breaking any of those laws . if the ai has a question , it could look at court precedent just like a judge would or ask humans ( with two - thirds of its points on the line , it should be pretty cautious ) . people have been looking for loopholes in law for thousands of years and there may not be any catastrophic ones left . ( it certainly would n't be violent ) i must be missing something if this is still an open problem .",9648,1671,2017-09-18T21:42:52.530,2017-09-20T20:10:38.970,"why is ai safety so much harder than isaac asimov 's "" three laws of robotics "" ?",ai-design ai-safety neo-luddism,1,6,1
901,4045,1,,2017-09-17T11:13:35.627,4,131,"i 'm developing a game ai which tries to master racing simulation . i already trained a cnn ( alexnet ) on ingame footage of me playing the game and the pressed keys as the target . i had two main issues with this setup : extracting the current speed from the speedometer , in order to feed it to the ai . that question was already solved here . during testing , i noticed that the ai can not make small adjustments on straight roads and cuts corners a lot . i 'm pretty sure that the second issue is caused by the game handling pressed keys binary ( e.g. ' a ' pressed - > 100 % left turn ) . the ai just ca n't make precise movements . in order to solve that issue , i want to emulate a joystick , which controls the game precisely . using pygame i already managed to capture my controller inputs as training data . the controller has two axes , one for turning , the other one for throttle / breaking . both axis can have any value between -1 and 1 : axis 0 : value -1 -&gt ; 100 % left value +1 -&gt ; 100 % right axis 1 : value -1 -&gt ; full break value +1 -&gt ; full throttle my goal is to train alexnet to output analogue raw axis values , given the current speed and captured frame , and feed its predictions into the joystick emulator . i found someone on github who tried something similar and could't achieve good results even on a modified alexnet . because of this , i was wondering whether it is even possible to modify the cnn to output analogue values instead of using it as an image classifier . my question is , whether it is worth putting the effort into editing alexnet , instead of using a whole different model . i found some models online like the nvidia end - to - end self driving model , which sadly only controls the steering angle , and seems to be made for low speed casual driving . thanks !",9578,9578,2017-09-17T12:03:44.510,2017-09-17T12:03:44.510,game ai - modify image classification model for analog output,deep-learning convolutional-neural-networks gaming tensorflow game-ai,0,0,1
902,4048,1,,2017-09-17T20:12:20.860,7,6893,"i am currently writing an engine to play a card game , as there is no engine yet for this particular game . i am hoping to be able to introduce a neural net to the game afterwards , and have it learn to play the game . i 'm writing the engine in such a way that is helpful for an ai player . there are choice points , and at those points , a list of valid options is presented . random selection would be able to play the game ( albeit not well ) . i have learned a lot about neural networks ( mostly neat and hyperneat ) and even built my own implementation . i am still unsure how best build an ai that can take into account all the variables in one of these types of games . is there a common approach ? i know that keldon wrote a good ai for rftg which has a decent amount of complexity , i am not sure how he managed to build such an ai . any advice ? is it feasible ? are there any good examples of this ? how were the inputs mapped ? edit : i have looked online and learned how neural networks work and usually how they pertain to image recognition or steering a simple agent . i 'm not sure if or how i would apply it to making selections with cards which have a complex synergy . any direction towards what i should be looking into would be greatly appreciated . about the game : the game is similar to magic : the gathering . there is a commander which has health and abilities . players have an energy pool which they use to put minions and spells on the board . minions have health , attack values , costs , etc . cards also have abilities , these are not easily enumerated . cards are played from the hand , new cards are drawn from a deck . these are all aspects it would be helpful for the neural network to consider .",9660,1671,2018-03-05T19:43:41.283,2018-10-15T23:36:49.367,teach a neural network to play a card game,neural-networks machine-learning gaming neat,4,3,6
903,4049,1,4083,2017-09-18T02:55:31.673,2,72,"i 've just watched the 9th episode of htm school about the "" boosting "" and "" inhibition "" ideas . however , i could n't find the neuroscience counterpart of these terms and concepts . since htm is a biologically - constrained theory , the "" boosting "" and "" inhibition "" concepts must have a neuroscience counterpart . what are they ? the video also discusses homeostasis and homeostatic regulation of neuronal excitability . do these concepts have something to do with "" boosting "" and "" inhibition "" ?",9614,2444,2018-08-20T20:51:20.457,2018-08-20T20:52:22.337,biological analogy for boosting and inhibition idea in hierarchical temporal memory ( htm ),terminology htm,1,0,
904,4050,1,,2017-09-18T05:39:58.897,1,85,"i was trying to categorical variable engineering following this paper . the code is the following : import random import pandas import numpy as np import tensorflow as tf from tensorflow.contrib import layers from tensorflow.contrib import learn from _ _ future _ _ import print_function from sklearn.preprocessing import labelencoder my dataset looks like the following . it 's has 2 independent variable ( ' x1 ' & amp ; ' x2')and 1 dependent variable ( ' lable ' ) . ' x2 ' is the categorical variable . i want to create an embedding vector for this variable and run the simple linear regression to predict ' label'using tensorflow . i could use any other method . but since linear regression is easiest to understand , i 'm trying that . df = pd.dataframe({'x1 ' : np.array([""a"",""a"",""b"",""c"",""b"",""c"",""b"",""c"",""c"",""b "" , "" a"",""b"",""a"",""c"",""a"",""a"",""c""]),'x2 ' : np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167 , 7.042,10.791,5.313,7.997,5.654,9.27,3.1 ] ) , ' label ' : np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221 , 2.827,3.465,1.65,2.904,2.42,2.94,1.3 ] ) } ) # for variable ' x1 ' , i 'm creating levels . encoder = labelencoder ( ) encoder.fit(df.x1.values ) x = encoder.transform(df.x1.values ) # recreating dependent variable list . y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221 , 2.827,3.465,1.65,2.904,2.42,2.94,1.3 ] ) # setting hyper - parameters training_epochs = 5 learning_rate = 1e-3 cardinality = len(np.unique(x ) ) embedding_size = 2 input_x_size = 1 n_hidden = 10 # setting up variables : embeddings = tf.variable(tf.random_uniform([cardinality , embedding_size ] , -1.0 , 1.0 ) ) h = tf.variable(tf.truncated_normal((embedding_size + len(df.x1 ) , n_hidden ) , stddev=0.1 ) ) w_out = tf.get_variable(name='out_w ' , shape=[n_hidden ] , initializer = tf.contrib.layers.xavier_initializer ( ) ) # embedding : embedded_chars = tf.nn.embedding_lookup(embeddings , x ) embedded_chars = tf.reshape(embedded_chars , [ -1 ] ) embedded_chars= embedded_chars + np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167 , 7.042,10.791,5.313,7.997,5.654,9.27,3.1 ] ) # multiplying with hidden layers : layer_1 = tf.matmul(embedded_chars,h ) layer_1 = tf.nn.relu(layer_1 ) out_layer = tf.matmul(layer_1 , w_out ) # define loss and optimizer cost = tf.reduce_sum(tf.pow(out_layer-y , 2))/(2*n_samples ) optimizer = tf.train.adamoptimizer(learning_rate=learning_rate).minimize(cost ) # run the graph init = tf.global_variables_initializer ( ) # launch the graph with tf.session ( ) as sess : sess.run(init ) for epoch in range(training_epochs ) : avg_cost = 0 . _ , c = sess.run([optimizer , cost ] , feed_dict={x : x , y : y } ) print(""ran without error "" ) while running the code , i 'm getting the following error . valueerror : shape must be rank 2 but is rank 1 for ' matmul_1 ' ( op : ' matmul ' ) with input shapes : [ 17 ] , [ 19,10]. i 'm not able to add the continuous variable with embedding variable . can anyone please guide me how to do it ? thank you !",9664,9647,2017-09-19T00:35:13.873,2017-09-19T00:35:13.873,categorical variable reduction using nn,neural-networks tensorflow,0,0,
905,4054,1,4055,2017-09-18T20:05:22.083,4,405,"is there any well defined method to define or represent evil in abstract logic , binary or ai form ? video games method of representing evil is relative to the player context ( thus subjective , and not pure abstract evil in an objective sense ) . what i am asking is there any data defined as well - known evil ? example : var x=666 ; if ( isevil(x ) ) { //do something . } remark : evil number descried in http://mathworld.wolfram.com/evilnumber.html does n't qualify as well - known evil data . following up : one of the main objectives of the question is to understand scientifically the limits of evil in ai according to my understanding of : https://en.wikipedia.org/wiki/evil i think it 's mandatory to explore "" evil "" in religion context in order to come up with valid model for evil . but i do n't want go into ( religion ) debates or any divergence at this stage . hence below points sums up my understanding : the only well - known evil source is the devil ( our creator declared the devil as the first common enemy for all humans ) . whispering is devil method of attack , if human followed the whisper it will lead to evil . and gradually human evil grow ... there are other points but i do n't see its related to ai in any means . based on the above , i asked myself : since ai is human creation , where the evil in ai will come from ? ? ! my answer is : directly from us and indirectly by following the devil . so all crimes committed by evil ai bounded to ai architect / designer / unethical hacker . the next stage in getting closer to model evil , is to define and classify the evil acts : definitions : define evil in ai context ( draft ver . 0.1 ) : committing crimes against nature , civilizations or humans . and reprogramming , modifying or attacking tech devices / machines to perform malicious agenda . crime is broad and relative to the party : example : breaking one government regulations based on the orders of other government . i mean as long each group of humans makes its own laws and regulations unified justice ca n't be applied on evil ai . if my assumption of bounding evil to crime is valid then evil classification inherits crime classification which seems well - defined : https://en.wikipedia.org/wiki/crime#classification_and_categorisation next step is to pick an easy to model crime class , prepare training data , ... do you agree with the follow up ? do you agree that boolean logic ca n't determine evil without ai ?",9685,9685,2017-09-23T14:17:06.233,2017-10-01T06:57:10.333,how to define or represent evil in logic,ai-design decision-theory ethics game-theory game-ai,3,3,3
906,4059,1,4070,2017-09-19T10:23:52.333,2,959,"i 'm currently developing a blackjack program that i posted on code review se about a month ago and , after making a few changes based on the users suggestions and my friends ' suggestions , i want to create an ai that essentially uses the mathematics of blackjack to make decisions . almost like an ai that can card count , but different because card counting is n't very effective with blackjack . i 've figured out some of the math myself based on what i 've read across multiple sites and books dedicated to the game , but i 'm just not sure how i can get started with it . at this point , all suggestions will be helpful . with that in mind , does anyone have any ideas or suggestions for how i might go about making a base plan for implementing it ? it does n't have to be language specific , but if it will help with an answer , the language i plan to use to do this would be python .",9699,1671,2017-12-21T21:13:03.630,2017-12-21T21:13:03.630,developing a blackjack ai,ai-design game-ai python,1,0,
907,4060,1,4062,2017-09-19T10:28:04.990,0,94,"i am a student of software engineering . we studied ai in last semester but it was all theory . now i am interested in practically using ai . i have searched ai tutorials everywhere but i ca n't find single one . i have studied all theory , okay not all but much theory to understand the concepts of ai . if anyone can guide me to the right direction or any book or tutorial where i can learn a little bit of using ai in practical , i would be very thankful ..",9700,,,2017-09-19T17:17:13.650,guidance for a beginner in the field of ai,ai-community,1,3,1
908,4061,1,4224,2017-09-19T13:06:22.270,3,166,"below is an excerpt in an instructor 's manual on ml that is explaining deep neural networks , using cat recognition ( what else ! ) from images as example . on how dl performs this feat , the excerpt said that , assume that the first layer returns the number of pixels that are brown / black / blue / red , and the second layer finds the most common color , and the third layer returns “ cat ” if previous layer had supplied “ brown ” . [ .. ] mathematically , this model would be , for the first layer , [ sum(r = 255 , g=255 , b=255 ) , ... , ... , sum(r=255 , g=0 , b=0 ) ] -- this is just a set of appropriately positioned relu functions ( okay , for r=234 , we ’d need two relu functions , so two layers , but you get the idea ) . the second layer would be a softmax layer . the third layer is simply an identity ! now i worked with deep nets , but i am not sure how i can structure a dl to do this . relu is simply a max(0,x ) , so how would i filter out pixel vals for example 128,128,128 and sum them up ? would n't the convolution layer play a role here too ? what would the layout of a simple deep net be that does what is described above ? thanks ,",9658,9658,2017-09-19T13:41:07.263,2017-10-08T08:12:04.967,"relu , sum and convolution layers to count pixels of certain color",neural-networks deep-learning convolutional-neural-networks,1,0,
909,4069,1,,2017-09-20T03:25:56.490,-1,87,how much power will it bring to its creator ?,9716,1671,2017-09-21T21:25:55.030,2017-09-21T21:25:55.030,"who will be the first to announce "" artificial intelligence "" has been created ?",machine-learning agi ethics,1,2,
910,4072,1,,2017-09-20T08:48:33.403,4,245,"to start , i 'm not a programmer / computer scientist / et al ... - i work in finance and have , through my job , self - thought myself vba for excel and outlook and would consider myself as being in the upper half of the population of people who write vba creating browser apps for scraping ( and interpreting site 's doms in doing so ) , creating arrays of multi - dimensional arrays ( arrays of 3d arrays - have n't gone for a 4d array as i have n't needed to ) and generally focused on making as efficient macros . i 've ( very ) basic understanding of java also ( i even created an basic android app ! ) . by pure accident , learning the above has given me a massive interest in all thing related to it developments and the massive development which i truly believe will be the development in human history ( for better or worse has yet to be determined ! ) is ai . i want to know all that i can about ai so was wondering if you all could assist an absolute noob on the subject . i know nothing of python ( which seems the language of choice for ai ) but anything that would help would be great . i 'm not completely naive to the fact the brightest minds in the it world are are working on this and concepts are going to be difficult for someone like me but i 'm willing to try ! any and all help / suggestions are welcome ! thanks in advance .",9712,169,2017-09-21T11:12:42.313,2017-09-21T11:12:42.313,can someone direct me to a sites and/or videos that can bring an absolute beginner up to speed with ai ?,neural-networks machine-learning deep-learning programming-languages,3,6,2
911,4076,1,,2017-09-20T12:37:03.733,5,178,"i have to analyse sequences of actions that look more or less like this json blob . the question i 'm trying to answer is whether there are recurring ( sub)patterns that different users adopt when asked to perform a certain specific task -- in this case , the task is to build a mathematical formula using this editor . in particular i 'd like to know if there are multiple significantly different ways in which people build the same expression . i thought of creating a markov model , but that would only give me the most likely sequence of actions of length n . an obvious alternative would be to build trees and count how many times a certain path occurs in the dataset . however , the nature of the expression - building process means that the sequences can be polluted by many confounding , non - significant actions ( such as streaks of undo - redo , deleting symbols , and the likes ) . i might go the "" longest common subsequence "" route , but i 'm not sure that would tell me if there are "" significantly different "" ways of building the same expression ( in quotes because , for now , i do n't have a rigorous definition of "" significantly different "" , but , for example , one way would be to drag and drop - in - place all the symbols in the correct order , and another way would be to drag all the symbols onto the canvas , and then place them in the correct spots ) . i thought this might be a nice challenge for some ai algorithm , but i 'm quite a noob at that , so i 'm open to suggestions .",9720,,,2018-05-16T11:24:15.590,detect patterns in sequences of actions,classification markov-chain detecting-patterns,1,1,
912,4080,1,4089,2017-09-20T18:40:17.130,4,1072,"i studied the article "" demystifying deep reinforcement learning "" extensively during the last days , while trying to implement the proposed algorithms myself . my goal is to have an agent learn by playing a simple board game against itself using the methods of deep reinforcement learning . the algorithm described in pseudo code in the chapter called "" deep q - learning algorithm "" is straight forward , but i can not wrap my head around the fact that the replay memory only gets initialized once and never gets cleared . besides the obvious issue of growing too large for the available memory , there seems to be a more fundamental flaw . in the beginning the games will appear almost random , because the q - function just has been initialized randomly . this means that bad moves will hardly be punished by the opponent . it makes sense to learn from those random encounters in the beginning , as we do not have any better data to learn from and this actually describes the observed behavior of the environment very well . but when the agent improves , the old memorized plays will no longer be valuable , as certain moves will no longer be played by the opponent and therefore the old games will no longer reflect the current behavior of the environment . that 's at least my interpretation . now i am wondering if this is simply missing from the pseudo code in the article or if my way of thinking is wrong in this regard . my question is , if we need to flush the replay memory regularly in the given setup and how often it should be done ?",9161,,,2017-09-22T01:05:41.510,is it necessary to clear the replay memory regularly in a dqn when an agent plays against itself ?,deep-learning reinforcement-learning,1,1,2
913,4084,1,,2017-09-21T06:36:17.840,5,142,"i 've posted this question on data science community , but have n't received an answer yet . so i also posted it here in the hope that more people could see it . i 'm learning about the restricted boltzmann machine ( rbm ) , and i just came up with two naïve understandings of this model . but it seems these two understandings are so different , can someone tell me how can i combine these two understandings ? my first understanding goes like this : the hidden units are just structural support and we do n't care about what those hidden vectors really are . we introduce those hidden units just for rbm to gain more expressive power for probability distribution : let 's say i have only one image , and i transform this image to a binary vector and feed this vector into a rbm with random variables ( all the weights and biases are chosen randomly ) . then , by turning on the machine , the first hidden vector would be constructed . but this hidden vector does not tell us anything , it would be only used to reconstruct a visible vector . ( my understanding for this reconstructed visible vector is this vector is a vector encoded in the defined rbm in the first place , we are not really construct something new , but we just happen to sample this vector from the defined rbm ) . and we just run this loop of construction and reconstruction for infinitely many times . finally , what we will get is just the probability distribution encoded in this rbm with random variables . my second understanding goes like this : rbm can be used to perform dimensionality reduction , and those hidden vectors are some abstract representations of the raw inputs : given a rbm , each hidden units of the rbm would be a classifier , and what it does is to check the input vector lies in which side of the hyperplane defined by this hidden units ( by its weights and bias ) . so , if we input an image into this rbm , the rbm will project this input vector onto many hyperplanes defined by all the hidden units , and thus for an input vector , the corresponding hidden vector is very important . it is some abstract representation . and we can further feed this representation into other learning models . so , these are my understandings , and the thing that troubles me the most is what the role that hidden vectors actually plays ? if you can answer this question by explaining how rbm is used for mnist that would be extremely helpful for me . thanks !",9739,1671,2017-09-25T18:13:00.020,2018-05-04T14:47:21.157,how can i combine these two understandings of rbm ( restricted boltzmann machine ) ?,neural-networks machine-learning,2,3,1
914,4085,1,4088,2017-09-21T08:27:28.160,9,3957,question is regarding deep reinforcement learning using policy gradients . cutting edge policy gradients algorithms are trpo ( trusted region policy optimization ) and ppo ( proximal policy optimization ) . when using single continuous action then normally you would use some random distribution ( for example gaussian ) for the loss function . the rough version is : $ l(\theta ) = log(p(a_1 ) ) * a$ where $ a$ is the advantage of rewards $ p(a_1)$ is characterized by and that comes out of neural network like in pendulum environment here : https://github.com/leomzhong/deepreinforcementlearningcourse/blob/69e573cd88faec7e9cf900da8eeef08c57dec0f0/hw4/main.py problem is that i can not find any paper on 2 + continuous actions using policy gradients ( not actor - critic methods that use a different approach by transferring gradient from q - function ) . do you know how to do this using trpo for 2 continuous actions in lunarlander environment ? https://gym.openai.com/envs/lunarlandercontinuous-v2/ is following approach correct for policy gradient loss function ? $ l(\theta ) = ( log(p(a_1))+log(p(a_2)))*a$,9740,2444,2019-05-02T16:04:02.497,2019-05-02T16:04:02.497,policy gradients for multiple continuous actions,deep-learning reinforcement-learning trpo,1,5,3
915,4094,1,4097,2017-09-23T09:19:23.177,3,757,what is the difference between local search and global ( or complete ) search algorithms ?,9788,2444,2019-05-02T13:59:50.987,2019-05-02T13:59:50.987,what is the difference between local search and global search algorithms ?,algorithm search definitions difference local-search,1,0,2
916,4095,1,,2017-09-23T16:32:35.447,4,143,"after reading this paper about monte carlo methods for imperfect information games with elements of uncertainty , i could n't understand the application of determinization step in author 's implementation of the algorithm for knockout game . determinization is defined as transformation from instance of imperfect information game to instance of perfect one . it means that all players should see the cards of each other after the determinization step . why ca n't the players see the cards of each other in the code above ?",9797,1671,2017-09-25T17:32:31.247,2019-04-25T14:02:40.603,determinization step in information set monte carlo tree search,monte-carlo-tree-search combinatorial-games imperfect-information,1,0,1
917,4098,1,,2017-09-24T07:04:56.917,3,606,"i am building a system that should take text without punctuation and automatically add punctuation . i found some papers about automatic punctuation , but they are mostly about spoken language understanding , they use cues such as prosody to detect potential places for punctuation . in my case the input is written text . are there papers about automatic punctuation of written text ? my current idea is to treat each punctuation mark as a class ( . , ; : ? ! ? ! ) and add a class for "" no punctuation "" . then , use e.g. an lstm and classify each word to one of the classes . an alternative approach is to first use a binary classifier to detect words after which there should be a punctuation mark , then use a multiclass classifier only on the punctuated words , to choose the right mark . which of these approaches , if any , is good ?",8684,4302,2018-10-08T12:24:05.333,2018-10-08T12:24:05.333,automatic punctuation,machine-learning natural-language-processing reference-request,1,0,
918,4100,1,,2017-09-24T19:07:53.807,3,1374,"i am developing a heuristic solution for blocks world problem . i tried using number of blocks out of place as my h(n ) . it seems little ineffective . can someone please point out a suitable heuristic for the problem and explain with few examples how it will work . blocksworld problem example : initial(starting state ) : stack 0 : d , b stack 1 : a , e stack 2 : c stack 3 : f goal state : stack 0 : a , b , c , d , e , f",9812,1671,2017-09-25T17:58:21.117,2017-09-25T17:58:21.117,blocks world game : a * heuristic solution approach,ai-design algorithm heuristics combinatorics,1,1,
919,4101,1,4103,2017-09-25T00:48:55.157,0,65,"not sure if this is the proper venue for this type of question . are there any visual elements , patterns , colors , graphs , maps , or even programming fonts that are used and even recurring in artificial intelligence and machine learning processes ? as a designer , it is not entirely clear what these "" look "" like realistically , as most generally associate ai with the kind of colors and textures generally seen in sci - fi films .",9815,,,2017-09-25T14:25:14.893,are there any visual elements commonly associated with ai ?,ai-design ai-community,1,0,
920,4111,1,4115,2017-09-25T20:21:10.377,3,529,"i just got into ai few months ago and i noticed most of the images in train - sets are usually low quality(almost pixelated ) . my questions is , does the quality of training images affect the accuracy of the nn ? i tried google but i could n't find an answer .",7266,1581,2017-09-26T19:25:49.557,2017-09-26T19:25:49.557,image quality for training nn,neural-networks machine-learning image-recognition,2,1,
921,4112,1,4123,2017-09-25T21:06:57.483,0,77,"i did a little bit of research on the most famous organizations and institutions that promote friendly ai in an open and collaborative way , like openai , miri and others . from what i have gathered , most of them have strong ties to the us with some influences from central europe and south east asia . but i did n't find a single instance where russia participates in any of those groups . so my question is , does russia contribute to open research concerning friendly ai ? are there any open ai projects with strong ties to russia ?",9161,9161,2017-09-26T20:55:51.633,2017-09-27T03:07:31.140,is russia contributing to open research concerning friendly ai ?,research strong-ai friendly-ai,1,7,
922,4113,1,,2017-09-25T21:45:57.080,2,193,"i building a deep learning model to detect what drug user use . i have many symptoms and duration of each drug . i create x and y data but , for example , lsd have an effect duration of 180 - 720 minutes . i really need make 540 arrays ? i really want a help . my lsd array : [ 28 , 180 ] , [ 28 , 720 ] , [ 29 , 180 ] , [ 29 , 720 ] , [ 30 , 180 ] , [ 30 , 720 ] , [ 31 , 180 ] , [ 31 , 720 ] , [ 32 , 180 ] , [ 32 , 720 ] , [ 33 , 180 ] , [ 33 , 720 ] , [ 34 , 180 ] , [ 34 , 720 ] , [ 35 , 180 ] , [ 35 , 720 ] , [ 36 , 180 ] , [ 36 , 720 ] , [ 37 , 180 ] , [ 37 , 720 ] , [ 1 , 180 ] , [ 1 , 720 ] , [ 38 , 180 ] , [ 38 , 720 ] , [ 12 , 180 ] , [ 12 , 720 ] , [ 9 , 180 ] , [ 9 , 720 ] , [ 24 , 180 ] , [ 24 , 720 ] , [ 17 , 180 ] , [ 17 , 720 ] , [ 7 , 180 ] , [ 7 , 720 ] , [ 4 , 180 ] , [ 4 , 720 ] , in first position i have differents symptoms and in second position duration . i just duplicated each symptoms and set min duration and max duration . but this return to me a perfection model . i know , i need add all minutes to each symptoms , but how i make this using python ? list of symptoms 0 - relaxamento 1 - euforia 2 - diminuicao da memoria a curto prazo 3 - boca seca 4 - habilidades motoras debilitadas 5 - olhos vermelhos 6 - humor 7 - aumento frequencia cardiaca 8 - aumento apetite 9 - concentracao debilitada 10 - sensacao de poder 11 - ausencia de medo 12 - ansiedade 13 - agressividade 14 - excitacao 15 - perda do apetite 16 - tremores 17 - dilatacao da pupila 18 - dentes anestesiados 19 - insonia 20 - movimentos descontrolados 21 - espasmos maxilar 22 - dor de cabeça 23 - visao turva 24 - nauseas 25 - desidratacao 26 - periodos de depressao 27 - perda total da memoria 28 - ilusões 29 - alucinações 30 - grande sensibilidade sensorial 31 - experiências místicas 32 - flashbacks 33 - paranoia 34 - perda da noção temporal e espacial 35 - confusão 36 - perda do controle emocional 37 - sentimento de bem - estar 38 - pânico 39 - sonolencia 40 - batimentos cardiacos diminuem 41 - insuficiencia respiratoria 42 - desanimo 43 - desinteresse pela vida familiar / profissional 44 - sensacao de estar no paraiso 45 - mal - estar 46 - incapacidade de sentir prazer 47 - incapacidade de sentir dor * * durations effects ( in minutes ) * * cannabis . 120 - 240 cocain . 30 - 40 ecstasy . 240 - 480 lsd . 180 - 720 heroin . 45 - 60 my full code : x = [ # cannabis [ 0 , 120 ] , [ 0 , 240 ] , [ 1 , 120 ] , [ 1 , 240 ] , [ 2 , 120 ] , [ 2 , 240 ] , [ 3 , 120 ] , [ 3 , 240 ] , [ 4 , 120 ] , [ 4 , 240 ] , [ 5 , 120 ] , [ 5 , 240 ] , [ 6 , 120 ] , [ 6 , 240 ] , [ 7 , 120 ] , [ 7 , 240 ] , [ 8 , 120 ] , [ 8 , 240 ] , [ 9 , 120 ] , [ 9 , 240 ] , # cocain [ 1 , 30 ] , [ 1 , 40 ] , [ 10 , 30 ] , [ 10 , 40 ] , [ 11 , 30 ] , [ 11 , 40 ] , [ 12 , 30 ] , [ 12 , 40 ] , [ 13 , 30 ] , [ 13 , 40 ] , [ 14 , 30 ] , [ 14 , 40 ] , [ 15 , 30 ] , [ 15 , 40 ] , [ 7 , 30 ] , [ 7 , 40 ] , [ 16 , 30 ] , [ 16 , 40 ] , [ 17 , 30 ] , [ 17 , 40 ] , [ 18 , 30 ] , [ 18 , 40 ] , # ecstasy [ 19 , 240 ] , [ 19 , 480 ] , [ 20 , 240 ] , [ 20 , 480 ] , [ 21 , 240 ] , [ 21 , 480 ] , [ 22 , 240 ] , [ 22 , 480 ] , [ 23 , 240 ] , [ 23 , 480 ] , [ 24 , 240 ] , [ 24 , 480 ] , [ 25 , 240 ] , [ 25 , 480 ] , [ 26 , 240 ] , [ 26 , 480 ] , [ 27 , 240 ] , [ 27 , 480 ] , [ 15 , 240 ] , [ 15 , 480 ] , # lsd [ 28 , 180 ] , [ 28 , 720 ] , [ 29 , 180 ] , [ 29 , 720 ] , [ 30 , 180 ] , [ 30 , 720 ] , [ 31 , 180 ] , [ 31 , 720 ] , [ 32 , 180 ] , [ 32 , 720 ] , [ 33 , 180 ] , [ 33 , 720 ] , [ 34 , 180 ] , [ 34 , 720 ] , [ 35 , 180 ] , [ 35 , 720 ] , [ 36 , 180 ] , [ 36 , 720 ] , [ 37 , 180 ] , [ 37 , 720 ] , [ 1 , 180 ] , [ 1 , 720 ] , [ 38 , 180 ] , [ 38 , 720 ] , [ 12 , 180 ] , [ 12 , 720 ] , [ 9 , 180 ] , [ 9 , 720 ] , [ 24 , 180 ] , [ 24 , 720 ] , [ 17 , 180 ] , [ 17 , 720 ] , [ 7 , 180 ] , [ 7 , 720 ] , [ 4 , 180 ] , [ 4 , 720 ] , # heroin [ 39 , 45 ] , [ 39 , 60 ] , [ 29 , 45 ] , [ 29 , 60 ] , [ 40 , 45 ] , [ 40 , 60 ] , [ 41 , 45 ] , [ 41 , 60 ] , [ 42 , 45 ] , [ 42 , 60 ] , [ 43 , 45 ] , [ 43 , 60 ] , [ 44 , 45 ] , [ 44 , 60 ] , [ 12 , 45 ] , [ 12 , 60 ] , [ 45 , 45 ] , [ 45 , 60 ] , [ 46 , 45 ] , [ 46 , 60 ] , [ 1 , 45 ] , [ 1 , 60 ] , [ 13 , 45 ] , [ 13 , 60 ] , [ 24 , 45 ] , [ 24 , 60 ] , ] "" "" "" # drogas 0 - cannabis 1 - cocain 2 - ecstasy 3 - lsd 4 - heroin "" "" "" y = [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , ] from sklearn.model_selection import train_test_split x_train , x_test , y_train , y_test = train_test_split(x , y , test_size=.5 ) from sklearn import tree my_classifier = tree.decisiontreeclassifier ( ) my_classifier.fit(x_train , y_train ) predictions = my_classifier.predict(x_test ) print(predictions ) from sklearn.metrics import accuracy_score print(accuracy_score(y_test , predictions ) ) sorry for my bad english :( thanks",7800,,,2018-01-04T07:40:36.313,how generate variation in datasets,deep-learning,1,2,1
923,4114,1,,2017-09-26T01:06:10.330,6,141,"are there any algorithms or software libraries that can be used to detect the similarity of concepts in text , regardless of articulation , grammar , synonyms , etc . ? for example , these phrases : outside , it is warm . outside , it is hot . outside , it is not cold . it is not cold outside . should be similar to this phrase : it is warm outside . ideally , the algorithm or software would be capable of generating a score from 0 to 1 , based on the concept similarity . the goal is to use this algorithm or software to map a large number of statements to a single , similar original statement . it is for this mapping of a given statement to the original statement that the aforementioned similarity score would be generated . does such an algorithm of software already exist ?",9836,5210,2018-07-15T16:10:19.960,2018-07-15T16:10:19.960,are there any projects that can measure the concept similarity between texts ?,neural-networks natural-language-processing,1,4,1
924,4117,1,,2017-09-26T08:38:02.313,3,976,"i have some images of the empty parking as shown below . i 'd like to use deep learning to extract the parking spots . but in the beginning , am confused whether there are several ways to do the following ; treat each parking spot as an object and use object detection to find the parking space . treat the parking spot as a few key points , instead of finding the spot , try to find the coordinates of key points on the parking line . any ideas on how would i work on these two methods",8508,1581,2017-09-26T19:25:38.170,2017-11-06T23:25:27.437,how to detect the empty parking spots ?,neural-networks convolutional-neural-networks image-recognition object-recognition,1,0,
925,4118,1,,2017-09-26T13:52:29.300,2,99,"i asked this question on /r / learnmachinelearning , but there was no answer so i 'm reposting it here . i was reading this article on detecting rectangles in an image , here . my doubt is in the part where the model works fine with detecting a single object , but struggles with two rectangles detection . the author reasons this as follows : we train our network on the leftmost image in the plot above . let ’s say that the expected bounding box of the left rectangle is at position 1 in the target vector ( x1 , y1 , w1 , h1 ) , and the expected bounding box of the right rectangle is at position 2 in the vector ( x2 , y2 , w2 , h2 ) . apparently , our optimizer will change the parameters of the network so that the first predictor moves to the left , and the second predictor moves to the right . imagine now that a bit later we come across a similar image , but this time the positions in the target vector are swapped ( i.e. left rectangle at position 2 , right rectangle at position 1 ) . now , our optimizer will pull predictor 1 to the right and predictor 2 to the left — exactly the opposite of the previous update step ! in effect , the predicted bounding boxes stay in the center . i do n't understand how this reasoning is correct , apart from the fact that when they try to flip the rectangles to mitigate this error , accuracy actually improves ( so there 's experimental observation , but not much theoretical reasoning ) . the reason i think so is because in case of single rectangle also the network has to learn for all differently placed objects just as in the two - rectangle - case , so there too , it should predict boxes in the somewhat the center only . i have to concede i am only a noob in this , so i would love to find out where i am wrong in my reasoning , because experimentally i am wrong ( i.e. accuracy does improve when rectangles are flipped ) . thoughts ? also if this is not the correct sub / forum for these type of questions , please feel free to guide me towards those that better suit the content .",4352,9647,2017-12-21T19:05:52.953,2018-01-20T19:39:13.280,object detection doubt,computer-vision object-recognition,1,0,1
926,4121,1,4122,2017-09-26T20:56:46.890,5,134,"i 've been reading about ai / deep learning , etc . to understand robots ( i.e. elon musk warning ) . but i must be missing something ... can the entire field be summed - up in this one sentence ? "" neural networks use the sigmoid function and gradient descent to fine - tune weights . "" for me , a real - world comparison is : 1 ) take a wood box and randomly drill different sized holes in the bottom of it . 2 ) fill the box with coins and shake it . 3 ) mark which holes have coins dropping out . 4 ) start over with a new wood box and drill new holes . 5 ) repeat until perfect sorting of coins is achieved . note : you can even have "" layers "" of boxes and paint it all black in the end , you have a finely tuned "" black box "" to "" sort "" data . ( referring to nn 's now ) but , what you do not have is ai - "" artificial intelligence "" . the whole world should call it "" recognition software "" instead . everyone knows that intelligence is to "" think outside the box "" . ( examples : observation , reflection , innovation ) so my question is : have mathematicians discovered any methods / algos that actually mimic intelligence ? ( what keywords should i google ? )",9847,9847,2017-09-27T21:42:48.457,2017-09-28T18:55:31.573,does any math / algos actually mimic human intelligence ?,neural-networks definitions terminology,3,5,1
927,4125,1,,2017-09-27T08:24:12.480,3,329,"i am currently working on my msc thesis , where i try to automatically convert description of an algorithm in natural language to source code in python ( the algorithms are quite simple , like revert an array of 100 elements ) using deep learning . the main problem is that i need a lot of data to do that . does anyone know any available datasets consisting of pairs { short natural language description , source code } ? i know the heartstone cards dataset ( really useful and close to my needs , but still not enough ) , the django dataset ( django code commented line by line - it does n't really contain the description of whole algorithm , it rather translates english to django code line by line ) . i tried to contact with few sites like for eg . sphere online judge but to no avail . every help would be appreciated .",9854,,,2017-12-04T09:16:03.210,is there any free dataset of source code along with natural language description ?,neural-networks machine-learning deep-learning datasets,1,0,
928,4126,1,4132,2017-09-27T09:49:33.510,1,368,"i have been playing around with the pong - v0 game . after few days , the ai was able to beat it . i stopped the script for a moment and ran it again . for my surprise it seems it started all over again . where and how does openai store the experience ? how can i continue the learning procedure in another computer ?",9856,1671,2018-05-10T20:42:35.277,2018-05-10T20:42:35.277,"openai gym : how is "" experience "" stored ?",open-ai,1,7,
929,4127,1,,2017-09-27T11:00:07.953,2,165,"i used this project for example(framework - caffe , arhitecture of net - mod of alexnet , 400 images are used for training ) . i have this result : or this : solver : net : "" ./cdnet / models / train.prototxt "" test_iter : 500 test_interval : 500 base_lr : 0.001 lr_policy : "" step "" gamma : 0.1 stepsize : 100000 display : 50 max_iter : 450000 momentum : 0.9 weight_decay : 0.0005 snapshot : 10000 snapshot_prefix : "" ./cdnet / models / training / cdnet "" solver_mode : gpu model of net : can anybody explain such behavior of accuracy and loss of my net ? what i am doing wrong ? author of tutorial has got this result :",9859,,,2017-09-27T11:00:07.953,can anybody explain such behavior of accuracy and loss of my net(caffe ) ?,neural-networks machine-learning convolutional-neural-networks image-recognition,0,0,
930,4128,1,4129,2017-09-27T13:14:05.297,0,381,and i know how the calculation are performed with help of above formula but i am not understanding why we are using this formula . can anyone explain the above formula ? ? ?,9863,9863,2017-09-28T05:11:23.433,2017-09-29T04:20:03.820,can anyone explain why we are taking negative sign in entropy calculation ( id3 decission tree algorithm ) ?,machine-learning,1,0,
931,4130,1,4157,2017-09-27T18:58:45.827,3,117,"as a researcher , i am getting interested in deep learning ( as everyone else : ) ) , and i decided to start with the variational eutoencoders , since i am more interested in unsupervised than supervised learning . i have already read tutorials on the general idea of the mechanism of vae , and what people usually suggest is to use tensorflow , keros , etc . to implement them . however , i do not just want to use machine learning , but also want to develop novel algorithms on top of the existing ones , so i want to deeply understand the mathematical details . and in my experience , in order to understand them , reading is not enough , you need to implement . any suggestions on where / how to start ? i have a general machine learning and programming background , but no experience on deep learning . i even do n't know what exactly back - propagation is used for , if this would help .",9609,,,2017-10-03T06:36:55.603,a good way to understand the mathematical details of variational autoencoders through implementation ?,neural-networks,1,0,
932,4131,1,,2017-09-27T20:18:39.897,4,374,"in example , if there is a simple feed - forward neural network with 3 input neurons , 3 hidden neurons , and one output neuron ; is it possible to predict a the value of an input neuron given the values and weights for the other two inputs and the output .",9423,9423,2017-09-28T00:21:12.350,2018-02-27T20:08:09.443,"in a neural network given partial inputs and complete outputs , is it possible to predict remainig inputs",neural-networks,2,2,1
933,4135,1,,2017-09-28T09:40:10.880,2,38,"this question is related to the usage of nn in critical systems ( those where a failure can cause life threatening situations - autopilots for example ) and the need for formal guarantees on their behavior . here is , for example , a paper that verifies nn used in the controls of an unmanned air - vehicle . there are numerous tools and techniques for the formal verification ( not just testing and simulation , but actual mathematical proof ) of , say , floating - point calculation ( properties like : will not overflow , will not accumulate an error greater than x , etc ) . now take a dnn where the weights are real numbers . that is the dnn as a concept . but there is the implementation of the nn , where the weights must be encoded as floating - points ( some even as low precision as 3-bits integers , apparently ) . and then , ( faster ) computation is done on these encodings . someone might argue that you lose precision , but others might answer that it may be a good thing since one must prevent a nn for over - fitting anyway ... questions : is there a way to quantify the effect of this encoding on the robustness / precision / stability - of - classification , in a comparable way to what we have in ( non learning ) software verification ? are there nn architectures ( like sum - product networks ( ? ) ) that are more amendable to offer such guarantees ?",9319,,,2017-09-28T09:40:10.880,loss of precision when encoding dnn weights,neural-networks deep-learning self-driving challenges,0,2,
934,4136,1,,2017-09-28T12:11:12.820,6,1168,"dnn can be used to recognize pictures . great . for that usage , it 's better if they are somewhat flexible so as to recognize as cats even cats that are not on the pictures on which they trained ( i.e. avoid overfitting ) . agreed . but when one uses nn as a replacement for numerical tables in an air collision avoidance system ( acas ) , it is primarily to reduce the "" required storage space by a factor of 1000 "" . for this usage , what we want from the nn is to say "" take a slight left turn "" or "" turn right hard "" if another ship comes slightly close on the right or rapidly close on the left , respectively . for this usage , where the answer is much simpler than recognizing a cat , is n't overfitting a good thing ? what would overfitting "" look like "" in this case and why would it be bad ? this question somewhat relates to this one , where a general idea seems to be "" machine learning is used for intractable things , you do n't need ml for tractable things "" . and while it is quite correct that acas can be implemented without nn , i would n't call nn "" useless "" for acas , because a factor 1000 reduction in required space will always come in handy .",9319,,,2017-09-30T19:10:46.590,is overfitting always a bad thing ?,neural-networks control-problem overfitting,3,0,1
935,4139,1,4146,2017-09-28T17:11:21.753,3,913,"for implementing a neural network algorithm that can play air hockey , i had two ideas for input , and i 'm trying to figure out which design would be most viable . the output must be two analog values that dictate the best position on half of the table for the robot to be at a specific point in time , evaluated 60 times per second . having consulted with a professor who has experience with implementing parallel algorithms , it was recommended to me to use a convolutional neural network with a single hidden layer , and directly process the image data as the input layer , after processing it to visualize a direct view of the table and somehow emphasize the puck and mallets with pre - processing . i have already started work on this and have successfully implemented object detection for the puck and mallets using opencv to get the center coordinates for all 3 entities . however , having been able to successfully and accurately pre - process these data at 60 times per second , my thought was to feed those ( after normalizing the values using the error function ) directly on the input layer and possibly implement a deep learning algorithm that employs more than one hidden layer . the problem is that i do n't have any experience implementing neural networks , so i 'm not even sure what type of layer would be best for this , or how i should seed the weights . another reason i want to consider my idea is that given the relatively few inputs and outputs , probably wo n't need a gpu to execute forward - propagation 60 times per second , whereas with the convolutional network my professor recommended , i know i 'll need to implement using cuda somehow . which of the two input methods would be most recommended for this , and if i were to try and implement my idea , what types of layers should i consider ? also any recommendations for existing frameworks to use for either approach would be highly appreciated .",9887,,,2018-02-18T02:19:20.447,what type of neural network would be most feasible for playing a realtime game ?,neural-networks ai-design game-ai hidden-layers real-time,2,0,1
936,4140,1,,2017-09-28T17:19:47.250,4,213,"notice that , in the following formula , at the very right , the term multiplied with is $ d_i$ $ $ w : = w + \alpha \sum_{i=1}^{n-1 } \nabla r(x_i^l , w ) \big \lfloor \sum_{j = i}^{n-1 } \lambda^{j - i } d_i \big \rfloor $ $ note that $ i$ is the index of the first sum . however , in the following formula , the term is $ d_m$ , which was $ d_i$ in the first equation . $ $ w_j : = w_j + \alpha \sum_{i=1}^{n-1 } \frac{\partial r(x_i^l , w)}{\partial w_j } \big \lfloor \sum_{m = i}^{n-1 } \lambda^{m - i } d_m \big \rfloor $ $ note that $ m$ is the index of the second sum . in my opinion , first equation seems reasonable . the in second one actually seems to work like discount factor . in another chess engine , giraffe , the author cited same paper , noted the first equation , but then proceeded on to implement second one . see also knightcap paper .",9886,2444,2019-04-02T14:34:09.910,2019-04-02T14:34:09.910,inconsistency in td - leaf algorithm in knightcap chess engine,machine-learning reinforcement-learning game-ai,2,0,1
937,4142,1,7357,2017-09-28T19:55:13.527,3,319,"given a virtual game map ( picture ) and a racing car at the map 's starting point , i 'm trying to build an algorithm that would help me generate a route that would get the car from the beginning to the endpoint . route definition : a very complex .xml file that includes all the data the car needs in order to navigate the road successfully . what i have : i have thousands of different maps and i could get any number of maps i 'd like . for every map , i have a complex external algorithm using picture analysis that builds a route for me - although this route is not so accurate . for some percentage of the maps , i also have a specific "" good working car solution "" - which is basically a manually built route file . what i 'm trying to achieve : basically , i 'm trying to improve the complex route generator algorithm which i do n't have access to . i want to use the percentage of manual routes i have and compare them to the routes that are generated for the same maps . from the comparison and the differences between the manual and the automatically created route files , i want to build a second - step smart algorithm that "" learns "" what should be changed in the automatically generated routes and have that algorithm run as a second step and make it as close as possible to the handmade route . my final goal is to be able to build an accurate route for maps i do n't have the manual route for . i want to be able to produce an accurate route for every map - using both the external algorithm and the one i will build on top of it . this entire subject is very new to me and i 'd like to get your advice as to how should i approach this complex problem , and what could make this task easier for me .",9890,9890,2017-09-28T21:41:54.470,2018-10-15T23:55:21.237,machine learning algorithm for xml manipulation,machine-learning algorithm learning-algorithms,1,0,
938,4147,1,,2017-09-29T14:35:49.217,2,152,"in their work continuous deep q - learning with model - based acceleration , the author demonstrate great results of applying imagination rollouts for model - based acceleration of learning . they test their algorithm on manipulation tasks in a mujoco simulation . i wonder whether similar accelaration would be possible in classic environments like pendulum - v0 .",9911,,,2017-09-29T14:35:49.217,is ilqg a good algorithm for model - based planning with simple environments ?,neural-networks reinforcement-learning models,0,0,
939,4148,1,,2017-09-29T15:38:22.220,2,180,"there 's a real possibility that self driving cars become more than just a high tech novelty and they start changing the market . as seen in logan , self driven commercial freight might be among the first . this makes sense to me . the automotive industry might be harder to overtake , but truckers are n't highly qualified . anybody can get a cdl . also in the movie , we see that the trucks are n't perfect . one causes a large accident , and sets spooked horses running frantically across the highway . sam harris has questions about the fundamental flaw of a technology that is expected to take lives into its own hands autonomously existing when it has to make a choice between one life or another . if a car is hurdling towards a mass of people on the side of the road through some mistake caused by road conditions , should it veer towards a nearby ditch where it may endanger its passengers ? that 's just one example of this . i 've spoken with my friend about this a bit , and we 've talked about asimov 's laws of robotics , which i think are a good starting point . as far as i know , there 's four laws . feel free to add if i 've missed any : a robot can not harm humanity , or through inaction allow humanity to come to harm . a robot can not harm a human , or through inaction allow a human to come to harm , except where this would conflict with the zeroth law . a robot must obey any orders given to it by a human , except where this would conflict with the zeroth or first laws . a robot must do everything it can to protect itself , except where this would conflict with the zeroth , first , or second laws . under this simple system , we already have our automated cars choosing to save a group of lives over one life . discussion about the worth of elderly lives versus children 's lives i think is unimportant . no company should be building machines that prioritize police over civilians or civilians over criminals . nobody over 60 is getting in the nissan they know cares more about everyone that 's not in the car than it does it 's passenger . the job of the machine is to make unbiased choices between saving one life or two . and it should always pick two . but one - for - one choices are going to likely be determined by factors like what action is the easiest to perform at x miles per hour . of course it makes the most sense that the vehicle itself is at the bottom of its own list of priorities . it 's a tool that can be easily replaced . but there 's one thing i 've not mentioned yet , and that 's the processing speed of the on board computer . if self driving cars prove incapable of making split second decisions because they take their sweet time analyzing , we wo n't want to ride them . rather interested in philosophy and ethics , the programming part is where my knowledge is lacking . i can assume some basic facts . you 'd have to make space in the vehicle for a computer , some kind of image recognition software would need to tell what 's a human . perhaps pets and whatnot could also be included . and maybe it would n't be unreasonable to think since the passengers do n't need to look out the windshield , that it could be replaced with a widescreen tv , or some other ancillary features . could we bridge the gap in knowledge here , and somebody give me an idea of whether this kind of thing is even possible ?",9914,,,2018-11-07T03:37:28.790,how would ai prioritize situational ethics ?,ai-design algorithm image-recognition philosophy,3,2,3
940,4150,1,,2017-09-29T20:19:57.870,2,35,"i 've only just started looking into machine learning , and so far most of the examples i 've seen involve starting with a training set of observations , each representing a value for a number of ' features ' , and using that to train a model - then evaluating the trained model against a second test set of data to allow refinements to the model . the basic operation the model is asked to do is to predict the value of one feature given others . when talking about art ( by which i would include music , poetry , etc ) , we can no longer assert that things are correct - but perhaps we could isolate a number of subjectively - judged features , such as whether a work is ' mysterious ' or ' joyful ' to a particular human judge , and then train the model to our judge 's taste . however , the problem here would seem to be that having a human being add judgement features to each of our data points sounds like a very slow task , and it might often be impractical to get to the number of ' assessed works ' that would allow the model to be trained . how could the process of training a model to produce art of a certain ' taste ' be done in a practicably short space of time ?",9918,9918,2017-09-30T10:25:46.863,2017-09-30T14:30:38.737,what techniques would allow a model to be trained to produce subjectively pleasant ' artistic ' output ?,machine-learning,1,0,1
941,4151,1,,2017-09-30T05:06:09.487,1,21,"let 's say i train an rnn on the sequence of characters in war and peace . after doing so is there a way i can take an arbitrary set of letters , e.g. hntask , and have it ( ideally ) tell me that the most probable sequence of that set is thanks ? also the algorithm must use only the characters in that set ; i.e. i do n't want it to guess that u comes after h , even if u appeared in the training examples . after each character it can only pick from remaining ones . i would assume one would somehow disable the illegal output neurons to do this . ( i know there are far simpler algorithms for transforming jumbled - up letters to english words . this is just a toy example for an important problem . )",9926,,,2017-09-30T05:06:09.487,can one use a trained recurrent neural network to find the most probable sequence of a set ?,recurrent-neural-networks,0,0,
942,4161,1,4264,2017-10-01T11:14:23.837,2,217,"my question assumes that a private researcher does n't have access to anything stronger than a modern pc with a high end gpu to implement his projects . he can also use cloud computing but with limited funds as well . is it still feasible to do research with those restrictions ? alphago used 1,202 cpus and 176 gpus to beat lee sedol . is this enormous power only required to achieve the final optimizations or have we already reached a state where high end research can only be done with larger funding ? please include in your answer examples of recent research results and the required infrastructure that was used to create them .",9161,,,2017-10-14T07:54:29.037,is private research with low funds still feasible in the field of ai ?,research,4,6,1
943,4162,1,4175,2017-10-01T12:17:42.547,2,136,i want to know what is the difference between a normal chip or processor and a processor designed for ai ?,9947,,,2017-10-03T02:35:47.710,hardware implementation of ann,neural-networks hardware,3,7,
944,4165,1,7338,2017-10-01T20:18:43.517,5,447,"it has been shown that it is possible to use unsupervised learning techniques to produce good feature detectors in cnns . i ca n't understand what drives specialization of those feature detectors . in publications https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf ( page 6 ) they show a very reasonable set of edge and blob detectors with little or no overlap . it goes against intuition - in absence of incentive to specialize , one would expect at least some duplication of learned kernels .",9803,9803,2017-10-04T06:02:22.887,2018-07-30T01:24:15.010,what makes learned feature detectors specialize in cnn ?,convolutional-neural-networks unsupervised-learning backpropagation,1,2,1
945,4167,1,4168,2017-10-02T02:14:48.800,1,493,"i am a first - semester grad student in robotics and have taken a course on machine learning for robotics . i am completely new to machine learning . i am to select and execute a problem statement on my own as a part of the course . i have selected the following problem statement - "" reinforcement learning for mapless navigation of mobile robots "" based on this research . the goal is to develop a mapless motion planner which enables a robot to navigate by avoiding obstacles . since i am completely new to machine learning and ai , i am not able to gauge whether the problem statement is challenging enough for a beginner to execute in a timeframe of 8 weeks ? ( given that i can dedicate around 6 hours a week ) i am planning to code in matlab since i am highly comfortable with matlab also , please leave any suggestions/ modifications to refine my approach and have a better understanding in this area . thank you !",9952,1671,2018-01-28T23:03:24.547,2018-01-28T23:03:24.547,reinforcement learning for robotic motion planning - problem statement ideas,deep-learning reinforcement-learning robotics matlab,1,2,
946,4170,1,,2017-10-02T16:51:04.940,6,362,"i 'm currently in the research stage of building a web app in asp.net where the user can input a url to an amazon product , then the app would determine how likely its reviews are to be genuine . i need help figuring out what algorithm to use in determining if a certain review is likely to be deceptive . i want my app to behave similarly to fakespot or reviewmeta . i realise a tool like this wo n't be 100 % accurate and that 's fine . so far i read parts of this book which seems to be recommended a lot to nlp newbies but have n't found anything that applies to such a specific problem . i also read this article but it 's based on hotel , restaurant and doctor reviews . i 'm trying to find a more general method that can be applied to any product . any help would be greatly appreciated !",9962,4302,2018-10-08T12:50:30.953,2018-10-08T12:50:30.953,how to determine if an amazon review is likely to be fake using text classification,machine-learning natural-language-processing algorithm classification,2,1,1
947,4176,1,4230,2017-10-03T09:42:23.333,5,12486,"this question covers in detail , what fuzzy logic is and how it relates to other math fields , such as boolean algebra and sets theory . this question is also very related , but the answers are focused more on general intuition and potential applicability . the only working system based on fuzzy logic , mentioned there , is mycin , which goes back to the early 70s . this quote from wiki summarizes my impression of it : mycin was never actually used in practice . from my experience in ai , the best tool to deal with uncertainty is bayesian probability and inference . it allows to apply not only a wide range of probabilistic tools , such as expectation , mle , cross - entropy , etc , but also calculus and algebra . can you call fuzzy logic a "" pure theoretical "" concept , which only played its role in the early development of ai ? are there real practical applications of fuzzy logic ? what problem would you recommend to solve and to code using fuzzy logic ?",9647,,,2017-10-08T22:48:07.803,are there real applications of fuzzy logic ?,ai-design applications problem-solving fuzzy-logic,3,0,6
948,4179,1,,2017-10-03T16:03:04.327,2,44,"i 'm currently developping an application which allows psychologists to manage their schedule and budget . as a proof of concept , i would like to create an intelligent appointment service . there can be 3 cases : i know the client , i need to guess the day and time for his next appointment i know the day , i need to guess which client and at what time i know nothing , i need to guess which client , which day and what time i 'm currently in the process of learning deep learning algorithms just to get a bit of theory , but it 's a little bit overwhelming . there are features i know i can extract from the appointments : day preference in the week ( always on monday , say ) reccurence ( every two weeks or such ) nb of days since last appointment whether the client was present or not to his last appointment etc .. i know there are things like "" features extraction "" that you can train a neural network to find the features itself , but all examples refers to image recognition or speech analysis . i want the algorithm to train on the existing and future appointments ( stored in a mongodb ) . i would also like that the algorithm trains live , that is if it proposes an appointment to the user and the user takes it , it should train positively . on the other hand , if the user navigates or change any parameter , the algorithm should adjust its weights accordingly . i also know i should start by extracting data from the db that will be transformed in a vector or matrix , then the algorithm is supposed to train on that data . is this correct ? how can i start and what kind of architecture do i need ?",9972,,,2017-10-03T16:03:04.327,recommendations on which architecture to use to guess appointment,neural-networks convolutional-neural-networks training recurrent-neural-networks backpropagation,0,0,1
949,4183,1,,2017-10-04T09:02:05.147,2,425,"i have to model an ai that should be able to understand clues and find the answer from a specified word database . i came across several papers that solves the problem with training neural networks or processing the clues with training several machine learning architectures on clues that exist on several databases . however , they all seem overkill for the course currently i 'm taking . so , what would be the most simple approach to solve that problem ?",9990,,,2017-10-05T02:42:46.113,solving crossword puzzles,neural-networks machine-learning ai-design training ai-community,1,2,
950,4186,1,5192,2017-10-04T17:01:41.900,2,658,"suppose i have cfg l that pumps out words in that language . i want a machine learning system that can detect if a word w came from l or not . it has access to a stream from l that is constantly producing words in l at random . in this case , the system can only be trained with positive examples . the system also has access to a verifier for l. the system ( if it chooses to ) can generate strings and verify that the string is in l or not . however , the system could just cheat and just use the verifier , so during the test phase , it gets disabled ( during the learning phase it use the verifier as much as it wants ) . moreover , l can be augmented and changed , and the system should adapt to that change . for instance , let 's consider this regex ( a regex is a cfg ) : ( aa ) * this language only produces a string of aa 's of even length . suppose i modified the regex to ( aa|bb ) * this system should adapt to recognize this new language . what kind of methods / approaches should i consider in my design ?",10001,9647,2017-10-05T16:40:51.353,2018-03-01T08:13:34.520,an ai system that learns the grammar of a cfg ( context - free grammar ),ai-design language-processing cfg,1,2,
951,4187,1,,2017-10-04T20:18:57.780,4,165,"let 's say we already have an intelligent system ( as a computer program ) . we give it some simple data from outside world , and system tries to predict it . the question would be : what kind of patterns this system should be able to recognize ? for example , system gets binary data stream(zeroes and ones ) . 1 ) should it recognize patterns like : 1 0 11 0 111 0 .... ? 2 ) should this system be able to recognize prime numbers in ascending order written with "" 1 "" with "" 0 "" between them ? is there any research about it ?",10004,1671,2017-10-05T20:26:27.880,2017-10-08T05:28:40.927,intelligence over simple data,reasoning pattern-recognition,3,3,0
952,4190,1,,2017-10-04T22:41:25.223,1,30,"being by no means an expert in the field , i do however know of solvers such as z3 that can generate a correct program provided you can express your constraints as a set of logical rules , such as here . however , in the example given above , logical rules are designed "" by hand "" from a set of examples , so the "" compilation step "" from the examples to the set of rules must be done in the user 's head . does a system automating this "" compilation step "" exist ( i.e automated rule set deduction from a set of examples ) , that would allow the user to see and make use of the set of rules ?",2364,,,2017-10-04T22:41:25.223,automated rule set deduction,logic,0,1,
953,4194,1,,2017-10-05T04:21:32.313,1,44,"image recognition can be used to classify images . but i wanted to find few parameters like height of person , his legs , his hand etc . will cnn helpful for this type of output ?",6212,,,2017-10-05T04:21:32.313,"can image recognition used to find height of a person whole , torso , legs etc",convolutional-neural-networks image-recognition,0,3,
954,4196,1,,2017-10-05T05:42:13.597,2,32,my network works on 32x32 normalized ( translationally ) but noisy images . its task it to determine whether image has simple symmetry ( horizontal / vertical ) . it needs to be reasonably robust to rotation ( up to 20 degrees ) . i approached this task with a simple perceptron - like net with 2 hidden layers . it performs reasonably well ( on limited amount of data that i have ) but i ca n't shake off the feeling that this design is absolutely the worst for what i need . it is hard to judge generalization capacity of the net ( overfits really easily ) it performs alot worse than a simple deterministic program that i wrote for the same task symmetry is such a simple concept but my neural network ( being feed forward type ) ca n't represent it efficiently . what i have in mind is a kind of rnn that decides what axis to fold the image along and then judging on how well folded parts of image match . are there papers on something like that ?,9803,,,2017-10-05T05:42:13.597,detecting symmetry in small images with rnn,classification computer-vision recurrent-neural-networks,0,0,1
955,4199,1,,2017-10-05T13:21:49.430,3,4582,"while working with darkflow , i encountered something that i ca n't understand . i understand that maxpooling with size=2,stride=2 would decrease the output size to half of its size . however , if the max - pooling is size=2,stride=1 then it would simply decrease the width and height of the output by 1 only . however , the darkflow model does n't seem to decrease the output by 1 . here is the model structure when i load the example model tiny-yolo-voc.cfg . source | train ? | layer description | output size -------+--------+----------------------------------+--------------- | | input | ( ? , 416 , 416 , 3 ) init | yep ! | conv 3x3p1_1 + bnorm leaky | ( ? , 416 , 416 , 16 ) load | yep ! | maxp 2x2p0_2 | ( ? , 208 , 208 , 16 ) init | yep ! | conv 3x3p1_1 + bnorm leaky | ( ? , 208 , 208 , 32 ) load | yep ! | maxp 2x2p0_2 | ( ? , 104 , 104 , 32 ) init | yep ! | conv 3x3p1_1 + bnorm leaky | ( ? , 104 , 104 , 64 ) load | yep ! | maxp 2x2p0_2 | ( ? , 52 , 52 , 64 ) init | yep ! | conv 3x3p1_1 + bnorm leaky | ( ? , 52 , 52 , 128 ) load | yep ! | maxp 2x2p0_2 | ( ? , 26 , 26 , 128 ) init | yep ! | conv 3x3p1_1 + bnorm leaky | ( ? , 26 , 26 , 256 ) load | yep ! | maxp 2x2p0_2 | ( ? , 13 , 13 , 256 ) init | yep ! | conv 3x3p1_1 + bnorm leaky | ( ? , 13 , 13 , 512 ) * * load | yep ! | maxp 2x2p0_1 | ( ? , 13 , 13 , 512 ) * * init | yep ! | conv 3x3p1_1 + bnorm leaky | ( ? , 13 , 13 , 1024 ) init | yep ! | conv 3x3p1_1 + bnorm leaky | ( ? , 13 , 13 , 1024 ) init | yep ! | conv 1x1p0_1 linear | ( ? , 13 , 13 , 125 ) -------+--------+----------------------------------+--------------- the bold text part is causing the confusion . my expectation what ( ? , 12,12,512 ) but it is not . it retains the same size ( 13,13 ) the corresponding model info from the .cfg file is : [ convolutional ] batch_normalize=1 filters=512 size=3 stride=1 pad=1 activation = leaky [ maxpool ] size=2 stride=1 why is the output height / width not decreasing by 1 ?",10019,,,2018-03-22T10:48:23.480,"max pooling size=2,stride=1 outputs same size",tensorflow,2,0,
956,4200,1,,2017-10-05T14:20:04.007,2,260,"after obtaining the result of algorithm fuzzy c means on the iris flower data set . results are to be checked for their correctness . so , how to derive it in matlab . i mean code for that . please suggest .",10020,1671,2018-01-28T23:02:55.883,2018-01-28T23:02:55.883,confusion matrix is useful to be applied to the result of fuzzy c means clustering or not,classification matlab,1,2,
957,4202,1,4208,2017-10-05T17:50:35.883,4,1139,"i was reading a little bit about the deepstack poker program : deepstack the first computer program to outplay human professionals at heads - up no - limit texas hold'em poker deepstack : expert - level artificial intelligence in heads - up no - limit poker the first article mentions processor used was an nvidia geforce gtx 1080 graphics card , but i 'm trying to get an understanding of how much memory this type of ai requires . related : how much has recent work in mathematical analysis of poker contributed to deepstack and other strong poker ai ?",1671,1671,2017-10-06T16:35:50.973,2017-10-06T16:35:50.973,how much memory does the deepstack poker program require ?,game-ai poker deepstack,1,0,1
958,4209,1,,2017-10-06T01:45:17.967,5,803,"i have been working for ages on a neuroevolution ai program , where cars learn how to race around a track . presently , i have a rudimentary fitness function that awards points for every degree traveled in the cw direction about the center of the window ( removes points in ccw dir ) and removes a certain amount of points for every collision that occurs . cars also lose points for every moment they stay still and/or are colliding with something . my end goal is to create cars that can complete a track full of obstacles , faster than a human controlled car . q : is there a better fitness function that would result in more efficient cars that : 1 . make better use of their sensors , 2 . are efficient in getting around the track ( do n't weave like a drunk driver ) and 3 . are faster in general ( cars are too cautious ... this is a race dammit ! ) ( half of the population seems to just spin around . ) edit : i have fully implemented my neuroevolution program . my implementation however , you 'll find that it is n't perfect . my question is how should i alter my fitness function to generate better driving cars . currently , the cars will only turn left or right if the outer sensors are activated . however , if the sensors directly in front are activated , the neural network is configured in such a way that "" ignores "" this signal . so the cars crash head - on into obstacles but avoid obstacles in their periphery . i think this is due to the fact that the fitness function ( which gives points for the displacement in 3 second time period ) is too generous and removes the incentive for cars to avoid obstacles . i 've tried altering the punishment for collisions and the reward for driving in the right direction but it still is n't performing the way i would like it to .",9708,9708,2017-11-03T02:59:07.010,2018-07-19T17:02:20.750,determining a fitness function for self driving car,neural-networks machine-learning genetic-algorithms unassisted-learning self-driving,3,6,
959,4211,1,4214,2017-10-06T09:16:12.273,4,189,"i recently started working on very simple machine learning codes in python and i came across a big problem : teaching the system to improve on its guesses . so this is what the code is about : i will have a list of organisms with their features stated in numerical values . i want to write a code that identifies that whether the organism is a cat or a fish or neither based on their characteristics . ( for example an organism with a high fur value and 4 legs is more probable to be a cat . ) my idea for the neural network is to have five input nodes(for the five characteristics ) and 2 output nodes ( one for how cat it is and one for how fish it is ) . the input nodes are multiplied by a weight value and then all summed together to produce one of the output nodes . this repeats itself for the other output . how much the system got wrong is just the difference between the value of the output nodes and how cat / fish the being actually is . but how can i use this information to correct the weights of the input nodes ? since the weights are randomly generated they can be in the "" wrong "" directions to begin with . for example if the subject is a cat then we should be expecting high fur and leg values . but what if the weight for the leg value is negative while the weight for the fur value is positive ? adding or multiplying the weight by the error wo n't bring us any closer to accurately determining the being . is my neural network flawed to begin with ? or is there a rule of thumb in choosing back propagation algorithms ? thanks .",10030,10030,2017-10-06T09:35:38.543,2017-11-12T16:20:24.247,finding an optimum back propagation algorithm,machine-learning backpropagation,1,1,
960,4213,1,,2017-10-06T14:03:38.610,4,186,"i 've been trying lately to search the internet to find a result for this but no useful results , unfortunately , we 're making an uber - like application during our discussion we needed a way to validate a rider 's rating after he completes his ride with a taxi , but the main question is , does uber use an algorithm to do so ? and how ?",10032,10032,2017-10-06T18:47:22.060,2018-04-16T17:58:15.187,how does ubers rating system algorithm work ?,algorithm,1,4,2
961,4215,1,,2017-10-06T14:47:48.343,4,365,"i 'm working on a system which reads large data from excel and tries to interpret something from it . if it does n't understand something , the user will correct it and slowly system will learn and try to do everything itself . is there any ai saas or anything that can help in this ? the use case is , i 'm reading bank statements from different banks and they all have different types of narration for similar transactions . i want to put them all into an accounting software without any human intervention .",10033,,,2018-01-08T11:36:56.540,what is the best ai or machine learning saas application ?,machine-learning,1,0,1
962,4216,1,,2017-10-06T18:00:12.530,2,60,"traditionally , "" strong ai "" refers to artificial general intelligence , the human mind understood as an algorithm ( searle , chinese room ) and artificial consciousness . but recent advances in artificial intelligence , most notably breakthroughs involving machine learning and neural networks , have led to strong ai being used in the context of an algorithmic intelligence that can outperform the top humans in specific tasks . ( this is sometimes qualified as "" strong narrow ai "" , but the qualifier seems less used time goes on . ) because the traditional usage of strong ai are based on theoretical concepts , where the current usage is based on practical , real - world results , does the new usage of "" strong ai "" supercede the traditional usage ? if this is the case , at what point does the wiki need to be updated ? is a major paper focused on re - coining the term necessary to formally reference the new usage of the term , or are numerous , informal uses sufficient ? related : "" strong "" has a precise , analytic , formal usage in the context of solved games . this game theoretic usage is similar in spirit to new usage of strong ai in that it connotes results that can be validated .",1671,1671,2017-10-06T18:46:13.040,2018-06-19T08:30:42.697,"is the traditional meaning of "" strong ai "" outmoded ?",strong-ai terminology history,1,0,1
963,4217,1,,2017-10-07T00:55:47.947,1,53,is there a technical term for an image classifier that classifies on a single class but is classifying on an amount like how full a glass of water is rather than different classes ?,2788,,,2017-10-07T17:06:30.263,is there a technical name for image classification on amount instead of class ?,deep-learning image-recognition,1,1,
964,4218,1,4648,2017-10-07T04:57:03.580,3,91,"i am having issues getting started with a multi class problem with multiple features and hoping someone could please point me in the right direction . i have data that is structured like this for training : item state code1 code2 code3 route --- --- --- --- --- --- item1 mi a1 33 blue route1 item2 tx a3 35 yellow route2 item3 nm a4 36 green route3 item4 nm a4 37 green route3 essentially i am trying to figure out where to even start . the goal is to know where to route the items based on the features state , code1,2 , and 3 . the route is dependent on a mix of the codes and state , and i want to build a model that says when i have code x , y , z and color xx , then it is probably route 1 ( some routes of course in the training data might have x , y as codes and a different z ) i am assuming i will need to one - hot encode the features like the state and codes ? but from there does anyone know of which type of model i should go for ? i would assume a neural net of some kind , i 've explored cnn 's and random forrest .",10040,,,2018-08-20T19:21:13.893,method for multi - class / category ?,neural-networks convolutional-neural-networks classification,3,3,
965,4219,1,4270,2017-10-07T09:54:51.200,5,2576,i am looking for something similar to ibm watson but open source .,10046,4302,2018-10-08T12:23:43.270,2018-10-08T12:23:43.270,is there any open source counterpart to the ibm watson ?,ai-design natural-language-processing watson,4,1,8
966,4223,1,,2017-10-08T04:58:09.930,3,763,i 'm trying to develop a project that recognises handwriting and converts into text . what are the algorithms and tools to be used ?,10061,9210,2017-11-09T19:05:59.973,2017-11-09T19:05:59.973,how to recognise handwriting and convert into text ?,pattern-recognition,1,3,
967,4233,1,,2017-10-09T06:04:50.650,4,1582,"how to train darkflow for my custom object really really fast during debugging in quad core pc and without gpu ? ( can i train with about 10 images and test with only those images , just to check if all convolutions are working as expected . and with 20 epoch ? ) . there is only once class and all license plates are similar in pattern with varying in angle and its digits in plate . i am using tiny yolo config and weight . so , what all parameters i should tune in yolo based .cfg file to do it ? i feel if tv like object can be detected with same training weight and config for tiny yolo then license plate too . overview : i am training for object , license plate , with darkflow . i tried with about 100 custom datasets i had created . this is only for initial poc , actual implementation will have more number of images . and upon testing with test images with trained graph , object is not highlighted rather highlighting squares are shown at random location within image and random in number , starting with 2 - 3 square boxes to lot many number . but non of those were highlighting to the actual license plate object . it took me 20 hour of training time to verify it . i used training images for testing as well , and also a plane black screen images to test what 's going on . but highlighting squares are still random in number and location even on blank screen image .",10083,,,2017-12-24T07:47:56.887,how to train for own dataset really really fast while debugging,deep-learning convolutional-neural-networks tensorflow,1,0,
968,4234,1,4249,2017-10-09T08:51:00.787,2,103,"i recently became interested in how creativity is generated in nn . my understanding of nns is that the output always known , given that it they are trained with target values , but how does one train a network to be creative , i mean in such cases would the task be to create something novel , but the actual target would not be known . how does creativity express itself in a rule based system , such as nn ? one would need to redefine the cost - function , but how does one imply creativity in a rule based system ?",6517,1671,2017-10-11T18:16:32.163,2017-10-11T18:16:32.163,how is creativity generated in a currently rule based neural network ?,ai-design,1,1,1
969,4236,1,4292,2017-10-09T13:13:35.987,-2,42,"i 'm working on a content based recommendation engine for ebooks . i create document vectors with 300 features for every ebook using a word2vec model trained on google news and determine recommendations based on the closest vectors . so far i have run tests on a dataset consisting of 200 books from the gutenberg project in four different categories . i find that a small group of books appears to be recommended a lot more then others . the most recommended book is recommended for over half the dataset . this book is theaetus , in which the most commmon tokens are fairly specific to the book ( tokens like socrates , theaetus ) . i can find no intuitive reason why this book would match so well with over half my dataset . is this common behavior when using document vectors to determine similarity ? are there any methods that would reduce this effect ?",10092,2444,2019-04-16T22:38:27.297,2019-04-16T22:38:27.297,over - exposure of certain items in content based recommendation engine,natural-language-processing word-embedding recommender-system,1,2,
970,4238,1,,2017-10-09T20:34:23.590,1,106,"i 've gotten curious about this topic and am wondering what the stack exchange community has to say about it . also , does anyone know of any professors / researchers who have published papers pertaining to this ?",10101,,,2017-10-13T09:02:38.120,to what extent can artificially intelligent agents reliably predict trends in financial markets ?,research multi-agent-systems real-time,3,1,
971,4245,1,,2017-10-10T16:33:45.657,2,182,"how are the layers in a encoder connected across the network for normal encoders and auto - encoders ? in general , what is the difference between encoders and auto - encoders ?",10118,2444,2019-04-14T10:02:07.033,2019-04-19T22:53:19.563,what is the difference between encoders and auto - encoders ?,machine-learning deep-learning difference autoencoders,3,0,
972,4247,1,,2017-10-11T00:55:26.993,2,660,"i am trying to replicate the fully convolutional networks ( fcn ) concept described here for semantic segmentation . it seems people have successfully trained such models by removing fully connected layers from the popular pre - trained models such as vgg and adding upsampling and unpooling layers . i understand that transpose convolution and unpooling in upsampling layers provide counterparts of convolution and max ( or average ) pooling in earlier downsampling layers respectively , but what are the counterparts for non - linearities such as relu ? what about dropout ? there seems to be no discussion of this in the video .",9653,2444,2019-05-04T15:13:34.840,2019-05-04T15:13:34.840,what are the counterparts of non - linearities and dropout in fully convolutional networks ?,deep-learning convolutional-neural-networks activation-function architecture dropout,1,0,1
973,4250,1,,2017-10-11T16:01:18.823,2,128,"i was going through "" wide & amp ; deep learning "" tensorflow tutorial & amp ; it 's quite simply explained the process . but i missed few of the things . if someone can please explain them to me , it will be of great help : 1 ) why occupation and native_country we are using tf.feature_column.categorical_column_with_hash_bucket and again using tf.feature_column.embedding_column ? 2 ) why in tf.feature_column.embedding_column , we are taking dimension=8 , even though they have more unique values ? 3 ) why we are using crossed_columns variables ? in the document there 's an explaination given . but i 'm not fully understanding it . the questions might sound silly , but i 'm not sure of the answers to these questions . thank you !",9664,7550,2017-10-12T06:42:08.630,2017-10-12T06:42:08.630,wide & deep learning explanation,neural-networks tensorflow,0,0,
974,4251,1,,2017-10-11T20:11:41.450,4,2519,"how can l leverage artificial intelligence and virtual reality to create intelligent automatic story generation ? my idea is to come up with a system that is empowering the user ’s imagination to create and experience stories that engage and immerse them . so they can have the control over the story creation / narrative ? a system that could enable the followings use case : 1- we sit down in our entertainment room . select a genre from a menu . pick the character types for the cast . decide on a generic plot or give the system the option of picking a random choice . sit back and enjoy . 2- let 's say , i want to watch a sci - fi movie set in paris with marlon brando and jim carrey . then the system instantly generate such movie . it has come to my attention that soartech developed a pattern - of - life capability for the u.s . army simulation : agent - based patterns of life for virtual environments that the university of twente in netherlands has been working on a project called the “ virtual storyteller ” . here is a research paper entitled the automatic generation of narratives . and more recently that ibm has developed a speech recognition sandbox i know that the answer probably requires tech , ai , speech , vr and ar , and watson integration specialists but any help is welcome . watson probably wo n't be a necessary component of the solution but as long it us an efficient voice recognition software with speech to animation features .",10143,1581,2018-11-01T17:49:26.643,2018-11-01T17:49:26.643,automatic story generator,ai-design natural-language,1,2,4
975,4257,1,4262,2017-10-12T21:46:05.993,2,160,"for example ... 1 ) if a dog is crossing the road , i 'd expect the car to try to avoid it . but what if this leads to .00001 % more risk for the driver ? what is the ' risk cut - off ' ? 2 ) what if a cockroach is crossing the road ? will the car have a list of animals okay to run over ? 3 ) what if a kid is crossing the street and avoiding it would kill the driver ? these questions seem to not really have an answer , yet self driving cars are almost ready . what are they doing about all of this ?",9648,,,2017-10-20T21:26:57.540,how will morality questions be settled in the domain on self - driving cars ?,ai-design self-driving,4,7,1
976,4271,1,,2017-10-14T16:25:09.157,3,122,"i want to use deep learning to estimate the value of a function based on some data . however , the loss function would be neither convex nor concave . can i know if it is a big deal in deep learning ? is training a deep network , when loss function is convex , the same as optimizing a convex problem or not ? i would be thankful if any paper has addressed this issue .",10191,,,2018-06-01T23:20:55.497,non - convex loss function in deep learning is a big deal ?,deep-learning,1,0,
977,4273,1,,2017-10-14T22:41:18.833,1,606,"if you walk your dog , he 'll be happy . if you do n't walk him , he 'll chew up your furniture . if he 's happy or if he chews your furniture , he 'll mess up your apartment . if he messes up your apartment , you 'll be annoyed . does your dog mess up your apartment ? are you annoyed at him ? write the knowledge base given above in propositional logic . convert the knowledge base into conjunctive normal form . use resolution to prove by contradiction that your dog made a mess . connect the two clauses to a new clause that the can be collapsed into ( see the example figure ) . example figure :",10195,,,2018-12-04T18:39:34.637,writing a knowledge base ( kb ) in propositional logic and converting the kb into conjunctive normal form,logic,1,0,
978,4279,1,4293,2017-10-16T10:29:30.813,7,128,"i 'm looking to design a neural network that can predict which runner wins in a sports game , where the amount of runners varies between 2 - 10 . in each case , specific data about the individual runners would be fed into the neural network . what design would be most advantageous for such a neural network ? essentially this is a ranking problem where the amount of inputs and outputs are variable .",4036,1671,2017-10-18T20:17:41.033,2017-10-18T20:17:41.033,neural network design when amount of input neurons vary,recurrent-neural-networks,1,6,
979,4280,1,,2017-10-16T12:33:16.407,1,35,"is there any existing software which analyzes idea flow in the text ( or between different texts , e.g scientific articles ) and for example visualizes it with some graph ?",10211,,,2017-10-16T12:33:16.407,any software to guess idea flow ( basic ideas / statements and their derivations ) throughout text ?,natural-language-processing text-summarization,0,0,1
980,4282,1,,2017-10-16T18:10:03.370,9,7532,i have a large dataset of vehicles with the ground truth of their lengths ( over 100k samples ) . is it possible to train a deep network to measure / estimate vehicle length ? i have n't seen any papers related to estimating object size using deep neural network .,10218,,,2019-01-18T19:58:23.217,measuring object size using deep neural network,machine-learning deep-learning computer-vision,3,2,3
981,4283,1,4837,2017-10-16T20:28:28.607,6,1881,"i 'm working on a image classification problem using neural - network . in the training data set , 90 % of the samples fall into 10 % of all categories , while 10 % of the sample fall into the other 90 % categories . so example is not evenly distributed among all categories . if we assume this distribution reflects the real world distribution , do i need to filter my dataset before training so that each category has similar number of samples ? thanks a lot !",10221,,,2017-12-24T17:32:11.583,does data skew matter in classification problem ?,neural-networks classification,2,3,1
982,4286,1,,2017-10-17T08:46:36.187,1,169,looking for an explanation of the linear regression estimation method in deep learning .,10229,4398,2018-01-17T23:05:26.113,2018-03-19T04:39:24.377,is deep learning the repeated application of linear regression ?,machine-learning deep-learning algorithm linear-regression,2,3,
983,4291,1,,2017-10-18T03:16:20.737,1,534,i selected the below data set for forecast and predict using artificial neural network as my final year project . https://archive.ics.uci.edu/ml/datasets/bank+marketing . i normalized the data set and try to train the network using matlab anntool . but i did n't get a good result . the data did n't plot along the curve . i want to train the network and predict for new input values . what is the matter with my network train . please help me .,10241,1671,2018-01-28T22:44:11.207,2018-01-28T22:44:11.207,forecasting and predict using matlab artificial neural network,neural-networks backpropagation prediction matlab data-science,0,2,
984,4294,1,,2017-10-19T05:11:43.337,5,214,"i ’ve been experimenting on several datasets and found something very strange while implementing ml . i ’ll explain after the code … import numpy as np from sklearn import datasets iris = datasets.load_iris ( ) # 4 features in np array - 150 rows case = 1 # change cases to see variation if case = = 1 : # first feature deleted iris.data = np.delete(iris.data,0 , 1 ) if case = = 2 : # first 2 features deleted iris.data = np.delete(iris.data,0 , 1 ) iris.data = np.delete(iris.data,0 , 1 ) if case = = 3 : # first 3 features deleted ( 1 feature left ) iris.data = np.delete(iris.data,0 , 1 ) iris.data = np.delete(iris.data,0 , 1 ) iris.data = np.delete(iris.data,0 , 1 ) if case = = 4 : # only second feature deleted from np array iris.data = np.delete(iris.data,1 , 1 ) if case = = 5 : # only third feature deleted from np array iris.data = np.delete(iris.data,2 , 1 ) if case = = 6 : # only last feature deleted from np array iris.data = np.delete(iris.data,3 , 1 ) # print iris.data # exit ( ) from sklearn.naive_bayes import gaussiannb gnb = gaussiannb ( ) pred = gnb.fit(iris.data , iris.target).predict(iris.data ) # pred = gnb.fit(iris.data , iris.target).predict(test_data ) from sklearn.metrics import accuracy_score print accuracy_score(iris.target , pred ) i ’m using the basic fisher iris dataset from sklearn , it has 150 rows and 4 columns ( features ) . using training data as test data . so i tried to remove a few features and see if accuracy changed . and i thought it would . but till case 1 , 2 , and 3 , i removed 1 , 2 and 3 features respectively and there was no change in the accuracy . it stayed 96 % . then on running cases 4 , 5 and 6 , the accuracy changed . why ? on comparing case 2 and 4 , both have second feature removed from the dataset , so clearly , removing second feature is responsible for change in accuracy ( as seen in case 4 ) so why does it not change in case 2 ? just because it had first feature removed too ? to balance out the second ? ( if that were true , case 1 would have given different accuracy ) why does accuracy not change in first 3 cases , but it does in the last 3 cases ? is ml dependent on the order in which features are feeded to the algorithm ? what am i missing here ? would be great if someone could clear this doubt . thanks !",10258,,,2017-10-19T09:05:02.993,is accuracy of a machine learning algorithm independent of the features ?,machine-learning,1,1,1
985,4296,1,4314,2017-10-19T10:47:53.943,1,188,"in the paper parameter space noise for exploration , the authors describe the noise that they add to the parameter vector as : $ $ \tilde{\theta } = \theta + \mathcal{n}(0 , \sigma^2i ) $ $ is $ i$ simply the identity matrix , or am i missing something ?",10261,2444,2019-04-01T13:52:40.123,2019-04-01T13:53:32.710,"what is $ i$ in the noise described in the paper "" parameter space noise for exploration "" ?",reinforcement-learning notation,1,1,
986,4297,1,4316,2017-10-19T12:39:44.217,1,208,"without using any of matlab 's neural network tools , i 'm writing a program to simulate an or gate with a perceptron . i have seen many tutorials , but i still ca n't understand why we need weights to train a perceptron for such a simple purpose . one way is to program the perceptron with the conditions ( 0,0)=0 . ( 1,0)=1 . ( 0,1)=1 . ( 1,1)=1 . so the two inputs to the perceptron would be either zeroes or ones . i do n't see the purpose of weights here . assuming weights are 1 , for the second training example , the output would be 1 * 1 + 0 * 1 = 1 . for the last example , it would be 1 * 1 + 1 * 1 = 2 . so an activation function which says if output & gt;= 1 , output = 1 else output = 0 ; end should suffice . this would successfully simulate an or gate . so why do i need to "" train "" any weights ?",9268,9268,2017-10-19T13:38:13.523,2017-10-20T21:29:03.757,why do we need weights when training a perceptron as an or gate ?,neural-networks artificial-neuron,2,3,
987,4298,1,,2017-10-19T14:03:09.607,6,555,"in the hill climbing algorithm , the greater value , compared to the current value , is selected , but i can not understand why it takes the larger value instead of the smaller one . why is that ? i greatly appreciate the inclusion of figures in your answers .",10265,2444,2019-03-02T10:54:30.007,2019-03-02T10:54:30.007,"why is the larger value , as opposed to the smaller one , chosen , in the hill climbing algorithm ?",algorithm search hill-climbing,2,1,2
988,4300,1,,2017-10-19T20:35:42.060,0,58,i 'm starting to get into the ai field and wanted as a side project to try and make an algorithm to play old games . the problem is i need to pass everything that is happening on the screen to the program and there is no real api to do that . does anyone know of a way to get the input any other way than using image recognition ? thank you in advance .,10271,,,2017-10-20T02:00:49.240,getting input about the enviorment from a game emulator,genetic-algorithms,2,1,1
989,4301,1,4302,2017-10-19T21:39:04.103,1,31,"in my research on video games path finding i 'm using ant colony optimization , not only to find the shortest path , but also to add some unpredictability and adaptiveness to bots path finding . it works the way as players move in the map , they add some pheromone to the map , so it adds up a probability that bots choose path like players . i have sent the paper , but judges said you need a benchmark and distinguish to previous works . can you tell me how can i benchmark this work ?",10273,2444,2019-05-27T22:04:33.227,2019-05-27T22:04:33.227,how can i benchmark an application of ant colony optimization ?,game-ai applications swarm-intelligence ant-colony,1,0,
990,4307,1,,2017-10-20T11:10:19.403,0,37,"i am new to this field and would like to know that for what kind of data types other than images , recommendation system can be created using machine learning . suppose for contents like audio or video , is it necessary to use data set of actual audio files or video files or just text information about the file is enough for this system ?",10279,16565,2019-04-04T20:38:06.403,2019-04-04T20:38:06.403,recommendation system based on content type,datasets recommender-system,1,0,
991,4309,1,,2017-10-20T15:25:36.953,1,16,"i 've been tinkering with an artificial life simulator , critterding for many years and lately i ve been branching the project and adding fitness functions . i m still quiet new to genetic algorithms and currently critters in the simulation have very complex and different sized dna files which i want to be able to cross , currently they can only reproduce asexually and evolve purely through mutation . for example , thier genome consists of : box part dimensions joint locations and angle constraints which connect those boxes and a list of neurons , thier properties and synaptic connections the number of boxes and thier joints can change by mutation as well as the number of neurons and synaptic connections and so i m having a hard time trying to think up a crossover approach which can handle these variables . so my question is , can anyone point me in the right direction for research material or give me insight into handling such dna structures for crossing ?",10282,,,2017-10-20T15:25:36.953,crossing complex and non uniform genomes ?,genetic-algorithms,0,0,1
992,4320,1,4334,2017-10-21T06:52:15.723,9,476,"this might sound silly to someone who has plenty of experience with neural networks but it bothers me ... i mean randomizing initial weights might give you better results that would be somewhat closer to what trained network should look like , but it might as well be the exact opposite of what it should be , while 0.5 or some other average for the range of reasonable weight value would sound like a good default setting ... why do initial weights for neurons are being randomized rather than 0.5 for all of them ?",10294,10294,2019-04-23T18:47:16.207,2019-04-24T07:26:02.753,why initial weights in neural network are randomized ?,neural-networks training,3,5,
993,4321,1,4325,2017-10-21T07:20:18.113,1,173,to classify images we are using the tensorflow incection v3 nn . is there a similar approach to classify sounds ? as for example to be able to recognize the person who is talking or classify a general sound ?,8546,,,2017-10-22T02:41:21.660,is there a way to classify sounds,classification tensorflow voice-recognition,1,1,
994,4322,1,4324,2017-10-21T07:39:59.093,2,179,"i understand that in reinforcement learning algorithms such as q - learning , to prevent selecting the actions with greatest q - values too fast and allow for exploration , we use eligibility traces . here are some questions ; does epsilon - greedy solve the same problem ? do these two approaches aim to attain same objective ? what are the advantages of each over the other ?",3017,4398,2017-10-23T22:48:49.130,2017-10-23T22:48:49.130,does eligibility traces and epsilon - greedy do the same task in different ways ?,algorithm reinforcement-learning unsupervised-learning,1,0,
995,4327,1,4348,2017-10-22T10:37:32.703,2,75,"ais that rely on mcts - like alphago - create their decision tree as the game progresses . do they start from scratch each game and build a new tree or do they keep the tree and grow it from game to game further ? besides the possible limitations of storage space for the search tree , i do n't see any obvious drawback in keeping and growing the tree , which seems to me to be the preferred option . are there other reasons to start from scratch each game ?",9161,,,2017-10-24T14:21:55.303,do ais based on mcts start each game from scratch ?,ai-design monte-carlo-tree-search,1,1,
996,4328,1,,2017-10-22T14:34:06.383,5,1636,i know a little about these subjects . i found them similar to each other . anybody can explain the differences between them ?,10314,2444,2019-02-19T14:47:23.080,2019-05-20T07:21:51.233,"what are the differences between machine learning , pattern recognition and data mining ?",machine-learning pattern-recognition difference data-mining,5,0,
997,4329,1,,2017-10-22T15:04:34.087,1,149,"i need to write a recurrent autoassociative network in python . i have read explorations in parallel distributed processing : a handbook o f models , programs , and exercises but i ca n't understand how are the activation updating cycles written . any information about this would be helpful . specially if i could find an algorithm for this .",10316,,,2017-10-22T15:04:34.087,how to write a recurrent auto associative network ?,neural-networks,0,1,
998,4330,1,,2017-10-22T15:58:05.937,0,655,does fitness proportionate selection select multiple individuals ? so i read on wikipedia and on multiple stack exchange threads about fitness proportionate selection or rather roulette selection but what i do nt understand is how do i not select multiple of the same individuals ? should i pop them from my array and recalculate the probabilities or after selection remove duplicates ? or is there some sort of purpose for having multiple of the same selected ?,6391,,,2018-02-26T05:58:41.617,does fitness proportionate selection select multiple individuals ?,algorithm genetic-algorithms evolutionary-algorithms,1,0,
999,4332,1,4909,2017-10-22T20:23:35.193,0,182,i want to create a project where my raspberry pi micro - controller can listen to my parrot talking and then talk back to him . i would need my rpi to listen to some of his phrases at least 100 times so that the pi could recognize that certain phrase and then say it back to him and also to know when to use it to talk back to him . i would also like to use it to teach him new phrases and new sounds . i would like to know if this is even possible and what hardware and software i would use to go about this ? i have already asked this in the raspberry pi stack exchange but have recieved no answers .,10324,,,2018-01-03T06:29:05.930,can the raspberry pi do audio learning ?,machine-learning emotional-intelligence,2,1,
1000,4338,1,,2017-10-23T08:57:52.940,1,316,"i want to implement dsd : dense - sparse - dense training for deep neural networks by han et al . in short , the paper suggest the following training scheme to improve the network accuracy : train as usual till convergence , prune the network and train with sparsity constraint , remove the sparsity constraint and let the pruned connections to recover . my question is about step 2 : train with sparsity constraint . the paper mentions training with a binary mask specifying the pruned weights to keep "" untouched "" so the sparsity constraint is satisfied , however that means implementing a dedicated layer that takes the binary mask as an additional blob and handles it accordingly . i wonder a simpler approach will give the same result : what if after the pruning step i keep the location of the pruned weights and then use dense training , but after each iteration to zero the originally pruned weights ? the forward path is the same , taking zero weights for the pruned weights anyway . but would it negatively affect the backward path - since the constraint is n't there , or is it equivalent to the formal training scheme ? thanks .",9233,9647,2017-10-25T06:29:27.027,2017-10-25T06:29:27.027,dense - sparse - dense cnn training,deep-learning deep-network training,1,0,
1001,4343,1,,2017-10-24T06:17:26.953,1,132,"i have a liberal arts background so i need help understanding this paper , particularly pages 26 to 30 . the authors test a four - camera system for localization , mapping , and obstacle detection for self - driving cars . the paper seems to say the multi - camera system can map the environment to within an average of 7 cm ( 2.8 inches ) of accuracy ( with the largest error being 16 cm or 6.3 ) and detect obstacles to within 10 cm ( 3.9 inches ) of accuracy . am i getting this right ? given that automotive lidar can detect objects to within 1.5 cm ( 0.6 inches ) of accuracy , and given that for driving purposes the difference between 1.5 cm and 7 cm , 10 cm , or 16 cm seems quite small , can a multi - camera system be used instead of lidar in a self - driving car application ? how do driving speeds affect things ? what crucial elements of the problem space might i be overlooking or misunderstanding ?",10350,,,2017-12-25T07:19:49.200,"‪can a multi - camera system be used for localization , mapping , and obstacle detection in self - driving cars to within 10 cm of accuracy ? whither lidar?‬",self-driving,2,0,
1002,4344,1,,2017-10-24T08:23:13.753,1,135,"i want to train a feedforward neural network to play a video game called puyo puyo 2 , using reinforcement learning . more specifically , i 'm trying q - learning but i 'm open to better alternatives . in this video game , you start with an empty 12x6 board which you fill with falling pairs of colored blobs called puyos . when 4 or more puyos of the same color are connected horizontally and/or vertically , they disappear and the puyos above them fall down , potentially creating new groups of puyos that disappear and so on , creating a chain ( here is a chain example : https://puyonexus.com/chainsim/image/5soqu.gif ) . my goal is to teach the neural network to build the longest chains possible given a random sequence of puyo pairs . the game 's information is incomplete though , as you only see the current and the next two pairs at a given time . what is the best choice for the output layer : a layer with 22 neurons , one for each possible way to place a pair at a given time , and a softmax activation function , or a layer with a single output node , with a linear activation function ? or something else ? also what do you think is a good number of hidden layers and neurons per layer and what activation function should i use for the hidden layers ? what should my target q - value be when i evaluate a move , so that i can train the network on each game situation ? right now i 'm thinking about using the maximum chain possible after the move is done , divided by half of the number of moves played so far . this is because if you play optimally and you are lucky enough , you can increase your maximum chain by 1 every 4 puyos you place , that is every two moves . thus my q - value would stay between 0 and 1 . what is the best update formula i can use ? is the one on wikipedia correct ? if i use a softmax output layer , how do i update the target q - value vector ? the update formula gives me an update but then the vector does n't represent probabilities anymore . can i add the update then take the softmax of the vector again just to make it probabilities again ?",10355,1671,2017-12-21T19:58:00.350,2017-12-21T19:58:00.350,help with implementing q - learning for a feedfoward network playing a video game,neural-networks machine-learning reinforcement-learning game-ai q-learning,0,0,
1003,4346,1,4347,2017-10-24T13:34:38.233,6,747,"i read through the publication mastering the game of go without human knowledge . it does n't seem to use gans , just a new form of search and reinforcement learning .",10359,1671,2017-12-22T17:38:09.817,2017-12-22T17:38:09.817,is the new alpha go implementation using generative adversarial networks ?,deep-learning ai-design game-ai unsupervised-learning go,1,0,
1004,4354,1,,2017-10-24T21:35:13.083,1,56,"it appears to me that the thoughts in our heads or at least mine are a result of the language they know . i do not have any thought that is not a consequence of the information that is contained within the language i know . i have thoughts that occur as the words we use in language and are particular to my voice the structure of that language forms constraints as to the depth of thought that my mind can possibly come up with as far as i can percieve . does this mean that an intelligence is limited to thoughts it 's able to associate words with since the minds language is a result of an recursive association of words associated to those thoughts which depend upon a leap of faith between the observer and an undeveloped mind . or is there simply a limit of conception bound by a language that is limited by its dictation and consequently a limit upon the perception of that so called intelligence . and thus a limit on any artificial creation or simulation of it bound by limits of dictation within that language and it 's knowledge of it , and does n't this mean that thoughts structure develops knowledge of language by association of the physical reciept of information external to it . so how do you approach an attempt at it 's recreation through a formal language that allready has its own structure .",9094,9094,2017-10-24T21:43:19.943,2017-10-25T13:33:14.620,consiousness and language,natural-language,2,0,
1005,4357,1,,2017-10-25T03:09:00.450,2,114,"so , i need to use an mlp to predict a 12x12 matrix composed of floating points . the matrices are as this one that follows : most matrices have this "" pattern "" . as input , i have 7 floating points , such as these : "" 2.0 "" , "" 0.23 "" , "" 239.10"",""3.5 "" , "" 12,0 "" , "" 10.6 "" , "" 0.62 "" . to simplify the output matrix , i 've converted it in a array with the 144 elements of the matrix . so far i 'm using mlpregressor from scikit.the problem is , there 's absolutely no pattern in the predicted results.the predicted results include negative numbers , numbers really big and no "" pattern "" for the indexes whatsoever . is there a way to adjust these things in the model or the problem is on my dataset ? thank you very much ! update : i was n't very clear about the problem , so i 'll try to explain it better . about the problem : predict water distribution in irrigation systems . i already have a program to simulate this , and works with rather good precision . the challenge now was to make an ai model to simulate this , instead of the old methods . about the inputs : inputs are one float representing the pressure of the sprinkler used to distribute water , two floats representing the speed and angle of the wind at the time data was collected . the 4 remaining are numbers representing some configuration of the sprinker ( these numbers change very little between instances , but they do have impact in the result , which is why i decided to mantain them in the dataset ) . about the output : output is an array of 144 elements , representing the 12 * 12 matrix . each element of the matrix contain a number representing the amount of water that was collected in that point . these collectors where evenly spread around the sprinkler . so the position of the matrix matter a lot , since in most cases the first and last lines and columns will have 0 or a close to 0 ( but positive ) number - this may vary depending especially on the speed and angle of the wind , but also on the sprinkler . about the dataset : i have available 75 instances . they are all stored in a csv file , where the 7 inputs and the 144 outputs are , each in one line .",9934,9934,2017-10-25T16:47:03.443,2017-10-25T16:47:03.443,using a mlp to predict a 12x12 matrix,mlp,1,3,1
1006,4361,1,4372,2017-10-25T09:51:34.497,1,102,"i 've been researching the following topic . or rather , i would like to but i ca n't find anything because i 'm not sure what to look for . i am interested weather there are some concepts or models that explain how humans ( or cognitive systems in general ) trade off remembering what to do in a particular situation versus thinking about the board state and devising a new solution . consider the following example : a person plays chess . on their turn they must decide what to do . they could act according to one of the following extreme strategies : remembering all possible board configurations and how the game ended ( we 'll call that memory ) use the known set of rules to simulate the game in their mind and choose the optimal path ( optimization ) i 'd say the best choice is a trade off between these two . but how ? when to choose one of the strategies ? is there a research field that is working on these kind of questions ? what would i look for ?",2937,2937,2017-11-02T08:41:52.233,2017-11-02T08:41:52.233,"trading off "" memory "" vs "" optimization """,human-like learning-algorithms chess combinatorial-games,2,1,1
1007,4365,1,,2017-10-25T15:04:00.207,3,137,"i have a huge text where a paragraph is written about a project execution . i need to parse that data and make sense out of it . for example the text states , "" est . 200k usd has been committed on the event of sales summit on 8th sep "" by this statement , i need to make sense of it as approx 200k usd is the revenue , there was a sales summit held on 8th sep , the amount is already committed . likewise many such statements will be there . i need to parse and make a conclusion out of it . is there anything anyone suggest ? what i have tried so far : am trying to find a text analytics tool which will group the text into few classifications . next am trying to write my own logic to tag each paragraph to certain master data . also trying to use a tool by name orange which is currently helping visualize the data , but not make sense of it . i think i need to be doing something with machine learning algorithms . not sure though .",10389,1581,2018-11-13T17:20:31.280,2018-12-13T20:01:35.283,text analytics for text analysis,machine-learning data-science,1,4,2
1008,4366,1,,2017-10-25T21:16:48.807,1,60,"i 'm running a3c ( asynchronous actor - critic agents ) to learn a game where an agent needs to catch 3 rewards . the input of my network , among other things , is the relative position of the 3 rewards against the agent . however , as the agent catches these rewards , my network ca n't just decrease its input size . what are ways of handling this ? my current solutions have been : set remaining features to zero - have tried , bad results just show the closest reward - loss of information , might not be optimal repeat existing reward positions to remaining features - we are giving more emphasis to some rewards ( the ones that are repeated ) over others",7496,1671,2017-10-26T20:35:04.420,2017-10-26T20:35:04.420,handling varied - size input with fixed - input network,neural-networks reinforcement-learning,0,2,
1009,4371,1,,2017-10-26T20:07:45.940,4,28,"i was wondering if anyone can think of any examples of integration of audio - visual inputs , or other modalities , failing or resulting in undesired outcomes in artificial cognitive systems ? i 'm interested in any issues , or artifacts , that have arisen when different information about the same event have been integrated in such a system . published , or discussed in print online , or even a ted talk would be great sources if you know of any . anecdotal evidence of such scenarios that you may have experienced are also of interest . thank you all for your time .",10419,,,2017-10-26T20:07:45.940,"examples of "" crossmodal "" integration failures in artificial cognitive systems",cognitive-science,0,0,
1010,4373,1,,2017-10-26T21:55:45.817,1,31,could a multi - camera slam system for self - driving cars that is accurate to under 10 cm ( 3.9 in ) at parking lot speeds ( i.e. very low driving speeds ) retain this level of accuracy at high driving speeds ( e.g. highway speeds or city driving speeds ) if the cameras were a ) 60 fps and b ) had a global shutter ?,10350,,,2017-10-26T22:21:49.140,could a multi - camera slam system that is accurate at low driving speeds be equally accurate at high driving speeds ?,computer-vision robots,1,0,
1011,4375,1,4401,2017-10-27T10:24:37.183,8,7003,"i talk about the robot from : hanson robotics , which was granted the right to citizenship from saudi arabia . i have found the following articles : your new friend is a humanoid robot source : theaustralian.com.au like amazon echo , google assistant and siri , sophia can ask and answer questions about discrete pieces of information , such as what types of movies and songs she likes , the weather and whether robots should exterminate humans . but her general knowledge is behind these players and she does n’t do maths . her answers are mostly scripted and , it seems , from my observation , her answer are derived from algorithmically crunching the language you use . sometimes answers are close to the topic of the question , but off beam . sometimes she just changes the subject and asks you a question instead . she has no artificial notion of self . she ca n’t say where she was yesterday , whether she remembers you from before , and does n’t seem to amass data of past interactions with you that can form the basis of an ongoing association . questions such as : “ what have you seen in australia ? ” , “ where were you yesterday ? ” , “ who did you meet last week ? ” and “ do you like australia ? ” are beyond her . why sophia the robot is not what it seems source : smh.com.au you can often fool this sort of software by introducing noise . that could be literal noise – machines are n't great at filtering out background noise , as anyone with a hearing aid will tell you – or it could be noise in the sense of irrelevant information or limited context . you could ask "" what do you think of humans ? "" and then follow up with "" can you tell more about it ? "" the second question requires the robot to define "" it "" , remember what it said last time , and come up with something new . in the case of the abc interview , the questions were sent to sophia 's team ahead of time so they were possibly pre - scripted . just like an interview with a human celebrity ! pretending to give a robot citizenship helps no one source : theverge.com sophia is essentially a cleverly built puppet designed to exploit our cultural expectations of what a robot looks and sounds like . it can hold a stilted conversation , yes , but its one - liners seem to be prewritten responses to key words . ( as piers morgan commented during an interview with sophia , “ obviously these are programmed answers . ” ) updates : inside the mechanical brain of the world ’s first robot citizen sophia , the uncanny robot ai , is not what you think . how much of robot sophia ’s speech is likely scripted with your understanding of the progress in nlp ?",10434,4302,2018-10-08T12:49:57.197,2018-10-08T12:49:57.197,are the dialogs at sophia 's ( the robot ) appearings scripted ?,natural-language-processing applications robots,1,4,2
1012,4376,1,,2017-10-27T12:44:01.850,3,68,"i have read ( here and here ) about the computational power of neural networks and a doubt came up . there is a way to reduce an ann to another ann ( not taking into count the training algorithm ) ? e.g. reduce a recurrent neural network to a multilayer perceptron , meaning that if i have a trained rnn , i can get a mp that maps the same inputs given to the rnn to the same outputs produced by the rnn . and if exists an answer to the above question , we can show the equivalence between neural networks , e.g. , all problems solved by an multilayer perceptron can be solved by a recurrent neural network but the opposite is not true , i.e. , mp is subset of rnn ( i do not know if this is true , is just an example ) . so , if we obtain this relationship between all neural networks , we can get a neural network x that is more powerful than others , so , we can throw away all other neural networks because x can solve any problem that other nn can . is this reasoning correct ? thanks .",9581,9581,2017-10-27T13:47:07.053,2017-10-29T01:04:45.197,reducibility and artificial neural networks,neural-networks,1,0,2
1013,4377,1,,2017-10-27T16:43:58.267,3,107,"i 'm very interested in writing a spiking neural network engine ( snn ) from scratch , but i ca n't find the basic information i need to get started . for example , i 've seen pictures of the individual signals that combine to form a neuron pulse in several research papers , with no information on the equations in use . it 's not the focus of the papers , and the authors assume the readers have that knowledge already . some papers reference software that provides this foundation ( nest , pynn , etc . ) , but the documentation for the software is similarly light on details . there is a ton of information out there on the more common network types , but snn have not yet made it into the mainstream . so where do i get this basic information ? has someone pulled together any recipes / examples / tutorials for an snn , as has been done with all the other network types ?",10402,,,2017-10-27T17:12:52.557,spiking neural network resources,neural-networks research neurons,1,2,
1014,4385,1,4387,2017-10-29T10:54:29.040,6,1065,"i am trying to train a cnn regression model using the adam optimizer , dropout and weight decay . my test accuracy is better than training accuracy . but as i know , usually train accuracy is better than test accuracy . so i wonder how this is happening .",10475,33,2017-10-29T23:31:20.673,2017-11-01T14:03:50.223,why my test error is lower then train error,convolutional-neural-networks,4,1,1
1015,4389,1,,2017-10-29T14:33:06.343,11,149,"i 'm trying to find a planning approach to solve a problem that attempts to model learning of new material . we assume that we only have one resource such as wikipedia , which contains a list of articles represented as a vector of knowledge it contains and an effort to read that article . knowledge vector and effort before we start , we set a size for the vector , depending on the number of different types of knowledge . for example , we can define the items in the vector to be ( algebra , geometry , dark ages ) , and then ' measure ' all the articles from this point of view . so , a math article will probably be ( 5,7,0 ) , since it will talk a lot about algebra and geometry but not about the dark ages . it will also have an effort to read it , which is simply an integer . problem given all the articles ( represented as knowledge vectors with an effort ) , we want to find the optimal set of articles that help us to reach a knowledge goal ( also represented as a vector ) . so , a knowledge goal can be ( 4,4,0 ) , and it 's enough to read an article ( 2,1,0 ) and ( 2,3,0 ) , since , when added , it adds up to the knowledge goal . we want to do this with minimal effort . question i 've tried to some heuristics to find an approximation , but i was wondering if there is any state of the art strategic planning method that can be used instead ?",10482,1671,2017-10-31T17:49:21.880,2019-04-30T11:02:05.697,strategic planning and multi dimensional knapsack problem,research heuristics knapsack-problem,1,4,2
1016,4391,1,4411,2017-10-30T02:13:23.367,4,120,"i have set myself the challenge of detecting the locations of players / bots in videos of a well known first person shooter game ( this is for a youtube series i 'm planning on doing ) . i 'm not sure which ai approach i should apply to this problem - i 'm a complete novice at this ! my first thought was that the face / head seems to have the most detail so i could train a convolution neural network on images of sprite heads and general background - however this seem not to work too well , i 've certainly not exhausted different network architectures / typologies but it was n't learning all that well . my second approach was to use a haar cascade . this seemed to be an obvious choice since it 's fast and good at detecting objects ( rather than multi - classifying ) . however my cascade stops after 5 or 6 stages ( using opencv ) as it seems to have reach a great accuracy , but it does n't detect when i feed it the training images , let alone other images . i also looked into pedestrian detection and got a stock version of that working . however this seemed to struggle when / if the sprites are crouching or in unusual positions ( and it is n't great on standing sprites tbh ) . so , is there a branch of machine learning / ai that is more applicable to this problem ? if not which should i continue to work on ?",10498,10498,2017-11-03T00:17:39.493,2018-03-01T06:25:38.560,what approach should i use to detect faces in video game footage ?,machine-learning ai-design training detecting-patterns,1,3,
1017,4394,1,4654,2017-10-30T23:01:56.720,10,733,"alpha go zero contains several improvements compared to its predecessors . architectural details of alpha go zero can be seen in this cheat sheet . one of those improvements is using a single neural network that calculates move probabilities and the state value at the same time , while the older versions used two separate neural networks . it has been shown that the merged neural network is more efficient according to the paper : it uses one neural network rather than two . earlier versions of alphago used a “ policy network ” to select the next move to play and a ” value network ” to predict the winner of the game from each position . these are combined in alphago zero , allowing it to be trained and evaluated more efficiently . this seems counter intuitive to me , because from a software design perspective this violates the principle separation of concerns . that 's why i am wondering , why this merge has been proven beneficial . can this technique - merging different tasks in a single neural network to improve efficiency - be applied to other neural networks in general or does this require certain conditions to work ?",9161,,,2017-12-04T15:51:35.273,why is the merged neural network of alpha go zero more efficient than two separate neural networks ?,neural-networks optimization architecture,1,0,3
1018,4395,1,,2017-10-31T00:44:22.317,5,110,"i want to design a simple model that predicts the movement of coordinates with rnns . in a typical three - dimensional lstm model , one feature is encoded as one hot encoding , and the x value is input as a three - dimensional matrix . ( e.g. , in seq2seq , abc = & gt ; [ [ [ 1,0,0],[0,1,0],[0,0,1 ] ] ] ) however , since i have to predict the x and y coordinate values , one hot encoding will inevitably produce a four - dimensional matrix . what should i do with back - propagation ? or should i design a 3-d matrix using scalar values without one hot encoding ?",10522,22296,2019-02-20T14:59:14.613,2019-02-20T14:59:14.613,how to design 4d deep recurrent neural networks using tensorflow ?,neural-networks tensorflow backpropagation,0,1,
1019,4396,1,5807,2017-10-31T01:26:08.780,4,768,"i 'm trying to answer the following question of chapter 6 exercises of artificial intelligence : a modern approach by peter norvig and stuart russell about dealing with constraints . this is in the context of constraint satisfaction problem and how you can re - formulate some problems with the constraints expressed as a bunch of binary constraints to use the generalized solver csp algorithm . but i 'm stuck with that exercise , i ca n't sketch a demonstration . 6.6 show how a single ternary constraint such as "" a + b = c "" can be turned into three binary constraints by using an auxiliary variable . you may assume finite domains . ( hint : consider a new variable that takes on values that are pairs of others values , and consider constraints such as "" x is the first element of the pair y. "" ) next , show how constraints can be eliminated by altering the domain of variables . this completes the demonstration that any csp can be transformed into a csp with only binary constraints . thanks in advance !",9574,9574,2017-11-06T12:15:53.827,2018-10-09T01:51:23.317,how to turn a ternary constraint into three binary constraints ?,ai-design russell-norvig,1,0,
1020,4397,1,,2017-10-31T10:05:28.143,2,68,"i want to develop a virtual assistant like google assistant , alexa , etc . , for mobile phones . what i should be researching apart from m / c learning and python ?",10529,1671,2017-10-31T17:36:09.223,2018-06-01T21:21:19.477,ai techniques and competencies necessary for creating a virtual assistant,machine-learning ai-design research,2,0,0
1021,4398,1,,2017-10-31T10:05:44.557,2,301,"i 'm having the following problem : ` i 'm training a multi - output cnn and using the relative values of the outputs in my loss function . the net is learning well , but as the absolute values of the outputs are not regularized in anyway in the loss function , the values of the outputs keep rising . this causes a situation where the result i need ( values of the outputs relative to each other ) are quite right , but the huge absolute values produce underflow resulting nan values at some point of the training . is there some way to constrain these absolute values ( perhaps elsewhere than the loss function ? ) i 'm using a custom loss function implementing a recovery angular error metric , so the loss function is somewhat complex . a weighed mean of the net outputs is fed to this function , so using both the net outputs and the weighed mean would lead to some quite problematic gradient derivation . would it be maybe possible to constrain the value range of the net outputs in the net structure itself ?",10528,,,2018-06-02T19:00:36.053,constraining the output value range of a cnn independent of the loss function,convolutional-neural-networks,1,0,
1022,4399,1,,2017-10-31T10:24:42.340,3,559,"so , my question is a bit theoretical . i have been trying to implement a perceptron based classifier with outputs 1 and 0 depending on the category . i have used 2 methods : the example by example learning method and batch learning method . i also have defined another method which will measure accuracy according to the formulae number_of_samples_classified_correctly / total_number_of_samples ( i 'm not sure this should be the correct definition for accuracy and you are welcome to suggest a better measure ) . now there are a few confusions i 'm facing . firstly , the accuracy of example by example learning is different from batch learning by 2 % . also the best accuracy achieved in both cases is depending on the slopes . so where exactly is the mistake?(batch learning algorithm= error*input_vector ( where error can be 1,-1 or 0 ) summed over all input vectors and then added to weights ) . for initial slope[1,-1 ] giving an accuracy of 88 % example by example learning for initial slope[1,-1 ] giving an accuracy of 88 % batch learning for initial slope[1,1 ] giving an accuracy of 84 % example by example learning for initial slope[1,1 ] giving an accuracy of 86 % batch learning",9947,6258,2017-10-31T16:43:17.867,2017-11-05T08:34:36.660,perceptron learning algorithm : different accuracies for different training methods,classification perceptron,2,3,0
1023,4409,1,,2017-11-01T00:09:06.357,3,44,"a question for developers of projects for pattern recognition . how best to organize the architecture of such a service ? at what stage do you conduct logic ? ( for example , for the recognition of a photo of a male blue jacket , a cascade of queries is performed : "" recognizing men "" - > "" recognizing the jacket "" - > "" recognizing the color of the jacket . "" ) does it make sense to implement all search options within a single neural network or is it better to create a set of individual neuronets that are confined to fairly simple tasks ?",7051,,,2017-11-05T13:40:10.120,image recognition service architecture,neural-networks ai-design pattern-recognition,2,0,
1024,4414,1,4495,2017-11-02T06:33:19.507,14,1385,"as you might know , capsule networks have been recently introduced by hinton . there also have been several heads up within his talks . as expected , the paper elaborates on the idea way theoretically ! however , as a fan of occam 's razor , i was wondering if anybody can simplify the idea behind the capsule networks or capsenets . thanks",10569,10569,2017-11-02T23:34:18.467,2017-11-12T06:45:05.930,what 's the main concept behind capsule networks ?,neural-networks deep-learning convolutional-neural-networks deep-network,2,0,9
1025,4415,1,,2017-11-02T09:43:49.843,4,1299,"i have a simple gauge displaying analog values ranging from 0 to 4 . here is an image of the gauge . unfortunately there is no way to get a analog or digital signal for the value . how do i read the value of the gauge ? my idea is to make an image every 5 minutes and get the value by analyzing it . i am thinking of manually generating reference images with the black needle in different positions and then compare it to the real image . since all the processing should be done on a raspberry pi without internet connectivity , a good approach would be a preconfigured docker image for image comparison which helps doing the image comparison locally , maybe supported by a python or php script . how to proceed ?",10586,,,2017-11-02T20:12:08.617,reading a value of a real gauge,image-recognition,1,3,2
1026,4419,1,,2017-11-02T20:17:38.433,2,40,"just a though passed through my head whatever method is adopted to achieve it . with programming or neural nets where do you think the location of that intelligence will be by these methods in the physical world . in me my intelligence or lack of it is behind my eyes and is carried around by my body but remains located there . know i am a stalwart of a dualistic nature , mind and material . but if iam a wrong and intelligence can be created purely from the material by programming , complexity or who knows , what still remains is , i may be alone , as i said that consious intelligence has a location so where will it be within a computer , it 's ram it 's processor or will the programming itself create a bi location with no precise boundary or absolute position of this ghost in the machine .",9094,,,2017-11-02T21:49:17.340,a.i and location,philosophy,1,0,
1027,4421,1,,2017-11-02T23:59:09.157,1,98,"i 'm working on a multi - player game that involves a board / map where there are walls , cover or open ground . the players then take turns moving units around and undertaking discrete actions , like move , fire etc . i 've been exploring adding an ai for single - player games , or to enhance multi - player games . i 've looked at : behaviour trees utility - based decisions goap the problem i see with all of these is that in order for the ai to use specific multi - unit tactics , like a pincer attack , or leap - frogging units ; that behaviour needs to be explicitly added separate to the above . so , how can a behaviour tree tell an ai how to identify it should use a flanking manoeuvre , i.e. move a unit around to hit from the side ? how does the ai identify that it 's a good plan , that is action across multiple "" moves "" ? how can the ai identify where to move the individual units in order to pull this off ( how does it identify the flank ) ?",2819,,,2017-12-04T19:29:42.040,how can an ai make tactical manoeuvres ?,ai-design game-ai,1,0,
1028,4423,1,,2017-11-03T06:30:25.853,3,105,"i bought an intel movidius neural compute stick a few weeks ago . even though i can use it with the examples , i want to actually use it for something ! the documentation is messy , and hard to work through . i 've already looked through some of the websites that have been mentioned in other posts . i specifically want to make an inference / reward - based ai that tries to guess a bunch of numbers over a period of time . the ai has to guess the best 10 guesses , and then it gets shown the set . the closer answers are rewarded , and the farther answers are penalised . in the end , i want it to try and guess the most likely set of numbers in a random number generator , kind of like how a quantum computer outputs answers . this could be used for all sorts of things ! even if it 's already built , i want to program it from scratch so i can learn how this sort of thing works ( teach a man to fish type of thing ) . so , considering my goals above , how should i start ? also , could this answer questions with close to the same accuracy as a quantum computer , if it 's fed enough data ?",10609,,,2018-08-08T16:16:58.627,programming an inference ai that computes the best outcomes like a quantum computer,reinforcement-learning tensorflow quantum-computing expert-system,1,1,1
1029,4424,1,,2017-11-03T08:58:53.623,2,34,is there a general adversarial network that can take multiple low quality images of a subject to create a higher quality image of the subject ? srgans just take a single low res image and make it high res but i need something that can take multiple low quality ( not necessarily low res ) images to create a higher quality image . in low quality i mean you could have one image with a reflection or partially blurry or has a small obstruction that another image might compensate for . would be good is if you could combine the lower quality images to create on higher quality image . is there any gan that could do this ?,2788,,,2017-11-03T08:58:53.623,is there a general adversarial network that can take multiple low quality images to create a higher quality image ?,machine-learning,0,0,
1030,4425,1,4426,2017-11-03T12:56:27.813,4,769,"i am fairly new to deep learning in general and i am currently facing a problem i want to solve using neural networks and i am unsure if it is a classification or regression problem . i am aware that classification problems are about classifying whether an input belongs to class a or class b ( or class c ... ) and regression problems are about mapping the input to some sort of continuous output ( just like the house pricing problem ) . i basically want to measure the body temperature of a person using a simple video camera . to me , this seems like more of a regression type of issue rather than classification , because of the actual continuous result values i want the neural network to produce from the input video frames , e.g. 39 ° celsius . but a question that came to my mind was : what if i use every integer value in the range from 35 ° c to 42 ° c as a possible output class ? this would make it a classification problem , am i right ? what would be the correct approach here and why ? classification or regression ? thank you !",10617,,,2017-11-04T09:03:41.727,deep learning – classification or regression approach ?,machine-learning deep-learning classification linear-regression,2,0,1
1031,4428,1,,2017-11-04T02:44:36.640,9,2282,"i was reading hinton 's new paper , "" dynamic routing between capsules "" and did n't understand the term "" activity vector "" in the abstract . a capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part . we use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters . active capsules at one level make predictions , via transformation matrices , for the instantiation parameters of higher - level capsules . when multiple predictions agree , a higher level capsule becomes active . we show that a discrimininatively trained , multi - layer capsule system achieves state - of - the - art performance on mnist and is considerably better than a convolutional net at recognizing highly overlapping digits . to achieve these results we use an iterative routing - by - agreement mechanism : a lower - level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower - level capsule . https://arxiv.org/pdf/1710.09829.pdf i thought a vector is like an array of data that you are running through the network . i started working through andrew ng 's deep learning course but it 's all new and terms go over my head .",10623,,,2017-11-10T10:58:19.057,what is an activity vector in neural networks ?,neural-networks,2,0,1
1032,4432,1,,2017-11-04T19:21:49.827,0,290,"i 'm looking to build a conversational ai . the goal is to create an ai that is your friend ( as opposed to personal assistants that are meant for carrying out commands like "" send a text , play that song ... "" etc ) . what are the underlying techniques involved ? is there any good open source project i can build on top of . what kind of training data do i need ? i looked at jasper - does nt look like a good fit based on a shallow evaluation . i 'm imagining that the user text needs to be parsed out ( like for eg using google 's syntaxnet ) . may be it queries some knowledge graph and then uses nlg . or it uses some kind of seq - seq modelling like lstm / rnn to generate responses .",6418,,,2018-01-04T09:38:03.213,build conversational ai,natural-language-processing intelligent-agent,2,3,
1033,4434,1,,2017-11-05T05:15:44.193,1,53,"given an ease with which a human can read a text scrambled in a special way , like the following passage popular a while ago aoccdrnig to a rscheearch at cmabrigde uinervtisy , it deosn't mttaer in waht oredr the ltteers in a wrod are , the olny iprmoatnt tihng is taht the frist and lsat ltteers be at the rghit pclae . the rset can be a toatl mses and you can sitll raed it wouthit porbelm . tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef , but the wrod as a wlohe . could you train a computer to restore the original ? are there any studies / known instruments that can achieve this ? i am talking about mildly scrambled / damaged text , like the above , not a totally random permutations or random omission of letters .",10655,,,2017-11-05T21:06:49.297,is it possible to restore scrambled / damaged text ?,machine-learning,1,0,
1034,4436,1,,2017-11-05T11:58:43.207,2,94,"i 've mostly seen ( e.g. in http://karpathy.github.io/2015/05/21/rnn-effectiveness/ ) that when training rnn 's on text for something like language modeling , the text is usually featurized character - by - character using a 1-hot encoding . for example , the text "" hello "" would be represented like { h : 1 , e : 0 , l : 0 , o : 0 } { h : 0 , e : 1 , l : 0 , o : 0 } { h : 0 , e : 0 , l : 1 , o : 0 } { h : 0 , e : 0 , l : 1 , o : 0 } { h : 0 , e : 0 , l : 0 , o : 1 } i was wondering if one could just as well use the ascii encoding of the text and feed the bits in one by one . so the input "" hello "" would be input like 0110100001100101011011000110110001101111 would the rnn have a disproportionately harder time having to figure out how the arbitrary and complex 8-bit ascii encoding should be used ? or would the ascii encoding lead to about the same performance as the nicer 1-hot encoding ?",10661,4302,2018-10-08T12:22:53.923,2018-10-08T12:22:53.923,training rnn 's on text : can you use an ascii encoding just as well as a one - hot character encoding ?,neural-networks machine-learning natural-language-processing recurrent-neural-networks lstm,0,0,
1035,4443,1,,2017-11-05T22:40:51.413,3,1514,"i 'm new in this argument , my question is : can convolution be applied in other contexts different from image recognition ? is there a good source to learn from ?",10676,4398,2017-11-05T22:52:04.350,2018-07-22T02:15:29.647,can convolutional neural networks be applied in domains other than image recognition ?,neural-networks convolutional-neural-networks,2,0,2
1036,4454,1,,2017-11-07T09:02:44.327,1,197,"please , can someone give advice what journals are good for first publication in the field of deep reinforcement learning ? i am in process of writing about research results of dqn related algorithms . i have 3 requirements - it should be indexed in one of these databases , otherwise , i can not receive grant money for research : https://www.scopus.com/ http://webofknowledge.com and it should not be very expensive to publish . it should be under 1000eur to publish , for example , open access license for elsevier "" artificial intelligence "" journal costs around 2400eur to publish . and it should not have very long review / publishing period . for example , elsevier "" information fusion "" journal currently gathers articles for july 2018 , that is 8 month period till publishing . is it normal ? can you please recommend some journals that qualify & amp ; you have had good experience publishing research ?",9740,9740,2017-11-07T09:45:22.050,2018-01-04T17:59:34.367,where to publish reasonable article in deep reinforcement learning ?,deep-learning reinforcement-learning,1,0,
1037,4456,1,,2017-11-07T14:10:09.410,23,8296,"what 's the difference between model - free and model - based reinforcement learning ? it seems to me that any model - free learner , learning through trial and error , could be reframed as model - based . in that case , when would model - free learners be appropriate ?",10720,2444,2019-02-16T19:01:36.767,2019-02-27T21:52:40.953,what 's the difference between model - free and model - based reinforcement learning ?,reinforcement-learning difference model-based model-free,5,3,5
1038,4457,1,,2017-11-07T14:42:18.287,0,100,can you good people direct me to sources to start learning the aspects of ai ?,10722,,,2017-11-07T15:03:41.597,i want to learn ai engineering but do n't know where to start,machine-learning,1,4,
1039,4460,1,,2017-11-07T21:00:12.733,4,169,"what would be the result of an ai subject to a data stream or information stream , fed to it from events in an "" external world "" if it could not find any pattern within that stream by the methods it has in determining patterns would the response be because of the limitation of its methods in determining patterns or would you be able to conclude that the information from those events are undetermined and therefore could be stated as random . or would the ai be unable to ever determine using its methods whether the information fed to it was from a random undetermined world because it needed an eternity of information as to determine whether the data was random or not . so what i guess i 'm asking is the pursuit of creating ai by mankind on a similar level of limitation by the accumulation of "" knowledge "" over time as is the other pursuits of mankind within science etc , and is only going to be achieved simultaneously with the acquisition of "" knowledge "" from these other pursuits . if determination of a simulated model comes from the physics of a determined world or not will there ever be a model because of the eternity of determining whether the information from the "" external world "" is determined or not .",9094,,,2018-12-29T19:01:30.337,pattern recognition,philosophy,2,3,1
1040,4463,1,,2017-11-08T05:19:17.283,2,40,"say i need to make an action such as "" walk "" perform for 2 seconds using a behavior tree which has access to a timer . the timer can do the following : start , time ( get value of timer ) , and reset . if the behavior tree should return success every time "" walk "" is completed , unless the timer reset or "" walk "" is still executing , how could such a thing be built ?",10741,,,2017-11-08T05:19:17.283,making behavior tree execute action for specific time ?,game-ai,0,0,
1041,4464,1,4466,2017-11-08T12:33:09.257,11,465,"i am software / hardware engineer for many years now . i am shamed to say , i know nothing about ai and machine learning . i have a strong background in digital signal processing , and various languages , like c , c++,swift , etc . is there any book / guide that start right from scratch with basic theory , and then goes with examples to real life applications , current tools , examples that you can run , etc ? not too academic , or statistic , start with basic philosophy and go to practice . thank you .",10749,1671,2017-11-09T20:00:20.483,2018-06-19T20:05:05.077,vetted sources of ai theory / tools / applications for an experienced programmer new to the field ?,neural-networks machine-learning,5,5,10
1042,4472,1,4485,2017-11-09T10:27:06.057,3,251,"i have a set of images that i already trained a cnn to classify successfully . i wonder if it would be possible to encode the images ( using xor in combination with a key of the same length as the image ) and train a new net on them . thinking logically , the features still exist in the same relation to each other , just in a different form ( encoded ) . considering that neural networks are incredible at pattern recognition , i assume that it would still be doable . for people , who can not imagine how a xor - encoded image would look like : for a human , it may look like rubbish , but the information is definitely there . would love to read your opinion .",10783,,,2017-11-12T09:04:28.110,would convolutional nn recognize patterns in encoded images ?,neural-networks convolutional-neural-networks image-recognition,3,0,1
1043,4476,1,,2017-11-10T00:24:02.657,-3,72,"this is an xor neural net i built in python . when i run it with random weights and keep the biases constant , it trains them perfectly and reaches the global minima , but when i add the biases in so that it has to train them too , it does n't work at all . i think its getting stuck in a local minima because of my training function but i do n't know for sure . attached is the code to only train the weights . # ! /usr / bin / env python import math import random class neuralnetwork : def _ _ init__(self ) : self.input = [ [ 0,0],[1,0],[1,1],[0,1 ] ] # four different sets of two inputs self.expected = [ 1,1,0,1],[0,1,1,1],[0,1,0,1 ] # optimal outputs for all three nodes self.weights = [ random.randrange(-10,10),random.randrange(-10,10)],[random.randrange(-10,10),random.randrange(-10,10)],[random.randrange(-10,10),random.randrange(-10,10 ) ] # self.weights = [ -20,-20,20,20,20,20 ] # optimal weights self.bias = [ 30,-10,-30 ] # one bias for each node # self.bias = [ 30,-10,-30 ] # optimal biases self.out = [ 0,0,0,0],[0,0,0,0],[0,0,0,0 ] # actual output for every node self.rmse= [ 0,0,0 ] self.error = [ 0,0,0,0],[0,0,0,0],[0,0,0,0 ] self.learnrate = 1 def nandgate(self ) : for i in range(0,4 ) : self.out[0][i ] = self.input[i][0]*self.weights[0][0]+self.input[i][1]*self.weights[0][1]+self.bias[0 ] self.out[0][i ] = 1 / ( 1 + math.exp(-self.out[0][i ] ) ) def orgate(self ) : for i in range(0,4 ) : self.out[1][i ] = self.input[i][0]*self.weights[1][0]+self.input[i][1]*self.weights[1][1]+self.bias[1 ] self.out[1][i ] = 1 / ( 1 + math.exp(-self.out[1][i ] ) ) def andgate(self ) : for i in range(0,4 ) : self.out[2][i ] = self.out[0][i]*self.weights[2][0]+self.out[1][i]*self.weights[2][1]+self.bias[2 ] self.out[2][i ] = 1 / ( 1 + math.exp(-self.out[2][i ] ) ) def calcrmse(self ) : for p in range(0,3 ) : self.rmse[p ] = 0 for p in range(0,3 ) : for i in range(0,4 ) : self.error[p][i ] = self.expected[p][i ] - self.out[p][i ] self.rmse[p ] + = self.error[p][i ] def train(self ) : for q in range(0,1000000 ) : self.nandgate ( ) self.orgate ( ) self.andgate ( ) self.calcrmse ( ) for i in range(0,3 ) : # self.bias[i ] + = self.learnrate*self.rmse[i ] for p in range(0,2 ) : self.weights[i][p ] + = self.learnrate*self.rmse[i ] # print(""iter : "" + str(q ) ) # print(""weights : "" + str(self.weights ) ) # print(""bias : "" + str(self.bias ) ) # print(""nand out : "" + str(self.out[0 ] ) ) # print(""or out : "" + str(self.out[1 ] ) ) print(""and out : "" + str(self.out[2 ] ) ) # print x = neuralnetwork ( ) x.train ( )",4744,,,2017-11-14T12:47:10.303,neural net not training biases correctly,neural-networks training,1,1,
1044,4477,1,,2017-11-10T06:19:58.130,2,122,"i 've seen some articles about text generation using lstms ( or grus ) for text generation . basically it seems you train them by folding them out , and putting a letter in each input . but say you trained it with text which includes the string : "" the dog chased the cat "" and also includes the string : "" the lion chased the ca "" both should be acceptable . but although they are very similar they differ entirely after the 4th character . so they will result in two very different vectors . further the longer you roll it out the more they will differ . how is it then possible for an lstm to learn that "" the [ something ] chased the cat "" is an acceptable phrase ? equally if you train it to try and learn to pair up parentheses . i can see how you could manually design it to do this , but how exactly could it be trained to do this just by entering strings like "" ( das asdasdas ) axd "" ? what i 'm getting at is that i do n't get how it could learn any sort of structure more than a markov model . any ideas ? ( also , i 've only ever seen one article that showed a lstm that can pair parentheses . so has this study ever been replicated ? ) i get that it is possible for an lstm to do this but i do n't get how it can learn to do this ! what i 'm getting at is you usually train things with the input phrase and then compare it to the expected phrase to get the error . but in text generation there might be millions of possible acceptable expected phrases ! so how can you compute an error ?",4199,4199,2017-11-10T06:26:29.593,2017-11-11T20:34:12.033,how are lstm 's trained for text generation ?,training lstm,1,0,
1045,4482,1,4483,2017-11-10T15:54:48.073,8,1215,"i invented a chess - like board game . i built an engine so that it can play autonomously . the engine is basically a decision tree . it 's composed by : a search function that at each node finds all possible legal moves an evaluation function that assigns a numerical value to the board position ( positive means first players is gaining the upper hand , negative means the second player is winning instead ) an alphabeta pruning negamax algorithm the main problem about this engine is that the optmization of the evaluation function is really tricky . i do n't know which factors to consider and which weights to put . the only way i see to improve the engine is to iterate games trying each time different combinations of factors and weights . however , it computationally seems a very tough feat ( can i backpropagate without using deeplearning ? ) . i would like to use reinforcement learning to make the engine improve by playing against itself . i have been reading about the topic , but i am still quite confused . what other reward is there in a game a part the win - or - lose output ( 1 or 0 ) ? if i use other rewards , like the output from the evaluation function at each turn , how can i implement it ? how do i modify the evaluation function to give better rewards iteration after iteration ?",10813,2444,2018-11-19T07:36:54.987,2018-11-19T07:36:54.987,a few doubts regarding the application of reinforcement learning to games like chess,reinforcement-learning game-ai game-theory combinatorial-games negamax,1,0,2
1046,4484,1,,2017-11-10T21:46:30.933,2,36,"is there examples of ai systems being taught grammatical rules , or have such rules built - in to them , in order to recognize commands and the like in natural language ? for example , is there a program , when given the instruction ( either by speech or by text ) in the form of "" open file-1 "" , will attempt to understand the command as consisting the parts "" open "" and "" file-1 "" , then respond according to how each part is classified ? the focus here is the fact that these kind of instructions are processed with built - in rules of grammar , rather than the rules being learned through observation of data .",1321,4302,2018-10-08T12:49:19.793,2018-10-08T12:49:19.793,"has there been "" prescriptive "" attempts at language - learning / comprehension in ai ?",natural-language-processing,1,2,
1047,4489,1,,2017-11-11T17:39:21.537,2,1704,"i want to build a personal assistant that listens to me continuously .. the flow looks like this : continuously record voice stream it to google speech api . get back the text in real time - > parse for intent etc .. problem is , google speech api gets expensive if you record for hours . a better way to do it is to only submit the parts where i 'm actually speaking to it .. then the cost of running this full time ( 17 hours a day , every day ) becomes very accessible . now my question is : how can i detect that a voice is present in the microphone stream ? i have a lot of noise in the background , dumb increase in volume detection is not a very good solution . i need something more intelligent . it does n't need to be very accurate - just good enough to not break my cloud computing budget . i 'm thinking that human voice sounds distinct enough that is not such a big problem to detect when is there . what do you recommend me to do , given this is a real time stream - not an audio file . the audio will be generated in chromium browser ( electron ) - with getusermedia api , and using node.js i plan to handle the streaming logic . note : there is a built in speechrecognition api in electron - but from my experience currently it does n't work ( not even after i give it my api key ) , and even if would have worked , i think it has the same cost problem . so this is why i 'm trying to provide my own implementation . i do n't know what i 'm doing , any insight is welcomed : ) thank you .",10650,1671,2017-11-12T22:13:25.073,2017-11-13T19:05:55.950,how to detect when human voice / speech appears in an microphone stream ?,intelligent-agent voice-recognition,1,1,
1048,4494,1,,2017-11-12T06:00:40.320,1,115,has there ever been a model ( neuro - physical ) proposed for the human unconscious brain ? i tried reading on unconscious mind but could n't find anything which answers the question . is there any literature related to it ?,1976,,,2017-11-17T19:52:55.357,has there ever been a model ( neuro - physical ) proposed for the human unconscious brain ?,human-like,1,4,
1049,4497,1,,2017-11-12T07:03:09.540,2,366,"i am trying to find a good evaluation function for a game with : a 7x7 tile board 2 players , given equal number(>=3 currently undetermined ) of stones placed randomly on the tiles a turn is consisted of a player moving a stone owned by that player , vertically or horizontally but not diagonally to a very next tile of itself a player loses when out of moves : a player is out of moves when every stone that player owns , has its very next tiles , except not for diagonals necessarily , occupied either by the board edge or other stones right now my evaluation function 's return value increases : if the total moves available to the player is increasing and/or average distance to the middle tile of the board is decreasing question : is there a better strategy ? or how can i improve my evaluation function ?",10843,10843,2017-11-12T22:32:00.190,2018-02-12T14:27:16.537,evaluation function for a go like game,ai-design algorithm heuristics go combinatorial-games,2,1,
1050,4499,1,,2017-11-12T11:20:46.507,1,56,"well , i am new to implementing ann 's and there is something that i want to know . it maybe a bit silly though . i just wanted to know that if we have a simple data set say dependent only on a single variable x1 , and if we want to train the ann to predict the data set values ( no new values , only the data set values needs to be predicted ) will sorting or arranging the data set in some way beforehand make the ann go to the optimum value quicker or it really does n't matter ? if arranging does matter than how do we arrange multidimensional data and what is the intuition behind it being faster ? just like in binary search for a random data set we get a performance gain of o(n^2)-o(nlogn ) over linear search similarly i want to know whether such performance gains are available in ann 's in any way ? note : i am aware this is already taken care by batch learning so i am specifically talking about example by example learning . also , i want to know if some arranged form of data set works well for certain forms of training e.g. momentum method , delta method . thanks in advance .",9947,9947,2017-11-17T05:46:07.337,2017-11-17T05:46:07.337,orientation of data set before training simple ann 's,neural-networks machine-learning datasets structured-data,0,0,
1051,4508,1,4523,2017-11-13T08:23:42.043,-1,65,"i want to capture text and letters on images ( png , jpeg , etc . ) . is it possible which algorithm / software can i use ? right now i am using r with the tesseract package but it 's not solving my problem : if letters on images are different colors and different format , sizes in those situations do n't work . i do n't have the any training data to compare text on the image with the existing data set . so in those situations is it possible ?",10862,69,2017-11-18T20:49:45.997,2017-11-18T20:49:45.997,text capturing on the images,deep-learning artificial-neuron,1,1,
1052,4509,1,4513,2017-11-13T08:49:01.553,9,598,"starting from last year , i have been studying various subjects in order to understand some of the most important thesis of machine learning like s. hochreiter , & amp ; j. schmidhuber . ( 1997 ) . long short - term memory . neural computation , 9(8 ) , 1735 - 1780 . however , due to the fact that i do n't have any mathematical backgrounds , i started to learn subjects like calculus multivariate calculus mathematical anaylsis linear algebra differential equations real anaylsis ( measure theory ) elementary probability and statistics mathematical statistics right now , i ca n't say i have done studying those subjects rigorously , but i know what the subjects above want to deal with . the thing is that i do n't know what i have to do at this point . there are many subjects that machine learning uses to solve many problems out there and i do n't know how to utilize them correctly . for example , reinforcement learning is now one of the most popular topic that hundreds of thousands of researchers are now doing their research to make a breakthrough of curse of dimensionality . but , as a future employee who 's gon na work in it companies , the task on the desk would n't be something i expected to do . is it important to have my own expertise to work in the fields ? if so , what kinds of subjects do i have to study right now ? for your convenience , i want to know more about markov process and markov decision process .",10860,2444,2019-02-19T11:31:31.100,2019-02-20T09:45:00.933,what do i need to study for machine learning ?,machine-learning getting-started,6,1,4
1053,4516,1,,2017-11-13T20:47:59.597,1,95,"i wish to write a bot that can use screen footage to play a game , specifically for the game ' nidhogg ' . to that end i have determined that a cnn should do the feature detection and a feedforward neural network should determine the action to take . for this question i wish to focus on the cnn . my idea was to first train the cnn as an autoencoder to aid in unsupervised pattern recognition and to add hidden layers after every epoch , but i am unsure whether this would actually be faster or lead to higher accuracy ( as opposed to immediately training all hidden layers at once ) . the reason i am unsure is because i imagined the following scenario : input & gt ; hidden1 & gt ; output if the output layer needs to approximate the input layer is it not wasteful to let it have its own weights backpropagated when it actually tries to determine the ' inverse ' function of the hidden layer ? is it possible to ' directly ' determine an inverse function for a layer even if consists of multiple filters ? if so , say i 've trained my first hidden layer so that the error is minimal and use it as an input layer whilst i add another hidden layer ; can i repeat my strategy ? input & gt ; hidden1 & gt ; output input(hidden1 ) & gt ; hidden2 & gt ; output",10883,,,2017-12-15T01:23:44.347,can cnn autoencoders be improved by treating the output layer as an inverted hidden layer ?,convolutional-neural-networks,1,0,
1054,4524,1,4528,2017-11-15T08:35:25.953,0,539,"i have implemented the hill climbing algorithm , with side away steps , which can increase the rate of success , because , when you do n't have new generated states , you can go back to previous level and choose from there another state . what should i do when new generated states have bigger distance to the goal than the parent state ? do i have to go back to the previous level and choose another state , from which i will continue ? or i continue with one of the states generated which has bigger distance ?",10794,2444,2019-03-02T10:50:29.347,2019-03-02T10:52:27.993,what should i do when the new generated state has bigger distance to the goal than the parent state ?,search hill-climbing,1,1,
1055,4525,1,4526,2017-11-15T10:03:45.220,1,276,"i know what meaning stride has when it is just an integer number ( by which step you should apply filter to image ) . but about ( 1 , 1 ) or even more dimensional stride ?",10929,10929,2017-11-15T10:14:21.910,2018-10-29T21:53:01.467,what is the meaing of 2d stride,convolutional-neural-networks,1,0,
1056,4530,1,,2017-11-16T09:44:50.243,3,83,"as we know in call centers there are certain sop(standard operating procedure ) , for example few are below , agent greeted customer agent verified the validity of customer before providing sensitive information agent is able to hold on conversation in decent way agent closed conversation with nice ending notes customer got all information what he wanted we are planning to transcribe the call records and then from the transcribed text , we want to verify above things(out of many ) from transcribed text using natural language processing . with the current state of ai technology is possible ? if yes can someone give some guidance to approach this type of problem ?",10955,4302,2018-10-08T12:22:04.933,2019-05-06T16:01:40.323,is it possible to verify sop of call center conversation using ai ?,ai-design natural-language-processing quality-control,1,0,1
1057,4532,1,,2017-11-16T15:00:37.470,9,149,"i am a phd student in computer science , and currently creating a state of the art overview in applications done in machine ethics ( a multidisciplinary field combining philosophy and ai , that looks at creating explicit ethical programs or agents ) . it seems that the field mostly contains theoretical arguments and there are relatively little implementations , even though there are many people with a technical background in the field . i understand that because ethics are involved , there is no ground truth and since it 's part of philosophy one can get lost in arguing over which type of ethics should be implemented and how this can be done best . however , in computer science , it is usual to even try a simple implementation to show the possibilities or limitations of your approach . what are the possible reasons there is so little done in explicitly implementing ethics in ai and experimenting with it ?",10958,,,2017-11-29T22:40:50.793,few implementations in machine ethics ?,ethics implementation,5,8,4
1058,4533,1,4537,2017-11-16T15:48:07.827,5,370,"with some knowledge of machine learning and deep learning , it seems very unlikely for ai to develop into the consciousness that we imagine . to me , consciousness requires a new framework that is very different from what we have today , as current forms of learning seem to be very ' shallow ' . what are the current limitations to artificial consciousness and sentience ? also is there a difference between artificial consciousness and sentience ?",9469,9647,2017-12-09T23:14:03.803,2017-12-09T23:14:03.803,current limitations to artificial consciousness,artificial-consciousness sentience,4,0,4
1059,4535,1,4538,2017-11-16T19:00:08.317,0,42,"i 'm new in genetic algorithms and i 'm reading the a. e. eiben and j. e. smith book introduction to evolutionary computing ( springer 2003 ) . on section 3.5 recombination , page 47 , second paragraph said : recombination operators are usually applied probabilistically according to a crossover rate p sub c [ ... ] . what does probabilistically means here ? i think it means that there are a probability that the operator is going to be applied or not .",4920,1671,2017-11-16T20:34:31.883,2017-11-16T20:34:31.883,applied probabilistically ... what does probabilistically mean ?,genetic-algorithms terminology,1,3,
1060,4560,1,,2017-11-20T06:06:19.840,0,26,"if some claim they are doing artificial intelligence by providing a voice assistant , what shall we expect from this voice assistant for the claim of doing "" artificial intelligence "" to be correct ? examples of potential qualifications : is replying "" hello "" to "" hello "" enough ? is repeating any word you said enough ? is identifying you by your voice enough ? shall it support corrections ? "" what is my favorite color?""/""red!""/""no , it 's blue . what is my favorite color?""/""blue ! "" shall it support emotions ? "" i would offer you better service if you could say thank you sometimes "" shall it support self - awareness ? "" dear master , may i ask you a question about me ? "" etc .",11018,,,2017-11-20T22:10:49.863,what qualifies a voice assistant to be an ai ?,definitions,1,0,
1061,4562,1,,2017-11-20T19:33:56.663,-1,421,"i am interested to know if it 's possible to use alice chat bot in our c / c++ program in offline mode ? i mean i can pass strings trough the alice in my c++ program and get the answer as a string(use it as a library / function ) is it possible ? edit : i asked this question from the author of mitsuku bot steve worswick and he answered : if you have an offline interpreter , the source files for alice are on the internet . also search for alice aiml for a full 3 times loebner prize winning chatbot ’s free to use source files . this assumes you have an offline aiml interpreter like program o or program ab . but i could n't understand what did he mean by aiml offline interpreter ? what do should this interpreter do ?",8818,8818,2017-11-21T02:45:13.420,2018-01-20T04:25:02.427,can i use alice chat bot in offline mode ?,chat-bots,1,0,1
1062,4570,1,,2017-11-21T21:32:19.650,2,59,"i am a newbie in the field of ai / ml . i am trying to implement predictive analytics model on the data generated and collected every minute from a device with sensors . i have two questions : what are various ml algorithms i can use to predict the number of people in a room given temperature , humidity , luminosity , and motion { 0 | 1 } ? what other things can we predict using the above data from the sensor that is deployed in a closed room ? context : the device sends temperature , humidity , luminosity , and motion(yes / no ) in real - time . i deployed this device in a closed room and started collecting data . now i want to use this data to predict the number of people in the room using the data collected . i believe a multiple ( linear / poly ) regression model will help me in achieving this but , wanted to know if there are any other algorithms or any other use cases i can look into .",11055,11055,2017-11-22T02:22:51.250,2017-11-22T02:22:51.250,what are suitable predictive analytics models for data from multiple sensors ?,machine-learning prediction linear-regression,0,5,3
1063,4571,1,,2017-11-21T21:34:49.517,3,1278,"i am very new to ai and ml alike . i am using google 's ocr to extract text from image like receipts and invoice . can someone please guide me to which techniques are used to make sense of the text . like i would like to extract date , name of business , address , total amount , etc . if someone can please direct me to right set of algorithm industry uses for machine learning before marking this question "" too broad "" , will be great . i know this question is too broad , but there are no one to one answers for these questions . i will delete this question once answered and if marked not useful to keep forum clean .",11056,1671,2017-11-21T22:26:26.760,2017-11-22T04:36:46.343,effective algorithms for ocr,machine-learning ocr,2,1,2
1064,4574,1,4630,2017-11-22T08:04:29.000,1,170,"i hope the experts here will bear with my basic questions . i have started on andrew ng 's machine learning course . it seems that machine learning is making correlations with known data based on as many parameters as possible . for example , if we collect data on existing property prices with information on the land area , built - in area , type of building , age of building , etc , it is possible to predict the price of another property if we input the value of the various parameters of this property . similarly , if we keep the images ( the black and white pixels ) of cats , we can tell whether a new picture is a cat if it bears some resemblance to the pixels of existing labeled cat images . this approach sounds great but is it practical ? how much effort and zetabytes of data do we have to keep just to reach the brain power of , say , a 3-year old , who can recognize dogs , cats , tigers , a mustang , trucks , a hamburger restaurant , and so on ? my next question is why does everyone have to repeat the effort of learning the same things ? if google has already learned cats , or if someone already has a program to recognize handwritten digits , can this knowledge be shared and re - used ? or is it just a matter of paying for them ?",11064,1671,2017-11-30T19:26:52.180,2018-01-01T22:20:02.853,a few basic questions,machine-learning,4,2,1
1065,4576,1,4605,2017-11-22T17:17:44.173,2,340,""" deep learning "" neural networks are now successful at image - recognition tasks that i would not have expected say 10 years ago . i wonder if the current state of the art in machine learning could generally tell the difference between the sound of a dog or cat moving around a house , and a person walking in the same area , taking as input only the sound captured by a microphone . i think i could generally tell the difference , but it is hard to explain exactly how . but this is also true of some tasks that deep learning is now succeeding at . so , i suspect it is possible but it 's not clear how you would go about it . i have found algorithms to detect human speech ( wikipedia:""voice activity detection "" ) but separating animal and human footsteps seems more subtle .",11078,11078,2017-11-22T22:52:45.540,2017-12-02T21:30:58.313,"could a cnn hear the difference between sound of a pet moving , and a person ?",deep-learning pattern-recognition security,2,1,1
1066,4577,1,,2017-11-22T19:26:29.110,1,16,"normally when doing a fit to some time series data ( e.g. , a polynomial fit ) , functions will return an associated error with each fitted point . i 'm now trying out scikit - learn 's support vector regression ( svr ) fitting instead , which does n't have any such return . there is a handy function in scikit - learn called validation_score that can tell me the accuracy score of various fits , from which i select the best one . so maybe that 's good enough ? but as a scientist , i 'm wondering if this will get slapped down in peer review of a publication . what 's the right way to propagate the errors of my time series data through the svr fit ?",11080,,,2017-11-22T19:26:29.110,should i be using a validation curve accuracy score in lieu of error bars for a model fit to time series data ?,models,0,0,
1067,4581,1,4586,2017-11-23T03:02:05.923,7,1076,"i 've been reading a lot about td - gammon recently as i 'm exploring options for ai in a video - game i 'm making . the video game is a turn - based positional sort of game , i.e. a "" units "" , or game piece 's , position will greatly impact it 's usefulness in that board state . to work my way towards this , i thought it prudent to implement a neural network for a few different games first . the idea i like is encoding the board state for the neural network with a single output neuron which gives that board states relative strength compared to other board states . as i understand , this is how td - gammon worked . however , when i look at other people 's code and examples / tutorials , there seems to be a lot of variance in the way they represent the board - state . even for something as simple as tic - tac - toe . so ; specifically for tic - tac - toe , which is a better , or what is the correct representation for the board state ? i have seen : 9 input neurons , one for each square . a 0 indicating a free - space , -1 the opponent and 1 yourself . 9 input neurons , but using different values such as 0 for the opponent , 0.5 for free and 1 for yourself ? could you use larger values ? like 0 , 1 and 2 ? 27 input neurons . the first 3 being square 1 , the next 3 being square 2 etc . every neuron is 1 or 0 . the first of the set of three indicates whether this square is free or not ; the second indicating whether the square is occupied by your opponent or not . in the end , only one in every 3 neurons will have a 1 , the other two will have a 0 . 18 input neurons . the first being 1 for the x player , the second being 1 for the o player and both being 0 for a blank then ; when branching into games where the specific pieces abilities come into play , like in chess , how would you represent this ? would it be as simple as using higher input values for more valuable pieces ? i.e. -20 for an opponents queen and +20 for your own queen ? or would you need something more complex where you define 10 + values for each square , one for each unit - type and player combination ?",2819,2819,2017-11-23T05:09:02.453,2018-06-17T02:39:11.737,how to represent tic - tac - toe vs checkers or chess for a neural network,neural-networks,3,0,2
1068,4583,1,4590,2017-11-23T16:38:53.243,0,398,"i have an ai class this semester . for our exam we also cover alpha - beta pruning . i found an old example , where i think we can stop already earlier . here is a picture of it . i think , because x wants to maximize his win , and finds 10 , he knows that he can not get better . therefore he cuts and puts 10 . i marked my "" improvement "" with red . thanks for your help . sascha",11098,,,2017-11-25T01:09:46.530,alpha - beta pruning,logic new-ai,3,1,0
1069,4585,1,,2017-11-23T22:22:39.840,-1,338,"if we say that we can measure intelligence and judge it with a scale of measurement . we get a quantity back from that measurment . the scale has no extremeties and has no limits in the quantity it measures unless the thing it measures has absolutes . is it possible then to concieve that ai can be created that surpasses the humans measured by the same scale . if it is , does n't it mean that the ai that has been created has not been determined by the limitations of those humans and its creation has not been determined from the intelligence associated with humans but would have to be created from something not directly attributable to intelligence and something more fundamental , and thus an ai more intelligent will be a consequence of this . or does the scale have absolutes and we have a limit in intelligence and maybe the artificial intelligence will be only created by someone that reaches that absolute measure , or any creation of which will be limited within the extremeties of the scale and be limited and not surpass the humans intelligence also confined within it ?",9094,9094,2017-11-23T22:48:38.947,2018-01-27T22:29:55.977,intelligence and judgement,philosophy,3,14,2
1070,4591,1,,2017-11-24T13:56:56.240,7,108,"i 'm building a neural net to predict the value of a piece of art with a wide range of inputs ( size , art medium , etc . ) and i would like to include the author as an input as well ( it is often a huge factor in the value of a single piece of art ) . my current concern is that the name of the author is n't an ideal numerical input for a nn ( i.e. if i just code each author with an increasing integer value i will be indirectly assigning more value to authors further down the list -_- ) . my thoughts were to create separate inputs for all the authors in my data set and then just use one hot encoding to better represent the input to the nn . this approach however runs into a problem when an author that is not included in my training data is used as an input to the nn ( i.e. a new author ) . i can get around this with an "" other author "" input field but i am worried that this wo n't be accurate as i would not have trained the the nn for this input ( all pieces of art with a valuation have an author ) . i have n't fully thought this through but i thought of perhaps training 2 nn 's , one for a valuation without an author and one for valuation with an author to ensure i have enough training data for an "" authorless valuation "" to still be reasonably accurate . i am still trying to conceptualize the best nn architecture before i get stuck into the implementation so if anyone has any suggestions / comments i would be very grateful ! thanks in advance , vince p.s . i am doing this as a small competition with a friend to test a nn vs the traditional commercial valuation techniques . please help me get a win for computer science over actuarial science .",11111,,,2018-07-31T09:36:38.867,neural network architecture for author name as an input ?,neural-networks,2,1,
1071,4594,1,4597,2017-11-24T17:59:59.923,2,376,"i 'd like to explore possibilities of applying deep learning on image noise reduction problem , more on photographic camera noise . what 's a good nn architecture to solve problems like this ? edit 25,nov,2017 : i have a small dataset of clean / noisy reference ( ~15k 4kres images ) acquired from digital camera . the target is to denoise other images from this camera type but without a reference photo .",8499,8499,2017-11-25T17:56:28.793,2017-11-25T17:56:28.793,applying nns to 2d image noise reduction ?,deep-learning,1,0,
1072,4599,1,4602,2017-11-25T22:39:06.307,0,1686,"i recently started to follow along with siraj raval 's deep learning tutorials on youtube , but i an error came up when i tried to run my code . the code is from the second episode of his series , how to make a neural network . when i ran the code i got the error : traceback ( most recent call last ) : file "" c:\users\dpopp\documents\machine learning\first_neural_net.py "" , line 66 , in & lt;module&gt ; neural_network.train(training_set_inputs , training_set_outputs , 10000 ) file "" c:\users\dpopp\documents\machine learning\first_neural_net.py "" , line 44 , in train self.synaptic_weights + = adjustment valueerror : non - broadcastable output operand with shape ( 3,1 ) does n't match the broadcast shape ( 3,4 ) i checked multiple times with his code and could n't find any differences , and even tried copying and pasting his code from the github link . this is the code i have now : from numpy import exp , array , random , dot class neuralnetwork ( ) : def _ _ init__(self ) : # seed the random number generator , so it generates the same numbers # every time the program runs . random.seed(1 ) # we model a single neuron , with 3 input connections and 1 output connection . # we assign random weights to a 3 x 1 matrix , with values in the range -1 to 1 # and mean 0 . self.synaptic_weights = 2 * random.random((3 , 1 ) ) - 1 # the sigmoid function , which describes an s shaped curve . # we pass the weighted sum of the inputs through this function to # normalise them between 0 and 1 . def _ _ sigmoid(self , x ) : return 1 / ( 1 + exp(-x ) ) # the derivative of the sigmoid function . # this is the gradient of the sigmoid curve . # it indicates how confident we are about the existing weight . def _ _ sigmoid_derivative(self , x ) : return x * ( 1 - x ) # we train the neural network through a process of trial and error . # adjusting the synaptic weights each time . def train(self , training_set_inputs , training_set_outputs , number_of_training_iterations ) : for iteration in range(number_of_training_iterations ) : # pass the training set through our neural network ( a single neuron ) . output = self.think(training_set_inputs ) # calculate the error ( the difference between the desired output # and the predicted output ) . error = training_set_outputs - output # multiply the error by the input and again by the gradient of the sigmoid curve . # this means less confident weights are adjusted more . # this means inputs , which are zero , do not cause changes to the weights . adjustment = dot(training_set_inputs.t , error * self.__sigmoid_derivative(output ) ) # adjust the weights . self.synaptic_weights + = adjustment # the neural network thinks . def think(self , inputs ) : # pass inputs through our neural network ( our single neuron ) . return self.__sigmoid(dot(inputs , self.synaptic_weights ) ) if _ _ name _ _ = = ' _ _ main _ _ ' : # initialize a single neuron neural network neural_network = neuralnetwork ( ) print(""random starting synaptic weights : "" ) print(neural_network.synaptic_weights ) # the training set . we have 4 examples , each consisting of 3 input values # and 1 output value . training_set_inputs = array([[0 , 0 , 1 ] , [ 1 , 1 , 1 ] , [ 1 , 0 , 1 ] , [ 0 , 1 , 1 ] ] ) training_set_outputs = array([[0 , 1 , 1 , 0 ] ] ) # train the neural network using a training set # do it 10,000 times and make small adjustments each time neural_network.train(training_set_inputs , training_set_outputs , 10000 ) print(""new synaptic weights after training : "" ) print(neural_network.synaptic_weights ) # test the neura net with a new situation print(""considering new situation [ 1 , 0 , 0 ] -&gt ; ? : "" ) print(neural_network.think(array([[1 , 0 , 0 ] ] ) ) ) even after copying and pasting the same code that worked in siraj 's episode , i 'm still getting the same error . i just started out look into artificial intelligence , and do n't understand what the error means . could someone please explain what it means and how to fix it ? thanks !",11083,,,2017-11-26T21:41:35.160,"error building neural net : valueerror : non - broadcastable output operand with shape ( 3,1 ) does n't match the broadcast shape ( 3,4 )",neural-networks machine-learning,1,0,
1073,4600,1,,2017-11-25T23:13:13.897,1,83,"i have a neural network that i 'm want to use to self - play connect four . the neural network receives the board state and is to provide an estimate of the states desirability . i would then , for each move , use the highest estimate , occasionally i will use one of the other moves for exploration . i intend to use td lamba to calculate the errors for each state to back propagate through the network . but i 'm confused about when this should actually occur . do i store the estimate of a state that is used and calculate the error based on the next state chosen ? or do i store a history of all states and back propagate only when the game is a win / lose / draw ? i guess overall i 'm not sure i understand when the update occurs , partially because i do n't quite understand how to implement the lambda ? like if i was to apply to back prop after every move , how would i even know the value of lambda at this time - step before i know how long the game will last ? when self - playing , is the error the difference between that "" sides "" last move ? i.e. i compare move 1 against move 3 , and move 2 against move 4 etc ?",2819,2819,2017-11-26T23:16:19.637,2017-11-26T23:16:19.637,when do you back - propagate errors through a neural network when using td lambda,neural-networks training backpropagation,0,1,
1074,4601,1,,2017-11-26T10:41:21.500,1,1314,"i am a c # senior developer and i got a task to try and predict the potential in each new client , or maybe the worth of each customer . i do n't have experience with machine learning , but i played with accord-framework.net and got some nice results on simple task . my data model for training is : geolocation , // the country of ip when registed . iso code string age , // number dateregistered , //date time email , //string can be broken to vendors as catergorial ( gmail , yahoo , microsoft and such ) emailvalidated , //is the email really exists . bool phonenumber , //string phonenumbervalidated , // is the phone number really exists campaignname , //string ( may be categirial ) useragent , //string should i make it categorial ? ( has info about browser , device , verndor , operation system and such , long string ) landedonpage , //string first url the customer entered from registeredfrompage , //string url of the page that the user registered from refererurl , //string url the client came to our site from , numberofpurchases , //the amount of times the customer puschase something on our site customervalueusd , //the total amount of usd the customer spent in our site the output shoud be customervalueusd i have a lot of data in the history , so i can back test it . my questions : does it make sense to do this task even though i do n't have an experience with machine learning ? how complicated is this task considering i 'm using a well known framework ? assuming that i 'm taking the task , which algorithm should i choose to perform this kind of task ? how should i build the training data ? see my comments , do you think my comments are ok to start with ? or maybe i can break the data directly ?",11133,15219,2018-04-26T17:11:20.607,2018-06-26T03:17:10.907,which algorithm should i choose for lead scoring,machine-learning,2,1,
1075,4606,1,,2017-11-27T17:17:55.997,1,50,"i have a problem that i have been trying to use decision trees to solve . there is a data set of pricing information for products sold by a company . the goal is to infer the pricing algorithm for each product based on the product 's name , type , and characteristics . however , the data is incomplete and may contain a few inaccuracies due to manual data entry . products can be priced using several possible algorithms : price is a fixed dollar amount ( widget costs $ 20 ) price is a percentage of msrp ( widget costs x% of msrp , note x could vary by product classes ) price is based on a cost per piece ( package of widgets costs $ 2 per piece ) using decision trees , i have been able to identify the products priced using algorithm # 1 , but have struggled to make progress on algorithm # 2 and # 3 . since price is a continuous variable , the leaf nodes always represent the average of the target variable . but this does n't seem to work for # 2 and # 3 , since these would be a function of other variables , not the dependent variable . is there a clever way to implement this using decisions trees ? any recommended alternate techniques that could identify the pricing algorithms used ? note , i am not interested in predictions , but rather explain the pricing rules / algorithm that fit the existing data . i am using sas enterprise miner , but may have access to other tools as well .",11153,,,2017-11-27T17:17:55.997,"decision tree does n't quite work , is there a better alternative ?",machine-learning,0,0,1
1076,4607,1,,2017-11-28T05:50:02.307,3,665,"from my understanding and text i found in research papers online : 1 ) pixel based object recognition : neural networks are trained to locate individual objects based directly on pixel data . 2 ) feature based object recognition : contents of a window are mapped to a feature space that is provided as input to a neural classifier . what do the above 2 definitions imply in a simpler language ? especially point number 2 . also , can someone point me to papers / resources / articles that would explain the above approaches in greater detail ?",11169,,,2017-11-28T09:45:11.927,what is the difference between ' pixel ' based object recognition and ' feature ' based object recognition ?,neural-networks machine-learning deep-learning computer-vision object-recognition,1,0,3
1077,4610,1,,2017-11-29T01:43:54.713,3,219,"goal - driven ais is the only kind of ai i am aware of . however , marcus hutter claims the following most , if not all known facets of intelligence can be formulated as goal driven or , more generally , as maximizing some utility function . it is , therefore , sufficient to study goal driven ai . which does n't necessarily imply that there are other types of ais ( apart from goal - driven ) , but ( at least , in the way it is phrased ) suggests that there are other types of ais . if there exist other types of ais , which are they ?",2444,2444,2017-11-29T19:04:37.350,2018-11-24T19:26:50.280,what other kind of ais exist apart from goal - driven ?,strong-ai definitions terminology,4,6,2
1078,4612,1,,2017-11-29T08:18:54.830,2,89,"while unifying two literals , can we unify a positive and negative literal . for e.g , p(x ) with ~p(a ) ? is this unifiable using the substitution { x / a}. russel & amp ; norvig textbook does n't say anything about unifying positive and negative literals while slides from university of toronto says it is possible .",4202,,,2017-11-29T08:18:54.830,first order logic : unification of positive and negative literals,algorithm,0,0,
1079,4613,1,,2017-11-29T09:49:06.553,4,208,"i have created a gomoku(5 in a row ) ai using alpha - beta pruning . it makes moves on a not - so - stupid level . first , let me vaguely describe the grading function of the alpha - beta algorithm . when it receives a board as an input , it first finds all repetitions of stones and gives it a score out of 4 possible values depending on its usefulness as an threat , which is decided by length . and it will return the summation of all the repetition scores . but , the problem is that i explicitly decided the scores(4 in total ) , and they do n't seem like the best choices . so i 've decided to implement a genetic algorithm to generate these scores . each of the genes will be one of 4 scores . so for example , the chromosome of the hard - coded scores would be : [ 5 , 40000,10000000,50000 ] however , because i 'm using the genetic algorithm to create the scores of the grading function , i 'm not sure how i should implement the genetic fitness function . so instead , i have thought of the following : instead of using a fitness function , i 'll just merge the selection process together : if i have 2 chromosomes , a and b , and need to select one , i 'll simulate a game using both a and b chromosomes in each ai , and select the chromosome which wins . 1.is this a viable replacement to the fitness function ? 2.because of the characteristics of the alpha - beta algorithm , i need to give the max score to the win condition , which in most cases is set to infinity . however , because i ca n't use infinity , i just used an absurdly large number . do i also need to add this score to the chromosome ? or because it 's insignificant and does n't change the values of the grading function , leave it as a constant ? 3.when initially creating chromosomes , random generation , following standard distribution is said to be the most optimal . however , genes in my case have large deviation . would it still be okay to generate chromosomes randomly ?",11207,,,2018-05-13T10:47:14.587,fitness function altenatives in genetic algorithms for game ai,genetic-algorithms game-ai minimax,1,1,1
1080,4614,1,,2017-11-29T10:41:16.767,2,122,"lets say i have a limited amount of training data ( 1000 documents each having 10000 values ) and after learning with those , the program is basically not allowed to fail . from what i 've read , even very easy tasks take a machine learning algorithm several thousand learning cycles until it is about 98 % reliable . which parameters need to be tweaked so that even with many many values , the program can make reliable decisions ?",10872,,,2018-03-29T21:45:51.307,what parameters to tweak to improve reliability ( machine learning ) ?,neural-networks machine-learning deep-learning,1,0,
1081,4616,1,,2017-11-29T13:38:30.750,5,281,"we all know how robots are getting more and sophisticated and more interesting what is the future of robotics in relation to ai , how and how will ai work with robotics in improving to affect our world in a positive way .",11210,1671,2017-11-29T18:06:12.230,2018-06-01T01:34:29.997,how have robots developed and how sophisticated might they be in the future ?,robotics,1,1,1
1082,4620,1,4627,2017-11-29T20:16:43.223,0,98,would it not even be more helpful ? it will have more time to transform the input with the extra zeros padded to the end .,9271,,,2017-11-30T19:07:50.017,what 's wrong with getting a dynamic rnn 's output at the end of a padded sequence ?,machine-learning recurrent-neural-networks,1,0,
1083,4621,1,,2017-11-29T20:57:01.877,1,45,"i read these things on the internet like my model determines the future scope ... "" or my model gives accurate readings about what the score would be ... "" what are these models ? how are they designed ?",11220,1671,2017-12-01T15:56:03.807,2017-12-01T15:56:03.807,what is a model and how is it designed ?,terminology models,1,5,
1084,4623,1,,2017-11-30T02:30:44.337,0,101,"technically speaking , could we code in natural language once we pass the turing test ? would passing the turing test at least simplify programming languages ' syntax ?",11224,4302,2018-10-08T12:19:52.467,2018-10-08T12:19:52.467,could we code in natural language once we pass the turing test ?,natural-language human-like programming-languages,2,1,
1085,4624,1,,2017-11-30T11:44:52.770,3,911,"from this link , alphago would take millenia to run in regular hardware . they generated 29 million games for the final result , which means it 's going to take me about 1700 years to replicate this . are these calculations correct ?",7496,,,2018-08-13T23:00:52.193,would it take 1700 years to run alphago zero in commodity hardware ?,neural-networks,1,1,
1086,4629,1,,2017-11-30T20:57:09.410,5,1621,"a blog post called "" text classification using neural networks "" states that the derivative of the output of a sigmoid function is used to measure error rates . what is the rationale for this ? i thought the derivative of a sigmoid function output is just the slope of the sigmoid line at a specific point . meaning it 's steepest when sigmoid output is 0.5 ( occuring when the sigmoid function input is 0 ) . why does a sigmoid function input of 0 imply error ( if i understand correctly ) ? source : https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6 we use a sigmoid function to normalize values and its derivative to measure the error rate . iterating and adjusting until our error rate is acceptably low . def sigmoid(x ) : output = 1/(1+np.exp(-x ) ) return output def sigmoid_output_to_derivative(output ) : return output*(1-output ) def train ( ... ) ... layer_2_error = y - layer_2 layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2 ) ... update apologies . i do n't think i was clear ( i 've updated the title ) i understand we do n't need to use sigmoid as the activation funtion ( we could use relu , tanh or softmax ) . my question is about using the derivative to measure the error rate ( full quotation from article above in yellow ) - > what does the derivative of the activation function have to do with measuring / fixing the "" error rate "" ?",10623,10623,2017-12-01T16:12:53.823,2018-06-22T20:49:44.953,how does an activation function 's derivative measure error rate in a neural network ?,neural-networks training,5,0,0
1087,4634,1,,2017-12-01T03:55:13.087,2,32,"in the paper ask me anything : dynamic memory networks for natural language processing the authors described a dynamic memory network in the context of question answering . then , they also tested the network on sentiment classification and part - of - speech tagging . in those applications , what is the ' question ' being inputted to the network ?",7419,,,2017-12-01T03:55:13.087,"what is the "" question "" when using dynamic memory networks to do part - of - speech tagging and sentiment analysis ?",deep-learning natural-language-processing,0,0,
1088,4635,1,4640,2017-12-01T08:44:23.637,1,41,"i have a mysql database that contains datetime for every check - in with an rfid card . i have millions of records in it . it shouts machine learning prediction for me . so i 'd like to predict popular times to see when most people use the terminal . i 'd like to represent it the same way as google does at places : this is the rare case when i do not ask for code , but keywords . i assume i have to predict number of check - ins between time ranges . also i am sure i have to make a dataset like this : ( these are the number of check - ins in hour ranges ) month | day | day - name | 0 - 1 | 1 - 2 | ... | 6 - 7 etc . jan | 1 | monday | 0 | 1 | | 12 jan | 2 | tuesday | 1 | 3 | | 15 so it can predict today 's popular times based on what day it is , also by day of given month ( as saturdays and sundays will be dead , also 25th of december . a good prediction should know these from the dataset ) . as i wrote this question i solved most of it it seems . the only thing i need is a keyword as i have little experience in this . what model fits this best ?",11263,,,2017-12-01T16:39:25.707,predicting popular times,machine-learning prediction,1,1,
1089,4641,1,4659,2017-12-01T19:14:27.803,-2,370,"i am using the following , fairly simple code to predict an output variable which may have 3 categories : n_factors = 20 np.random.seed = 42 def embedding_input(name , n_in , n_out , reg ) : inp = input(shape=(1 , ) , dtype='int64 ' , name = name ) return inp , embedding(n_in , n_out , input_length=1 , w_regularizer = l2(reg))(inp ) user_in , u = embedding_input('user_in ' , n_users , n_factors , 1e-4 ) artifact_in , a = embedding_input('artifact_in ' , n_artifacts , n_factors , 1e-4 ) mt = input(shape=(31 , ) ) mr = input(shape=(1 , ) ) sub = input(shape=(24 , ) ) def onehot(featurename ) : onehot_encoder = onehotencoder(sparse = false ) onehot_encoded = onehot_encoder.fit_transform(modality_durations[featurename].reshape(-1 , 1 ) ) trn_onehot_encoded = onehot_encoded[msk ] val_onehot_encoded = onehot_encoded[~msk ] return trn_onehot_encoded , val_onehot_encoded trn_onehot_encoded_mt , val_onehot_encoded_mt = onehot('modality_type ' ) trn_onehot_encoded_mr , val_onehot_encoded_mr = onehot('roleid ' ) trn_onehot_encoded_sub , val_onehot_encoded_sub = onehot('subject ' ) trn_onehot_encoded_quartile , val_onehot_encoded_quartile = onehot('quartile ' ) # model x = merge([u , a ] , mode='concat ' ) x = flatten()(x ) x = merge([x , mt ] , mode='concat ' ) x = merge([x , mr ] , mode='concat ' ) x = merge([x , sub ] , mode='concat ' ) x = dense(10 , activation='relu')(x ) batchnormalization ( ) x = dense(3 , activation='softmax')(x ) nn = model([user_in , artifact_in , mt , mr , sub ] , x ) nn.compile(loss='sparse_categorical_crossentropy ' , optimizer='adam ' , metrics=['accuracy ' ] ) nn.optimizer.lr = 0.001 nn.fit([trn.member_id , trn.artifact_id , trn_onehot_encoded_mt , trn_onehot_encoded_mr , trn_onehot_encoded_sub ] , trn_onehot_encoded_quartile , batch_size=256 , epochs=2 , validation_data=([val.member_id , val.artifact_id , val_onehot_encoded_mt , val_onehot_encoded_mr , val_onehot_encoded_sub ] , val_onehot_encoded_quartile ) ) here 's the summary of the model : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ layer ( type ) output shape param # connected to = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = user_in ( inputlayer ) ( none , 1 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ artifact_in ( inputlayer ) ( none , 1 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ embedding_9 ( embedding ) ( none , 1 , 20 ) 5902380 user_in[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ embedding_10 ( embedding ) ( none , 1 , 20 ) 594200 artifact_in[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ merge_25 ( merge ) ( none , 1 , 40 ) 0 embedding_9[0][0 ] embedding_10[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ flatten_7 ( flatten ) ( none , 40 ) 0 merge_25[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ input_13 ( inputlayer ) ( none , 31 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ merge_26 ( merge ) ( none , 71 ) 0 flatten_7[0][0 ] input_13[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ input_14 ( inputlayer ) ( none , 1 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ merge_27 ( merge ) ( none , 72 ) 0 merge_26[0][0 ] input_14[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ input_15 ( inputlayer ) ( none , 24 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ merge_28 ( merge ) ( none , 96 ) 0 merge_27[0][0 ] input_15[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dense_13 ( dense ) ( none , 10 ) 970 merge_28[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dense_14 ( dense ) ( none , 3 ) 33 dense_13[0][0 ] = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = total params : 6,497,583 trainable params : 6,497,583 non - trainable params : 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ but on the fit statement , i get the following error : --------------------------------------------------------------------------- valueerror traceback ( most recent call last ) & lt;ipython - input-71 - 7de0782d7d5d&gt ; in & lt;module&gt ; ( ) 5 batch_size=256 , 6 epochs=2 , ----&gt ; 7 validation_data=([val.member_id , val.artifact_id , val_onehot_encoded_mt , val_onehot_encoded_mr , val_onehot_encoded_sub ] , val_onehot_encoded_quartile ) 8 ) 9 # nn.fit([trn.member_id , trn.artifact_id , trn_onehot_encoded_mt , trn_onehot_encoded_mr , trn_onehot_encoded_sub ] , trn.duration_new , /home / prateek_dl / anaconda3 / lib / python3.5 / site - packages / keras / engine / training.py in fit(self , x , y , batch_size , epochs , verbose , callbacks , validation_split , validation_data , shuffle , class_weight , sample_weight , initial_epoch , steps_per_epoch , validation_steps , * * kwargs ) 1520 class_weight = class_weight , 1521 check_batch_axis = false , -&gt ; 1522 batch_size = batch_size ) 1523 # prepare validation data . 1524 do_validation = false /home / prateek_dl / anaconda3 / lib / python3.5 / site - packages / keras / engine / training.py in _ standardize_user_data(self , x , y , sample_weight , class_weight , check_batch_axis , batch_size ) 1380 output_shapes , 1381 check_batch_axis = false , -&gt ; 1382 exception_prefix='target ' ) 1383 sample_weights = _ standardize_sample_weights(sample_weight , 1384 self._feed_output_names ) /home / prateek_dl / anaconda3 / lib / python3.5 / site - packages / keras / engine / training.py in _ standardize_input_data(data , names , shapes , check_batch_axis , exception_prefix ) 142 ' to have shape ' + str(shapes[i ] ) + 143 ' but got array with shape ' + --&gt ; 144 str(array.shape ) ) 145 return arrays 146 valueerror : error when checking target : expected dense_14 to have shape ( none , 1 ) but got array with shape ( 1956554 , 3 ) how do i resolve this error ? why is the final layer expecting ( none,1 ) when according to the summary ( ) it has to output ( none,3 ) ? any help would be greatly appreciated .",8371,8371,2017-12-01T19:46:28.840,2017-12-04T21:44:44.757,problems trying to predict a categorical variable in keras,neural-networks deep-learning keras,1,1,
1090,4643,1,,2017-12-01T21:59:34.857,4,1645,"i 'm attempting to make a bot for the connect 4 competition on http://riddles.io my bot is n't horrible , like it 's getting up the ladder , but it can not compete with the winning bots . i 'm using a neural network which is fully connected with one hidden layer . internally it uses the sigmoid function as the activator in each neuron . i 've trained it over 500,000 games with td - lambda back propagation . the alpha and beta values ( i.e. learning rates ) are set to 0.1 each , and the lambda for the eligibility trace is set to 0.7 . there are 2 outputs nodes , one to give the value of this position for player 1 , the other to give the value of the position for player 2 . upon a win , these are back - propagated with -1 for a loss and 1 for a win , on a draw they are both back - propagated with a 0 . there is a bias input and weight for every neuron as well . all of the weights are initialised to a random value between + -4*sqrt(6 / totalnumberweights ) . the board state is represented to the network as : for each space in the board , 2 values : for the first value if player 1 occupies this space it 's a 1 , otherwise it 's a 0 for the second value if player 2 occupies this space it 's a 1 , otherwise it 's a 0 if both are 0 it would mean it 's a free spot for each space on the board another 2 values : for the first value , if placing a token here would result in a connect 4 for player 1 , it 's a 1 , otherwise a 0 for the second value , if placing a token would get player 2 a connect 4 , then it 's a 1 otherwise a 0 so if no one would win by placing a token here , it 's two 0 values for these two inputs two final values , the first indicating whether it 's player 1 's turn or not , then second indicating if it 's player 2 's turn or not what i see when my bot is playing is that it makes what i assume are somewhat clever moves , like it 's preparing for both horizontal and diagonal moves . but when the other bot will get a connect 4 on the next move , my bot fails to place a token there . the best bot in the comp seems capable of setting itself up to get three in a row with a free space on both sides , so that it will definitely get a connect 4 . again , my bot does not seem to be able to see this coming . what i think the issue is , other than potentially my learning rates , is that i 've represented the board in a bad way . is there a better way to represent it to the network so that it can more accurately estimate the value of a board state , and so that it does n't fail to identify an immediate connect 4 threat ?",2819,2819,2017-12-01T23:34:04.463,2019-01-27T03:02:55.513,is this a good way to represent connect 4 to a neural network ?,neural-networks game-ai combinatorial-games,0,3,
1091,4644,1,,2017-12-01T23:39:24.043,2,128,"i would like to measure time for forward and backward times on tf - slim ( over all network and per - layer ) like caffe does . however , it just logs the step / iteration time , and i have no idea of how to do that without changing the source and recompiling ( takes a lot of time on my machine ) i also would like to log a not - rounded loss value ( it rounds to 4 digits after dot )",11289,,,2017-12-01T23:39:24.043,how to measure times on tensorflow slim ?,tensorflow,0,0,
1092,4645,1,4646,2017-12-02T14:55:15.743,4,115,the naive bayes ' generative algorithm is often represented by the following formula $ $ why do we have the approximation $ p(y ) \approx 1 $ ? why do we have the approximation ?,11303,2444,2019-04-19T18:39:51.593,2019-04-19T18:39:51.593,why is the denominator ignored in the bayes ' rule ?,machine-learning math naive-bayes bayes-theorem,1,1,1
1093,4647,1,,2017-12-02T22:32:26.803,6,175,"there has been recent uptick in interest on xai or e x plainable a rtificial i ntelligence . here is xai 's mission as stated on its darpa page : the explainable ai ( xai ) program aims to create a suite of machine learning techniques that : produce more explainable models , while maintaining a high level of learning performance ( prediction accuracy ) ; and enable human users to understand , appropriately trust , and effectively manage the emerging generation of artificially intelligent partners . here is the link to a recent new york times piece on the same - the article does a good job of explaining the need for xai from a human interest perspective as well as providing a glancing outlook on the techniques being developed for the same . the force behind xai movement seems to center around the ( up & amp ; coming ? ) concept of right to explanation - the requirement that applications based on ai that significantly impact human lives via their decisions be able to explain to stakeholders the factors / reasons leading up to said decision . my question is as follows : how is it reasonable , the right to explanation , given the current standards at which we hold each other accountable ?",11166,,,2018-02-18T09:56:16.283,explainable ai and the right to explanation,philosophy ethics,1,4,3
1094,4650,1,,2017-12-04T06:03:27.163,2,750,i am currently looking for ai use cases for telco . what are the different ai use cases for telcos / communication service providers ?,11346,19413,2019-03-10T20:19:55.447,2019-03-10T20:19:55.447,what are ai use cases for communication service providers ?,applications,2,0,
1095,4655,1,4707,2017-12-04T14:42:49.947,13,1568,"given a neural network $ f$ that takes as input $ n$ data points : $ x_1 , \dots , x_n$ . we say $ f$ is permutation invariant if $ $ f(x_1 ... x_n ) = f(pi(x_1 ... x_n))$$ for any permutation $ pi$ . could someone recommend a starting point ( article , example , or other paper ) for permutation invariant neural networks ?",11359,2444,2019-04-25T21:01:55.643,2019-04-26T13:44:37.133,permutation invariant neural networks,neural-networks machine-learning reference-request,3,6,3
1096,4660,1,4668,2017-12-04T23:12:58.733,2,2839,"as i am trying to make an ai with reinforcement learning , i have found out and implemented a lot of things such as both these topics ( nns and rl ) separately . but when trying to combine them , i have ran into trouble . i have not been able to find or think of a way to properly do backpropagation with rl . so what i was trying to do was a local search for all actions and then use a neural net for the q(s , a ) function . how would one do the backpropagation in such a neural net ? up to this point , i have only done things with gradient descent . should one use a different algorithm ? could one calculate the q(s , a ) value based on the output of the neural net with a discount factor ? this is what the formula would suggest , but i could not find any confirmation .",10364,1671,2018-07-09T20:36:29.230,2018-08-21T03:43:41.180,how to combine backpropagation in neural nets and reinforcement learning ?,neural-networks machine-learning reinforcement-learning backpropagation,2,3,4
1097,4663,1,4665,2017-12-05T01:22:40.723,2,203,"collecting and labeling training data for a supervised learning tasks is incredibly time - consuming and costly . for instance ; let 's say you wrote a script that went on google images and got you 5000 pictures for each of 10 classes . you then use an unsupervised algorithm to cluster them . then , you train another , supervised algorithm using the labels from the scraper as ground truth . obviously , your network will perform more poorly than one with perfectly labelled data , but is there a way to guesstimate how much ? perhaps there are 50 mislabeled images in each class . that would most likely be better than 500 mislabeled images , but i 'm wondering if there is a way to predict how much ( even if it is by someone 's rules of thumb or something like that ) .",8829,1581,2017-12-08T20:42:24.380,2017-12-18T20:38:17.267,what is the effect of mislabeled training data ?,neural-networks machine-learning datasets,1,0,
1098,4666,1,4667,2017-12-05T07:34:53.323,1,1385,"i 'm training two cnns ( alexnet e googlenet ) in two differents dl libraries ( caffe e tensorflow ) . the networks was implemented by dev teams of each libraries ( here and here ) i reduced the original imagenet dataset to 1024 images of 1 category -- but setted 1000 categories to classify on the networks . so i trained the cnns , varying processing unit ( cpu / gpu ) and batches sizes , and i observed that the losses converges fastly to near zero ( in mostly times before 1 epoch be completed ) , like in this graph ( alexnet on tensorflow ) : in portuguese , ' épocas ' is epochs and ' perda ' is loss . the weight decays and initial learning rate are the same as used on models that i downloaded , i only changed the dataset and the batch sizes . why my networks are converging this way , and not like this way ?",11289,,,2017-12-05T17:55:48.067,what fast loss convergence indicates on a cnn ?,deep-learning convolutional-neural-networks training tensorflow,1,2,
1099,4669,1,,2017-12-06T02:14:32.017,2,61,"let 's suppose i have 5 images , all of which i assure you are of the same item , but from various angles and perhaps different lighting conditions . i now supply you with an additional image , and i want a score of how likely this image is to contain the item depicted in the first five pictures . let us suppose that the item is n't too complex . it wo n't be a pile of fabric with a pattern on it , dropped several different ways , and wo n't be a keychain with keys in different conformations . it will also be more complex than just a blue ball shot from different angles . how might you approach the problem of scoring this image ?",11410,,,2017-12-08T17:17:56.303,identifying if an image contains an object with very small ( five image ) training data set,image-recognition training,2,0,1
1100,4671,1,,2017-12-06T09:09:36.967,1,278,"i am trying to build a neural network suitable to measure similarity between pairs of images . in particular i am interested in shoes . i have a query image ( e.g. a shoe that i just took a picture of ) and i want to find similar shoes in a database ( several thousands of images ) . i tried using mac feature ( e.g. max pool over the entire spacial dimension on last ( or some other ) convolution layer of say vgg16 ) ( here is a link to a paper https://arxiv.org/pdf/1511.05879.pdf ) . the two mac vectors are compared using cosine similarity . that works , but among the top matches there are always a few very strange shoes ( e.g when i submit a query image with a boot i find sandals among other boots with extremely high similarity score ) . what would be a better way of doing that ? something more robust to finding shoes similar in shape to the query image . thanks !",11417,,,2017-12-06T09:09:36.967,similarity of images ( cbir ) with cnn features,convolutional-neural-networks,0,0,
1101,4672,1,,2017-12-06T19:00:23.507,0,356,"can an ai learn to play chess if you give it nothing but "" the goal is to win "" as starting criteria ? if not , what is the minimum information the ai would need to be "" seeded "" with in order to learn to play chess ? what techniques could be used to create an ai that learns to play chess independently ?",7801,33,2017-12-11T20:13:41.930,2018-12-12T06:53:16.953,can an ai learn how to play chess without instructions ?,gaming game-ai chess combinatorial-games,7,2,
1102,4675,1,4676,2017-12-07T02:19:36.767,3,503,"currently ai is advancing fast in deep learning : entire human chess knowledge learned and surpassed by deepmind 's alphazero in four hours . as a layman , i 'm taking this as a quite powerful searching algorithm , using artificial neural networks to identify the patterns of each game . however , how good is ai doing in math ? for example , the key to the theory of the game nim is the binary digital sum of the heap sizes , that is , the sum ( in binary ) neglecting all carries from one digit to another . this operation is also known as "" exclusive or "" ( xor ) or "" vector addition over gf(2 ) "" . is ai good enough to discover / invite operations / logics such as "" exclusive or "" , or , more advanced , abstract algebra in finite field ?",11440,9647,2017-12-09T23:14:14.253,2018-09-25T21:16:16.993,how good is ai in math ?,strong-ai,3,1,1
1103,4677,1,,2017-12-07T14:32:11.810,7,290,"i am using keras to train different nn . i would like to know why if i increment the epochs in 1 , the result until the new epoch is not the same . i am using shuffle = false , and np.random.seed(2017 ) , and i have check that if i repeat with the same number of epochs , the result is the same , so not random initialization is working . here i attach the picture of the resulting training with 2 epochs : and here i attach the picture of the resulting training with 3 epochs : also , i would like to know why the training time is not ( 3/2 ) and how is it possible that some of them have less accuracy with one more epoch . thanks a lot !",11391,,,2018-05-31T22:09:15.300,why does ' loss ' change depending on the number of epochs chosen ?,neural-networks training optimization keras,1,1,1
1104,4678,1,4704,2017-12-07T17:17:44.947,3,517,"i was thinking , what if we could combine artificial intelligence ( neural network for image recognition ) , computer hardware and a security camera for identify any breaking into our backyard at 12:00am - 8:00am ? of course my current knowledge leads me to only a simple question . so , in order to have a general idea : ¿ have been this already solved using a commercial or free software ? ¿ can this be done using tensorflow ? ¿ is there any free set of images with millions of them to teach any ai distinguish between a man and another moving object ? ¿ approximate hardware requirements for doing this ? if this question could be silly please mark it as off - topic . i based this idea on autonomous driving car , they can both recognize images and drive at the same time . unless they have within a super computer i guess maybe the previous idea can be fulfill . update 1 : i found this can convnets be used for real - time object recognition from video feed ? but i guess it could be outdated . right now i 'm in the land of "" maybe "" ( lack of knowledge ) .",11451,11451,2017-12-07T20:18:49.093,2017-12-10T21:04:32.487,real time image processing for object recognition using security cameras,neural-networks image-recognition,1,9,
1105,4680,1,,2017-12-07T21:00:24.577,4,61,"i 'm trying to get up to speed on the latest research regarding indoor localization , scene classification , navigation in changing environment , etc . any advice would be appreciated , but i 'm especially interested in recent research papers from vetted sources .",11455,1671,2017-12-08T19:03:45.647,2019-03-11T20:49:33.250,current research on indoor localization and navigation in changing environment ?,machine-learning path-planning scene-classification,1,4,1
1106,4681,1,,2017-12-08T03:15:30.290,1,25,"i 'm trying to understand how to build the ann on cognitrons , so i have read theory for that topic and found the scheme : as i got neurons are subdivided in two classes : the exciting and the inhibitory . i have written these classes : exciting neuron : typedef float signal ; typedef std::vector&lt;signal&gt ; sigvec ; class inhibitory_neuron { public : inhibitory_neuron ( ) { } ~inhibitory_neuron ( ) { } virtual signal call(const sigvec & amp;e_inputs , const sigvec & amp;i_inputs ) { unused(i_inputs ) ; signal sum = 0 ; for ( auto i = std::begin(e_inputs ) ; i ! = std::end(e_inputs ) ; + + i ) { sum + = * i ; } return sum ; } } ; here e_inputs are c -factors ( sum of c is 1 ) . but in the formula : i ca n't get it ... whaz out_i there ? and below my exciting neuron class is : class exciting_neuron : inhibitory_neuron { protected : std::unique_ptr&lt;sigvec&gt ; m_e_weights ; std::unique_ptr&lt;sigvec&gt ; m_i_weights ; public : exciting_neuron(sigvec & amp;&amp;e_weights , sigvec & amp;&amp;i_weights ) : m_e_weights{std::make_unique&lt;sigvec&gt;(std::move(e_weights ) ) } , m_i_weights{std::make_unique&lt;sigvec&gt;(std::move(i_weights ) ) } { } ~exciting_neuron ( ) { } signal call(const sigvec&amp ; e_inputs , const sigvec&amp ; i_inputs ) override { auto ew = * m_e_weights ; auto iw = * m_i_weights ; auto ei = e_inputs ; auto ii = i_inputs ; if ( ew.size ( ) ! = iw.size ( ) || iw.size ( ) ! = ii.size ( ) ) { throw std::invalid_argument(""wrong input size . "" ) ; } signal e_sum = 0 , i_sum = 0 ; for ( unsigned int j = 0 ; j & lt ; ew.size ( ) ; + + j ) { e_sum + = ew[j ] * ei[j ] ; } for ( unsigned int j = 0 ; j & lt ; iw.size ( ) ; + + j ) { i_sum + = iw[j ] * ii[j ] ; } signal n = ( 1 + e_sum ) / ( 1 + i_sum ) - 1 ; return n & gt;= 0 ? n : 0 ; } } ; here e_inputs are a -factors ( exciting ) and i_inputs are b -factors ( inhibitory ) . but how should the ann structure look ? i mean i ca n't get where should i put an exciting neuron and an inhibitory ... has it some samples or rules of the structure , i could n't find ?",9684,9684,2017-12-08T03:24:42.393,2017-12-08T03:24:42.393,the ann is based on cognitrons,artificial-neuron,0,1,
1107,4682,1,,2017-12-08T07:16:45.903,1,234,"i have a ann but when i added two or more features as inputs to it , the accuracy of my ann has been decreased . i have a remaining useful life ( prediction of rul ) problem that i want to predict . when i 've used features like rms and kurtosis or together , in spite of fact that the system should be improved , it is getting worse . why might this be happening ? what are the potential reasons for this degradation in performance ? i know that when we added more nodes in layers ( like hidden layers ) , overlearning can happen . would that be related to my problem re : using more than two features ?",11465,1671,2017-12-08T19:11:28.630,2017-12-08T19:11:28.630,how to improve accuracy of ann,neural-networks prediction rul,0,0,
1108,4683,1,,2017-12-08T14:48:36.857,6,3861,what is the fundamental difference between convolutional neural networks and recurrent neural networks ? where are they applied ?,11256,2444,2019-05-13T21:29:16.820,2019-05-25T08:23:17.223,what is the fundamental difference between cnn and rnn ?,neural-networks convolutional-neural-networks recurrent-neural-networks difference,3,0,4
1109,4692,1,4693,2017-12-09T01:27:18.407,4,340,"i have a 10 gb file of a time series 1d signal . i want to find some patterns within this signal , i know cnn 's are great for this but the problem is i do n't have any training data . now i could of course spend an entire week slowly making 100 versions of a certain pattern to train the cnn with . but maybe there is some other way ? maybe there is a way for the neural network to work out patterns on its own and simply categorize them ? like this is pattern a , this is pattern b. my ultimate goal is to look at any size data and find the occurrences of patterns within the data . does anyone have any idea how this problem could be solved ? i am just starting with machine learning so i am slowly learning of what 's possible in this field . thanks",11476,,,2017-12-09T01:42:04.313,cnn for pattern recognition without a training set,neural-networks convolutional-neural-networks pattern-recognition detecting-patterns signal-processing,1,0,1
1110,4694,1,,2017-12-09T03:04:51.340,2,73,"did alan turing expect the ais to be aware they were being turing tested while the game was being played ? i think it 's slightly harder to look like a duck while pretending to be a duck than when you believe you are a duck . it is a step further . aside from all other criticisms of original article , did alan turing have the time and motivation to address the point in any later writing ?",2023,75,2017-12-09T22:30:54.887,2018-03-23T20:48:37.987,is the turing test a blinded experiment ?,strong-ai history turing-test,1,1,1
1111,4695,1,,2017-12-09T03:31:09.143,2,40,"two practical questions regarding the use of autoencoders . for the size of the nn , let 's say i have 50 inputs , so layer sizes go like 50 - 25 - 12 - 6 - 2 - 6 - 12 - 25 - 50 how much data i generally need to train it and how does it depend on the size of net ? i 'd like to weigh samples differently - e.g. there is a time component to the problem the last samples are most recent , so should weigh more . is there an easy way to incorporate it ?",11478,9647,2017-12-09T14:04:02.413,2018-01-12T04:40:45.207,training data for autoencoders,neural-networks,1,0,
1112,4696,1,4747,2017-12-09T10:20:49.837,9,717,"is it possible to give a rule of thumb estimate about the size of neural networks that are trainable on common consumer grade gpus ? for example : the emergence of locomotion ( reinforcement ) paper trains a network using tanh activation of the neurons . they have a 3 layer nn with 300,200,100 units for the planar walker . but they do n’t report the hardware and time ... but could a rule of thumb be developed ? also just based on current empirical results , so for example : x units using sigmoid activation can run y learning iterations per h on a 1060 . or using activation function a instead of b causes a n times decrease in performance . if a student / researcher / curious mind is going to buy a gpu for playing around with these networks , how do you decide what you get ? a 1060 is apparently the entry level budget option , but how can you evaluate if it is not smarter to just get a crappy netbook instead of building a high power desktop and spend the saved $ on on - demand cloud infrastructure . motivation for the question : i just purchased a 1060 and ( clever , to ask the question afterwards huh ) wonder if i should have just kept the $ and made a google cloud account . and if i can run my master thesis simulation on the gpu .",11429,11429,2017-12-09T19:11:08.780,2018-01-30T09:04:28.727,"what size of neural networks can be trained on current consumer grade gpus ? ( 1060,1070,1080 )",neural-networks,3,0,4
1113,4697,1,,2017-12-09T11:18:02.577,2,96,"in the current rush of artificial intelligence research , fueled by nn , independent of the paper i choose , the nn are always trained by themselves . sure , there are architectures that combine cnn and rnn or lstms in a way that can help to solve multiple problems interacting ( like labeling images with human readable text snippets ) , but independent of this , they always learn by themselves . supervised networks just get a bunch of examples to learn from . reinforcement algorithms run around virtual spaces falling over hundreds of times before learning to walk properly . this might sound silly but no one helps them and no one plays with them . children , researchers , puppies , dolphins , ... all intelligent beings we are aware of interact with each other . there are many forms of learning , one very important one being social learning which includes observational learning . we imitate , copy and learn from others all the time . there is also a new idea of looking at knowledge that suggests that we all are n't as independent as our culture might have taught us to think we are . that knowledge really lies in the connections between individuals instead of inside of each agent itself , just like the connections of neurons are what makes the brain work , not the neurons themselves . could n't such interaction between learning agents be enormously valuable ? if you throw 5 agents in a room ( nn a knowledge based agent a simple pre - programmed one etc ) and let them learn from each other through observation and inspiration , what effects could one expect ? i am not talking about ensemble learning as this just throws several hypotheses together and takes the average over all results . i am talking about a complex way of agents interacting with each other during learning . is there any research on this idea ? if so , what were the results ? this is the only one i found : learning to communicate with deep multi - agent reinforcement learning",11429,9687,2017-12-09T14:33:27.990,2017-12-09T14:33:27.990,"why are neural networks always trained "" by themselves "" ?",unassisted-learning intelligent-agent multi-agent-systems,1,0,
1114,4698,1,,2017-12-09T11:21:31.313,4,180,"how would you solve the problem of identifying certain customer in a grocery store ? suppose our client is already signed - up on our website with an unique i d given to him . to come in to the store , firstly he must place a phone with a qr code in front of the reader , so that our server is informed on a specific client entering the shop . then after successful authentication he heads towards the shelf with goods and pick some item . identifying product name turned out not to be a challenge , as opposed to person . from my perspective , the solution is to make a several photo of face from a different angle while person coming in . quickly train cnn and feed it with face images when customer picks goods in order to choose the corresponding one . so , what are your thoughts on this issue ? what approach would you take to work it out ?",11482,1671,2017-12-09T23:17:17.467,2018-03-09T22:14:09.190,how to identify the face of a certain customer in a grocery store ?,image-recognition facial-recognition,1,2,2
1115,4700,1,,2017-12-09T21:20:16.713,2,97,"at this moment , i am able to use nn to identify object such as human when given a frame from the camera . once locate the object , then i can feed the human object image to either nn that 's designed to classify male or female . let 's say i get 1 frame per second from camera and perform detection , the objective is to track number of male and female walk pass the camera within the given hours . my question is , the same person in multiple frames will be over counted . i could n't wrap my head around how can i train a nn to understand that this is the same person without dive into facial recognition ? i 'm sure there is some tracking technique that i just do n't know . one little constraint , if the person left the camera frame and come back into it later , it is fine to treat it as two people . any help or direction will help ! thank you all in advanced !",11493,,,2018-01-12T05:40:35.973,tracking object,neural-networks computer-vision object-recognition,1,3,
1116,4703,1,,2017-12-10T00:35:09.153,2,160,"i 'm testing various learning rates and neural network configurations . i 'm testing over 10,000 games with the first 2000 having random starting moves and a general randomness throughout of about 20 % , i.e. 20 % of moves are random . in all configurations i initiate the weights to random values . what i 've found is that in all comfigurations , player 1 will win the majority of games , or player 2 will . there 's no 50/50 split . is this expected / normal ?",2819,,,2017-12-10T00:35:09.153,is it expected that during self - play reinforcement learning that player 1 or player 2 wins the majority of games ?,neural-networks reinforcement-learning unassisted-learning,0,10,1
1117,4705,1,4897,2017-12-10T08:34:39.257,1,757,i have deep interest in ai and want to start learning how to implement current method . i know about java and c++ ; are these languages sufficient ? i 'd appreciate suggestions regarding free online courses / websites that utilize java / c++ . if i lag some knowledge which is required before starting ai then please let me know .,11498,1671,2018-03-22T20:32:03.207,2018-03-22T20:32:03.207,how and where can i start learning ai if i have considerable knowledge of java & c++ ?,getting-started java c++,3,1,10
1118,4706,1,4710,2017-12-10T19:29:25.527,4,273,"there seem to be so many sub - fields , so i 'm interested in getting a better understanding of the approaches . i 'm looking for information on a single framework per answer , in order to allow for granularity without the overall answer getting too long . for instance ; deep learning neural networks would be a single answer .",5356,1581,2017-12-12T21:22:14.500,2019-03-19T20:29:26.660,what are different approaches used in machine learning ?,machine-learning definitions terminology,2,0,3
1119,4709,1,4717,2017-12-11T03:03:06.380,1,130,"i heard that your ml model 's quality depends directly on the quality and the quantity of data you use . so i was thinking that can question answers be used as data to train an algorithm which can solve any high school science problems ? because we do have a gazillion number of high school books with millions of question - answers which are both high in quality as well as quantity . p.s : i do n't have any in - depth knowledge in any of the ai fields , so please answer accordingly !",11506,,,2018-01-07T15:41:33.890,can we make an algorithm which can solve any high school ( science ) problem using ml and dl ?,machine-learning deep-learning,3,4,
1120,4711,1,4729,2017-12-11T04:44:30.330,3,710,"trying to understand the vgg architecture and i have these following questions . i understand the general understanding of increasing filter size is because we are using max pooling and so its image size gets reduced . so in order to keep information gain , we increase filter size . but the last few layers in the vgg architecture , the filter size remained same when vgg was max pooling from 14x14 to 7x7 image size , the filter size remained same at 512x512 . why was n’t there the need to increase filter size there ? also few consecutive layers , in the end , was constructed with both same filter and image size , those layers were built just to increase accuracy ? ( experimentation ? ) and i could n’t wrap around that visualization at final filters have the entire face as the feature as i understood through convolution visualizing ( matt zieler video explanation ) . but max pooling causes us to see only a subset part of the image right ? when filter size is 512x512 ( face as the filter / feature ) the image size as 7x7 , so how does entire face as a filter will work on images when we are moving over small subset of the image pixels ?",11508,11508,2017-12-11T04:53:53.160,2017-12-13T03:18:12.600,trying to understand vgg convolution neural networks architecture,convolutional-neural-networks image-recognition,1,0,
1121,4713,1,,2017-12-11T15:53:38.130,1,55,"i 'm solving this problem similar to consumer - producer of materials ( i.e. sand ) . this is the graph of the problem : where req ( e0 and e1 ) are units that require material , and prod ( c0 and c1 ) produce material . i 'm looking for the first random solution or similar that meets the constraints i.e e0 require 10 , shared from c0 or / and c1 . what would be a valid approach ? i just think in genetic algorithms .",11525,,,2017-12-11T15:53:38.130,which algorithm would you use to solve a multiple producer - consumer problem with constraints ?,genetic-algorithms optimization,0,0,1
1122,4714,1,,2017-12-11T16:28:06.863,2,218,"i am planning to build an app which will count the number of sqauts from videos . assuming that the user and camera do not move , are there ways i can count the number of squats ? do such models to understand human activity and pose exist ?",11387,,,2018-01-11T01:14:42.493,is it possible to count the number of squats with computer vision techniques ?,computer-vision,1,0,1
1123,4725,1,4727,2017-12-12T12:14:09.153,9,888,"currently i 'm doing a project that 's about creating an ai to play the game gomoku ( it 's like tic tac toe , but played on a 15 * 15 board and requires 5 in a row to win ) . i have already successfully implemented a perfect tic tac toe ai using q learning and having game states / actions stored in a table , but for a 15 * 15 board the possible game states become too large too implement this project . my question is , should i use neural networks or genetic algorithms for this problem ? and more specifically , how should i implement this ?",11537,,,2017-12-13T18:13:30.030,neural networks vs genetic algorithms in games like tic tac toe ?,neural-networks machine-learning genetic-algorithms combinatorial-games,1,1,3
1124,4737,1,,2017-12-13T13:26:14.637,1,61,"pieter abbeel says that having access to the dynamics model , that is p(s ' | s , a ) , is unrealistic because it assumes we know the probability that we will reach all future states . i do n't understand how this is unreasonable ? could someone explain this to me like in a simple way ?",11566,2444,2017-12-22T17:22:46.880,2017-12-22T17:22:46.880,why is a dynamics model unrealistic in q - learning ?,reinforcement-learning q-learning,1,0,
1125,4738,1,,2017-12-13T13:40:10.167,2,43,"i am thinking of an application where i want to teach a neural network to adjust some parameters on another device for a specific purpose . i say "" adjust some parameters "" because i need the output to "" turn the knobs "" of something else , not only binary "" push buttons "" what kind of neural network do i need ? is it a specific architecture ? the nns i have used so far were sequential and outputting vectors like [ 0 ] or [ 1 ] and classifiers with outputs like [ 1,0,0,0 ] [ 0,1,0,0 ] [ 0,0,1,0 ] [ 0,0,0,1 ] for example but now i need outputs like [ 0.25 , 0.99 , 0.14 , 0.54 ] , 0.25 for knob 1 0.99 for knob 2 0.14 for knob 3 0.54 for knob 4 thank you for your answers",11567,,,2017-12-20T17:09:48.610,"which nn arch ? - given input vec x - > output vector y made of "" not binary "" , floating point values",neural-networks,1,1,
1126,4740,1,4745,2017-12-13T19:25:59.450,5,968,"in the paper human - level control through deep reinforcement learning , the dqn architecture is presented , where the loss function is as follows $ $ l_i(\theta_i ) = \mathbb{e}_{(s , a , r , s ' ) \sim u(d ) } \left [ \left ( r + \gamma \max_{a ' } q(s ' , a ' ; \theta_i^- ) - q(s , a ; \theta ) \right)^2\right ] $ $ where $ r + \gamma \max_{a ' } q(s ' , a ' ; \theta_i^-)$ approximates the "" target "" of $ q(s , a ; \theta)$ . but it is not clear to me why . how can existing weights approximate the target ( ground truth ) ? is n't $ r$ is a sample from a the experience replay dataset ? is $ r$ a scalar value ?",11566,2444,2018-11-19T18:21:18.537,2018-11-19T18:21:18.537,"why is the target $ r + \gamma \max_{a ' } q(s ' , a ' ; \theta_i^-)$ in the loss function of the dqn architecture ?",deep-learning reinforcement-learning q-learning,2,0,2
1127,4743,1,,2017-12-14T12:39:33.200,4,483,"brief idea i want to create an artificial intelligence to compete against other players in a board game . game explanation i have a board game similar to ' snakes and ladders ' . you have to get to a final field before your opponent does . but instead of depending on luck ( throwing the dices ) this game uses something like ' food ' . you can go as far as you 'd like but it costs food to move ( the more you move the more one extra field costs ) and you can only get food on some special fields . and there are n't any snakes or ladders so you have to run the whole part . there are some more rules for example you can go backwards and are only allowed to go into the goal if you 've got less than some amout of ' food ' and there are some extra fields with other special effects . for one player if there was only one player as there is n't anything like ' luck ' in this game i theoretically could just compute every single method to find the one and only best method . practically i should use an algorithm that requires less computational power . for two or more players the challenge comes with the other player(s ) . i can not visit an already taken field . and some other fields give me bonuses depending on my relative position to the other player ( i 'll just talk about two player games ) . for example only if i 'm behind him that special field gives me some extra food . my question first of all , i have to say that i 'm kind of a beginner ( i programmed nn , ga and rl ) when it comes to artificial intelligence ( do n't tell me it could be to difficult for me . i know but i 'm willing to learn and at least i want to try it ;) ) . what would be ideally is if i had some kind of a neural network that knows the field bonuses and i would give my position , the opponents position , the food and so on ( the state of the game ) and it would compute a value between -100 and 100 ( assuming fields from 0 to 100 ) of how many fields i should go ( forward or backward ) . i read a bit about q - learning , deep reinforcement learning and deep neural networks . is this the right approach to solving my problem ? and if yes , have you got any more concrete idea ? the multiple actors and the sheer endless possibilities for moving depending on endless states make it hard for me to think of anything . or is there a different , way better way that slipped past me ? thanks for reading all this and for your help ;)",11585,1671,2017-12-14T16:53:08.563,2017-12-15T19:37:33.080,how do i create an ai for a two players board game ?,deep-learning deep-network game-ai multi-agent-systems combinatorial-games,2,1,
1128,4748,1,4749,2017-12-15T09:56:33.757,5,2948,"i have a two - class classification problem , where false positive error has a very big cost compared to the false negative - error . is there a way to design a classifier for such problems ( preferably with an implementation of the algorithm ) ?",11601,9947,2017-12-15T13:44:16.507,2018-08-07T19:01:36.563,classifier that minimizes false positive error,neural-networks classification training,3,0,1
1129,4750,1,,2017-12-15T16:03:57.683,1,2587,"i have a computer science degree and have done ai courses and projects . i want to study ai in a good university , probably online because in my country there are no good universities . which online ai masters degrees do you recommend me ? do you recommend me to try to study a "" normal "" master 's degree instead of an online one ?",11604,16565,2018-10-05T20:50:52.647,2018-10-05T20:50:52.647,which online ai masters degrees do you recommend me ?,academia,2,2,
1130,4751,1,,2017-12-15T16:37:35.293,2,77,"i am interested in learning more about the capabilities of ai , one of my ideas with practical functionality is using images of the rear of a log hauling truck to measure the individual logs using ai . the diameter and length of the log determine the board foot of that log . the diameter is the variable , with most lengths being the same 8 ft or 16 ft length . the ai would have to measure the diameter usually in 1-inch increments . i assume you would have to train the ai using pictures of logs that were manually measured . the images would mostly be straight on rear shots , not much off angle . however not every tree grows perfectly round so it will need to be able to measure those to maximize board feet . using ai long term it would be interesting if it could suggest the best way to cut the log gaining the most lumber board feet from the log . what would be used to develop something like this ? obviously some type of image recognition with learning .",11605,,,2018-03-20T06:41:53.253,measuring logs from a picture,machine-learning image-recognition,1,0,
1131,4752,1,,2017-12-15T17:15:42.727,8,989,"i am trying to detect a tv channel logo inside a video file , so simply given an input .mp4 video , detect if it has that logo present in a specific frame , say first frame , or not . we have that logo in advance ( although might not be the % 100 same size ) and the location is always fixed . i already have a pattern matching - based approach . but that requires the pattern be % 100 same size . i would like to use deep learning and neural network to achieve that . how can i do that ? i believe cnn can have a higher efficiency ?",9053,16565,2018-11-08T16:07:40.343,2018-11-08T16:07:40.343,use ai or neural network for logo detection,neural-networks convolutional-neural-networks pattern-recognition detecting-patterns,2,2,3
1132,4757,1,4758,2017-12-15T20:39:56.260,2,129,"even though modern chess playing programs have demonstrated themselves to be as strong ( or stronger ) than even the best human players for nearly 20 years now ( 1997 when ibm 's deep blue defeated the world chess champion gary kasparov ) , why would a game like chess still be considered a valuable research subject in artificial intelligence ? in other words , what can be gained by continuing to advance ai in areas that have already surpassed human capabilities . for instance , as recently as november 2017 , google successfully challenged their deep learning technology against one of the worlds strongest chess playing programs .",11606,,,2017-12-15T21:17:40.370,why is chess still a benchmark for artificial intelligence,deep-learning,1,2,2
1133,4760,1,,2017-12-16T05:32:04.693,-1,68,"i am building model with medical dataset using deep learning methods . medical dataset consists of both numerical data such as age , sex and images of xray scans(1024 x 1024 ) . labels consists of types of cancer . i believe that ages and sex gon na affect output of network . but including images will make the network biased towards images , because images will occupy most of the input layer . how can i design the input layer of network ? additional information : i am not using cnn but normal deep learning network with two hidden layers",11614,,,2017-12-27T13:37:46.167,input layer in deep learning,deep-learning deep-network,1,2,
1134,4762,1,4862,2017-12-16T09:56:29.880,-1,60,"i 've modelled a micromanipulation domain with 22 subtasks : grasp : absinit , pregrasp , close graspvertical : pregraspinit , pregrasp , close , remove rotate : touchleft , touchright , closeleft , closeright rotateside : touchleft movebox : up , right , left task : 90right , 90rightside , pickplace , pickplaceleft , graspvertical , stoprotating , done for example , if the user executes the macro “ touchleft ” , the robotgripper moves with ( x=30,y=-30 ) . unfortunately the environment is noisy so in reality the touchleft - command works only sometimes . in certain condition another value like ( x=28,y=-30 ) is necessary . so my question is of how to get a robust hierarchical architecture ? short note : i 've heard something about associative skill memories but i 'm unsure if this works .",11571,,,2017-12-27T23:14:53.043,skill primitives are not robust enough,reinforcement-learning,1,0,
1135,4764,1,,2017-12-16T16:16:22.330,2,116,"as self - driving technology is improving , there are so many companies developing self - driving cars like google , uber , etc . is it possible that we wo n't need any private / paid self - driving cars and the "" self - driving taxi "" becomes ubiquitous in the city ? if we assume that there are such taxis everywhere , would transportation become extremely low cost or free ? ( the self - driving car company could benefit from broadcasting advertisements for advertising agencies . )",11621,1671,2018-01-02T22:42:35.153,2018-10-19T02:45:03.577,could the deployment of self - driving cars make rides free ?,self-driving social,3,0,2
1136,4766,1,5072,2017-12-16T18:51:09.517,4,1731,"capsule networks seem to be a good solution for problems , which make up hierarchical complexity ( ( ( eyes , nose , ears - > face ) ; ( fingers , nails , palm - > hand ) ) - > human ) . nlp domain is a very clear hierarchical complexity problem , because there are words , sentences , paragraphs and chapters , whose meaning change based on the style of lower levels . is there any research papers or software tools on capsule networks and nlp , which i should be aware of ? is there related research papers , which have been investigating hierarchical complexity within the domain of nlp , which could be easily translated to capsule network ?",11626,11626,2017-12-18T11:52:47.350,2018-06-22T19:47:32.280,has capsule networks or similar systems been used for nlp ?,neural-networks natural-language-processing,2,3,2
1137,4769,1,4771,2017-12-17T04:44:32.630,3,146,"providing that more and more decisions about human life are ( to be ) decided by machine ( like access to loans , housing , scholarship , jobs , healthcare , insurance , etc . ) at the same time , in many countries there are laws and codes of conduct against ( negative / positive ) discrimination does there any industrial - accepted way to examine the ai system its legal compliance ? i believed that acm / ieee software engineer professional code of conduct can be applied here , but also like to learn more about auditing process from examiner side as well , if there 's any . thank you .",11535,11535,2017-12-17T05:17:45.823,2018-03-15T21:21:07.203,legal compliance and ai auditing framework,ai-design algorithm self-driving ethics ai-safety,3,1,1
1138,4770,1,,2017-12-17T06:12:44.007,2,151,"i have a macbook air and asus rog 702 with a built in geforce 1070 gpu . i 'm considering getting an nvidia volta for ai applications ( tensorflow / pytorch ) to run on either the mac or windows . would they even be compatible ? and if so , what box would i need to house the gpu ? would it be system agnostic ? ( i was thinking aikito node ) . thank you ! all info is appreciated by this novice : d",9316,9608,2017-12-19T21:16:54.540,2018-05-01T20:57:30.840,using nvidia volta v with mac or windows externally,hardware,1,0,1
1139,4774,1,,2017-12-17T23:00:00.030,1,102,"i am trying to find literature on a network architecture that takes the following as in input : action ( like ' up ' , ' down ' , etc ) image of current state and outputs : image of next state i already have a lot of training data for the inputs . however , i am trying to find relevant literature / architecture for this problem .",11644,,,2019-05-07T20:03:05.840,neural network that predicts game state based on actions,computer-vision game-ai,2,1,
1140,4776,1,4777,2017-12-18T13:27:20.397,6,149,i was watching a lecture on policy gradients vs bellman equations . and they say that the bellman equation indirectly creates a policy . while the policy gradient directly learns a policy ? why is this ?,11566,,,2018-01-17T14:42:57.753,why does bellman equation solve an indirect policy ?,reinforcement-learning,1,0,
1141,4778,1,4781,2017-12-18T19:38:08.683,1,57,"tldr : is there an ai available that can recognize employees in a factory and tell when they entered and left pre - defined areas ? i work in a factory where we gather cycle time data from various inputs ( computer interfaces , bar code readers and rfid tags ) . we follow both parts moving along the production line and persons working on those parts because they move from stations to stations during their day and can be more than one at the same place to help one another . our goal is to know how many people are working on each part at any given time , but the problem is that it takes time for people to check themselves in when they get to their workstation and they can also forget to do it . many employees asked me if i could find a way to track them automatically so they would n't need to check in everywhere they go . and then i stumbled upon microsoft workplace safety demo and yitu system ( this one is scary ) but they both seem to be a little overkill for my needs . after learning about these , here is the ideal ai features that i 'd need : can use video feeds to detect new people in the workspace and prompt someone ( via e - mail or text message ) for identification . can use video feeds to recognize persons that are already known and document every time someone has entered or left a pre - defined zone ( workstation ) in the video feed . wo n't force me to have a camera for each workstation . allows one workstation to span on more than one video feed . as of now , i found nothing available that does this , but i may have missed lots of things because this is not what i am used to work with . so maybe you know something that could help me with that ?",9361,,,2018-06-29T07:56:23.487,searching an ai to recognize and locate persons in a factory,computer-vision facial-recognition,2,2,
1142,4779,1,,2017-12-18T21:07:10.847,2,492,"i 'm creating a decision tree and at the very root level itself , i 'm getting negative information gain . as per my knowledge , information gain is always & gt ; 0 .... any explanation .... please .... please look at the node below ..... [ 69+,42- ] / \ / \ [ 56+,33- ] [ 11+,11- ] ig = h([69 + , 42- ] ) - h([56+,33-],[11+,11- ] ) = { ( -69/111 * lg(69/111 ) -42/111 * lg(42/111 ) ) - 89/111 * ( -56/89*lg(56/89 ) -33/89*lg(33/89 ) ) - 22/111 * ( -11/22*lg(11/22 ) -11/22*lg(11/22 ) ) } = -0.004 it turns out that for every feature , the ig & lt ; 0 . what should i do to decide the feature at the root node ? thanks",8720,8720,2017-12-18T21:18:55.663,2018-11-18T11:01:10.203,getting negative information gain ( using entropy as a measure of disorder ),machine-learning decision-theory,1,0,2
1143,4780,1,4782,2017-12-18T23:23:25.590,7,142,"i am a software engineer who has taken an interest in machine learning about a year ago . i work on a couple of projects where a large amount of data is collected , and am wondering what i can do ( or practice ) in order to start recognizing opportunities to implement machine learning . until now , i 've spent some time building various classifiers with tensorflow , caffe , etc . using pre - defined data sets for random things i 've found online ( whether it be photos , spreadshets , etc ) - and i just want to be able to use it at work . what is the best way to start getting in the frame of mind where i start recognizing where i can apply it ? i found this article : https://hbr.org/2017/10/how-to-spot-a-machine-learning-opportunity-even-if-you-arent-a-data-scientist but it did n't seem too helpful to me . if there are any other thoughts and/or resources online that you could point me to , that would be helpful . thanks .",11667,,,2018-01-02T16:14:15.443,how to start looking for machine learning opportunities in projects,machine-learning,2,2,1
1144,4783,1,,2017-12-19T03:04:18.840,3,524,"model - based rl creates a model of the transition function . tabular q - learning does this iteratively ( without directly optimizing for the transition function ) . so , does this make tabular q - learning a type of model - based rl ?",11566,2444,2019-02-16T19:05:37.663,2019-02-16T19:14:31.143,is q - learning a type of model - based rl ?,reinforcement-learning q-learning definitions model-based,2,1,
1145,4784,1,,2017-12-19T07:53:43.403,1,662,"i have a video which is capture from a moving car and video showing plenty of details like pools , human , cars / buses , roads , etc , etc . the video i am playing in unity3d and camera showing that video and user is allow to click on video . i want to get different information from the videos like if someone clicked on video ( like clicked on a bus object ) , then , unity ui should show that it is a bus object . it something like object detection in video within unity environment . from last couple of days i searched and found different things like : opencv for unity3d an asset on asset store but cost is the problem . so is there any tut or custom way available that i should write it myself ? yolo but its in python and i found no integration guide that i follow and integrate it in unity . emgucv unity is also an option but its github do n't conver any guide , i downloaded its project there is not sample or docs . and its paid version cost is about 400$. is there any third thing compatible with unity3d ? or do i can use / integrate above given projects in unity ?",11671,11671,2017-12-20T04:32:38.577,2017-12-20T04:32:38.577,object detection in video,image-recognition tensorflow object-recognition,0,3,1
1146,4786,1,,2017-12-19T21:21:32.760,1,184,"i 'm learning about neat from the following paper : http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf i 'm having trouble understanding how adjusted fitness penalizes large species and prevents them from dominating the population , i 'll demonstrate my current understanding through an example and hopefully some one will correct my understanding . let 's say we have two species , a and b , species a did really well last generation and were given more children , this generation they have 4 children and thier fitnesses are [ 8,10,10,12 ] while b has 2 and thier fitnesses are [ 9,9 ] so now thier adjusted fitnesses will be a[2,2.5,2.5,3 ] and b[4.5,4.5]. now onto distributing children , the paper states : "" every species is assigned a potentially different number of offspring in proportion to the sum of adjusted fitnesses f'i of its member organisms "" so the sum of adjusted fitnesses is 10 for a and 9 for b thus a gets more children and keeps growing , so how does this process penalizes large species and prevent them from dominating the population ?",11684,1671,2018-03-05T19:06:20.840,2018-06-03T21:12:20.553,adjusted fitness in neat algorithm,neural-networks machine-learning genetic-algorithms fitness-functions neat,1,0,1
1147,4790,1,,2017-12-20T15:43:29.047,-2,107,"this is the question which always is being discussed . will artificial intelligence be the undefeatable enemy for human beings ? like the slaughter bots ( killer microdrones technology ) can kill a human or an living thing in just some seconds . and it neither could be deceived by its target nor it could be stopped by any source . once it 's set to kill the target , it can never be stopped and it identifies its target . it is also the innovation of ai . see microdrones technology introduction .",11706,9161,2019-03-13T19:36:27.837,2019-03-13T19:36:27.837,will / can ai be the worst enemy to humanity ?,strong-ai neo-luddism superintelligence automation ai-takeover,1,0,
1148,4791,1,,2017-12-20T15:54:43.057,2,30,"so , i have this huge amount of data , which has 7 vector features ( float from 0 to 1 ) . i am trying to build a kind of recommendation system , with a twist ( it uses agents and negotiations and narratives ; narratives meaning , that there will be temporal and partial order causal link dynamics , or "" short term memory "" ) . there is network effects in the data , since the more agents i have , the more potential there will be for match making ; in simulation phase i will be just using randomness , instead of real data , but i believe the real dataset will also be more or less random : of course i think there will be nice clusters of correlations for the vectors , but i also want to understand the more general aspects of the problem , instead of just this specific use - case . the agents will be generating a list of artifacts in co - operation and also evaluate how the generated list of artifacts match their preferences . the agents will be recommending items from their own list ( and some times from a list of their related agents ) and after each suggestion , there is negotiation . if there would not be network effect in the data , i would use simple heuristics , which would measure the diff of each feature . however , in network effect world this does not work very well , because it will make many agents unable to co - operate . i was thinking of something , where some features are preferred features by the agents in different circumstances ( depending on the other agents ) , this will enable some elasticity to the system and imo avoid the simple problems , of which many recommendation systems suffer ( lack of context awareness ; humans do not like the likes of other people , we have flexible preferences , which depend upon our mood ) . since this sounds like a very generic problem , it seems like i have just discovered a problem , which has been elegantly solved by someone else from the academia years ago , but i just do n't know the name for this concept . instead of depending on strict evaluation criteria ( only accept diff x per feature ) , how to choose evaluation function to make the agents more co - operative ? i was thinking something , where diff has to be narrow for at least one feature , but may be big for three worst matching features ; so that there would be essence of few features and diversity of several . edit : as i have spent some more time with this problem , i have a hunch about which issues might be related : in real world networks , which depend upon energy efficient co - operation and signaling , scale - free networks seem to emerge ; effective agent - based model in network effect scenario should probably form co - operation networks , which follows some kind of fitness function . due to scale - free networks , it should be assumed , that the agents using essential features ( which are common in data ) should be ranked higher and agents using diverse features ( which are rare in data ) should be ranked lower ; in scale - free networks ( like facebook ) it has been observed , that all but 3 % of nodes form edges to more popular nodes : this makes signaling efficient . as a reference , in deep learning capsule networks have had some state - of - the - art success with this similar hierarchical "" recommendation for upper tiers "" mentality ( there are much more lower layer features , than upper layer features ) . so , i have this idea , that when we use agent - based models for solving problems with network effects , the evaluation function should evaluate agents skill of passing the problem for more popular agent , which could delegate it to the best agent or attempt to solve it on it 's own .",11626,11626,2017-12-30T04:19:48.763,2017-12-30T04:33:18.227,"how to choose evaluation functions for features , when network effects are in place ( multi - agent systems ) ?",multi-agent-systems linear-regression,1,0,1
1149,4796,1,,2017-12-21T03:46:18.017,-1,2184,can anybody suggest on how to get - started with artificial intelligence with python specifically ? what are some : -reliable resources i can utilize -basic test problems to solve -online tutorials and classes thanks,11716,1671,2017-12-21T19:10:04.210,2018-01-25T23:28:16.370,how to start learning ai in python,getting-started python ai-basics,3,5,1
1150,4819,1,4820,2017-12-22T17:17:54.323,1,63,i ca n't understand the red box . what do does that 1 do before { c(i)=j } ? also how does all the algorithms work ? may someone explains the to steps more clearly by one example !,11747,,,2017-12-22T19:40:24.343,how does this part of algorithm works?(k - means ),algorithm unsupervised-learning,1,0,
1151,4821,1,,2017-12-22T20:09:12.370,1,43,"i 'm working on architecture for a game ai where , due to the nature of the game , the classical approach seems likely be sufficient to beat most humans -- the endgame is tractable and traditional game - solving and is worth pursuing due to certain intrinsic properties of the game . ( strategy is a direct application of minimax , although there is a great deal of nuance in that determination . ) the simplified rules for the game can be found here and there is a free app ( no ads , no purchases , no data collected ) where you can try the mechanics for the basic , non - trivial game . ( tutorial takes about 10 minutes . ) at some point , we 'd like to integrate some form of local nn , but size of the database for learning / training would be dependent on how much volume the user is willing to devote to their "" mbrane "" . ( these will initially be mobile devices such as phones and tablets . new ipad pro has upt to 512 gb , but most devices will be much smaller . ) is this even feasible ? today ? at some point in the future ? uses : "" tuning "" ai behavior to the human player weighting complex stability states based on current board state to determine optimal placement tuning to human behavior we want each discrete , local ai to evolve uniquely , in conjunction with human play , and adapt itself to the preferences of their respective humans . hardness : most simply , we want ai hardness to be determined by a win / loss ratio against the human player . hard = human never wins . easy = human always wins . this forms a spectrum , so you might have settings for 2/3 w / l or 1/3 w / l . these do n't have to be precise , but the ai should tailor it 's play strength for a subsequent game based on the outcome of the previous game . individuality : we want the ai 's to learn not only by self - play , but by play with their humans . it 's ok if the process is glacial b / c the basic evaluation functions and perfect endgame will provide inherent ai strength . the main thing is that the discrete ai 's develop uniquely . ai 's will be able to play other ai 's . stability states there are three simple stability states that arise out of the mechanics . these states can be strong or weak . these can be a hard t / f , or , since the game is quantitative , values between 1 and 0 , based on the respective regional deltas ( neutrality ) . "" flipping "" of a region refers to the changing of the dominant player in the region . stability ( weak : can the region be flipped with a single placement ? strong : can the polarity of a region be flipped by any number of placements ? ) epistability ( weak : will the region flip under prior resolution ? strong : such that the region can not be epistabilized by further placements ? ) metastability ( weak : can the region 's position in the resolution order be meaningfully altered with a single placement ? strong : can the position in the resolution order be meaningfully altered by any number of placements ? ) complex stability states the simple stability states combine into 8 "" complex stability states "" , based on true / false for metastability / epistability / stability : ttt superstability ( "" super - stably stable "" ) ftt semistability ( "" semi - stably stable "" ) tft mendaxastability ( "" super - unstably stable "" ) fft demistability ( "" semi - unstably stable "" ) ttf contrastability ( "" super - stably unstable "" ) ftf nonstability ( "" semi - stably unstable "" ) tff antistability ( "" super - unstably unstable "" ) fff khaotictivity ( "" semi - unstably unstable "" ) there are two hierarchies , the ordering / weighting of metastability , epistability and stability in the complex states ( here the order is reversed for ease of interpreting the linguistic description of these states , ) and the ordering / weighting of the complex states themselves , currently with superstability as the least important , and khaotictivity as the most important . ( i.e. a strongly superstable region can not be flipped , and does not need to be reinforced ; a khaotic regions is very much "" in play "" ) it seems to me that deep learning might be very usefully applied in determining the hierarchies based on any give board state .",1671,,,2017-12-22T20:09:12.370,mini ( local ) nn for mobile ?,neural-networks game-ai evolutionary-algorithms game-theory combinatorial-games,0,0,
1152,4822,1,,2017-12-22T20:49:51.510,2,905,"i 'm really new to neural networks . i 'm trying to make a neural network with genetics algorithms which will make a snake learn to look for the food and avoid hitting his tail . the thing is that i think that i 've done it , but as there 's no walls the snake learns to go one direction only without making a 180 turn [ gif here ] . i 've tried to incentivate mutations that make them turn by decreasing the score of the snakes that always takes the same directions , but it do n't works . i 've only made them dumber , needing more breeds to reach another "" smart "" linear snake . i 've made a network with 5 inputs : food position relative to my position and direction ( 2 inputs . x and y ) nearest wall ( my tail ) if i turn left nearest wall ( my tail ) if i do not turn nearest wall ( my tail ) if i turn right the output are 3 , being the first one turning left , the second do not turn and the third going right . i make the snake go the highest one of the 3 outputs . i 've added 1 hidden layer of 8 neurons ( inputs + outputs premise ) . the way i calculate the score is : each step , 1 point . each food eaten , 10 points . if the snake lasts too much time without eating food , dies . if hits his tail , dies . then i save each time the direction this snake has gone ( up , right , down , left ) and increment them by one . when the snake dies , i weight the final score by the difference between the lowest and highest values . if the difference is high , they receive a big penalty ( down to 0.25 of their score ) . this way if a snake is pretty much linear gets a high penalty and if a snake does a cool pattern , gets a low penalty . also , i keep a record of each time a direction change happens compared to the last direction change , so if a snake keeps going in circles do n't gets a high score because of "" cool pattern method "" for going all 4 directions . with all this , i do n't understand why my best snakes are always the linear ones :-/ i spawn 20 snakes and get the best 4 of each generation when everyone dies . for generations i use neataptics.js and for neural networks i use synaptics.js . i have a fiddle here : http://jsfiddle.net/llorx/gunsct5r/ at line 10 you can see the network definition . at 211 you can see the snake "" view "" ( food position and walls . where it gets the inputs ) and at 164 you can see the score weight calculation depending on steps taken that i mentioned before . all inputs are normalized from 0 to 1 . i 'm sure that i 'm doing , not one , but a lot of things pretty bad , as i 'm a newbie on this , but some light on this will be really cool .",11751,11751,2017-12-22T21:02:12.060,2018-01-01T09:21:22.497,trying to make snake learn different directions,neural-networks genetic-algorithms,2,3,2
1153,4830,1,,2017-12-23T13:50:07.207,3,322,"recently , deepmind 's alphazero chess algorithm did better than the prior best chess software stockfish . i read an arxiv paper about it but i 'm not sure if : is there a value given for each piece ( e.g. 1 for pawn , 3 for knight , 9 for queen , etc . ) to train the algorithm , or does the algorithm learn this by himself ? i read that the algorithm uses monte carlo tree search , but what are the key improvements to prior chess algorithms already using mcts ? is there a hope for being able to run it an average computer ? they said it required 9 hours learning ( starting with nearly 0 knowledge except rules ( and maybe value for piece ? ) ) , and 24 millions of games . is it something doable in maybe 1 month with a average computer ?",11764,1641,2018-08-07T18:51:20.200,2018-08-07T18:51:20.200,"alphazero chess algorithm , monte carlo search",deep-learning monte-carlo-tree-search chess,1,2,1
1154,4831,1,,2017-12-23T18:41:29.570,7,1253,"do off - policy policy gradient methods exist ? i know that policy gradient methods themselves using the policy function for sampling rollouts . but ca n't we easily have a model for sampling from the environment ? if so , i 've never seen this done before .",11566,2444,2018-11-19T17:23:05.213,2018-11-19T17:23:05.213,do off - policy policy gradient methods exist ?,reinforcement-learning,1,0,
1155,4832,1,,2017-12-23T21:23:29.133,2,43,"let say that we have four straight string with colors red , green , blue and yellow . these strings are tied up randomly . we know the current state of string ( like starting point , where we should we start untying from ) and the final state ( where current string must look like after untying them ) . here is a very simple example of the problem : at every move , we are allowed to replace only two nearby strings . for example , to solve the shown problem , we should do the following replacements : 1 ) replace green and yellow 2 ) replace blue and green but in a programming environment , how can i calculate the movements to untie the strings ? what will be the algorithm specifically to solve such type of problems ? here is another variation of the question . its not possible to solve this problem according to the answer .",11770,9947,2018-03-11T02:00:39.003,2018-05-18T17:08:38.880,string node untying algorithm,algorithm logic,1,0,
1156,4833,1,,2017-12-23T23:19:35.863,0,68,"imagine this theoretical situation : a group of people are asked to provide a solution for an imaginary problem via email . then an ai service runs through their written solution , analyzing the kind of words and sentences they use , essentially exploring how they faced the problem . then the service finds a personality portfolio of each person . is there such an ai service which can find such personality portfolios by analyzing the text they use describing their solution to a problem ? if there is not a current service , how might it be approached ? regards , koppany",11772,4398,2018-01-03T06:19:31.337,2018-01-03T06:19:31.337,what ai service can define personality portfolio based on text ?,philosophy emotional-intelligence text-summarization,1,1,
1157,4834,1,,2017-12-24T06:15:24.523,2,157,"i am using hog ( histogram of oriented gradients ) for car detection from a video . i have used the matlab function extracthogfeatures ( ) , it has given me a feature vector but how do i differentiate between different features of different objects . after this i extracted the corner points(313x1 ) and brisk points ( 172x1 ) but they show nothing just empty vectors .",3751,1671,2018-01-28T23:06:39.327,2018-01-28T23:06:39.327,extracting specific features using hog,image-recognition computer-vision object-recognition matlab,0,0,
1158,4841,1,,2017-12-24T23:59:57.663,1,89,"i have a large number of observations . each observation contains : dependent variable : a scores ranging from 0 - 100 independent variable : a large article i want to know which words or phrases predict a higher score . i know that for discrete dependent variables , naive bayes can be used for this task . how do i conduct this analysis with a continuous dependent variable ?",11782,11782,2017-12-25T00:09:55.857,2017-12-25T00:09:55.857,natural language processing with a continuous dependent variable,machine-learning deep-learning training natural-language-processing learning-algorithms,0,2,
1159,4843,1,,2017-12-25T11:43:17.483,-2,329,"i am thinking of making a simple and interesting research on training a rnn , to create source codes . the objective is to have a set of simple repl programs in java , and create a source code for a program that is not present in the databas . ex : database = > [ palindrom , prime , fibonacci ] ai will be able to create a source code to print all the fibonacci numbers that are prime , or create a tribonacci number etc . i am wondering how to break a code into a training input ! i have found some link rnn2source deep generation ai - programmer deep coder can anybody explain me what is the ideal way , to approach this problem , and how to break down the source code into a training set ?",11789,11789,2017-12-25T12:06:33.480,2017-12-25T22:03:19.217,how to train a simple neural network to create source code,neural-networks genetic-algorithms recurrent-neural-networks,1,0,1
1160,4845,1,,2017-12-26T01:43:16.810,1,42,"i am trying to train chess data through cnn . to proceed reinforcement learning , i had divided into two - "" current network "" and "" reinforcement network "" . for each checkpoint file stored in different directory , would like to call the weight and proceed chess games and learn only "" reinforcement network . "" thus i had made an instance for each network , current and reinforcement , however , regardless of its sequence , if a network stores saver.restore the other network returns error while same saver.restore process . i doubt this happens because i had used same variable_scope . maybe this is because in tensorflow there already exists same scope_variable i guess . any chance i could use two different already - learned network simultaneously on tensorflow ? i use python / tensorflow1.1.0 . any advice will be appreicated",11797,,,2017-12-26T01:43:16.810,regarding tensorflow : how to avoid duplicate use of scope / variable_names,convolutional-neural-networks reinforcement-learning tensorflow,0,0,
1161,4849,1,4867,2017-12-26T13:19:57.850,1,154,"i 'm trying to learn more about ai by trying to program a neural network . first i 'm trying to understand writing my own perceptron but i 'm struggling to get a basic perceptron working correctly . i 've tried writing a few basic perceptrons to do very basic tasks for example trying to classify a point as above or below a line y = x. the problem i 'm facing is it seems that when training the perceptron my weights start increasing exponentially and it does n't seem like i 'm getting anywhere . if i start with a perceptron with two weights representing an x and a y value of a point on a graph : starting weight values of 1 and 1 using the sign function of the output being more than or equal to 0 outputs +1 and less than 0 is -1 . i 'm using the training data with examples ( 1,2 ) , ( 2,1 ) , ( 3,3 ) , ( 2,1 ) . i think i 'm correct in saying if the perceptron outputs the correct value then you do n't need to adjust the weights , if it 's incorrect then the weights can be updated using new_weight = old_weight + ( ( expected_ouptut - false_output ) * input ) by the first iteration i have weights of -4 and -2 respectively , the second passes , the third i.e ( 3,3 ) with weights ( -4,-2 ) changes the weights to ( 53,55 ) . i 'm fairly sure my math is correct , and it seems like the weights are increasing exponentially without making any difference to getting a working perceptron . are there any errors i 'm making with trying to design a working perceptron ? thanks ! sorry this may get slightly long ! edit : i think i may have been making mistakes doing it manually , i ’ve written the code now that seems to be partly working . the way my code works is by running test firstly with test cases , followed by the results from those test cases in the same order within the lists . followed by the weights and finally the test case to run it from . so running in ghci : test [ ( 5,2 ) ] [ ( -1 ) ] ( 1,1 ) ( 5,2 ) this works correctly , first it tests the coordinates ( 5,2 ) in the perceptron with weights ( 1,1 ) and checks to see if it is equal to -1 , if it is not equal to one it will run it again and alter the weights with the alter function until it does equal -1 , when this happens it then checks the perceptron again and outputs with the final tuple ( 5,2 ) which is the test case to check if it ’s working . this works correctly , however when working with larger sets it does n’t work . next i tried test [ ( 1,2),(2,5),(3,1),(7,0),(6,5),(4,4 ) ] [ 1,1,(-1),(-1),(-1),1 ] ( 1,1 ) ( 5,1 ) where the first two lists are the test cases and the results , then the weights ( 1,1 ) and finally after running all of the test cases trying to teach the perceptron it then runs the perceptron on the final tuple ( 5,1 ) and outputs the result , it should be ( -1 ) as it is below y = x , however it outputs 1 . pseudocode : testfunction if(test cases remaining ) : if calling sign on the output of the perceptron ! = result for test case then : call same test case again but alter weights else : call testfunction again will same lists minus the first test arguments and results else if(no more test cases , all have been tested and removed , or none specified ) : call sign on the output of the perceptron for the final tuple provided with the altered weights code in haskell : module main where -- function to take training data , results from that data to test the perceptron -- also takes weights and finally outputs the result of the test case test : : [ ( float , float ) ] -&gt ; [ float ] -&gt ; ( float , float ) -&gt ; ( float , float ) -&gt ; float test [ ] [ ] weights testcase = sign(perceptron testcase weights ) test ( t : traindata ) ( r : trainresult ) weights testcase | ( output /= r ) = test ( [ t ] + + traindata ) ( [ r ] + + trainresult ) ( alter weights output t r ) testcase -- add t and r | otherwise = test traindata trainresult weights testcase where output = sign(perceptron t weights ) -- function that computes x1*w1 + x2*w2 perceptron : : ( float , float ) -&gt ; ( float , float ) -&gt ; float perceptron coordinate weights = ( ( ( ( fst coordinate ) * ( fst weights ) ) ) + ( ( snd coordinate ) * ( snd weights ) ) ) -- function that finds the new weights by doing w1 = w1 + ( expected_result - perceptron output)*x1 and similar for w2 alter : : ( float , float ) -&gt ; float -&gt ; ( float , float ) -&gt ; float -&gt ; ( float , float ) alter ( w1,w2 ) output ( x1,x2 ) result = ( ( w1 + ( ( result - output ) * x1 ) ) , ( w2 + ( ( result - output ) * x2 ) ) ) -- activation function sign : : float -&gt ; float sign n | n & gt;= 0 = 1 | otherwise = ( -1 )",11795,11795,2017-12-26T18:45:15.477,2017-12-28T19:34:48.623,creating a working perceptron,neural-networks perceptron,2,2,1
1162,4850,1,4870,2017-12-26T15:00:08.953,1,34,"i am reading learning from logged implicit exploration data it says formally , given a dataset of the form s = ( x , a , r_a ) * generated by the interaction of an uncontrolled logging policy what is such a policy ?",11808,,,2017-12-28T22:08:14.853,what is uncontrolled logging policy ?,reinforcement-learning,1,0,
1163,4851,1,,2017-12-26T15:57:06.827,4,379,"lets pretend we had a list of facts ( similar to prolog tuples ) that define some knowledge about some entities . e.g. doing(clean , data ) done(collect , data ) todo(train , model ) todo(write , paper ) what methods could i use to generate sentences like : you should be cleaning the data you collected , then you need to train your model and write your paper .",11809,,,2017-12-28T21:22:17.360,how could you generate sentences from lists of facts,natural-language-processing,1,0,
1164,4859,1,,2017-12-27T14:28:25.243,2,120,"i was wondering if its possible to classify or learn to estimate the minimum value in a table if the values are integer and represented 32 bits , and we can input all variable at the same moment like in soc or something .",10186,,,2018-05-27T09:23:57.567,search minimum value with learning machine algorithm,machine-learning research classification genetic-algorithms,0,8,
1165,4863,1,4885,2017-12-28T00:29:30.293,-3,117,"recently i have had the urge to make a chat - bot . i want to train my chat - bot on a large set of data . this raised my question on whether xml , sqlite , or storing it in a raw json text file will be faster for storing questions with possible answers . what are your thoughts on this ?",,,2018-01-02T02:27:20.497,2018-01-02T02:27:20.497,xml speed versus sqlite,neural-networks,1,0,
1166,4864,1,4887,2017-12-28T11:12:28.850,7,6764,what is the concept and how does one calculate bottleneck values ? how do these values help image classification ? please explain in simple words .,11837,6252,2018-03-22T19:37:09.923,2018-03-22T19:37:09.923,what is the concept of tensorflow bottlenecks ?,deep-learning tensorflow terminology,3,0,6
1167,4868,1,,2017-12-28T20:52:32.340,0,73,"i started to study nn recently . so i understand principles with which i should define input and output layers . but i ca n't find any guide / directions how to build hidden layers : how many layers do i need , how many neurons per layer , what activation functions should i use etc for different types of tasks . so i am searching for some guide like : try to start with ... layers of ... neurons . if you get ... result , please increase number of neurons ... , but if you get ... result , you should try to decrease number of neurons . or something similar .",11845,,,2018-01-02T13:04:46.157,is there any common principle/ build algorithm for deep nn structure ?,deep-learning deep-network hidden-layers,2,2,
1168,4873,1,4875,2017-12-29T07:42:14.020,-4,102,i need to have a full brief on recurrent neural network . with the explanation how to train recurrent neural network ? ?,11851,,,2017-12-29T11:15:18.387,how to train recurrent neural network ?,recurrent-neural-networks,1,1,
1169,4879,1,,2017-12-29T21:45:58.837,0,350,"for instance , one task would be to detect if with an android phone in hand , i 'm panning the camera toward a circle shape in a 2d space . what is the best technology set and embedded application approach can be used for these types of motion independent pattern recognition tasks ?",9053,4302,2018-11-01T23:26:27.703,2018-11-01T23:26:27.703,use mobile device camera for moving pattern recognition,neural-networks machine-learning pattern-recognition detecting-patterns embedded-design,1,6,1
1170,4886,1,4888,2017-12-31T09:17:59.750,3,183,"is it possible with any of machine learning methods to train machine to tie shoe lace ? if possible how data should be interpreted for the training ? if we are using reinforcement learning , how will it learn to reach the best rewards ?",6687,6687,2017-12-31T12:39:44.510,2018-01-05T00:57:14.577,can we teach machine to tie shoe lace ?,machine-learning reinforcement-learning,1,5,2
1171,4889,1,,2017-12-31T18:04:41.053,5,2397,"for example i need to detect classes for mnist data . but i want to have not 10 classes for digits but also i want to have 11th class "" not a digit "" . so that any letter ( except "" o "" of course : ) ) , any other type of image or random noise would be classified as "" not a digit "" . or with cifar-10 i want to have 11th "" unknown "" class to classify any image that contain something out of classes range . so how to implement such feature ? maybe there are some examples somewhere , preferable with keras .",11845,,,2019-01-08T16:18:48.567,"how to implement an "" unknown "" class in nn classification ?",image-recognition classification,2,0,1
1172,4890,1,4893,2017-12-31T18:47:23.170,2,88,"machine learning and nn trainings as a part of ml is based on data that was gotten from real world and inserted into virtual space by humans . meanwhile nn are also used for data generation . each year the more and more texts , images , sounds etc become more realistic and can not be determinated from real world data even by a human . so it is really possible that machines will start to learn with data that was generated by other machines , because it will look as real even for a human , but naturally will be not related to real world . q : will it lead to some machine learning collapse ? q : might it lead to some changes in human 's perception of the world , because people get a very big part of their knowledge using computers , connected to the internet ? q : is anyone thinking about this potential problem ?",11845,1671,2018-01-02T22:21:07.177,2018-01-02T22:41:50.427,machine learning and machine generated content conflict problem,machine-learning image-recognition models datasets social,2,3,
1173,4892,1,,2017-12-31T19:14:13.233,1,70,"are there any algorithms , or any evidence to decide or to suggest it would be better to connect a neuron node in a layer l , in a neural network to particular nodes in the previous layer l-1 of the neural network as well as to particular nodes in the next layer l+1 of the neural network ? this is obviously contrived , but here is a illustration of what i mean . the thick line with arrow indecates an edge that leads to l-1 layer neuron",11893,11893,2018-01-01T23:05:16.293,2018-01-02T15:17:08.087,algorithms that connect neurons to previous layers as well as next,neural-networks machine-learning architecture,0,4,1
1174,4907,1,,2018-01-03T01:49:59.060,4,71,"the basis of my question is that a cnn that does great on mnist is far smaller than a cnn that does great on imagenet . clearly , as the number of potential target classes increases , along with image complexity ( background , illumination , etc . ) , the network needs to become deeper and wider to be able to sufficiently capture all of the variation in the dataset . however , the downside of larger networks is that they become far slower for both inference and backprop . assume you wanted to build a network that runs on a security camera in front of your house . you are really interested in telling when it sees a person , or a car in your driveway , or a delivery truck , etc . let 's say you have a total of 20 classes that you care about ( maybe you want to know minivan , pickup , and so on ) . you gather a dataset that has plenty of nice , clean data . it has footage from lots of times of the day , with lots of intra - class variation and great balance between all of the classes . finally , assume that you want this network to run at the maximum possible framerate ( i know that security cameras do n't need to do this , but maybe you 're running on a small processor or some other reason that you want to be executing at really high speed ) . is there any advantage , computationally , to splitting your network into smaller networks that specialize ? one possibility is having a morning , an afternoon / evening , and a night network and you run the one corresponding to the time of day . each one can detect all 20 classes ( although you could split even farther and make it so that there is a vehicle one , and a person one , and so on ) . your other option is sharing base layers ( similar to using vggnet layers for transfer learning ) . then , you have the output of those base layers fed into several small networks , each specialized like above . finally , you could also have just one large network that runs in all conditions . question : is there a way to know which of these would be faster other than building them ? in my head , it feels like sharing base layers and then diverging will run as slow as the "" sub - network "" with the most additional parameters . similar logic for the separate networks , except you save a lot of computation by sharing base layers . overall , though , it seems like one network is probably ideal . is there any research / experimentation along these lines ?",8829,,,2018-07-28T22:02:05.237,is one big network faster than several small ones ?,deep-learning convolutional-neural-networks,1,0,2
1175,4908,1,,2018-01-03T05:51:20.047,1,26,"say i have 500 variables and i believe those variables can be shown in a 4-dimensional latent representation which i want to learn . what i have for training is 100k samples , and those samples are coming mainly from 3 unbalanced groups : 1st group has 1k samples , 2nd group has 49k samples , and 3rd group has 50k samples . do you think i can learn a meaningful representation by training a ( variational ) autoencoder with this data ? is there a reason that requires all samples to come from the same distribution ? if not , is there a reason that requires balanced classes ?",9609,9609,2018-01-09T07:08:52.287,2018-01-09T07:08:52.287,does it make sense to train an autoencoder using data from different distributions ?,neural-networks unsupervised-learning,0,1,
1176,4910,1,6520,2018-01-03T10:48:13.130,4,307,"i 've seen data sets for classification / regressions tasks in domains such as credit default detection , object identification in an image , stock price prediction etc . all of these data sets could simply be represented as an input matrix of size ( n_samples , n_features ) and fed into your machine learning algorithm to ultimately yield a trained model offering some predictive capability . intuitively and mathematically this makes sense to me . however , i 'm really struggling with how to think about the structure of an input matrix for game - like tasks ( chess , go , seth blings mario kart ai ) specifically ( using the chess example ) : how would you encode the state of the board to something that a model could train on ? is it reasonable to think about the board state as a 8x8 matrix ( or 1x64 ) vector with each point being encoded by a numerical value dependent on the type of piece and color ? assuming a suitable representation of the board state , how would the model be capable of making a recommendation given that each piece type moves differently ? would it not have to evaluate the different move possibilities for each piece and propose which move it "" thinks "" would have the best long term outcome for the game ? a follow up on 2 - given the interplay between a moves made now and moves made n moves into the future how would the model be able to recognize and make trade - offs between moves which may offer a better position now vs those that offer a position n moves in the future - would one have to extend the board state input to a vector of length 1x64n where n is the total number of moves for expected for an individual player or is this a function of a different algorithm which should be able to capture historical information which training ? i am unsure if i 'm overthinking this and am missing something really obvious but i would appreciate any guidance in terms of how to approach thinking about this .",11933,1671,2018-01-03T17:30:59.280,2018-05-27T13:26:14.883,how would you encode your input vector / matrix from a sequence of moves in game like tasks to train an ai ? e.g. chess ai ?,deep-learning ai-design training combinatorial-games chess,1,2,1
1177,4911,1,4912,2018-01-03T11:49:19.257,3,48,"suppose i have a classification problem with a stream of training - samples constantly arriving over time . i can not keep all training - samples in memory , but i still want to train a classifier that will have the "" wisdom "" of all samples , and additionally , i want the classifier to become better whenever it gets new samples . i thought of the following idea . suppose we have enough memory to keep 100 samples . then , for each run of 100 samples , we will train a different sub - classifier . we will have a meta - classifier that will classify based on voting between all existing sub - classifiers . over time , we will have more and more sub - classifiers , so hopefully the meta - classifier will improve with time - it will have a "" wisdom of the crowds "" effect . has this method been tried before ? specifically , has it been tried in a deep - learning sequence - classification setting ?",8684,,,2018-01-03T17:22:04.860,creating a classifier for simpler classifiers trained on few training samples,machine-learning deep-learning classification,1,0,
1178,4914,1,,2018-01-03T14:31:20.780,2,26,"i was wondering any examples of the following ; para generation : for eg , given x similar paragraphs , are you able to build a model to learn the style and generate a new para that is a paraphrase of the x paras . similar in meaning but diff wording . drawing conclusions from x given articles . he has a list of conclusions , check the x articles can provide evidence to the conclusions . eg , given conclusion “ city is not safe ” , look for evidence such as “ murders ” and “ thefts ” . glady appreciate , betty",11930,,,2018-01-03T19:25:03.207,para generation and drawing conclusion from x give articles,machine-learning,1,1,
1179,4917,1,,2018-01-03T20:23:06.233,3,111,"the intel 8080 had 4500 transistors and ran at 2 - 3.125 mhz . by comparison , the 18-core xeon haswell - e5 han 5,560,000,000 transistors and can run at 2 ghz . would it be possible or prudent to simulate a neural network by backing a chip chock - full of a million interconnected , slightly modified intel 8080s ( sped up to run at 2 ghz ) ? if each one modeled 100 neurons you could simulate a neural network with 100 million neurons on a single chip . edit : i 'm not proposing that you actually use a million intel 8080s ; rather i 'm proposing that you take a highly minimal programmable chip design like the intel 8080 's design and pattern it across a wafer as densely as possible with interconnects so that each instance can function as one or a few dozen fully programmable neurons each with a small amount of memory . i 'm not proposing that someone take a million intel 8080s and hook them together .",3323,3323,2018-01-09T02:55:58.890,2019-01-22T04:32:32.657,could a large number of interconnected tiny turing - complete computer chips be patterned across a wafer to simulate a programmable neural network ?,artificial-neuron hardware neuromorphic-engineering,4,0,1
1180,4918,1,,2018-01-04T02:06:35.053,1,131,"i 'm a student , and currently into image processing project and coding using opencv . recently , i watched sebastian thrun from udacity in tedtalks talked about alphago and i 'm totally interested in the idea . i have read this question too : merged neural network in alphago . i was wondering if same approaches can be used in my project . i 'm going to perform color enhancement method for any natural images . and of course , color sampling is a tricky task now . it 's a lot of work , i have to prepare condition for each key - color sampling given and also prepare & amp ; pick the best enhancement function for it . i 'm able to do it already using opencv . but i was wondering if i could load tons of sample pictures instead , have my system test them against each other , and figure out its own enhancement rules from all testing . i 'm not that familiar with deep learning , we do n't even have deep learning course at my university , but i 'm interested in the idea and ready to learn . i 'm not even sure if this can be done or not , but i wonder what kind of approaches should i learn to achieve my goal ? is deep learning -- > neural network a good start ? in my case , to which method in deep learning should i go with ? any reference / advice will be highly appreciated . thanks .",11943,1671,2018-01-04T21:30:07.087,2018-01-04T21:30:07.087,deep learning approaches for color enhancement testing,neural-networks deep-learning getting-started ai-basics,0,3,
1181,4929,1,,2018-01-06T01:31:03.133,1,244,"i coded a small rnn network with tensorflow to return the total energy consumption given some parameters . there seem to be a problem in my code . it ca n't overfit the training data when i use a batch size > 1 ( even with only 4 samples ! ) . in the code below , the loss value reaches 0 when i set batchsize to 1 . however , by setting batchsize to 2 , the network fails to overfit and the loss value goes toward 12.500000 and gets stuck there forever . i suspect this has something to do with lstm states . i get the same problem if i do n't update the state with each iteration . or maybe the cost function ? a help is appreciated . thanks . import tensorflow as tf import numpy as np import os from utils import loaddata epochs = 10000 learningrate = 0.0001 maxgradnorm = 5 seqlen = 1 nchannels = 28 nclasses = 1 nlayers = 2 nunits = 256 batchsize = 1 numsamples = 4 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # trainingfile = "" ./training.dat "" x_values , y_values = loaddata(trainingfile , seqlen , numsamples ) x = tf.placeholder(tf.float32 , [ batchsize , seqlen , nchannels ] , name='inputs ' ) y = tf.placeholder(tf.float32 , [ batchsize , seqlen , nclasses ] , name='labels ' ) keep_prob = tf.placeholder(tf.float32 , name='keep ' ) initializer = tf.contrib.layers.xavier_initializer ( ) xin = tf.unstack(tf.transpose(x , perm=[1 , 0 , 2 ] ) ) lstm_layers = [ ] for i in range(nlayers ) : lstm_layer = tf.nn.rnn_cell.lstmcell(num_units=nunits , initializer = initializer , use_peepholes = true , state_is_tuple = true ) dropout_layer = tf.contrib.rnn.dropoutwrapper(lstm_layer , output_keep_prob = keep_prob ) # [ lstm ---&gt ; dropout ] ---&gt ; [ lstm ---&gt ; dropout ] ---&gt ; etc ... lstm_layers.append(dropout_layer ) rnn = tf.nn.rnn_cell.multirnncell(lstm_layers , state_is_tuple = true ) initial_state = rnn.zero_state(batchsize , tf.float32 ) outputs , final_state = tf.nn.static_rnn(rnn , xin , dtype = tf.float32 , initial_state = initial_state ) outputs = tf.transpose(outputs , [ 1,0,2 ] ) outputs = tf.reshape(outputs , [ -1 , nunits ] ) weight = tf.variable(tf.truncated_normal([nunits , nclasses ] ) ) bias = tf.variable(tf.constant(0.1 , shape=[nclasses ] ) ) prediction = tf.matmul(outputs , weight ) + bias prediction = tf.reshape(prediction , [ batchsize , seqlen , nclasses ] ) cost = tf.reduce_sum(tf.pow(tf.subtract(prediction , y ) , 2 ) ) / ( 2 * batchsize ) tvars = tf.trainable_variables ( ) grad , _ = tf.clip_by_global_norm(tf.gradients(cost , tvars ) , maxgradnorm ) optimizer = tf.train.adamoptimizer(learning_rate = learningrate ) train_step = optimizer.apply_gradients(zip(grad , tvars ) ) sess = tf.session ( ) sess.run(tf.global_variables_initializer ( ) ) iteration = 1 for e in range(0 , epochs ) : train_loss = [ ] state = sess.run(initial_state ) for i in xrange(0 , len(x_values ) , batchsize ) : x = x_values[i : i + batchsize ] y = y_values[i : i + batchsize ] y = np.expand_dims(y , 2 ) feed = { x : x , y : y , keep_prob : 1.0 , initial_state : state } _ , loss , state , pred = sess.run([train_step , cost , final_state , prediction ] , feed_dict = feed ) train_loss.append(loss ) iteration + = 1 print(""epoch : { } /{}"".format(e , epochs ) , "" iteration : { : d}"".format(iteration ) , "" train average rmse : { : 6f}"".format(np.mean(train_loss ) ) )",11974,,,2018-08-07T15:02:05.900,tensorflow : ca n't overfit training data with batch size > 1,neural-networks machine-learning deep-learning tensorflow recurrent-neural-networks,0,0,1
1182,4933,1,,2018-01-07T09:44:07.823,5,660,"what kind of knowledge is required to jump into the field of ai ? what mathematics is required ? how good i should be in mathematics ? currently , i have just started programming . i would be grateful if you suggest me reading material .",11992,2444,2019-04-08T16:28:17.430,2019-04-08T16:34:30.753,what kind of knowledge is required to jump into the field of ai ?,ai-basics getting-started math,5,2,10
1183,4937,1,,2018-01-07T16:07:12.053,0,87,there is more information recently that alphazero has been trained to be the best chess program after 4 hours of learning ( in chess ) . i am wondering how the ai network could have been modeled for this program ?,11994,1671,2018-01-07T21:25:00.477,2018-08-21T09:02:10.317,how the ai network can be modeled for alphazero ?,deep-learning ai-design game-ai combinatorial-games,2,5,
1184,4949,1,,2018-01-08T16:58:14.890,4,338,i am trying to understand if robotic process automation is a field which requires expertise in machine learning . does the algorithm behind rpa use machine learning expect from ocr ?,12013,1671,2018-01-09T00:38:19.030,2018-05-01T20:57:39.443,is robotic process automation related to ai ?,machine-learning robotics ai-basics automation,2,1,1
1185,4953,1,,2018-01-09T09:48:31.757,4,357,"can viola jones algorithm be used to detect the facial emotion . actually it was used in creating harr - cascade file for object and facial detection , but what confused me is whether it can be used to train for emotion detection . if not , what algorithms can i use ? and what are the mathematical bases ? ( i.e. what mathematics should i be studying ? )",12021,1671,2019-05-16T19:02:54.233,2019-05-16T19:02:54.233,viola jones algorithm,algorithm image-recognition emotional-intelligence math facial-recognition,2,1,1
1186,4955,1,,2018-01-09T13:31:07.907,3,54,"i have a large dataset of skin images , each one associated with a hydration value ( percentage ) . now i 'm looking into predicting the hydration value from an image . my thinking : train a cnn on the dataset and evaluate the model with a mean square error regression . first , does this sound like a sensible way to try this ? second , i 'd like to run the model on mobile . can you recommend any examples with caffe2 ( or alternatively tensorflow ) or diagrams that might explain a similar task ?",12024,,,2018-08-16T15:01:15.120,predict value from image set,convolutional-neural-networks prediction linear-regression,1,0,2
1187,4956,1,,2018-01-09T14:54:46.683,1,163,can someone please point me to where i can read up on why non linearities that can produce values larger than 1 or smaller than 0 work . my understanding is that neurons can only produce values between 0 and 1 and that this assumption can be used in things like cross entropy . are my assumptions just completely wrong ?,12026,,,2018-02-09T14:21:31.417,why does relu ( and other non linearities ) work ?,convolutional-neural-networks tensorflow backpropagation,2,1,1
1188,4957,1,,2018-01-09T18:26:56.430,2,82,"imagine we have 2 air conditioner systems ( aa ) and 2 "" free cooling "" systems which mix external and internal air ( fc ) in a closed box which always tends to warm up . for each system , we have to find turn on and off temperatures ( for some hysteresis , let 's say between the range 20 - 40 each one ) to optimize the energy consumption . as we do n't know the relation between these parameters and the energy consumption ( and we do n't intend to know them ) , we treat the problem as a black - box function . till now , the problem would be solvable via a bayesian optimizer ( eg . with gaussian process acquisition function ) . but there is a problem : the best configuration may change between seasons , and even days ! a simple bayesian optimizer maybe could deal with these changes limiting the data it takes into account by , for example , the last 15 - 30 days . but this would deal with the change after the consumption increased . so , the idea is introduce some contextual variables which would help the system prevent these changes ( eg . the external and internal temperature , and the vectors of variation of external and/or internal temperature , the weather prediction , whatever ) . also , some of these variables we can take into account might be internal of the system , which means while these influence the best configuration , the actual configuration also influences these variables ! and this becomes a reinforcement learning problem . 1 ) is there a way ( documented or experimental ) to know which variables ( both internal or external ) influences the optimal configuration of these aa / fc systems ? 2 ) based on the first question , which would be the best approach ? 2.1 . ) no features . this might be considered a multiarmed bandit problem for continuous reward . ( fix posterior to scenario change , if there is a scenario chagne ) 2.2 . ) only external features to predict the scenario change . this might be considered a contextual multiarmed bandit - problem . ( foresee the scenario change ) 2.3 . ) consider only system - internal features . this can be considered a reinforcement - learning problem . ( fix immediatly the scenario change ) 2.4 . ) consider both external and internal features . this can be considered a reinforcement - learning problem where some of the states are not influenced by the configuration . ( foresee the scenario change , and if something fails , fix immediatly ) .",6114,6114,2018-01-10T08:50:26.763,2018-01-10T08:50:26.763,which features and algorithm could optimize this air - conditioner problem ?,algorithm reinforcement-learning optimization,0,0,1
1189,4960,1,4968,2018-01-09T22:35:46.167,2,169,"i studied machine learning when i was in university , a couple years ago . i used it for my master thesis ( decision trees , ensembles , svm and word embedding mostly ) and for other projects either personal and academics ( genetic algorithms , q - learning ) . then i studied neural networks by myself , from the pure theory of the perceptron , backpropagation , gradient descent , etc ... ( with bengio 's deep learning book ) to the various ramifications of convnets , deconvnets , auto encoders , lstm , deep reinforcement learning and other various papers . i took a several months break and there are already new types of networks that i do n't know ... so hard to keep up with the state of the art . however , my knowledge is purely theoretical . i want to embed machine learning in my future career but getting a job in the field requires practical expertise , which i lack . i am working full time now as a software engineer , so what do you think may be a possible path to follow in order to gain expertise ? given that i have a strong mathematical background and more than ten years of coding on my back , is there any resource on practical rules of thumbs , suggestions , real case studies , etc ... which i can invest time onto ? i mean something beyond the vanilla "" introduction to machine learning "" , something more real and concrete . do you think that attempting kaggle challenges from scratch would be stimulating ? or too confusing ? do you think delivering a personal project ( i.e. not a byproduct of my current job ) in which i employ one of such methods to solve a potentially real problem would be enough to be relevant in my cv ?",12030,1671,2018-01-10T17:05:06.383,2018-01-10T19:15:58.280,i 'm not new to ml / dl but my knowledge is mostly theoretical : resources to improve practical expertise ?,machine-learning getting-started praxis,1,1,2
1190,4964,1,,2018-01-10T13:21:00.913,3,763,"i am making my speech recognition project for pc ( working on windows 8) and am new in this area . the project should have basic functionality like dictation with accuracy in email , notepad , etc . , and should respond to local commands of pc . i am using sphinx4 for my speech recognition project . i 'm trying to determine if there is there is a better open source api than cmu sphinx in terms of accuracy and large vocabulary ? does kaldi ( deep neural network based ) perform better than cmu sphinx ( hmm based ) ? are the two platforms better for different kinds of applications ? essentially : i want to increase my speech recognition system accuracy and vocabulary . which might be most optimal : kaldi or cmu sphinx ? also wondering what is the difference between a speech api and a speech engine , and , more generally , as a developer what i will require to develop my software ? please help me to get a clear understanding about above questions and , if possible , provide some speech recognition developer or researcher community links . any and all comments , suggestions and answers are welcome !",12043,1671,2018-01-12T20:03:14.943,2018-05-16T15:24:21.307,speech recognition software implementation by open source api,neural-networks voice-recognition getting-started software-evaluation,1,2,1
1191,4965,1,6418,2018-01-10T16:52:52.467,11,13305,"i am working on a problem where i need to determine whether two sentences are similar or not . i implemented a solution using bm25 algorithm and wordnet synsets for determining syntactic & amp ; semantic similarity . the solution is working adequately , and even if the word order in the sentences is jumbled , it is measuring that two sentences are similar e.g. - 1 ) python is a good language . 2 ) language a good python is . my solution is determining that these two sentences are similar . what could be the possible solution for structural similarity ? how will i maintain structure of sentences ?",9428,1671,2018-01-10T17:04:08.890,2019-05-09T21:19:15.330,sentence similarity in python,natural-language-processing python,3,2,5
1192,4969,1,4974,2018-01-10T20:05:55.703,0,43,what is the difference between a histopathological image and a natural image when training a neural network ?,9560,75,2018-01-27T17:01:41.253,2018-01-27T17:01:41.253,histopathological image vs. natural image,neural-networks machine-learning deep-learning,1,0,
1193,4975,1,,2018-01-11T19:22:11.197,3,424,"i 'm currently writing an alpha - beta - pruning algorithm for a board game . now i need to come up with a good evaluation function . the game is a bit like snakes and ladders ( you have to finish the race first ) , so for a possible feature list i came up with following : field index should be high in the lower fields my fuel should be high , when coming to the end it should be low ( maximum of ' 10 ' required to enter the goal ) all ' power - ups ' must be spent to enter the goal , so prioritize them if it is possible to enter the goal ( a legit move ) , do it ! there could be some more for some special cases . i 've read somewhere that it is the best ( and easiest ) to combine them in a linear function , for example : i = field index p = power - ups f = fuel -&gt ; 0.75 * i - 5 * p - 0.25 * |(f - max_field_index / i)| since i ca n't ask an expert and i 'm not an expert by myself i have nobody to ask if those parameters are good , if i 've forgot something or if i 've combined the factors correctly . the parameters are n't that big of a deal because i could use a genetic algorithm or something else to optimize them . my problem and question is : what do i have to do to find out how to put together my features optimally ( how can i optimize the function / parameter arrangement itself ) ?",11585,,,2018-01-12T04:09:39.310,how do i write a good evaluation function ?,evolutionary-algorithms heuristics minimax,1,0,
1194,4976,1,,2018-01-11T22:07:19.627,2,67,"cio nn cio nn stands for c ontroller i nput o utput n erual n etwork note due to a typo the "" nearon "" means "" neron "" for this we have to redefine the nearon 2 inputs 2 outputs 4 weights ( each input and output have their own weights ) internal memory cell ( any byte or bit or block size with variable size ) activation function ( defines what weights and what inputs activate this nearon ) memory storage function ( defines what and when this cell should store said memory or memory stream ) memory transpose function ( once activated any stored memory that the activation function can trigger will be played / pushed into the nerual network ) forget function ( defines when and/or how and/or why these memories can be destroyed / removed based on activation function with memory states and any input stateses itself ) how would i implement this in the form of code ? please take note of the spec . ( this is non profit / gnu v3 ) this would look something like these : these would be arranges like this : which we can build it into this : it can be trained like this : do normal nn training from the inputs to the input outputs like a hidden layer nn then trained to be controlled like this : then set the known nn dataset ( inputs ) to be corrected to actual or correct values via the ci ( controller input ) which will be outputed on the "" input + controller input "" output by training like this : you have a normal nn with hidden layers which can be trainned ( this can be done with cnns ) with backpropergation , and a cost function ( use least amount of nerons ) etc now you can allow the cio nn to retrain / teach itself with supervised or unsupervised learning . you can combine the "" input output "" and the "" input + controller input "" output with another nn which can then connect with this nn in a similiar way that a neron connects to a neron",1282,1282,2018-01-12T21:54:30.257,2018-01-12T21:54:30.257,how would i implement this new type of nn,ai-design training deep-network,0,0,1
1195,4979,1,,2018-01-12T13:05:47.300,5,326,"at my work we 're currently doing some research into data visualisation for highly inter connected data , basically graphs . we 've been implementing all sorts of different layouts and trying to see which fits best , but , due to the nature of the problem --it 's a visual thing - we needed to come up with some automated way to analyse the result so welcome up with a bunch of metrics to analyse our layouts . so far , the most important metrics have been information density , edge crossings , node overlap and edge length . this gives us some good results and has allowed us to fine tune our layout algorithms . however , when a new graph is loaded , we noticed that humans still tend to fiddle a lot with the structure of the layout . moreover , it seems that our metrics do a good job of predicting where a user is likely to mess around . graph layout is a tough problem , so after some discussion , the idea of just throwing data at a neural network and let it figure it out came up . none of us are experts , or even experienced in ai . i 'm the one with the most contact with ai methods . all i 've ever done were simple nn models , no convolution , feedback or feedforward or anything of the sorts , but it seems to me this should be doable . maybe it 's my lack of expertise here but i have n't been able to find any good information on this sort of application for nns , so i was hoping someone here could point me in the right direction . what sort of model is best for such a situation ? and why ? is this actually possible or would it be super complicated ? has anyone ever tried something like this before ? if it helps , our input data ( for v1 , i guess ) would be two arrays of variable length , one for the nodes and another for the relationships between them and the output data would be an array with the node xy coordinates . any help would be greatly appreciated ! cheers !",12070,1671,2018-01-12T18:17:38.623,2018-12-05T00:01:15.260,neural network for data visualization,neural-networks machine-learning reinforcement-learning research graphs,2,2,5
1196,4984,1,4985,2018-01-13T03:48:19.280,3,296,"i have some episodic datasets extracted from a turn - based rts game in which the current actions leading to the next state does n’t determine the final solution / outcome of the episode . the learning is expected to terminate at a final state / termination condition ( when it wins or losses ) for each episode and then move on to the next number of episodes in the dataset . i have being looking into q learning , monte carlo and sarsa but i am confused about which one is best applicable . if any of the mentioned algorithm is implemented , can a reward of zero be given in preliminary states before termination state of each episodes at which it will be rewarded with a positive / negative ( win / loss ) value ?",12081,,,2018-01-13T20:28:09.730,which reinforcement learning algorithms are efficient for episodic problems ?,reinforcement-learning game-ai unsupervised-learning monte-carlo-tree-search q-learning,1,0,2
1197,4986,1,,2018-01-13T13:52:26.407,4,66,"i have a dataset with 2,23,586 samples out of which i used 60 % for training and 40 % for testing . i used 5 classifiers individually , svm , lr , decision tree , random forest and boosted decision trees . svm and lr performed well with close to 0.9 accuracy and recall also 0.9 but tree based classifiers reported an accuracy of 0.6 . after a careful observation , i found out that svm and lr did not predict the labels of 20,357 samples identically . so can i apply voting and resolve this conflict wrt prediction outcome ? can this conflict be due to an imbalanced dataset ?",12088,16909,2018-08-08T20:45:04.810,2018-08-08T20:45:04.810,can i combine two classifiers that make different kinds of errors to get a better classifier ?,classification,1,3,
1198,4987,1,,2018-01-13T15:02:40.960,6,246,"some argue that humans are somewhere along the middle of the intelligence spectrum , some say that we are only at the very beginning of the spectrum and there 's so much more potential ahead . is there a limit to the increase of intelligence ? could it be possible for a general intelligence to progress infinitely , provided enough resources and armed with the best self - recursive improvement algorithms ?",12089,2444,2019-04-19T15:22:11.460,2019-04-19T15:22:11.460,is there a limit to the increase of intelligence ?,philosophy strong-ai agi,5,0,1
1199,4988,1,4992,2018-01-13T15:14:53.140,6,133,"i 'm quite new to neural network and i recently built neural network for number classification in vehicle license plate . it has 3 layers : 1 input layer for 16 * 24(382 neurons ) number image with 150 dpi , 1 hidden layer(199 neurons ) with sigmoid activation function , 1 softmax output layer(10 neurons ) for each number 0 to 9 . i 'm trying to expand my neural network to also classify letters in license plate . but i 'm worried if i just simply add more classes into output , for example add 10 letters into classification so total 20 classes , it would be hard for neural network to separate feature from each class . and also , i think it might cause problem when input is one of number and neural network wrongly classifies as one of letter with biggest probability , even though sum of probabilities of all number output exceeds that . so i wonder if it is possible to build hierchical neural network in following manner : there are 3 neural networks : ' item ' , ' number ' , ' letter ' ' item ' neural network classifies whether input is numbers or letters . if ' item ' neural network classifies input as numbers(letters ) , then input goes through ' number'('letter ' ) neural network . return final output from number(letter ) neural network . and learning mechanism for each network is below : ' item ' neural network learns all images of numbers and letters . so there are 2 output . ' number'('letter ' ) neural network learns images of only numbers(letter ) . which method should i pick to have better classification ? just simply add 10 more classes or build hierchical neural networks with method above ?",12090,,,2018-01-17T03:25:18.633,is it better to make neural network to have hierchical output ?,neural-networks classification,3,0,
1200,4989,1,,2018-01-13T15:16:21.933,0,403,what is the relation between back - propagation and reinforcement learning ?,9560,2444,2018-11-07T16:23:28.823,2018-11-07T16:23:28.823,what is the relation between back - propagation and reinforcement learning ?,neural-networks reinforcement-learning definitions backpropagation ai-basics,1,0,
1201,4990,1,,2018-01-13T16:20:28.183,1,477,"i play a racing game called need for madness ( some gameplay : https://www.youtube.com/watch?v=nc5ufz-t0a8 ) . nfm is a racing game , where the player can choose different cars and race and crash the other cars , and you can play on different tracks too . the game has a fixed frame rate , so you can assume that the same sequence of button presses will always arrive at the exact same position , rotation , velocity , etc . of the car . i want to make a bot which could race faster than i can . what would be the best way to go about doing this ? is this problem even suited for deep learning ? i was thinking i could train a neural network where the input would be the current world state ( position of the player , position of the checkpoints you have to through and all the obstacles ) , and the output would be an array of booleans , one for each button . during a race , i could then keep forward propagating from the input to the booleans . however , i 'm not so sure what i would do after the race is over . how do i back propagate after the race to make the nn be less likely to make bad moves ?",12091,1671,2018-01-17T00:41:47.397,2018-01-17T00:41:47.397,how to teach an ai to race optimally in a racing game ?,neural-networks deep-learning game-ai pathfinding,0,2,1
1202,4991,1,4995,2018-01-13T20:13:58.970,0,531,"i do n't understand why google translate translates the same text in different ways . here is the wikipedia page of the 1973 film "" enter the dragon "" . you can see that its traditional chinese title is : 龍爭虎鬥. google translates this as "" dragons fight "" . then , if we go to chinese wikipedia page of this film , and search for 龍爭虎鬥 using ctrl - f , it will be found on several places : but if we try to copy hyperlink of chinese page into google translate , it will be word "" tiger "" from somewhere : even more , if we try to translate chinese page into english using build - in chrome translate , it will be sometimes translated as "" enter the dragon "" , in english manner : why it gives different tanslations for the same chinese text here ?",12095,4302,2018-10-08T12:18:34.753,2018-10-08T12:18:34.753,google translate : different translations for the same text,machine-learning natural-language,4,1,
1203,5000,1,5007,2018-01-14T12:43:05.573,1,504,"i am new to neural networks , i 've only started studying and learning about the subject a year ago , and i just started building my first neural network . the project is a little bit ambitious : a browser extension for children 's safety , it checks for sexual or abusive content , so that it replaces that content with a placeholder , the user will have to insert a password to show original content . i did n't find a dataset online so i decided to build my training dataset . so , i started by writing a web crawler , it starts collecting images , meanwhile implementing data augmentation techniques . it basically resizes images ( to 95x95 ) , crops them , rotates , changes colors , adds blur , black and white , noise ... etc . the problem is that after applying these techniques , i noticed that some images are not even recognizable by a human subject . i mean that even though i know that picture contains sexual content , it does n't even appear to be sexual anymore . so , do i have to label it as sexual or not sexual ? notice that it 's easier for me to consider it as sexual , if every image produces about 50 edited images , i 'd only have to label the original image , what follows is that all 50 images get the same label . is it okay to do just that ? this is a sample of what i get after doing data augmentation , notice that some pictures are not recognizable by humans . for example , look at the result after editing images hue and saturation , a human ca n't recognize this result , is it okay to label it : not sexual ? i would n't recognize the picture on the right if i did n't see the original one . edit : i just tested this on human subjects ( my brothers ) , they did n't recognize the squirrel on the right . thank you !",12113,12113,2018-01-14T17:54:41.790,2018-01-15T18:05:50.603,how to label edited images after data augmentation,neural-networks training datasets python,1,3,
1204,5005,1,5028,2018-01-14T19:11:56.920,2,92,"i was watching a documentary on netflix about alphago , and at one point ( ~1:10:16 from the end ) , one of the programmers uses the term "" heavy node , "" which i assume has to do with neural networks . i did a little bit of research but could n't find anything on what that term means . the closest i could get was this wikipedia page on heavy path decomposition : https://en.wikipedia.org/wiki/heavy_path_decomposition , which seemed like it could be somewhat related , but i was n't sure how exactly . has anyone heard of this term being used ? does anyone know what it means ? for context , in the documentary the line is that if it ( the network / player ) creates something new not in the heavy node , then they do n't see it .",12119,,,2018-01-16T16:31:57.280,what is a heavy node in neural networks ?,neural-networks deep-learning,1,1,1
1205,5010,1,5038,2018-01-15T10:31:37.033,-1,75,"in recent years , china has made rapid progress in manufacturing and scientific research as evidenced by their successful teleportation of a single quantum entangled photon to a satellite in orbit . my question is , what major contributions have chinese ai researchers made in the field of artificial intelligence ?",10913,10913,2018-01-15T18:17:17.790,2018-01-17T03:35:52.357,what noteworthy contributions have chinese ai researchers made in the field of artificial intelligence ?,ai-design research history ai-community,1,1,
1206,5011,1,,2018-01-15T13:02:15.013,6,2239,"i am trying to develop a machine learning algorithm to identify topological features within 3d cad models ( i.e. slots , pockets , holes , bosses etc ) for the input data i have decided to use the adjacency of the faces to identify the features within the model . i have developed a pre - processing algorithm which computes the adjacency between every face in the model and also whether the relationship is concave or convex in nature . below is an example of a 3d model and the relationships between each faces represented in a graph format . i was thinking of using some sort of supervised learning training method where the training data would include all of the adjacency info for the model with labels defining the features that exist within the model such as : slot - made up of faces f7 , f8 , f9 hole - made up of faces f15 pocket - made up of faces f10 , f11 , f12 , f13 and eventually once the model is sufficiently trained it would be able to identify the features in an un - seen model and determine which faces make up those features . i am not sure how i would go about pre - processing the input data ( i.e. it would be of variable length since the number of faces may not be the same from model to model ) . i am also struggling to understand which type of machine learning algorithm would be best suited to this application any help on this problem or even pointing to some resources to help understand would be appreciated",12125,,,2018-06-28T16:08:21.523,using machine learning to identify cad model features,neural-networks machine-learning convolutional-neural-networks,3,1,4
1207,5018,1,,2018-01-16T06:20:51.670,5,198,"i am not looking for proof of ai ( too broad , i know ) concepts trying to solve a specific business problem , instead for a proof ( or dis - proof ) or a model ( or a source of the same ) where a machine is capable of general thinking and decision making . collecting data as it see fit ( use any channel at their disposal ) . communicate as it see fit ( with whom , how , where and when ) . develop and evolve an agenda . note - i wrote an article more than 5 years ago - why a software can not be intelligent . since then i have not read a convincing argument against it . edit intelligence is the ability to take a decision in the unseen , unfamiliar , and unvisited circumstances . so , my definition of artificial intelligence is - a machine doing essentially the same ( doing , not replicating or simulating ) . i am looking for a proof or disproof that machine is capable of doing things ( written above ) , not the following instances where machine as simulated a certain aspect or property of intelligence . machine replicating a certain human behavior to achieve a goal .",12138,1671,2018-01-17T17:26:11.910,2018-03-31T15:28:07.313,has it been theoretically proven or dis - proven that a machine can generally think and communicate on its own ?,philosophy agi theory,3,3,3
1208,5019,1,,2018-01-16T07:11:01.590,1,538,"i have a pedestrian dataset and would like to estimate human height in a video survillance using person detection techniques like yolo darknet or ssd ( single shot detectors ) . would this technique work ? also , the videos that i have are in a constrained environment with good illumination . the idea is to get the coordinates from the bounding box and try to estimate pixel height . after getting the pixel height , some correlation could be estimated between pixel height and real world height . note that i wo n't be using camera calibration .",11800,11800,2018-01-16T09:49:37.117,2019-03-14T20:15:30.843,human height estimation using person detection techniques,machine-learning datasets,1,5,
1209,5022,1,,2018-01-16T10:17:43.680,4,426,"due to recursive self - improvement , ai could lead to an intelligence explosion improving on itself year over year exponentially . assuming the proper environment was created to allow an ai to self - improve , how fast would this occur ? with human intervention we might say ai could improve similar to gdp or science growth say 3 % per year . but this intervention would be additive to the self - recurring improvements made by the ai . what is a reasonable annual rate of self - growth and what would the processing power of the ai be after 10 years given an initial value of say 1pflop ? example : human rate = 3 % annually ai rate = 1 % of current processing power annually year 1 . 1 pflop year 2 . 1 pflop + ( 1 pflop * 3 % ) + ( 1 pflop * 1 % ) = 1.04 pflop year 3 . 1.04 pflop + ( 1.04 pflop * 3 % ) + ( 1.04 pflop * 1.04 % ) = 1.082 pflop year 4 . 1.082 pflop + ( 1.082 pflop * 3 % ) + ( 1.082 pflop * 1.082 % ) = 1.126 pflop year 5 . 1.126 pflop + ( 1.126 pflop * 3 % ) + ( 1.126 pflop * 1.126 % ) = 1.172 pflop",12139,1671,2018-01-17T01:20:43.730,2019-01-04T05:17:30.303,at what rate could ai theoretically self - improve ?,machine-learning philosophy unsupervised-learning theory,2,5,2
1210,5027,1,,2018-01-16T15:29:53.320,3,66,"for supervised learning , humans have to label the images computers use to train in the first place , so the computers will probably get wrong the images that humans get wrong . if so can computers beat humans ?",12145,1671,2018-01-17T00:57:58.863,2018-01-17T00:57:58.863,how can computers beat humans at image recognition ?,image-recognition theory,1,2,
1211,5041,1,,2018-01-17T11:53:21.057,4,190,"many of the architectures that do semantic segmentation like segnet , dilatednet ( yu and koltun ) , deeplab , etc . do not work on high resolution images . for such benchmarks like cityscapes , what is a standard / practical approach for such methods to perform on the benchmark ? i 've tried to look into the paper , but i could n't find such details . there 's an article mentioning that they output at 1/8 of input images than do interpolation ( usually 2 , 4 or 8 times ) from their results , but the article does not specify which upsampling techniques are the most reasonable one .",3098,3098,2018-01-17T12:03:30.943,2018-05-28T03:56:36.300,semantic segmentation how to upsampling,deep-learning self-driving,1,0,
1212,5042,1,,2018-01-17T12:49:10.813,1,42,i 've been studying a recommender system which uses a collaborative deep learning approach and bayesian learning . it has the following nn representation : i need to know the working of stacked denoising autoencoders . here is the link to the paper : http://www.wanghao.in/paper/kdd15_cdl.pdf,10118,16565,2019-04-04T20:38:39.597,2019-04-04T20:38:39.597,how do stacked denoising autoencoders work,neural-networks deep-learning learning-algorithms recommender-system,0,0,
1213,5043,1,,2018-01-17T15:50:13.960,0,89,"what are the connections between ethics and artificial intelligence ? what are issues that have arisen , especially in the business context ? what are issues that may arise ?",12171,1671,2018-01-17T17:23:30.043,2018-09-20T00:28:54.457,what are the connections between ethics and ai,philosophy ethics,4,1,2
1214,5048,1,5061,2018-01-18T00:46:14.930,1,34,"consider an environment , where an agent intends to move from cell "" a "" to cell "" g "" , avoiding obstacles ( cells marked with shading ) . the agent can move forward , rotate 90º to the left , or rotate 90º to the right and can identify the type of cell in front of him . classify the characteristics of the environment , according to : acessible or non - acessible ; deterministic or non‐deterministic ; episodic or non‐episodic ; discrete or continuous ; static or dynamic .",12182,,,2018-01-18T22:42:52.817,how to classify this environment ?,intelligent-agent,1,2,
1215,5051,1,5056,2018-01-18T04:07:11.523,2,429,"the above environment is deeptraffic now consider this situation in the above environment , the red car ( we control it with our rl agent ) is on the extreme right lane . during the exploration phase , we take a ' move right ' action , which ofcourse will result in the car not moving right , but the other cars will be moving , state changes due to the rules of the environment . i 'm using cnn to solve this , the state representation is the image itself and its a q - learning algorithm as described in dqn paper from deepmind . in the above situation i mentioned , wo nt the agent think due to ' move right ' action the state has changed , which is not really the case ? and when remembering the state transition ( s , a , r , s ' ) should i remember the actual action ' move right'(invalid ) or ' do nothing'(correct as per env ) ?",5030,,,2018-01-18T16:09:13.940,rl agent 's view of state transitions,reinforcement-learning,1,3,1
1216,5054,1,,2018-01-18T13:20:41.763,3,106,"i 'm struggling to understand the underlying mechanics of cnns so any help is appreciated . i have a network with a relu activation function which does perform signifigantly better than one with sigmoid . this is expected as relu solves the vanishing gradient problem . however , my understanding was the reason we implement nonlinearities is to separate data which can not be separated linearly . but if relu is linear for all values we care about it should n't work at all ? unless , of course , neurons are defined for negative values but then my question becomes "" why does relu solve the vanishing gradient problem at all ? "" , since the derivative of relu for x&lt;0 = 0",12026,,,2018-08-16T20:00:46.517,"if neurons are only defined for values between 0 and 1 , how does relu differ from the identity ?",machine-learning image-recognition tensorflow,1,1,
1217,5057,1,5062,2018-01-18T18:32:34.333,3,103,"in the book "" reinforcement learning "" by sutton there is a discussion of the k - armed bandit problem , where the expected reward from the bandits changes slightly over time ( is non - stationary ) . instead of updating the q values by taking an average of all rewards , the book suggests using a constant step - size parameter , so as to give greater weight to more recent rewards . thus : $ $ q_{n+1 } = q_n + \alpha ( r_n - q_n)$$ where is a constant between 0 and 1 . until here i understand . the book then asserts that : we call this a weighted average because the sum of the weights = 1 . my question : can someone please explain what that last line means , and why it is true ?",12201,,2018-11-26T18:37:32.457,2018-11-26T18:37:32.457,k - armed bandit and reinforcement learning,machine-learning reinforcement-learning unsupervised-learning game-theory math,1,5,
1218,5058,1,5418,2018-01-18T21:00:06.967,5,116,"i have a model that predicts sentiment of tweets . are there any standard procedures to evaluate such a model in terms of its output ? i could sample the output , work out which are correctly predicted by hand , and count true and false positives and negatives but is there a better way ? i know about test and training sets and metrics like auroc and auprc which evaluate the model based on known data , but i am interested in the step afterwards when we do n't know the actual values we are predicting . i could use the same metrics , i suppose , but everything would need to be done by hand .",8385,,,2018-02-24T16:39:04.620,how do i statistically evaluate a ml model ?,machine-learning classification,1,0,3
1219,5059,1,,2018-01-18T21:58:58.450,2,283,"i am trying to do some experiments with some intelligent agents , but i 'm not sure how significant they will be in the future . can someone explain some interesting scenarios , maybe some use - cases of intelligent agents in the future ? for instance it can be used as a virtual assistant instead of a real call agent . but what can be a more appealing application in the future ?",9053,1581,2018-11-01T17:30:24.143,2018-11-01T17:30:24.143,the future of intelligent agents ?,ai-design intelligent-agent,1,2,2
1220,5063,1,,2018-01-19T01:19:43.947,2,45,"by open up i mean slightly open up so that a theoretical structure of panels with no width looks three - dimensional . the original structure being an ideal object where any number several panels can occupy the same region of space ( plane ) . to concretize what i mean by rigid structure of panels , let 's take what 's on my profile picture . i 'm including here a larger version of that object : an origami figure is folded from a square and , unlike this simple example in the image , it can be very convoluted , with layers upon layers . let 's say that if i have an ideal , theoretical and flat model of an origami picture and by flat i mean the faces are on planes but not necessarily on a single plane . for example all of faces of the figure in the image would be in one plane , but there could be figures with more planes ; think of animals with ears , flippers , etc . i would like to open up those parts that hinge on a theoretical segment ( relatively easy ) or curve or make a triangle of the corners of those faces that have two adjacent faces with no connections , open up a set of several faces that allow such opening of which the image is a good example . so far i have tried programming rules for different structures suchs as flaps , ends , wrap - arounds ... however there are three no small problems . first , it 's extremely challenging to take into account all the corner cases and possibilities . i suspect it sounds simpler that it really is , but i do n't want to digress with explanations . second , the code is not maintenable . it 's difficult to put into words rules that have to be visualized . third it 's very difficult to unit test and debug . i strongly suspect that there must be some artificial intelligence techniques for doing this . would you be so kind as to point me in the right direction ? just in a general way , without needing to go much into detail . let me know if i should include more info or code .",12210,1671,2018-01-19T20:44:54.667,2018-01-19T20:44:54.667,how to open up a rigid structure made of connected panels ?,algorithm heuristics,0,3,
1221,5067,1,,2018-01-19T11:32:39.073,3,715,"i am currently writing an engine to play a card game and i would like for an ann to learn how to play the game . the game is currently playable , and i believe for this game a deep - recurrent - q - network with a reinforcement learning approach is the way to go . however , i do n't know what type of layers i should use , i found some examples of atari games solved through ann , but their layers are cnn ( convolutional ) , which are better for image processing . i do n't have an image to feed the nn , only a state composed of a tensor with cards in the player 's own hand and cards on the table . and the output of the nn should be a card or the action ' end turn ' . i 'm currently trying to use tensorflow but i 'm open to any library that can work with nn . any type of help or suggestion would be greatly appreciated !",12217,12217,2018-01-23T14:25:09.793,2018-10-21T16:00:29.817,what layers to use in a neural network for card game,neural-networks deep-learning reinforcement-learning tensorflow game-ai,2,2,2
1222,5069,1,,2018-01-19T15:07:30.837,0,46,"as far as i understand , the hill climbing algorithm is a local search algorithm that selects any random solution as an initial solution to start the search . then , should we apply an operation ( i.e. , mutation ) on the selected solution to get a new one or we replace it with the fittest solution among its neighbours ? this is the part of the algorithm where i am confused .",6095,2444,2019-03-02T11:03:17.787,2019-03-02T11:03:17.787,should the mutation be applied with the hill climbing algorithm ?,search optimization hill-climbing,1,0,
1223,5070,1,,2018-01-19T17:17:28.953,-1,74,"i do not have background in artificial intelligence related algorithms etc . but i have a scheduling problem that needs to be resolved . for example , employees need to be scheduled for on call . they might be on vacation . can you guide as to what algorithms i need to look into , suitable for a beginner ?",12203,1671,2018-01-19T21:02:20.463,2018-02-21T16:10:01.147,artificial intelligence and automatic scheduling,learning-algorithms getting-started ai-basics,1,3,
1224,5075,1,5079,2018-01-20T10:58:40.943,0,119,"in the delta rule the equation to adjust the weight with respect to error is : - where is the learning rate and e is the error the graph for e vs w would look like the one below with e in the y axis and w in the x axis in other words we can write i want to know , what is the proof behind the gradient of a curve being equal / proportional to the distance between the two co - ordinates in the x - axis -or- ( ∂e/∂w ) times step is a small shift on f(w ) not w.so why does the difference between w(n+1 ) and wn be equal to f(w ) i found a similar question some - confusion - of - gradient - descent , but the accepted answer doesnot have a proof .",11789,11789,2018-01-23T13:56:48.110,2018-03-19T21:57:48.580,what is the proof behind the gradient of a curve being equal / proportional to the distance between the two co - ordinates in the x - axis,machine-learning backpropagation learning-algorithms gradient-descent,2,1,
1225,5080,1,,2018-01-21T14:54:11.900,1,25,are there a finite set of computable functions constructing deep neural network which can form or implement any c.e . function or computable function ? or does there exist a finite set of computable function by which every c.e . function can be implemented by combination and connection(like connection in dnn ) ?,9237,,,2018-01-21T14:54:11.900,are there a finite set of computable functions constructing deep neural network which can form or implement any c.e . function or computable function ?,deep-network reference-request,0,7,
1226,5081,1,,2018-01-21T16:36:58.270,2,338,this ai is really human - like and allegedly does n't give pre - programmed responses . it 's makers robots without borders say the project is open source but i could n't find the code anywhere .,12254,,,2018-04-12T04:58:32.700,can anyone find the source code for the chatbot luna ?,chat-bots,1,5,
1227,5084,1,,2018-01-22T03:50:21.097,1,200,"i am currently working on a defect detection algorithm but i only have a few samples of defects.i googled for defect detection datasets and i found this one : http://resources.mpi-inf.mpg.de/conferences/dagm/2007/prizes.html which has a few hundreds of original images of defects . my idea is : imagenet = > defect dataset from internet = > own defect dataset step 1 . training a model with imagenet initialization using the defect dataset found in the internet ( + non - defect images + augmented data ) step 2 . using the output model of step 1 ( which will be more similar to my own data),do transfer learning using my own defect dataset ( defects + non - defects + augmented ) . do you think this a good way to get good results ? based on : https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0 should defect images consider as low similar with imagenet 's images ? or similar to model because a both inputs are images ? some webpages said because they both are images , they are similar but some webpages said because these images are too different to the images used to train the imagenet model so i got confused about this . if i skip step 1 , i do nt think i get anything good because i have less than 100 images . any advise or comment will be appreciated .",12261,12261,2018-01-22T06:28:04.447,2018-01-22T06:28:04.447,transfer learning from model trained in a similar dataset,training tensorflow object-recognition keras,0,0,
1228,5085,1,,2018-01-22T10:47:03.950,4,91,"i am trying to build a neural network that takes in a single string , ex : "" dog "" as an input , and outputs 50 or so related hashtags such as , "" # pug , # dogsarelife , # realbff "" . i have thought of using a classifier , but because there is going to be millions of hashtags to choose the optimal one from , and millions of possible words from the english dictionary , it is virtually impossible to search up the probability of each it is going to be learning information from analyzing twitter posts ' text , and its hashtags , and find which hashtags goes with what specific words .",12264,,,2019-05-24T11:02:01.823,what machine learning algorithm should be used to analyze the relationship between strings ?,neural-networks ai-design algorithm,3,3,1
1229,5086,1,,2018-01-22T13:54:46.703,1,19,"i have a sample set of data about leads that gets generated every day . leads are nothing but a user expressing request to be our partner or not . sample data set is as shown below leadid , createdate , status , leadtype 810029,24-dec-17 12.00.00.000000000 am , open , leadtype1 806136,30-dec-17 12.00.00.000000000 am , open , leadtype2 812134,31-dec-17 12.00.00.000000000 am , open , leadtype2 806147,31-dec-17 12.00.00.000000000 am , open , leadtype1 806166,01-jan-18 12.00.00.000000000 am , open , leadtype2 28002,04-mar-16 12.00.00.000000000 am , open , leadtype2 808156,01-jan-18 12.00.00.000000000 am , open , leadtype1 808162,01-jan-18 12.00.00.000000000 am , open , leadtype2 806257,07-jan-18 12.00.00.000000000 am , open , leadtype1 832091,17-jan-18 12.00.00.000000000 am , open , leadtype2 838079,17-jan-18 12.00.00.000000000 am , open , leadtype1 66001,26-mar-16 12.00.00.000000000 am , open , leadtype1 70001,28-mar-16 12.00.00.000000000 am , open , leadtype2 806019,23-dec-17 12.00.00.000000000 am , open , leadtype2 822064,12-jan-18 12.00.00.000000000 am , open , leadtype1 834043,14-jan-18 12.00.00.000000000 am , open , leadtype2 836053,16-jan-18 12.00.00.000000000 am , open , leadtype1 838119,19-jan-18 12.00.00.000000000 am , open , leadtype2 as you can see lead types can be of leadtype1 or leadtype2 and this get generated every day . in order to make sense of data i created the following plot using python the supporting code is as follows . note i am just a noob to python and ai but i want to check if this proves a valid use case for machine learning and what should be my approach import numpy as np import pandas as pd import matplotlib.pyplot as plt # % matplotlib inline in_file = ' lead_data.csv ' mydf = pd.read_csv(in_file,encoding='latin-1 ' ) fig , ax = plt.subplots(figsize=(15,7 ) ) # g = mydf.groupby(['r4gstate','leadtype']).count()['status'].unstack ( ) g = mydf.groupby(['r4gstate','status']).count()['leadtype'].unstack ( ) g.plot(ax=ax ) # ax.set_xlabel('r4gstate ' ) ax.set_xlabel('r4gstate ' ) ax.set_ylabel('number of leads ' ) ax.set_xticks(range(len(g ) ) ) ; ax.set_xticklabels([""%s "" % item for item in g.index.tolist ( ) ] , rotation=90 ) ; basically i just read the csv , curated the data ( i have cleaned the original csv ) to keep what is meaningful for me . i also created grouping of number of leads month - year wise so that i can see the historical lead generated every month . i want to know if machine learning helps me to predict number of lead generated in next coming months based on previous months data . if the answer is yes then is linear regression the right path to explore further",12266,,,2018-01-22T13:54:46.703,can number of leads be predicted based on previous months,machine-learning linear-regression python,0,0,
1230,5087,1,,2018-01-22T14:22:23.820,1,13,"forgive what might be a basic question . i 'm just experimenting with ml / al and i have a small problem set and i 'd like to see if it can be solved with ml / ai . basically , given a set of objects with multiple features , i 'd like to create a process for recommending one automatically to a user . i 'm thinking that some sort of clustering algorithm may be the best approach . however , one main challenge i 'm trying to wrap my head around is that i do n't know in advance how many distinct clusters will evolve ... there may be scenarios where we feature x is really important , but other scenarios where a user will say feature y is important . secondly , what is my input set ? for each training sample , i will have 1 selected object , and n-1 unselected objects . but i do n't want to "" train "" that the unselected objects are "" bad "" because they could be selected in a future training example . finally , i do n't have a large training set already , so i would like to use feedback ( user input , "" this was a bad choice "" or "" use this object instead . "" ) from the process to further refine the algorithm . is this feasible ? are there any established patterns for this sort of process ? thanks in advance .",11449,,,2018-01-22T15:18:57.320,recommend item from set based on features,machine-learning learning-algorithms,1,0,
1231,5092,1,5101,2018-01-22T15:56:40.457,1,355,i am currently learning about cnn 's and i am confused on how filter / kernels are initialized beside their size ? say if you want a filter of 3x3 how are the inner values initialized a the start ? http://setosa.io/ev/image-kernels/ do you just use those predefined image - kernels as a start ? or are they randomly initialized and retrained from backpropagation ? i am pretty confused on this matter because so far of all the lecture i have taken no one really talk about this yet and i want to know it now .,12242,1671,2018-01-22T22:35:46.073,2018-01-23T09:56:39.900,how are kernel 's input values initialized in a cnn network ?,convolutional-neural-networks image-recognition,1,1,1
1232,5093,1,5099,2018-01-22T17:00:35.630,1,447,"i am implementing neural network to train hand written digits . following is the cost function , in log(1-(h(x ) ) , if h(x ) is 1 , then it would result in log(1 - 1 ) ( i.e ) log(0 ) . so i m getting math error . i m initializing the weights randomly between 10 - 60 . i m not sure where i have to change and what i have to change ! thanks for your help .",12273,,,2018-01-23T07:01:42.500,neural network cost function implementation,neural-networks machine-learning,1,3,
1233,5094,1,,2018-01-22T19:39:50.210,1,17,i 'm reading the book introduction to evolutionary computing and on the chapter about evolution strategies said that we have to modify the strategy parameter sigma ( standard deviation or mutation step size ) before using in to modify the object parameter and i do n't understand why . why do we have to modify sigma before the mutation of object parameters ? maybe because if we do it we will have the mutated object parameters and the strategy parameters that have generated them .,4920,,,2018-01-22T19:39:50.210,es- modify sigma before mutate object parameters,evolutionary-algorithms,0,0,
1234,5096,1,,2018-01-22T21:40:56.423,10,910,"i am trying to develop a neural network which can identify design features in cad models ( i.e. slots , bosses , holes , pockets , steps ) . the input data i intend to use for the network is a n x n matrix ( where n is the number of faces in the cad model ) . a ' 1 ' in the top right triangle in the matrix represents a convex relationship between two faces and a ' 1 ' in the bottom left triangle represents a concave relationship . a zero in both positions means the faces are not adjacent . the image below gives an example of such a matrix . lets say i set the maximum model size to 20 faces and apply padding for anything smaller than that in order to make the inputs to the network a constant size . i want to be able to recognise 5 different design features and would therefore have 5 output neurons - [ slot , pocket , hole , boss , step ] would i be right in saying that this becomes a sort of ' pattern recognition ' problem ? for example , if i supply the network with a number of training models - along with labels which describe the design feature which exists in the model , would the network learn to recognise specific adjacency patterns represented in the matrix which relate to certain design features ? i am a complete beginner in machine learning and i am trying to get a handle on whether this approach will work or not - if any more info is needed to understand the problem leave a comment . any input or help would be appreciated , thanks .",12125,,,2018-12-09T05:43:59.447,using neural network to recognise patterns in matrices,neural-networks machine-learning convolutional-neural-networks pattern-recognition detecting-patterns,3,1,4
1235,5098,1,,2018-01-23T05:02:16.247,5,61,"i am new to machine learning and ai , so forgive me if this is obvious . i was talking with a friend on how to solve this problem , and neither of us could figure out how to do it . say i have a grid area of 100x100 blocks , and i want a robot to build a horizontal 100x100 grid , and 3 blocks high . i am given a random , but known starting surface , always 100x100 but the height of the random surface can vary from 1 to 5 blocks . i have an extra reserve of blocks i can pick up , so do nt have to worry about running out . the robot can move in any direction , even diagonally at some cost penalty . the robot can obviously move a 4 high block to fill in a 2 high , so each is at the design height of 3 . this sounds like a reinforcement learning problem , but would any one be able to explain more detail how i would do this , to a ) minimize the amount of moves , and b ) to get to the design surface .",12284,,,2018-06-29T16:34:31.287,move blocks to create a designed surface,reinforcement-learning,2,0,
1236,5103,1,,2018-01-23T14:12:52.357,0,228,"trying to figure out what google is offering in the article at this link https://www.nytimes.com/2018/01/17/technology/google-sells-ai.html . i also need to know if their offerings are limited to visual scans and if so , the range of photographical types it can work with ( heat maps for example , or continuous video feed ) . we are a small road construction company looking for ways to start using ai on some of our projects so i am in the process of gathering information on exactly what it can do .",12292,1671,2018-01-26T21:34:41.263,2018-09-25T08:01:15.967,interested in what google ai can do . know nothing about ai at all,machine-learning google,2,2,2
1237,5104,1,,2018-01-23T14:24:36.013,2,127,"i am working with a project which is a agent based pedestrian simulation in java and its is animated with the help of javafx . i 've tried to read all the social force model papers but my understanding of those articles are none . so i tried an own approach which got trashed after failing time after time . my approach was that each agent calculated its surrounding and first calculated the distance to each of the agents on the field and if that distance was below a constant then the agent calculates the angle of that agent which is too close to it and moves accordingly to the calculated angle . this approach did nt work for me because the "" avoidance code "" is not efficient enough and the agents just do nt know where to go when they meet and just stays in place . i am asking for guidance to how i can approach this problem in a better way . double [ ] check(vector&lt;pedestrian&gt ; peds , pedestrian p1 ) { for ( pedestrian p : peds ) { if ( p.getpedestrianid ( ) ! = this.id ) { double distance = ipedestrian.distance_formula(gettranslatex ( ) , gettranslatey ( ) , p.gettranslatex ( ) , p.gettranslatey ( ) ) ; if ( distance & lt;= danger ) { system.out.println(""danger "" ) ; return ipedestrian.angle(gettranslatex ( ) , gettranslatey ( ) , p.gettranslatex ( ) , p.gettranslatey ( ) , p1 ) ; } } } return new double [ ] { speed , 0 } ; } public void move(vector&lt;pedestrian&gt ; peds , pedestrian p ) { double [ ] new_steps = this.check(peds , p ) ; if ( side = = sidechooser.left ) { settranslatex(gettranslatex ( ) + new_steps[0 ] ) ; settranslatey(gettranslatey ( ) + new_steps[1 ] ) ; } else { settranslatex(gettranslatex ( ) - new_steps[0 ] ) ; settranslatey(gettranslatey ( ) - new_steps[1 ] ) ; } } math formulas : static double distance_formula(double thisx , double thisy , double otherx , double othery ) { return math.sqrt(math.pow(otherx - thisx , 2 ) + math.pow(othery - thisy , 2 ) ) ; } static double [ ] angle(double x1 , double y1 , double x2 , double y2 , pedestrian p ) { double angle = math.todegrees(math.atan2(y2-y1 , x2-x1 ) ) ; angle + = math.ceil(-angle/360 ) * 360 ; //double angle = math.todegrees(math.atan2(y2-y1 , x2-x1 ) ) ; if ( p.getsidechoosen ( ) = = sidechooser.left){//if the pedestrian is from the left side if ( angle & lt ; 45 || angle & gt ; 315)//front return new double[]{-speed/5 , 0 } ; else if ( angle & gt;= 135 || angle & lt;= 225 ) //back return new double[]{speed*1.4 , 0 } ; else if ( angle & gt;= 45 || angle & lt;= 90)//north - east return new double[]{0 , speed } ; else if ( angle & gt ; 90 || angle & lt;= 135 ) //north - west return new double[]{speed*1.2 , speed } ; else if ( angle & gt;= 270 || angle & lt;= 315 ) //south - east return new double[]{0 , -speed } ; else if ( angle & gt ; 225 || angle & lt;= 270 ) //south - west return new double[]{speed*1.2 , -speed } ; else return new double[]{speed , 0 } ; } else { if ( angle & lt ; 45 || angle & gt ; 315)//back return new double[]{speed*1.4 , 0 } ; else if ( angle & gt;= 135 || angle & lt;= 225 ) //front return new double[]{-speed/5 , 0 } ; else if ( angle & gt;= 45 || angle & lt;= 90)//north - west return new double[]{speed*1.2 , -speed } ; else if ( angle & gt ; 90 || angle & lt;= 135 ) //north - east return new double[]{0 , -speed } ; else if ( angle & gt;= 270 || angle & lt;= 315 ) //south - west return new double[]{speed*1.2 , speed } ; else if ( angle & gt ; 225 || angle & lt;= 270 ) //south - east return new double[]{0 , speed } ; else return new double[]{speed , 0 } ; } }",12291,,,2018-01-23T16:08:52.917,agent collision avoidance java,multi-agent-systems,1,0,
1238,5107,1,5740,2018-01-23T18:45:30.723,5,2402,"below is a quote from cs231n prefer a stack of small filter conv to one large receptive field conv layer . suppose that you stack three 3x3 conv layers on top of each other ( with non - linearities in between , of course ) . in this arrangement , each neuron on the first conv layer has a 3x3 view of the input volume . a neuron on the second conv layer has a 3x3 view of the first conv layer , and hence by extension a 5x5 view of the input volume . similarly , a neuron on the third conv layer has a 3x3 view of the 2nd conv layer , and hence a 7x7 view of the input volume . suppose that instead of these three layers of 3x3 conv , we only wanted to use a single conv layer with 7x7 receptive fields . these neurons would have a receptive field size of the input volume that is identical in spatial extent ( 7x7 ) , but with several disadvantages . my visualized interpretation how can you see through the first cnn layer from the second cnn layer and see a 5x5 sized receptive field ? there were no previous comments stating all the other hyperparameters , like input size , steps , padding , which made this very confusing to visualize . edited : i think i found the answer https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807 but i still do n't understand it . in fact , i am more confused than ever .",12242,12242,2018-01-24T16:08:03.693,2019-01-30T17:28:22.307,how can 3 same size cnn layers in different ordering output different receptive field from the input layer ?,deep-learning convolutional-neural-networks image-recognition,3,0,
1239,5108,1,5109,2018-01-23T21:06:43.630,-1,33,i trained data for recognizing the fingerprint of my friends . i am feeding testing data to local machine by hardware . what should i do to run this as a server and get the data over network in least time and in most efficient way ?,12302,,,2018-01-27T06:00:52.580,best way to build api for input data and get response classified by tensorflow ?,tensorflow,1,1,
1240,5110,1,,2018-01-23T22:19:57.100,3,414,"i want to develop an eclipse voice plugin on a mac that helps me jot down high level classes and stub methods like create a class that inherits from x , add a method that returns string could somebody help me point out the right material to learn to achieve that ? i do n't mind using an existing solution if it exists . as far as i understand , i would have to use some siri interface and use nltk to convert the natural text into commands . maybe there 's some chatbot library that saves me some boilerpate nlp code to directly jump on to writing grammar or selecting sentence patterns .",12303,,,2018-07-29T18:23:58.867,chatbot to write code,chat-bots,3,4,2
1241,5111,1,,2018-01-24T01:46:31.200,6,2288,"i 'm going through andrew ng 's course which talks about yolo but he does n't go into the implementation details of anchor boxes . look through the code , each anchor box is represented by two values , but what exactly are these values representing ? ? as for the need of anchor boxes , i 'm also a little confused about that -- as far as i understand , the ground truth labels have around 6 variables : 1 ) p_o which check if it 's an object or background , 2,3 ) bx , by ( which are the center coordinates ) 4,5 ) bh , bw which are the ( height and width of the box ) 6 ) c ( object class , which depends on how many class labels you have , so you can have multiple c ) as for creating the bounding box , bh is divided by 2 , with one half from the center points ( bx , by ) to the top , and the other half to the bottom . if we train our classifier , would n't the prediction boxes be close to the ground truth labels as training progresses ? so if our ground truth label has a high height , small width as boxes for some images , and low hight and large width for other images , would n't our classifier automatically learn to differentiate between when to use one over the other , as it is being trained ? if so then what is the use of anchor boxes ? and what are those numbers representing anchor boxes representing ? thank you .",3460,,,2019-05-05T13:02:10.800,confusion regarding anchor boxes in yolo,neural-networks convolutional-neural-networks computer-vision,1,0,2
1242,5113,1,,2018-01-24T04:30:21.830,3,194,"so i am currently trying to create a program that when provided a list of descriptive words or a passage of text , would create a piece of abstract art based on the feelings evoked by those words . i figured that ai would probably be the best way of achieving the results i am looking for . even though i do have quite a bit of experience with programming , i have just about zero knowledge on the subjects of machine learning , artificial intelligence , neural nets , etc . another issue is that i simply do n't have much time to write this program and i simply do n't have the luxury of thoroughly learning about this subject . i came across a really cool python thingy called stackgan that is specifically made for text to image generation . my plan was to take a bunch of pictures of abstract art and for each one write down a list of descriptive words or emotions that they evoke . i figure that there must be some way for me to then feed the pictures plus associated words / description into the neural net as training data but have no idea how to do that and the fact that there is very little documentation about the program does n't help . even after spending a few days trying to make sense of the code i am completely lost as to use stackgan . so .... my question is : how exactly do i set up the training data and train stackgan to do what i am trying to do ? the github 's readme.md mentions some sort of cnn - rnn text embedded in the images used for training , and i have no idea what this means , if its necessary , or how i would add such data because i ca n't find any sort of thorough instructions . also if you have any suggestions on alternative more user friendly libraries that would be greatly appreciated . stackgan link : https://github.com/hanzhanggit/stackgan-v2",12309,,,2018-01-24T04:30:21.830,how do i make and use a dataset for stackgan,machine-learning python,0,0,2
1243,5114,1,,2018-01-24T19:31:41.960,3,6870,"before i start i want to let you know that i am completely new to the field of deep learning ! since i need a new graphics card either way ( gaming you know ) i am thinking about buying the gtx 1060 with 6 gb or the 1070 ti with 8 gb . because i am not rich , basically i am a pretty poor student ;) , i do n't want to waste my money . i do n't need deep learning for my studies i just want to dive into this topic because of personal interest . what i want to say is that i can wait a little bit longer and do n't need the results as quick as possible . so here is my question : can i do deep learning with the 1060 ( 6 gb seem to be very limiting according to some websites ) or the 1070 ti ? is the 1070 ti overkill for a person hobby deep learner ? or should i wait for the new generation nvidia graphics card ? thank you very much in advance !",12324,1671,2018-01-25T18:41:49.510,2018-01-26T21:27:56.327,1070 ti or 1060 6 gb for deep learning,deep-learning hardware-evaluation,3,1,2
1244,5115,1,5178,2018-01-24T21:38:41.783,3,85,"imagine two languages that have only these words : man = 1 , deer = 2 , eat = 3 , grass = 4 and you would form all sentences possible from these words : man eats deer . deer eats grass . man eats . deer eats . german : mensch = 5 , gras = 6 , isst = 7 , hirsch = 8 possible german sentences : mensch isst hirsch . hirsch isst gras . mensch isst . hirsch isst . how would you write a program that would figure out which words have same meaning in english and german ? it is possible . all words get their meaning from the information in which sentences they can be used . connection with other words define their meaning . we need to write a program that would recognize that a word is connected to other words in the same way in both language . then it would know those two words must have the same meaning . if we take word "" deer "" ( 2 ) it has this structure in english 1 - 3 - 2 2 - 3 - 4 in german ( 8) : 5 - 6 - 8 8 - 6 - 7 we get the same structure ( pattern ) in both languages : both 8 and 2 lie in first and last position , and middle word is the same in both languages , the other word is different in both languages . so we can conclude that 8=2 because both elements are connected with other elements the same way . maybe we just need to write a very good program for recognizing analogies and we will be on the right track to creating ai ?",12251,4302,2018-10-08T12:17:50.863,2018-10-08T12:17:50.863,figure out the meaning of words,natural-language-processing statistical-ai,2,4,
1245,5116,1,5124,2018-01-24T23:21:22.067,3,275,"as many papers point out , for better learning curve of a nn , it is better for a data - set to be normalized in a way such that values match a gaussian curve . does this process of feature normalization apply only if we use sigmoid function as squashing function ? if not what deviation is best for the tanh squashing function ?",12327,9947,2018-01-25T19:40:24.993,2018-01-25T19:40:24.993,data - set values feature scaling : sigmoid vs tanh,neural-networks datasets,2,0,1
1246,5121,1,5122,2018-01-25T12:20:43.140,0,97,"i got a ' homework ' to solve from college but i have no idea how to do it ... can anyone get it ? a neural network with the following structure is given : one input neuron , four elements in the hidden layer , one output neuron . the output neuron is bipolar , the neurons in the hidden layer are linear . the weights between the input neuron and the neurons in the hidden layer have the following values : w11 = -3 , w12 = 2 , w13 = -1 , w14 = 0.5 , while between neurons in the hidden layer and the starting neuron : : w21 = +2 , w22 = -0.5 , w23 = -3 , w24 = +1 ( no threshold input in both layers ) . what will the network response be like if the number 3 is given to the input neuron ?",12341,1671,2018-01-26T21:25:22.237,2018-01-26T21:25:22.237,neutral network - how to solve this ?,neural-networks ai-basics,1,0,1
1247,5123,1,,2018-01-25T13:54:23.187,1,38,"i am writing my thesis in the field of ( deep ) metric learning ( dml ) . i am training a network in the fashion of contrastive / triplet siamese networks to learn similarity and dissimilarity of inputs . in this context , the ground truth is commonly expressed as a binary . let 's take an example based on the similarity of species : image a : german shepard ( dog ) image b : siberian husky ( dog ) image c : turkish angora ( cat ) image d : gray wolf ( wolf ) image a and b are similar : same species , same sub - species ( canis lupus ) - > 1.0 = = true image a and c are dissimilar : different species ( canis lupus vs. felis silvestris ) - > 0.0 = = false image a and d ? same species , but different sub - species - > 0.8 which metric learning approaches use a continuous ground truth for learning ? i could imagine that there is a lot of research out there using a continuous ground truth in classification settings . for instance to learn that the expression of a face is "" almost ( 60 % ) happy "" , or more controversial , an image of a person depicts a "" 70 % attractive person "" . also in this fields i would be happy for hints / links . remarks : i do n't ask for opinions on whether this makes sense or not . could someone please attach the tag metric learning ? i do n't have enough reputation .",12345,,,2018-01-25T13:54:23.187,continuous ground truth in supervised ( metric ) learning ?,deep-learning computer-vision,0,0,
1248,5125,1,,2018-01-25T16:52:10.797,1,59,will it be possible to model the problem of odd - even distinction of an integer ( not binary string representation ) using neural networks ?,11835,,,2018-01-25T16:52:10.797,modelling odd - even distinction of an integer with neural networks,neural-networks deep-learning,0,5,1
1249,5126,1,,2018-01-25T18:31:10.407,4,97,"i am training an ann for classification between 3 classes . the ann has an input layer , one hidden layer and a 3 node output layer . the problem i am facing is that the output being produced by the 3 output nodes are so close to 1 ( for the first few iterations at least , and so i am assuming the problem propagates to future outputs as well ) the weights are not being updated ( or hardly updated ) due to overflow ( about $ 10^{-11}$ ) . i can fix the overflow problem ( but i do n't think it is the culprit ) . i think such low values of error is the main culprit , and i can not figure what is causing such low values of error . what will cause the network to behave more responsively , that is , how will i be actually able to grasp the weight updates and not something in the order of $ 10^{-11}$ ? the data set contain values in the order of $ 10 $ 's , and the weights randomly initialized are in the order of $ 0 & lt ; w & lt ; 1 $ . i have tried feature normalization but it is not that effective .",9947,2444,2019-04-12T21:59:36.427,2019-04-12T21:59:36.427,what are some concrete steps to deal with the vanishing gradient problem ?,neural-networks machine-learning backpropagation datasets gradient-descent,1,0,
1250,5129,1,,2018-01-25T22:33:45.687,2,317,"i 'm having trouble with accuracy evaluation at the end of session . training process looks like this : with tf.session ( ) as sess : sess.run(tf.global_variables_initializer ( ) ) epochs = 10 batch_size = 128 for e in range(epochs ) : shuffle_indices = np.random.permutation(np.arange(len(inputx ) ) ) x_train = inputx[shuffle_indices ] y_train = inputy[shuffle_indices ] epoch_loss = 0 for i in range(int(len(inputx ) // batch_size ) ) : start = i * batch_size batch_x = x_train[start : start + batch_size ] batch_y = y_train[start : start + batch_size ] _ , c = sess.run([optimizer , cost ] , feed_dict={x : batch_x , y : batch_y } ) epoch_loss + = c print(e + 1 , ' / ' , epochs , ' loss : ' , epoch_loss ) at the end of this session i want to evaluate reached accuracy by calculating mean value of percentual differences between prediction and testy for each of 3 output layer neurons . percentual_differences_for_neuron1 = # insert your advice here accuracy_for_neuron1 = tf.reduce_mean(percentual_differences_for_neuron1 ) print('accuracy of neuron 1 is ' , ' ' ' another advice ' ' ' ) what 's the best way of doing this in tensorflow ?",12327,12327,2018-01-25T23:04:02.267,2018-06-12T10:24:45.057,tensorflow training evaluation,neural-networks tensorflow,1,0,
1251,5134,1,,2018-01-26T12:45:35.187,2,134,"my goal is to build a neural net that can find patterns between a hash and a word on it 's own . so that it returns the word of any hash that i will input . unfortunatally my skill in the area of neural net isn´t advanced , and i want to use this project to learn more . so i use a german dictionary and encode it via one_hot encoding . then i generate the sha256 value of every word inside ( before i have done this i cleaned the file and wrote every word in another line ) it . so i got an big array with the shape of 20000x20000 for the words and another for the hashs . so then i used the a example of the keras homepage for binary classification because the one_hot values are represented by ones and zeros . so if i want to predict a hashs i get these error : error when checking : expected dense_1_input to have shape ( 20000 , ) but got array with shape ( 1 , ) . so i do n't know if this model is working for my problem but i could n't convert one hash into a size of 20000x20000 . ( the hash will one_hot encoded for that prediction ) . so how could i get it to accept different shaped hashs / one hash only ? is there a way to train the model with each hash after another for example with a for loop ? ! edit : so i figured out that i can convert a list of characters into a numpy.array with 2 dimensions . so i hot_encoded every character and create a list of them , these list i passed inside the np.array(words,ndim=2 ) . so this i have done for my hashs aswell . then after i run the code i got this error : valueerror : setting an array element with a sequence so i tried to reshape the array with the .reshape(20000 ) command but nothing chaged . so what to do with that ? edit2 : i figured out now that the problem is that enhot_encoding generates diffrent sized "" arrays "" for each word , and if i fill this into a real array and this into a neuronal net it have to return this error . but still the question is : how to convert single words and hashs to a format that i can train a neuronal net with and get usefull output so i can enter any hash and it should return some kind of word(lable ) . if you need the actual code please inform me and i will upload it`s current state . code : model = sequential ( ) model.add(dense(64 , input_shape=20000 , activation='relu ' ) ) model.add(dropout(0.5 ) ) model.add(dense(64 , activation='relu ' ) ) model.add(dense(units=64 , activation=""relu "" ) ) model.add(dropout(0.5 ) ) model.add(dense(19957 , activation='sigmoid ' ) ) model.compile(loss='binary_crossentropy',optimizer='rmsprop ' , metrics=['accuracy ' ] ) print(""fitting data ... "" ) model.fit(test_hashs,test_words , epochs=10,batch_size=128 , verbose=1 ) train_y = input(""input a hash that is not contained in the training data : "") # train_x = pd.series(hashlib.sha256(str.encode(train_y)).hexdigest ( ) ) train_y = pd.series(train_y ) # test_x = pd.get_dummies(train_x ) test_y = pd.get_dummies(train_y ) model.save(""first_test "" ) print(model.evaluate(test_y ) ) # score = model.evaluate(test_x , test_y , batch_size=128 , ) print(""score : "" + score ) prediction = model.predict(test_x , verbose=1 ) for i in prediction : print(i )",12367,,2018-04-17T20:44:51.500,2018-04-17T20:44:51.500,keras pattern finding between hash and word,unsupervised-learning keras python,1,0,
1252,5137,1,,2018-01-26T14:47:12.340,1,59,"i am new in machine learning . i have taken a course in vision and we are required to do a project . i am thinking of data mining medical lab report images . my code must take an image and jpg file and then extract important information from it like lab where test has been done , patient name , test type and more important various data like heamoglobin , rbc , etc in case of blood test report . i can build an ocr , but , problem which i am stuck at is in case of data which generally forms a table like structure . so , i want to find that tabular structure on which i can just apply matrix extraction to find various datas . i 'm looking for assistance with two basic things : is my approach of finding tables and then extracting data is correct ? if yes , then can you point out some good papers or implementation to find tabular structure . ( p.s.- do n't mention tabular ) any approach which is state - of - the - art or good ? ( paper or implementation )",12370,1671,2018-01-26T20:36:18.793,2018-01-26T20:36:18.793,data extraction from medical reports,ai-design computer-vision ai-basics,0,0,1
1253,5138,1,5154,2018-01-26T20:20:16.840,2,167,"i 'm relatively new to ai , and i 've tried to create one that "" speaks "" . here 's how it works : 1 . get training data e.g ' jim ran to the shop to buy candy ' 2 . the data gets split into overlapping ' chains ' of three e.g [ ' jim ran to ' , ' ran to the ' , ' to the shop ' , ' the shop to ' ... ] 3 . user enters two words 4 . looks through the chains to find if the two words have been seen before . 5 . if they have , finds out which word followed it and how many times . 6 . work out the probability e.g : if ' or ' followed the two words 3 times , ' because ' followed the two words 1 time and ' but ' followed it 1 time it would be 0.6 , 0.2 and 0.2 7 . generate a random decimal 8 . if the random decimal is in the range of the first word ( 0 - 0.6 ) pick that one or if it 's in the range of the second word ( 0.6 - 0.8 ) pick that word or if it 's in the range of the third ( 0.8 - 1 ) pick that word 9 . output the word picked 10 . repeat from 4 but with the new last two words e.g if the last words had been ' to be ' and it picked ' or ' the new last two words would be ' be or ' . it does work , but it does n't stick to a particular topic . for example , after training with 800 random wikipedia articles : in the early 1990s the frequency had a plastic pickguard and separate hardtail bridge with the council hoped that the bullet one replaced with the goal of educating the next orders could revert to the north island or string of islands in a new urban zone close to the west . as you can see the topic changes many times mid - sentence . i thought of increasing the number of words it considered from two to three or four , but i thought it might start simply quoting the articles . if i 'm wrong please tell me . any help is greatly appreciated . if i have n't explained clearly enough or you have any questions please ask .",12376,4302,2018-10-08T12:17:25.203,2018-10-08T12:17:25.203,how can i improve this word - prediction ai ?,natural-language-processing probabilistic,1,2,
1254,5153,1,,2018-01-26T23:32:35.670,2,76,"i have seen people using stacked softmax layers right at the output of neural networks designed for classification . i 'm trying to understand this . does it give any additional value ? i think this could "" sharpen "" decisions on the boundaries . model.add(dense(10 , activation='sigmoid ' ) ) model.add(dense(1 , activation='sigmoid ' ) ) seen here .",7364,1671,2018-01-27T22:09:54.847,2018-01-27T22:09:54.847,stacked softmax layers before output,neural-networks machine-learning,0,1,
1255,5155,1,5164,2018-01-27T07:43:03.613,2,64,"is there any previous work on computing some sort of prominence score based on the prevalence of features in an image ? for example , let 's say i am classifying images based on whether or not they have dogs in them . is there a way to compute how prominent that feature is ?",9608,,,2018-04-28T00:04:21.207,"computing a "" prominence score "" ( computer vision )",machine-learning computer-vision statistical-ai,1,0,
1256,5156,1,,2018-01-27T10:50:42.060,3,82,"at some point in time during the evolution , because of some factors , some beings first started to become conscious of themselves and their surroundings . that conscious experience is beyond some mere sensory reflexive actions trained . can that be possible with ai ?",3015,1671,2018-01-27T22:09:01.793,2018-01-27T22:09:01.793,can the first emergence of consciousness in evolution be replicated in ai ?,philosophy evolutionary-algorithms artificial-consciousness,2,1,1
1257,5167,1,,2018-01-28T02:59:24.203,1,587,i have started to make a chatbot . it has a list of greetings that it understands and responds to with its own list of greetings . how would a bot / script learn a new greeting or a synonym to a word it already knows ?,4173,4302,2018-10-08T12:17:04.910,2018-10-08T12:17:04.910,how would a(n ) ( chatbot ) ai learn synonyms ?,natural-language-processing chat-bots unassisted-learning,3,0,1
1258,5169,1,,2018-01-28T07:56:26.750,2,185,a human player plays limited games compared to a system that undergoes millions of iterations . is it really fair to compare alphago with the world # 1 player when we know experience increases with the increase in number of games played ?,12394,1671,2018-04-14T02:44:25.220,2018-04-14T08:23:57.840,is it fair to compare alphago with a human player ?,philosophy game-ai alphago,5,1,
1259,5174,1,,2018-01-28T09:53:44.280,4,1342,"i read about minimax , then alpha - beta pruning and then about iterative deepening . iterative deepening coupled with alpha - beta pruning proves to quite efficient as compared alpha - beta alone . i have implemented a game agent that uses iterative deepening with alpha - beta pruning . now i want to beat myself . what can i do to go deeper ? like alpha - beta pruning cut the moves , what other small change could be implemented that can beat my older ai ? my aim to go deeper than my current ai . if you want to know about the game , here is a brief summary : there are two players , four game pieces and a 7-by-7 grid of squares . at the beginning of the game , the first player places both the pieces on any two different squares . from that point on , the players alternate turns moving both the pieces like a queen in chess ( any number of open squares vertically , horizontally , or diagonally ) . when the piece is moved , the square that was previously occupied is blocked . that square can not be used for the remainder of the game . the piece can not move through blocked squares . the first player who is unable to move any one of the queens loses . so my aim is to cut the unwanted nodes and search deeper .",12384,1671,2018-01-30T23:48:58.257,2018-04-08T19:57:53.713,what else can boost iterative deepening with alpha beta pruning ?,game-ai combinatorial-games combinatorics,3,2,1
1260,5176,1,,2018-01-28T11:08:39.637,0,75,"so , i have seen few pictures re - created by a neural network or some other machine learning algorithm after it has been trained over a data set . how , exactly is this done ? how are the weights converted back into a picture or a memory which a neural net is holding ? a real life example would be when we close our eyes we can easily visualize things we have seen . based on that we can classify things we see . now in a neural net classification part is easily done , but what about the visualization part ? what does the neural net see when it closes its eyes ? and how to represent it for human understanding ? for example a deep net generated this picture : source : deep nets generating stuff there can be many other things generated . but the question is how exactly is this done ?",9947,9947,2018-01-29T11:44:19.117,2018-06-29T02:17:58.913,how to know what kind of memory is stored in the connection weights ?,neural-networks machine-learning image-recognition,2,0,
1261,5177,1,,2018-01-28T11:15:09.723,1,53,can anyone explain what information the formula gives us . what does the notations mean ? where i can find more material about what does the log probability function do ?,10046,9947,2018-01-28T22:23:38.650,2018-01-28T22:23:38.650,"please explain this "" log probability function "" ? what does each part mean ?",math probabilistic,1,0,
1262,5185,1,5188,2018-01-29T04:48:11.113,0,376,"what is the application of generative adversarial networks having been successfully trained ? splitted into two part as g and d , the g is for creation , and the d for a decider ? then there is a assumption that is the input of gan have to be continued ?",9237,,,2018-12-20T17:13:47.197,what is the purpose of the gan,deep-learning,1,0,
1263,5186,1,,2018-01-29T06:50:50.310,8,367,i recently read that google has developed a new ai that anyone can upload data to and it will instantly generate models i.e. an image recognition model based off that data . can someone explain to me in a detailed and intuitive manner how this ai works ?,10913,10913,2018-02-17T09:23:58.050,2018-02-27T01:55:08.437,what is an intuitive explanation of how google 's automl works ?,machine-learning ai-design research,1,0,2
1264,5193,1,,2018-01-30T14:53:22.540,4,255,"i would like to do some practical implementation of artificial intelligence planning ( of course something a bit simple and easy ) . is there any website where i can pick an algorithm , say a * or hill climbing or calculate heuristic values , code it and visualize how it works ? example : for machine learning , the above i.e. pick a learning method , say linear regression , code it and visualize how it works in https://www.kaggle.com/ note : if you find the tags to be inappropriate for this question , then i am sorry . i could n't find ( or i do n't know ) the appropriate tag any input will be appreciated . thank you :)",12438,1671,2018-01-31T16:49:48.463,2019-01-27T23:48:20.697,is there any website where i can visualize how ai planning works ?,machine-learning ai-design getting-started,6,1,1
1265,5194,1,,2018-01-30T23:02:22.490,1,239,"let 's say we have a cluster of 20 - 2000 heterogenous compute nodes . consider for example the parallel solution of the helmholtz equation : now we want to distribute the solution process and , to make things easier , we split the problem in a fine - grained way ( partial solution of the system matrix ) . we could train an ai with the time taken to solve the subproblem depending on multiple factors ( for example , size of the mesh , needed precision , etc ) and let the ai choose the optimal distribution and division of the problem based on the available data . i 'm new to the area of artificial intelligence . are there any open source frameworks which could accomplish this task ? how would you estimate the required amount of compute power to train the network ?",12448,,,2018-01-30T23:02:22.490,application of ai to task scheduling problems on heterogenous platforms,machine-learning prediction optimization,0,0,
1266,5201,1,,2018-02-01T05:56:15.930,0,216,"number of layer of dnn and computational complexity of it are correlated after optimization , but how to estimate it before designing dnn ?",9237,,,2018-03-03T13:31:53.173,the connection between number of layer of dnn and computational complexity of it,deep-learning deep-network reference-request,1,0,
1267,5203,1,,2018-02-01T10:17:51.863,3,128,gold showed that a language can be learned only if it contains a finite set of sentences . we know that deep neural networks can implement any function . does this contradict the gold 's result ? what is the relation or difference between the definition of learnability of vapnik and gold and the definition of learnability of neural networks ?,9237,2444,2019-05-03T15:46:12.270,2019-05-03T15:46:12.270,what is the relation between the definition of learnability of vapnik and gold and learnability of neural networks ?,neural-networks machine-learning deep-learning learning-theory relation,0,1,
1268,5208,1,5209,2018-02-01T20:07:14.110,0,228,"i 'm trying to understand how to effectively plan and write a neural network but running into problems with understanding how they should be written . i 'm working with classification with writing a backpropagation neural network however any other types / examples would be greatly appreciated . one of my problems is understanding the possibilities of them , for example if you have a single neuron i assume you can have 2 ^ 1 possibilities , like in my example i ca n't imagine how i could predict something more than above or below a straight line . similarly i assume with 3 neurons therefore 2 ^ 3 possibilities you could predict if something is one of the 4 quadrants of a graph and then predict if it 's in the upper part or lower part of the quadrant so 8 classifications . is this true or is a network capable of predicting more / less than that ? on to my specific problem . i 've written a single neuron / perceptron that can predict whether something is above or below a straight line graph given the correct training data and using a sign activation function . following from this i 'm trying to write a neural network that can predict whether something is in the 1st , 2nd , 3rd or 4th quadrant of a graph . one idea i have had is to have 2 input perceptrons , the first taking the x value , the 2nd taking the y value , these then try and predict individually whether the answer is on the right or left of the centre , and then above or below respectively . these then pass their outputs to the 3rd and final output neuron . the 3rd neuron uses the inputs to try and predict which quadrant the coordinates are in . the first two inputs use the sign function . the problems i 'm having with this is to do with the activation function of the final neuron , one idea was to have a function that somehow scaled the output into a integer between 0 and 1 , so 0 to 0.25 would be quadrant 1 , and so on up to 1 . another idea would be to convert it to a value using sin and representing it as a sine wave as this could potentially represent all 4 quadrants . another idea would be to have a single neuron taking the input of the x and y value and predicting whether something was above or below a graph ( like my perceptron example ) , then having two output neurons , which the 1st output neuron would be fired if it was above the line and then passed in the original x coordinate to that output neuron . the 2nd output neuron would be fired if it was below then pass in the original x value as well to determine if it was left or right . are these ideas adequate examples of writing a network ? also my final question would be if you wanted your network to have 8 possible outputs would you need 8 output neurons or could you represent the 8 values with a single more advanced activation function ? thanks !",11795,,,2018-02-02T06:27:53.587,planning a neural network,neural-networks machine-learning ai-design artificial-neuron,1,0,2
1269,5210,1,,2018-02-02T11:24:24.060,0,82,"i had this idea of training for example a cnn on images , and having output branches at several of its intermediate layers . the early layers ' output branch might then predict high - level class of detected objects ( supposedly able to do this because less info is needed for a high - level classification than a very specialised one ) , and the later layers giving more detailed labels of the sub - class of the earlier high level class . i have been searching for research on this type of setup but could n't really find anything . is there a name for this idea , or is this an open question / idea ?",2522,,,2018-02-03T15:46:34.080,is there any research on neural networks with multiple outputs for hierarchical label classification ?,neural-networks convolutional-neural-networks classification backpropagation,1,0,
1270,5211,1,,2018-02-02T15:40:15.847,4,86,"if i train a speech recognition model using data collected from n different microphones , but deploy it on an unseen ( test ) microphone - does it impact the accuracy of the model ? while i understand that theoretically an accuracy loss is likely , does anyone have any practical experience with this problem ?",12502,,,2018-02-10T18:48:34.000,can variations in microphones used in training set and test set impact the accuracy of speech recognition models ?,deep-learning voice-recognition,2,2,
1271,5220,1,5222,2018-02-02T22:41:35.850,1,329,"the match got a lot of press , and i doubt anyone is surprised that alpha zero crushed stockfish . see : alphazero destroys stockfish in 100 game match to me , what 's really salient is that "" much like humans , alphazero searches fewer positions that its predecessors . the paper claims that it looks at "" only "" 80,000 positions per second , compared to stockfish 's 70 million per second . "" for those who remember matthew lai 's giraffechess : however , it is interesting to note that the way computers play chess is very different from how humans play . while both humans and computers search ahead to predict how the game will go on , humans are much more selective in which branches of the game tree to explore . computers , on the other hand , rely on brute force to explore as many continuations as possible , even ones that will be immediately thrown out by any skilled human . in a sense , the way humans play chess is much more computationally efficient - using garry kasparov vs deep blue as an example , kasparov could not have been searching more than 3 - 5 positions per second , while deep blue , a supercomputer with 480 custom ” chess processors ” , searched about 200 million positions per second 1 to play at approximately equal strength ( deep blue won the 6-game match with 2 wins , 3 draws , and 1 loss ) . how can a human searching 3 - 5 positions per second be as strong as a computer searching 200 million positions per second ? and is it possible to build even stronger chess computers than what we have today , by making them more computationally efficient ? those are the questions this project investigates . [ lai was tapped by deepmind as a researcher last year ] but what i 'm interested in at the moment is the decision speed in these matches : - what was the average time to make a move in the alphazero vs. stockfish match ?",1671,,,2018-02-03T02:34:14.947,what was the average decision speed pf alpha zero in the recent stockfish match ?,game-ai chess alphazero,1,1,
1272,5221,1,,2018-02-03T01:06:48.493,1,134,"i have tried several environment libraries like openai gym / gridworld but now i am trying to create a toy environment for experimentation . the environment i 've created is as follows : state : grid with n rows by m columns , represented by a boolean matrix . each grid cell can be empty or filled and the grid starts empty . action : one of the m columns to be filled , which must have at least the top row empty . next state : once a column is chosen , the lowest unfilled cell in that column is filled . this works from bottom up like a very simple version of tetris . reward : after every action , a reward equal to the number of empty columns is awarded . therefore in a sample world of 5 rows by 3 column , starting off with an empty grid , the maximum attainable reward would be by filling column wise first . this policy will give a maximum total reward of 2 * 5 + 1 * 5 = 15 . ( 2 free columns by 5 row action , once first column is filled then 1 free column by 5 row action . ) this very simple environment is trained using dqn with a single ff layer . the agent only took a few episodes to converge and is able to produce the maximum attainable reward . in a next toy environment , i 've made it a little more complex . i modified the very first action to be random choice of any column . i have retrained the rl model with the new environment modification . however , after convergence , the agent does not attain max score of 15 for all possible starting columns . i.e. if column 1 was randomly chosen first , max score might be 15 , however column 2 or 3 was randomly chosen first , max score might only reach 11 or 9 . in theory , the optimum policy would be for the agent to fill column that was randomly chosen first - i.e. repeat the first randomly chosen action . i have tried several ways to tweak my input parameters ( e.g. episilon_decay_rate , learning_rate , batch_size , number of hidden nodes ) to see if the agent could act optimally for all possible starting columns . i also tried ddqn and sarsa . the only way i could make the agent perform optimally is by reducing gamma ( discount factor ) to 0.5 or below . are there any explanations to why the agent only works for small discount factors in this example ? also , are there alternative ways to obtain the optimum policy ?",12505,1847,2018-02-05T23:04:15.513,2018-02-06T08:36:59.880,agent in toy environment only learns to act optimally with small discount factors,reinforcement-learning,1,4,0
1273,5230,1,,2018-02-05T14:19:24.217,3,2577,which freely available code/ software is the fastest way to train and test data on ? i 'm looking for a gui software / code that let me test data - sets without investing too much time in coding . i found some projects online but i wanted to hear your suggestions in order to avoid investing time in testing many different ones . thanks,12518,9947,2018-02-05T23:04:21.883,2019-03-15T09:52:07.880,freely available graphical user interface for training a network on a dataset,machine-learning software-evaluation,3,0,2
1274,5232,1,,2018-02-05T17:58:46.973,-1,87,when is a measurable function a bayesian decision function ? how do i prove this ? can you give an example with standard or weighted binary classification ?,9560,2444,2019-05-01T17:20:07.680,2019-05-01T17:20:07.680,when is a measurable function a bayesian decision function ?,machine-learning math bayes proofs,2,0,1
1275,5234,1,,2018-02-05T23:00:14.127,8,760,"it was recently brought to my attention that chess experts took the outcome of this now famous match as something of an upset . see : chess ’s new best player is a fearless , swashbuckling algorithm as as a non - expert on chess and chess ai , my assumption was that , based on the performance of alphago , and the validation of that type of method in relation to combinatorial games , was that the older ai would have no chance . why was alphazero 's victory surprising ?",1671,,,2018-02-23T19:49:04.850,why were chess experts surprised by the alphazero 's victory against stockfish ?,chess alphazero,3,0,3
1276,5235,1,5254,2018-02-06T04:46:48.667,3,164,which problems are considered to be the toughest problems in artificial intelligence / machine learning ?,9483,1671,2018-02-07T22:19:29.337,2018-02-08T12:51:44.313,problems in artificial intelligence,machine-learning ai-design ai-basics,1,7,2
1277,5239,1,5241,2018-02-06T14:19:27.290,7,615,"so this is an introductory question . whenever i read any book about neural nets or machine learning , their introductory chapter says that we have n't been able to replicate the brain 's power due to its massive parallelism . now , in the modern times transistors have been reduced to the size of nano - meters , much smaller than the nerve cell . also we can easily build very large supercomputers . computers have much larger memories than brain . can communicate faster than brain ( clock pulse in nanoseconds ) . can be of arbitrarily large size . so my question is why can not we replicate the brain 's parallelism if not its information processing ability ( since brain is still not well understood ) even with such advanced technology ? what exactly is the obstacle we are facing ?",9947,,,2018-04-04T16:50:55.393,what makes animal brain so special ?,neural-networks neurons brain,3,1,3
1278,5246,1,5247,2018-02-07T11:20:37.190,10,2429,"for instance , the title of this paper reads : "" sample efficient actor - critic with experience replay "" . what is sample efficiency , and how can importance sampling be used to achieve it ?",12574,2444,2019-02-25T15:05:42.603,2019-03-18T14:32:48.860,"what is sample efficiency , and how can importance sampling be used to achieve it ?",reinforcement-learning statistical-ai importance-sampling,2,0,3
1279,5252,1,5255,2018-02-07T23:01:24.403,2,254,a lot of people seem to be under the impression that combining gofai and contemporary ai will make models more general . i 'm particularly interested in reasoning through analogy or case - based reasoning .,9271,1671,2018-02-14T21:27:39.623,2018-02-14T21:27:39.623,what are some interesting recent papers that synthesize symbolic ai with deep learning ?,deep-learning gofai,1,0,
1280,5256,1,,2018-02-08T14:39:26.087,3,157,"so , i am trying to create an ai to handle the construction layout of a real time strategy game like age of empires ii . the process has too many steps to be handled effectively by brute force , but also has enough structural requirements that it also can not just be done randomly . assuming a limited area to work with , which can be represented by tiles , the ai must be able to place several structures within the area . a layout once fully created can be given a score based on the pathable distance between certain structures as well as a few other factors . if paths between certain structures become completely blocked , the ai has failed . the starting area that is defined by tiles is also random in nature , containing a few predefined elements that the ai can not control , which include randomly placed terrain and resource nodes . i know this problem case has been quite vague , but the actual question relates to the type of ai that would best fit solving this issue . i currently have an algorithm that runs through what it believes are the best few results in a series of steps trying to create the most optimal layout , but it fails with many starting layouts , which require manual adjustments to the ai before it can process them properly . even then , it is just an approximation at best , since it starts out assuming that the steps i gave it to check contain the most optimal layout . here is an example of what a finished layout might look like . it does n't state which colors are which objects , but it might help in determining what type of ai i should be looking at using .",12607,12607,2018-02-08T15:10:17.527,2018-11-06T02:00:39.567,rts construction layout ai,algorithm game-ai prediction game-theory path-planning,1,0,
1281,5258,1,5868,2018-02-08T17:51:28.833,4,295,"so i read somewhere there are 2 different views of artificial intelligence . one is the normal conventional approach where we use ml and ai logical inference programs to mimic a human brain . another view is that we can not create an intelligent agent just by the above procedure . we humans or animals are only intelligent due to our social interactions . if we think about it , the second view has a lot of credibility since a brain in a jar is not at all intelligent and will not evolve things that make an animal possess intelligent traits like language , logical reasoning , etc . whereas an ant or bee colonies are apparently intelligent due to their collective information gathering even if their brain are basically hard - wired logical sensors which just perform an action on a given stimulus . so my questions are : are all this artificially intelligent agents / programs being created or will be created in near future , just a set of rules and probabilities ? what is the current progress and views on collective / swarm intelligence ? and what is the level of intelligence or brain capabilities required by an agent which if works collectively ( like bees / ants broadly super - organisms ) can exceed normal human intelligence ? edit : if we look at it nodes in a neural net are also kind of ants / bees . they just perform a hardwired calculation and output the result . can we think of an ann as swarm intelligence ? a collective answer explaining the reasoning for the views is highly appreciated . thanks in advance ! !",9947,9947,2018-02-10T09:43:41.970,2018-04-02T18:04:07.530,swarm intelligence vs normal human intelligence,philosophy swarm-intelligence brain super-organism,2,2,2
1282,5259,1,,2018-02-08T19:01:32.327,0,61,"once the artificially intelligent machines are able to identify objects , we might want to teach them how to value different things differently based on their utility , demand , life , etc . how can we accomplish this and how did we start to value things ?",3015,9947,2018-02-09T15:41:41.487,2018-02-09T15:41:41.487,is understanding value for different features next step for object recognition ?,machine-learning artificial-neuron object-recognition,2,0,
1283,5265,1,,2018-02-10T01:54:04.027,2,960,"i 'm trying to implement some image super - resolution models on medical images . after reading a set of papers , i found that none of the existing models use any activation layer for the last layer . what 's the rationale behind that ?",10569,,,2019-03-26T13:04:33.133,what activation function is not used at the final layer of super resolution neural models ?,deep-learning deep-network computer-vision,3,0,1
1284,5266,1,5273,2018-02-10T03:28:13.393,1,80,"i 'm wondering if these 2 specific programs already exist and if not how hard would it be to write them : a program that would figure out ( by only "" reading "" large amounts of texts in human language 1 and 2 ) which words in second language have the same meaning as a word in first language . you would give for input texts in both languages and for output you would get for every word in first language a list of words in second language that are most similar to it with a probability that they mean the same thing . a program that would figure out which words have the most similar meaning by analyzing large amounts of texts in one human language . i 'm planning on writing these two programs and it would be nice if i could get existing programs that do this so that i could compare results of my program to those of existing programs .",12251,4302,2018-10-08T12:16:29.977,2018-10-08T12:16:29.977,existing programs that find out words with same meanings,machine-learning natural-language-processing,2,5,
1285,5270,1,5282,2018-02-10T13:54:10.053,1,68,"introduction tracking control is a technique which is similar to pid control and has the aim to implement a robust feedback loop for generating action - signals . in contrast to a line following robot , tracking control is oriented in a time - action space . on the x - axis the timecode is presented , for example 0 seconds , 0.5 seconds and 1 seconds , while on the y - axis the value of the signal is plotted . the value can be the position of a robot , the angle of an inverted pendulum or the x - position of an object . even in simple problems like the inverted pendulum , the number of parameters on the y - axis is greater than 1 . the spline of all parameters of the time are equal to a movement pattern . the problem is how to summarize the different values to a single signal . this is needed for determine the similarity in a “ learning from demonstration ” experiment . the human - operator is doing an action , and the aim of the robot is to reproduce the movement pattern . i 've searched a bit the topic in google scholar and found a paper : a humanoid robot standing up through learning from demonstration using a multimodal reward function but i 'm a bit unsure , because there is so much math and it is also dedicated to zmp biped walking . what i 'm searching is more a general idea of how to compress different splines into one reward function . description of the bug tracking a single spline which is plotted in a diagram is easy . the difference between the current value and the desired value is measured and the feedback - controller reduces the difference . if the number of splines is 2 this concept fails . for example , spline # 1 represents the angle and spline # 2 the velocity over time . the current state has 2 variables and the desired state has 2 variables . how can i program a steering - controller for a multi - spline ?",11571,11571,2018-02-10T20:13:31.733,2018-02-12T05:09:52.063,tracking control with multimodal reward,reinforcement-learning,1,1,
1286,5274,1,,2018-02-11T05:27:00.877,21,4710,"is most development or theory geared towards the idea that consciousness is an emergent phenomenon ? that once we put enough complexity into our system , it will become self - aware ? or is this even a problem that people are attempting to tackle right now ?",12648,1671,2018-02-14T19:14:16.120,2019-03-07T22:38:16.073,what are the current theories on developing a conscious ai ?,philosophy theory artificial-consciousness,7,7,11
1287,5279,1,5283,2018-02-11T16:03:14.763,2,53,"i 'm developing a multi - armed bandit which learns the best information to display to persuade someone to donate to charity . suppose i have treatments a , b , c , d ( which are each one paragraph of text ) . the bandit selects one treatment to show to a person . the person is given $ 1 and has to decide how much ( if any ) to donate , in increments of one cent . the donation decision is recorded and fed to the multi - armed bandit , who will then re - optimize before another person is shown a treatment selected by the bandit . how should i program the bandit if my objective is to maximize total donations ? for example , can i use thompson sampling , and if a participant donates $ 0.80 , i count that as 80 successes and 20 failures ?",12656,,,2018-02-12T11:28:25.123,programming a bandit to optimize donations,ai-design training,1,2,
1288,5285,1,5293,2018-02-12T13:05:53.910,1,64,"i 'm reading "" recurrent neural network based language model "" of mikolov et al . ( 2010 ) . although the article is straight forward , i 'm not sure how word embedding $ w(t)$ is obtained : the reason i wonder is that in the classic "" a neural probabilistic language model "" bengio et al . ( 2003 ) - they used separate embedding vector for representing each word and it was somehow "" semi - layer "" , meaning - it have n't contains non - linearity , but they did update word embeddings during the back - propagation . in mikolov approach though , i assume they used simple one - hot vector , where each feature represent presence of each word . if we represent that 's way single word input ( like was in the mikolov 's paper ) - that vector become all - zeros except single one . is that correct ?",12691,2444,2019-04-16T22:35:10.617,2019-04-16T22:35:10.617,"how is the word embedding represented in the paper "" recurrent neural network based language model "" ?",natural-language-processing recurrent-neural-networks word-embedding papers,1,0,
1289,5288,1,,2018-02-12T15:41:14.503,2,26,"the way i understand it is that hidden units are added to capture higher order interactions offering more capacity to the model . now , the bm family are energy based undirected networks , meaning there 's no forward computations ; instead , for each input configuration x , a scalar energy is calculated to asses this configuration . ( the higher the energy , the less likely for x to be sampled from the target distribution . ) the probability distribution is defined through the energy function by summing over all possible states of the hidden part h . so , my question is : how do we calculate the values of these hidden units ? or do we not explicitly compute these valus and instead approximate the marginal "" free energy "" which is the negative log of the sum over all possible states of h ?",12672,12672,2018-02-12T23:15:55.083,2018-02-12T23:15:55.083,how do we calculate the hidden units values in a ( restricted ) boltzmann machine,boltzmann-machine,0,0,
1290,5289,1,5481,2018-02-12T18:52:04.187,2,37,"in order for the generalized bell membership function to retain its defined shape and domain , two restrictions must be placed on the b parameter : 1 ) b must be positive and 2 ) b must be an integer . using backpropagation to tune the membership parameters ( a , b and c in the bell ) , it appears to be possible that the correction to b will break one or both of these restrictions on b. can someone please explain to me how we can use backpropagation to tune the b parameter ( as well as a and c ) without violating 1 ) and 2 ) ?",12544,,,2018-04-30T19:13:07.277,"tuning the b parameter , anfis",neural-networks machine-learning fuzzy-logic,1,0,1
1291,5290,1,,2018-02-13T05:23:58.633,3,134,"i have purchasing history data for grocery shopping . i am trying to get abnormally frequently purchased items under certain conditions . for instance , i am trying to find frequently purchased items , when customers shop online and are willing to pay an extra shipping fee . in order to find items that are particularly ( or abnormally ) frequently purchased under that situation ( through online stores by paying shipping fee ) , how and what machine learning algorithm should i apply and identify those items ? i found arules r package which is using the association rules with purchasing history and tried to apply it . but it seems the package might be based on different principle from my idea . anyone has an idea about my problem ? if there is an r package related to the problem , it would be perfect .",12713,9947,2018-02-14T18:39:01.800,2018-02-14T18:39:01.800,predict frequently purchased items under certain conditions with customer purchasing history data,machine-learning unsupervised-learning learning-algorithms r,2,4,
1292,5300,1,,2018-02-14T10:04:17.840,1,55,"in box2d is a platform given with a ball on top . the platform can be moved to left and right , and the object is moving according to standard gravity of 9.8f which is set in the box2d setting . to make the task a bit harder , there are two platforms at the same time . left in the image is the demonstration and right is the repetition . the repetition platform gets the same input signals but starts with a slightly different position of the object . the position of the ball can be retrieved in realtime from the program . the human operator is sending a “ left ” command , the platform is moving and the ball is rolling . how the controller looks like for the repetition platform if both balls should be in the same position ? my efforts to solve the problem i 've created the sourcecode in c++ which is 420 lines long i 've programmed a manual controller for testing if the physics engine works i read some papers about dynamic movement primitives , learning from demonstration and tracking control but it seems , that something is missing for solving the problem . any advice is welcome . sourcecode the controller is executed in the mainloop . it works currently as a normal pid - controller , which measures the difference between the x - positions of both balls and executes a left or right counteraction . the idea is , that the human - operator controls the demonstration and this modifies the pid - control loop in the agent . but in most cases , it works not very well . the controller is not very intelligent . mainloop : myagent.automode ( ) ; class agent { public : void automode ( ) { int posa = myphysics.mybox[6].positioncenter.x-200 ; int posb = myphysics.mybox[9].positioncenter.x-400 ; int diff = posb - posa ; std::cout & lt;&lt ; mysettings.framestep & lt;&lt ; "" "" & lt;&lt;diff & lt;&lt ; "" \n "" ; if ( diff&gt;0 ) { std::thread t1 ; t1 = std::thread(&amp;agent::autoleftrun , this ) ; t1.detach ( ) ; } if ( diff&lt;-0 ) { std::thread t1 ; t1 = std::thread(&amp;agent::autorightrun , this ) ; t1.detach ( ) ; } } ;",11571,11571,2018-02-14T12:32:23.923,2018-02-14T12:32:23.923,balance problem with a ball,control-problem,0,7,
1293,5308,1,,2018-02-14T19:43:20.667,6,110,"greedy algorithms are well known , and although useful in a local context for certain problems , and even potentially find general , global optimal solutions , they nonetheless trade optimality for shorter - term payoffs . this seems to me a good analogue for human greed , although there is also the grey goo type of greed that is senseless acquisition of material ( think plutocrats who talk about wealth as merely a way of "" keeping score "" . ) technical debt is an extension of development practices that fall under the algorithmic definition of greed ( short - term payoff leads to trouble down the road . ) this may be further extended to any non - optimized code in terms of energy waste ( flipping of unnecessary bits ) which will only increase as everything becomes more computerized . so my question is : what are other vices that can arise in algorithms ?",1671,,,2019-01-02T17:46:21.257,algorithms can be greedy . what are some other algorithmic vices ?,philosophy ethics theory,3,0,1
1294,5314,1,,2018-02-14T21:42:22.173,2,2446,"i 'm interested in working on challenging ai problems , and after reading this article ( https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/ ) by deepmind and blizzard , i think that developing a robust ai capable of learning to play starcraft 2 with superhuman level of performance ( without prior knowledge or human hard - coded heuristics ) would imply a huge breakthrough in ai research . sure i know this is an extremely challenging problem , and by no means i pretend to be the one solving it , but i think it 's a challenge worth taking on nonetheless because the complexity of the decision making required is much closer to the real world and so this forces you to come up with much more robust , generalizable ai algorithms that could potentially be applied to other domains . for instance , an ai that plays starcraft 2 would have to be able to watch the screen , identify objects , positions , identify units moving and their trajectories , update its current knowledge of the world , make predictions , make decisions , have short term and long term goals , listen to sounds ( because the game includes sounds ) , understand natural language ( to read and understand text descriptions appearing in the screen as well ) , it should probably be endowed also with some sort of attention mechanism to be able to pay attention to certain regions of interest of the screen , etc . so it becomes obvious that at least one would need to know about computer vision , object recognition , knowledge bases , short term / long term planning , audio recognition , natural language processing , visual attention models , etc . and obviously it would not be enough to just study each area independently , it would also be necessary to come up with ways to integrate everything into a single system . so , does anybody know good resources with content relevant to this problem ? i would appreciate any suggestions of papers , books , blogs , whatever useful resource out there ( ideally state - of - the - art ) which would be helpful for somebody interested in this problem . thanks in advance .",12746,12746,2018-02-15T13:05:46.507,2018-02-20T07:37:32.120,training an ai to play starcraft 2 with superhuman level of performance ?,machine-learning ai-design computer-vision knowledge-representation reasoning,2,3,2
1295,5317,1,,2018-02-15T03:54:17.247,1,161,i often develop bots and i need to understand what some people are saying . examples : - i want an apple - i want an a p p l e how do i find the object ( apple ) ? i honestly do n't know where to start looking . is there an api that i can send the text to which returns the object ? or perhaps i should manually code something that analyses the grammar ?,12752,2193,2018-12-06T21:18:23.517,2018-12-06T21:18:23.517,how to find the subject in a text ?,algorithm natural-language-processing ai-basics getting-started,2,5,1
1296,5318,1,,2018-02-15T05:55:24.767,4,538,"i have been trying to use cnn for a regression problem . i followed the standard recommendation of disabling dropout and overfitting a small training set prior to trying for generalization . with a 10 layer deep architecture , i could overfit a training set of about 3000 examples . however , on adding 50 % dropout after the fully - connected layer just before the output layer , i find that my model can no longer overfit the training set . validation loss also stopped decreasing after a few epochs . this is a substantially small training set , so overfitting should not have been a problem , even with dropout . so , does this indicate that my network is not complex enough to generalize in the presence of dropout ? adding additional convolutional layers did n't help either . what are the things to try in this situation ? i will be thankful if someone can give me a clue or suggestion . ps : for reference , i am using the learned weights of the first 16 layers of alexnet and have added 3 convolutional layers with relu non - linearity followed by a max pooling layer and 2 fully connected layers . i update weights of all layers during training using sgd with momentum .",12754,,,2019-04-12T19:01:14.567,what to do if cnn can not overfit a training set on adding dropout ?,convolutional-neural-networks linear-regression dropout,2,2,
1297,5320,1,5323,2018-02-15T11:38:38.140,2,162,"this is a kind of biological and philosophical question . so , the recent concern in ai is that an ai agent may go rogue with prominent people voicing their concerns . now say , we have created an ai ( you are free to use your own definition of what makes an ai intelligent ) which has gone rogue with powers given in this question . now , the broad view of today 's biology is that everything we do is to further our genes down the future ( leaving aside small technical details ) . it is even widely accepted that we are just machines whose controller are the genes . everything we do is controlled / hardwired by the genes with some avenue of learning from experiences . also genes only further their own interest . scientist george price even wrote a mathematical equation proving all our acts are selfish and only furthering the interest of our genes ( article ) . also richard dawkins is a pioneer of this idea ( this is only to show i have n't pulled the idea out from air ) . now , my question is that what will possibly be the motivation of an ai agent to go rogue ? it does n't have genes whose interest it needs to further . we all do something for an end result . what is the end result a rogue ai might try to achieve / attain and why ?",9947,,,2018-02-26T15:29:38.990,motivation that drives a rogue ai agent,philosophy biology genes,2,1,1
1298,5322,1,,2018-02-15T14:16:52.027,5,866,"i was reading gary marcus 's a critical appraisal of deep learning . and one of his criticisms is that neural networks do n't incorporate prior knowledge in tackling a problem . my question is , has there been any attempts at encoding prior knowledge in deep neural networks ?",10913,10913,2018-02-20T06:31:53.027,2018-07-08T07:06:44.577,can prior knowledge be encoded in deep neural networks ?,neural-networks ai-design knowledge-representation world-knowledge,4,5,
1299,5325,1,,2018-02-15T16:31:48.240,2,129,"i have completed week 1 of andrew ng 's course . i understand that the cost function for linear regression is defined as $ j ( \theta_0 , \theta_1 ) = 1/2m*\sum ( h(x)-y)^2 $ and the $ h$ is defined as $ h(x ) = \theta_0 + \theta_1(x)$ . but i do n't understand what and represent in the equation . is someone able to explain this ?",12762,2444,2019-03-02T11:31:51.940,2019-03-02T14:31:39.037,understanding a few terms in andrew ng 's definition of the cost function for linear regression,linear-regression math,4,1,
1300,5329,1,,2018-02-15T18:29:45.160,4,131,"i 'm looking for an industry standard framework for joining multiple neural networks in a modular way . assume we have two or more neural networks trained to perform certain tasks . by feeding the outputs of some networks into the inputs of others we might obtain higher functionality , but to test multiple hypotheses , we would need a way to rapid prototype those configurations . the source of this image is here & mdash ; a proposition of this modular ann architecture in a thesis i 'd be interested in knowing : do any frameworks or libraries like this even exist ? if so , do they support distributed models , so that models do n't have to be hosted in the same process or on the same machine ? do they allow hosting models to be generated from different deep learning frameworks ?",12765,4302,2018-07-29T04:27:58.717,2018-11-29T13:02:50.520,framework for joining multiple modular artificial neural networks,neural-networks software-evaluation topology software-architecture,1,2,
1301,5332,1,,2018-02-16T05:12:24.747,5,2053,"i 'm trying to implement q - learning ( state - based representation and no neural / deep stuff ) but i 'm having a hard time getting it to learn anything . i believe my issue is with the exploration function and/or learning rate . thing is , i see different explanations in the sources i am following so i 'm not sure what 's the right way anymore . what i understand so far is that q - learning is td with q - val iteration . so a time - limited q - val iteration step is : q[k+1](s , a ) = ∑(s ' ) : t(s , a , s ' ) * [ r(s , a , s ' ) + γ * max(a'):q[k](s',a ' ) ] where : q = q - table : state , action -&gt ; real t = mdp transition model r = mdp reward func γ = discount factor . but since this is a model - free , sample - based setting , the above update step becomes : q(s , a ) = q(s , a ) + α * ( sample - q(s , a ) ) where : sample = r + γ * max(a'):q(s',a ' ) r = reward , also coming from percept after taking action a in step s. s ' = next state coming from percept after taking action a in step s. now for example , assume the following mdp : 0 1 2 3 4 0 [ 10 t ] [ s ] [ ? ] [ ? ] [ 1 t ] discount : 0.1 | stochasticity : 0 | t = terminal ( only exit action is possible ) s = start with all of the above , my algo ( in pseudo code ) is : input : mdp , episodes , percept q : s , a -&gt ; real is initialized to 0 for all a , s α = .3 for all episodes : s = mdp.start while s not none : a = argmax(a ) : q(s , a ) s ' , r = percept(s , a ) sample = r + γ * max(a'):q(s',a ' ) q(s , a ) = q(s , a ) + α * [ sample - q(s , a ) ] s = s ' as stated above , the algorithm will not learn . because it will get greedy fast . it will start at 0,1 and choose the best action so far . all q - vals are 0 so it will choose based on arbitrary order on how the qvals are stored in q. asume ' w ' ( go west ) is chosen . it will go to 0,0 with a reward of 0 and a q - val update of 0 ( since we do n't yet know that 0,0 , exit yields 10 ) in the next step it will take the only possible action exit from 0,0 and get 10 . at this point the q - table will be : 0,1,w : 0 0,0,exit : 3 ( reward of 10 averaged by learning rate of .3 ) and the episode is over because 0,0 was terminal . on the next episode , it will start at 0,1 again and take w again because of the arbitrary order . but now 0,1,w will be updated to 0.09 . then 0,0,exit will be taken again ( and 0,0,exit updated to 5.1 ) . then the second episode will be over . at this point the q - table is : 0,1,w : 0.09 0,0,exit : 5.1 and the sequence 0,1,w->0,0,exit will be taken ad infinitum . so this takes me to learning rates and the exploration functions . the book ' artificial intelligence : a modern approach ' ( 3ed , by russell ) first mentions ( pages 839 - 842 ) the exploration function as something to put in the val update ( because it is discussing a model - based , value iteration approach instead ) . so extrapolating from the val update discussion in the book , i 'd assume the q - val update becomes : q(s , a ) = ∑(s ' ) : t(s , a , s ' ) * [ r(s , a , s ' ) + γ * max(a'):e(s',a ' ) ] where e would be an exploration function which according to the book could be something like : e(s , a ) = & lt;bigvalue&gt ; if visitcount(s , a ) & lt ; & lt;minvisits&gt ; else q(s , a ) the idea being to artificially pump up the vals of actions which have not been tried yet and so now they 'll be tried out at least minvisits times . but then , in page 844 the book shows pseudo code for q - learning and instead does not use this e in the q - val update but rather in the argmax of the action selection . i guess makes sense ? since exploration amounts to choosing an action ... the other source i have is the uc berkeley cs188 lecture videos / notes . in those ( reinforcement learning 2 : 2016 ) they show the exploration function in the q - val update step . this is consistent with what i extrapolated from the book 's discussion on value iteration methods but not with what the book shows for q - learning ( remember the book uses the exploration function in the argmax instead ) . i tried placing exploration functions in the update step , the action selection step and in both at the same time .. and still the thing eventually gets greedy and stuck . so not sure where and how this should be implemented . the other issue is the learning rate . the explanation usually goes "" you need to decrease it over time . "" ok .. but is there some heuristic ? right now , based off the book i am doing : learn(s , a ) = 0.3 / visitcount(s , a ) . but no idea if it is too much or too little or just right . finally , assuming i had the exploration and learn right , how would i know how many episodes to train for ? i 'm thinking i 'd have to keep 2 versions of the q - table and check at which point the q - vals do not change much from previous iterations ( similar to value iteration for solving known mdps ) .",12773,1671,2018-02-16T17:46:09.450,2018-02-16T17:46:09.450,how to implement exploration function and learning rate in q learning,reinforcement-learning q-learning data-science,2,0,
1302,5333,1,,2018-02-16T05:39:17.747,5,116,"i 'm working on a project related to machine q&amp;a , using the squad dataset . i 've implemented a neural - net solution for finding answers in the provided context paragraph , but the system ( obviously ) struggles when given questions that are unanswerable from the context . it usually produces answers that are nonsensical and of the wrong entity type . is there any existing research in telling whether or not a question is answerable using the info in a context paragraph ? or whether a generated answer is valid ? i considered textual entailment but it does n't seem to be exactly what i 'm looking for ( though maybe i 'm wrong about that ? )",12775,12775,2018-02-16T13:45:11.513,2018-08-16T16:27:09.707,methods to tell if a question can be answered from a paragraph,neural-networks machine-learning classification natural-language-processing,1,1,1
1303,5336,1,5487,2018-02-16T10:17:09.810,3,337,"i am fairly a newbie to neural networks . i wanted to ask if it is possible to train a nn to identify only one type object ? for instance , a table from a large set of images , where the nn should be able to identify if new images are tables . if yes can you please guide me in the direction to get started ? edit : i wanted to ask if it is possible to train a nn to identify only one type object ? ... so my understanding as of now is that if i train an nn on one class and one class alone with a fairly large amount of data then i can get it trained in a small amount of time rather than training it on a huge amount of data and consuming a larger amount of data ... objective is to train a large no of such smaller nns to create an ensemble of nns .",2904,2904,2018-02-16T20:42:40.030,2018-03-02T08:29:16.080,training a yes / no nn for image classes,neural-networks deep-learning classification computer-vision ai-basics,1,1,1
1304,5338,1,,2018-02-16T12:21:49.247,2,131,any small application based on real world application of ai which can be done easily at home for a beginner who is trying to make use of his basic programming skills into ai at the beginning level .,12780,1671,2018-02-16T17:32:16.760,2018-02-16T20:36:01.557,can anyone suggest a small application based on an artificial intelligence which can be done by a beginner in ai ?,game-ai ai-basics applications getting-started python,2,3,2
1305,5339,1,,2018-02-16T13:13:37.437,1,42,"so i really do n't get this question because i always thought the agent program is the same as agent 's function , but i read somewhere that this is statement might be true so is this statement actually true ? if so then why ? if i have an agent function why i can not implement it in an program ? ! this is the definition of function : the function is implemented as the agent program .",12782,,,2018-02-16T13:13:37.437,prove that there might be an agent function which can not be implemented by any agent program,machine-learning intelligent-agent,0,2,
1306,5343,1,5344,2018-02-16T19:36:22.933,3,181,i was reading a paper recently about improving the learning of an ann using weight normalization : weight normalization : a simple reparameterization to accelerate training of deep neural networks i wanted to check my understanding / interpretation of it with the experts here . normally the output of a neuron is equal to the sum of every incoming neuron multiplied by their respective weights and then pushed through an activation function : output = activation ( sum ( x multiplied by w ) ) in the example in this paper it seems that instead of using each individual weight they are actually calculating a normalized weight to substitute by taking the square root of all summed weights ( the euclidean norm ? ) - we 'll call that z - and then plugging it into the calculation as : normalized_weight = ( scalar / z ) multiplied by w. they never saw what they used for scalar btw ... can anyone confirm if i am correct in my understanding and if not could they correct me . the maths goes a little over my head so any pseudocode is welcome .,12788,1671,2018-02-16T20:05:28.010,2018-02-16T20:15:06.580,weight normalization,neural-networks ai-basics,1,2,1
1307,5347,1,,2018-02-16T22:44:52.420,1,73,i have been reading a lot lately about some very promising work coming out of uber 's ai labs using mutation algorithms enhanced with novelty search to evolve deep neural nets . https://www.arxiv-vanity.com/papers/1712.06563/ i 'd like to ask the community if anyone is able to understand how this is done or has any ideas on how to do this ? i am unclear on if the novelty should reward novel structures within the ann or novel behavior . i am guessing the latter but would love to hear from others on this topic . i checked the other items in ai.stackexchange for novelty searches but they did n't answer this question so i do not believe this is already answered .,12788,1671,2018-02-18T21:45:10.973,2018-02-18T21:45:10.973,novelty search mutation algorithm,deep-learning genetic-algorithms evolutionary-algorithms,0,4,1
1308,5348,1,,2018-02-17T07:59:05.443,1,1476,"what is the difference between ai architecture and ai models ? are both of them the same ? if not , please distinguish both of them and give example of each .",12803,2444,2019-05-25T21:46:09.573,2019-05-25T21:46:09.573,what is the difference between ai architecture and ai model ?,ai-basics models difference architecture,6,0,1
1309,5349,1,5351,2018-02-17T08:29:32.363,2,41,"i 'm trying to identify numbers and letters in license plate . license plate images are taken at different lighting condtion and converted to gray image . my concern with type of data for training is : gray image : since they are taken at different lighthing condition , gray image have different pixel intensity for same number . which means , i have to get many training data for different lighting condition to train . edge image : they lack enough pixel information since only edge is white while others(background ) are black . so i think they will be very weak for translational difference like shearing or shifting . i want to get some information about which type of image is better for training number in different lighting condition . i wish to use edge image if they do n't differ much since i can prepare edge image right now .",12090,,,2018-02-17T10:37:51.787,"in number classification using neural network , is training with edge image better than gray image ?",neural-networks machine-learning image-recognition,1,0,1
1310,5353,1,,2018-02-17T11:08:12.603,1,220,"the more i think about machine learning the more i realize the importance of finding similarities by using analogies as a way of learning . if i want to categorize words into hierarchical tree this method would work i think , if not tell me why ? find two sentences that contain same words but their order is different and some words can also be different . find another two such sentences . evaluate strength of analogy between these 4 sentences . the stronger it is the more probability that words mean similar thing , that they belong to same category . when you learn categories you can do analogies on abstract sentences that contain these found categories and in this manner you can build hierarchy of categories . for example : deer eats grass . ... is to ... man eats deer . as cow eats grass is to ? if we can find sentence man eats cow . being used in texts that we analyze then we have confirmed that cow and deer belong to category animals that eat grass and that human eats them .",12251,1671,2018-02-18T21:42:43.747,2018-02-18T21:42:43.747,analogies and similarity,machine-learning natural-language-processing theory computational-linguistics,0,4,2
1311,5355,1,,2018-02-17T16:39:27.940,2,94,"i 'm studying a master 's degree in artificial intelligence and i need to learn how to use the java neural network simulator , javanns , program . in one practice i have to build a neural network to use backpropagation on it . i have created a neural network with one input layer with 12 nodes , one hidden layer with 6 nodes and one output layer with 1 node . i 'm using kaggle 's titanic competition data with this format following dataquest course getting started with kaggle for titanic competition : pclass_1,pclass_2,pclass_3,sex_female , sex_male , age_categories_missing , age_categories_infant , age_categories_child , age_categories_teenager , age_categories_young adult , age_categories_adult , age_categories_senior , survived 0,0,1,0,1,0,0,0,0,1,0,0,0 1,0,0,1,0,0,0,0,0,0,1,0,1 0,0,1,1,0,0,0,0,0,1,0,0,1 1,0,0,1,0,0,0,0,0,1,0,0,1 0,0,1,0,1,0,0,0,0,1,0,0,0 0,0,1,0,1,1,0,0,0,0,0,0,0 1,0,0,0,1,0,0,0,0,0,1,0,0 0,0,1,0,1,0,1,0,0,0,0,0,0 if you want see the same data better in an spreadsheet : but they preprocess the data to use it with linear regression and i do n't know if i can use these data with backpropagation i think something is wrong because when i run backpropagation in javanns i get these data : opened at : sat feb 17 17:29:40 cet 2018 step 200 mse : 0.5381023044692738 validation : 0.11675894327003862 step 400 mse : 0.5372328944712378 validation : 0.11700781497209432 step 600 mse : 0.5370386219557437 validation : 0.11691717861750939 step 800 mse : 0.5370348711919518 validation : 0.11696104763606407 step 1000 mse : 0.5369724294992798 validation : 0.11697568840154722 step 1200 mse : 0.5369697016710676 validation : 0.11665485957481342 step 1400 mse : 0.5370053339270906 validation : 0.11684215268609244 step 1600 mse : 0.5370121961199371 validation : 0.11670833992558485 step 1800 mse : 0.5370200812483633 validation : 0.11673550099633925 step 2000 mse : 0.5367923502149529 validation : 0.11675956129361797 nothing changes , it is like it does n't learn anything . how many hidden layers does the network have with how many nodes on each hidden layer ? maybe the problem is that the data have been prepared to be used in linear regression and i using it with backpropagation . i have only created the neural network , i have n't implemented the backpropagation algorithm because it is already implemented in javanns .",4920,4920,2018-02-19T16:20:24.040,2018-02-19T16:20:24.040,data prepared to linear regression . can i use it with backpropagation ?,neural-networks backpropagation hidden-layers,0,10,
1312,5358,1,,2018-02-18T06:26:23.680,1,26,"i am new to neural networks . i am trying to model the run - off vs. time in a water channel after a storm event given that i know the permeability of the material in the channel , total precipitation , and some other single valued parameters for a particular event .. i have a database of run off histories , and the values of the associated parameters ( permeability , total precipitation , etc ) i want my model to give me a runoff vs. time history when i enter the associated parameters . i do not know how to train my model . do i just stack all the time histories in my database together and feed them together ? all examples in books use one time history to train the model . i m confused .",12814,,,2018-02-18T06:26:23.680,how to train a recurrent neural network with multiple series,neural-networks recurrent-neural-networks,0,1,
1313,5359,1,,2018-02-18T08:41:34.297,1,106,"from russell - norvig : a csp is strongly k - consistent if it is k - consistent and is also ( k − 1)-consistent , ( k − 2)-consistent , . . . all the way down to 1-consistent . how can a csp be k - consistent without being ( k - 1)-consistent ? i ca n't think of any counter example for this case . any help would be appreciated .",12608,,,2018-02-18T19:00:54.843,does k consistency always imply ( k - 1 ) consistency ?,linear-algebra,1,1,
1314,5362,1,,2018-02-18T16:47:56.780,1,124,"the problem : i want to classify a trajectory if it has some properties , for example i want to create a simple 0/1 classifier for circular trajectories . if a target is moving in a circular trajectory the network should produce 1 , if not it should produce 0 . my input and data set : what i have is data set with cartesian coordinates in 2d so x , y , vx , vy . i have a dataset of 10000 trajectory , 5000 circular , 5000 rectilinear . so i feed the network with a tensor [ 10000 , 4 , 1 ] the question : i 'm trying to use a network with three layers , input layer with 4 neurons , hidden layer with 2 ltsm and one fc layer with sigmoid activation function . is it possible to feed the network with a tensor [ 4x1 ] each time ? or do i need to provide the information in batches ? or what ? is the design of my basic network correct ?",12824,,,2018-02-18T16:47:56.780,trajectory classification using rnn,neural-networks tensorflow recurrent-neural-networks,0,0,1
1315,5364,1,,2018-02-18T17:08:36.020,-1,95,"what ai algorithms and processes would be involved to simulate a personal trainer to provide a virtual solution so people , athletes and schools can have custom workouts to lessen injuries , optimize athletic performance , lose weight , gain weight , and improve strength ?",12825,10913,2018-02-20T21:45:13.277,2018-02-21T10:09:05.190,ai algorithms and processes to simulate a personal trainer,machine-learning ai-design healthcare,1,1,1
1316,5369,1,5421,2018-02-19T01:45:21.330,0,1563,i am in the process of getting back into ai programming after some time out and have been building my neural net in c#.net . i managed to get all of the feed - forward stuff working very eloquently but i am not using sigmoid as the activation function ; instead i am using leaky relu as i heard it is best for deep learning . i began working through the back - propagation material today and ran into a road - block as i need to know the derivative of leaky relu in order to calculate the changes in weights . my calculus is lacking so i was hoping someone might be able to help . here is the code for the leaky relu function which i got from https://www.codeproject.com/articles/1220276/reinventing-neural-networks : private double relu(double x ) { if ( x & gt;= 0 ) return x ; else return x / 20 ; } thanks in advance . -mike,12788,12957,2018-02-27T18:55:00.770,2018-06-22T06:21:56.937,can anyone show me the derivative of leaky relu in c # ?,neural-networks backpropagation ai-basics,2,1,
1317,5370,1,5409,2018-02-19T12:07:13.790,1,124,"i 'm new to this , know only the theory part of the stuff . i want to train a neural network for detection of currently single class(will be extending to detect more classes ) i came across transferred learning , that explains as how to use a pre trained nn to train new data . i selected a framework pytorch which has a nice tutorial explaining transfer learning we have a pytorch implemented single shot detector as well . this is my current situation the data i want to train is different from the once trained already i.e 20 classes those have already been trained . i currently have a very limited labeleb data training set . the solution is to freeze the weights of the initial few layers , and then train the nn . i am confused as what exactly is meant by initial few layers ? this is a useful post i found online [ learning note ] single shot multibox detector with pytorch — part 1 explaining how the single shot detector works . can anyone help me how to perform these two tasks what are the initial few layers here in this case ? how exactly can i freeze them ? what are the changes i need to make while training the nn to classify one or more new classes ?",11038,,,2018-02-23T13:05:57.450,train new data set using pre trained single shot detector(vgg16 ) ( transferred learning ),deep-learning convolutional-neural-networks,1,1,
1318,5371,1,,2018-02-19T13:18:50.290,2,59,lately i 've been wondering . is there 's a way to locate redundant / unnecessary / misleading inputs by analysis of weights in the first layer ?,12327,12327,2018-02-19T17:53:27.933,2018-02-23T09:30:16.750,identify unnecessary inputs of nn,neural-networks,1,1,
1319,5372,1,,2018-02-19T16:54:33.217,1,38,in the facenet paper there mentions an gradient algorithm called ' adagrad'(adaptive gradient ) referenced to this paper which has apparently been used to calculate the gradient of the triplet loss function . after referring to the paper also i find it hard to understand how to calculate this adaptive gradient . any ideas regarding this matter ? would love to hear any explanations or ideas towards understanding this concept . thank you .,12843,,,2018-02-19T16:54:33.217,how to calculate adaptive gradient ?,machine-learning deep-learning optimization gradient-descent facial-recognition,0,0,1
1320,5376,1,,2018-02-19T22:19:28.513,1,34,i have a problem in which the dimensions of the input are increasing in row and column at each timestep . what method for preprocessing could be done or are there any architectures used for solving such a case ?,12849,,,2018-02-19T22:19:28.513,dealing with input to recurrent net with changing dimensions,neural-networks convolutional-neural-networks reinforcement-learning,0,0,
1321,5377,1,5379,2018-02-20T00:11:50.153,4,1208,"i have a map . i need to colour it with $ k$ colours , such that two adjacent regions do not share a colour . how can i formulate the map colouring problem as a hill climbing search problem ?",12519,2444,2019-03-02T11:02:24.677,2019-03-02T11:02:24.677,how can i formulate the map colouring problem as a hill climbing search problem ?,search hill-climbing,2,0,
1322,5385,1,,2018-02-21T06:19:23.060,1,93,"two stanford university researchers , dr . michal kosinki and yilun wang have published a paper that claims that ai can predict sexuality from a single facial photo with startling accuracy . this research is obviously disconcerting since it exposes an already vulnerable group to a new form of systematized abuse . the research can be found here https://osf.io/zn79k/ , here https://psyarxiv.com/hv28a/ and has even been highlighted by newsweek magazine here http://www.newsweek.com/ai-can-tell-if-youre-gay-artificial-intelligence-predicts-sexuality-one-photo-661643 above is an image of composite heterosexual faces and composite gay faces from the research . ( image courtesy of dr michal kosinki and yilun wang ) my question is , as knowledgable members of the ai community , how can we scientifically debunk / discredit this research ?",10913,,,2018-02-21T17:18:04.107,is the research by stanford university students who use logistic regression to predict sexual orientation from facial images really scientific ?,research ethics social,1,2,
1323,5389,1,,2018-02-21T17:15:53.053,2,60,"i have a customer purchasing dataset and the data set is from a retailer having an online store and offline stores . so , customers have two options in their shopping channel , online or offline . in an online shopping , there is a shipping fee however if a basket size is larger than $ 50 there is no shipping fee . i found pieces of evidence that customers are trying to add some of items to make their basket size larger than $ 50 when their baskets are near and a little bit below the $ 50 , because their shipping fee can be waived by doing that . in this situation , i am trying to identify and characterize items that were purchased only because of the shipping threshold by using a machine learning algorithm . if there is no shipping threshold , $ 50 , the customers would not purchase the items , but they purchased some items to make their basket size larger than $ 50 . i have not observed those kinds of items ( added items because of the shipping threshold ) . is there any machine learning algorithm that i can identify those kinds of items ? i think i need to use some of unsupervised machine learning algorithm . another challenging part is that each customer has different characteristics so i probably need to consider it as well . how can i detect those kinds of items ? ?",12713,1671,2018-02-21T17:43:46.017,2018-06-22T14:48:19.570,detect observations under certain conditions,machine-learning unsupervised-learning detecting-patterns,1,0,1
1324,5392,1,5406,2018-02-22T08:08:59.070,3,66,i am absolutely new in ai area . i would like to know how to mathematically / logically represent the sense of sentences like : the cat drinks milk . sun is yellow . i was at work yesterday . so that it could be converted to computer understandable form and analysed algorithmically . any clue ?,12907,9947,2018-02-22T22:06:50.683,2018-02-23T15:37:27.007,represent sense / meaning of sentences mathematically,natural-language-processing sentience,2,1,1
1325,5393,1,,2018-02-22T10:40:58.157,1,40,"i am trying to perform classification task using keras and tensorflow . however , the learning converges after achieving an accuracy of 57 % . all my inputs and outputs are categorical data . am i using the correct approach to train ? are there any other example in the web which is trying to solve the similar problem ? data.csv religion , caste , qualification , marital_status , sex , nature_activity 2,20,5,1,1,10 2,20,5,1,1,10 2,20,5,1,1,10 2,20,5,1,1,10 1,3,5,1,1,13 1,4,4,1,2,3 1,3,4,2,1,1 1,3,3,2,1,1 source code from keras.models import sequential from keras.layers import dense import pandas from sklearn.metrics import classification_report from sklearn.preprocessing import labelencoder from keras.utils import np_utils import numpy from sklearn.model_selection import train_test_split from keras.wrappers.scikit_learn import kerasclassifier import sys import matplotlib.pyplot as plt # fix random seed for reproducibility numpy.random.seed(7 ) # load pima indians dataset dataframe = pandas.read_csv(""data.csv"",header=none ) dataset = dataframe.values # split into input ( x ) and output ( y ) variables x = dataset[:,0:4].astype(float ) y = dataset[:,4 ] x_train , x_test , y_train , y_test = train_test_split(x , y , test_size=0.0015 , random_state=42 ) encoder = labelencoder ( ) encoder.fit(y_train ) encoded_y = encoder.transform(y_train ) one_hot_enc_y = np_utils.to_categorical(encoded_y ) model = sequential ( ) model.add(dense(500,input_dim=4,activation='relu ' ) ) model.add(dense(100,activation='relu ' ) ) model.add(dense(50,activation='relu ' ) ) model.add(dense(3,activation='softmax ' ) ) # compile model model.compile(loss='categorical_crossentropy ' , optimizer='sgd ' , metrics=['accuracy ' ] ) # fit the model history = model.fit(x_train , one_hot_enc_y , validation_split=0.33,epochs=300 , batch_size=5 ) # summarize history for accuracy plt.plot(history.history['acc ' ] ) plt.title('model accuracy ' ) plt.ylabel('accuracy ' ) plt.xlabel('epoch ' ) # summarize history for loss plt.plot(history.history['loss ' ] ) plt.title('model loss ' ) plt.ylabel('loss ' ) plt.xlabel('epoch ' ) plt.legend(['train ' ] , loc='upper right ' ) plt.show ( ) # evaluate the model scores = model.evaluate(x_train , one_hot_enc_y ) print(""\n%s : % .2f%% "" % ( model.metrics_names[1 ] , scores[1]*100 ) ) sys.exit ( )",12910,,,2019-02-22T09:01:53.270,improving learning rate for classification task,neural-networks classification tensorflow keras,1,3,
1326,5398,1,6495,2018-02-22T16:39:30.320,1,164,"in some atari games in the arcade learning environment ( ale ) , it is necessary to press fire once to start a game . because it may be difficult for a reinforcement learning ( rl ) agent to learn this , they may often waste a lot of time executing actions that do nothing . therefore , i get the impression that some people hardcode their agent to press that fire button once when necessary . for example , in openai 's baselines repository , this is implemented using the fireresetenv wrapper . further down , in their wrap_deepmind ( which applies that wrapper among others ) , it is implied that deepmind tends to use this functionality in all of their publications . i have not been able to find a reference for this claim though . my question is : is it common in published research ( by deepmind or others ) to use the functionality described above ? i 'd say that , if this is the case , it should be explicitly mentioned in these papers ( because it 's important to know if hardcoded domain knowledge was added to a learning agent ) , but i have been unable to explicitly find this after looking through a wide variety of papers . so , based on this , i 'd be inclined to believe the answer is "" no "" . the main thing that confuses me then is the implication ( without reference ) in the openai baselines repository that the answer would be "" yes "" .",1641,,,2018-05-23T08:46:19.700,is it common in rl research with atari / ale to automatically press fire to start games ?,reinforcement-learning research deepmind,1,0,
1327,5399,1,5401,2018-02-22T16:56:56.350,5,784,"it is suggested that the number of hidden units in a layer should be in powers of 2 because it helps converge faster . is it a fact and if it is , how this helps the nn learn faster . does it have to do something with how the memory is laid down ?",12901,12901,2018-02-22T17:53:48.460,2019-01-29T10:37:16.573,why number of hidden units in a layer are suggested to be in powers of 2 ?,deep-learning optimization hidden-layers,1,3,1
1328,5400,1,5402,2018-02-22T18:01:00.163,1,41,when we augment data for training are we also changing the distribution of data and if its a different distribution why do we use it to train a model for original distribution ?,12901,,,2018-02-22T18:09:58.070,does augmenting data changes the distribution of augmented data ?,machine-learning deep-learning,1,0,
1329,5408,1,,2018-02-23T10:25:50.713,2,56,"how is it that word embedding layer ( say word2vec ) brings more insights to the network compared to a simple one hot encoded layer ? i understand how word embedding carry some semantic meaning , but it seems that this information would get "" squashed "" by the activation function , leaving only a scalar value and as many different vector could yield the same result , i would guess that the information is more or less lost . could anyone bring me insights as to why a network may utilize the information contained in a word embedding ?",12931,2444,2019-04-16T22:32:07.857,2019-04-16T22:32:07.857,intuition on how word embeddings bring information to a network,neural-networks machine-learning natural-language-processing word-embedding,2,0,
1330,5410,1,,2018-02-23T15:10:26.070,3,634,"for people who are not in academia , could you please provide some insight into the current stage of developments in agi area ? are there any projects that had breakthroughs recently ? maybe some news source to follow on this topic ?",12935,1671,2018-02-23T21:47:02.720,2018-05-06T23:30:52.507,what is current state of agi development ?,agi ultraintelligent-machine superintelligence,2,6,2
1331,5414,1,,2018-02-23T19:05:54.593,4,138,"artificial networks model systems with a set of inputs and outputs and expected behavior . to train a network for modeling such systems , hundreds , thousands , or millions of example inputs - output pairs may be required . this is called a labelled data set , and the network and its optimization algorithm are meant to find a set of network parameters that best match the i / o of the artificial network with the i / o of the system . are there any systems , for which sufficient labelled data sets exist , that have yet to be successfully modeled with artificial networks of any type ( recurrent , deep , convolution , etc ) ?",12941,4302,2018-09-26T21:24:25.683,2018-09-29T08:45:54.133,what kinds of systems have so far failed to be modeled via supervised artificial network training ?,neural-networks deep-learning recurrent-neural-networks models reliability,3,6,
1332,5415,1,5492,2018-02-23T20:55:36.540,-1,126,"i have a game application with characters that have to cross mazes . the game can generate thousands of different mazes and the characters can move according to users choice and cross the maze manually . we needed to add the possibility to show a correct way out of each maze . therefore we added the possiblity to move the characters according to an xml file . this xml file is very complex , usually around thirty - fifty thousands of rows . lets say its in the following structure ( but much more complex ) : & lt;maze - solution&gt ; & lt;part id=""1""&gt ; & lt;sector number=""1""&gt ; & lt;action&gt ; & lt;equipment&gt;heavy&lt;/equipemnt&gt ; & lt;movement&gt ; & lt;start - position&gt;1250&gt;&lt;/start - position&gt ; & lt;angle&gt;23.43&lt;/angle&gt ; & lt;duration&gt;0.44&lt;/duration&gt ; & lt;/movement&gt ; & lt;action - type&gt;run&lt;/action - type&gt ; & lt;character&gt;1&lt;/character&gt ; & lt;protection&gt;none&lt;/protection&gt ; & lt;/action&gt ; & lt;action&gt ; & lt;equipment&gt;light&lt;/equipemnt&gt ; & lt;movement&gt ; & lt;start - position&gt;4223&gt;&lt;/start - position&gt ; & lt;angle&gt;233.43&lt;/angle&gt ; & lt;duration&gt;0.32&lt;/duration&gt ; & lt;/movement&gt ; & lt;action - type&gt;walk&lt;/action - type&gt ; & lt;character&gt;1&lt;/character&gt ; & lt;protection&gt;none&lt;/protection&gt ; & lt;/action&gt ; & lt;action&gt ; & lt;equipment&gt;heavy&lt;/equipemnt&gt ; & lt;movement&gt ; & lt;start - position&gt;1231&gt;&lt;/start - position&gt ; & lt;angle&gt;84.134&lt;/angle&gt ; & lt;duration&gt;0.454&lt;/duration&gt ; & lt;/movement&gt ; & lt;action - type&gt;run&lt;/action - type&gt ; & lt;character&gt;2&lt;/character&gt ; & lt;protection&gt;none&lt;/protection&gt ; & lt;/action&gt ; & lt;action&gt ; & lt;equipment&gt;heavy&lt;/equipemnt&gt ; & lt;movement&gt ; & lt;start - position&gt;932&gt;&lt;/start - position&gt ; & lt;angle&gt;34.43&lt;/angle&gt ; & lt;duration&gt;0.50&lt;/duration&gt ; & lt;/movement&gt ; & lt;action - type&gt;duck&lt;/action - type&gt ; & lt;character&gt;1&lt;/character&gt ; & lt;protection&gt;none&lt;/protection&gt ; & lt;/action&gt ; & lt;/sector&gt ; & lt;sector number=""2""&gt ; & lt;action&gt ; & lt;equipment&gt;heavy&lt;/equipemnt&gt ; & lt;movement&gt ; & lt;start - position&gt;1250&gt;&lt;/start - position&gt ; & lt;angle&gt;23.43&lt;/angle&gt ; & lt;duration&gt;0.44&lt;/duration&gt ; & lt;/movement&gt ; & lt;action - type&gt;run&lt;/action - type&gt ; & lt;character&gt;1&lt;/character&gt ; & lt;protection&gt;none&lt;/protection&gt ; & lt;/action&gt ; & lt;action&gt ; & lt;equipment&gt;light&lt;/equipemnt&gt ; & lt;movement&gt ; & lt;start - position&gt;4223&gt;&lt;/start - position&gt ; & lt;angle&gt;233.43&lt;/angle&gt ; & lt;duration&gt;0.44&lt;/duration&gt ; & lt;/movement&gt ; & lt;action - type&gt;walk&lt;/action - type&gt ; & lt;character&gt;1&lt;/character&gt ; & lt;protection&gt;none&lt;/protection&gt ; & lt;/action&gt ; & lt;action&gt ; & lt;equipment&gt;heavy&lt;/equipemnt&gt ; & lt;movement&gt ; & lt;start - position&gt;1231&gt;&lt;/start - position&gt ; & lt;angle&gt;84.134&lt;/angle&gt ; & lt;duration&gt;0.454&lt;/duration&gt ; & lt;/movement&gt ; & lt;action - type&gt;run&lt;/action - type&gt ; & lt;character&gt;2&lt;/character&gt ; & lt;protection&gt;none&lt;/protection&gt ; & lt;/action&gt ; & lt;action&gt ; & lt;equipment&gt;heavy&lt;/equipemnt&gt ; & lt;movement&gt ; & lt;start - position&gt;932&gt;&lt;/start - position&gt ; & lt;angle&gt;23.43&lt;/angle&gt ; & lt;duration&gt;0.44&lt;/duration&gt ; & lt;/movement&gt ; & lt;action - type&gt;duck&lt;/action - type&gt ; & lt;character&gt;1&lt;/character&gt ; & lt;protection&gt;none&lt;/protection&gt ; & lt;/action&gt ; & lt;/sector&gt ; & lt;sector number=""3""&gt ; & lt;/maze - solution&gt ; at the moment , we have the ability to analayze each maze using a cnn algorithm for image classification and generate an xml that represents a way out of the maze - meaning that if the characters will be moved according to that file , they will cross the maze . that algorithm has been tested and can not be changed by any means . the problem is that most of the times the generated file is not the best one possible ( and quite often it is very noticeable ) . there are different , faster , better ways to cross the maze . we also have thousands ( and we can get as many as needed ) files that were created manually for saved mazes and therefore they are representing an elegant and a fast way out of the maze . the ideal goal is that someday , our program will learn how to generate such a file without people creating them manually . to conclude , we have plenty of xml files generated by a program compared to the hard - coded xml files . there are thousands of pairs - the file the program generated , and the "" ideal "" file version that a person created . ( and we can get infinite number of such pairs ) is there a way , using those thousands of pairs , to make a second step algorithm that will "" learn "" what adjustments should be made in the generated xml files to make them more like the hard - coded ones ? i 'm not looking for a specific solution here but for a general idea that will get me going . i hope i made myself clear but if i missed some info let me know and i will add it .",9890,12630,2018-03-03T10:50:36.027,2018-03-03T19:48:22.920,a smart way to adjust xml files according to handwritten ones,machine-learning convolutional-neural-networks algorithm,1,3,
1333,5419,1,,2018-02-24T18:12:56.687,0,99,"i 've been trying to puzzle our resnet and i think i have an intuitive explanation for it , but i want to make sure it does n't stray too far from the actual theory to be misleading . again , since i started out on this from a point of relative ignorance , i could be mistaking / missing some fundamental concept which would prevent my metaphorical explanation from accurately modeling reality . here again is the link to the medium article i 've written . https://medium.com/@commongibbon/explaining-resnet-with-crayons-74740bc8a519 i have n't yet published it , so i 'd welcome any feedback . thank you ! edit : after reading this through a few more times and comparing against the literature , i 'm pretty confident i have it right . i 'm still more than open to any feedback you might have !",12953,12939,2018-02-26T15:51:54.187,2018-08-28T02:00:54.280,does my explanation of resnet make rational sense ?,neural-networks deep-learning convolutional-neural-networks,1,2,
1334,5420,1,,2018-02-24T21:15:45.637,1,121,"i trained my model resnet-50 for ucmerced_landuse dataset and this my loss graph but i have a problem when i try my first picture river give me the right class river then give me the first class river predicted for all the next picture i test for example : input picture is = > pic a result class a input picture is = > pic b result class a input picture is = > pic c result class a .... this is my code repository my test code that give me always the first class import tensorflow as tf import numpy as np from os import path as path from os import makedirs import cv2 classes = [ ' agricultural ' , ' airplane ' , ' baseballdiamond ' , ' beach ' , ' buildings ' , ' chaparral ' , ' denseresidential ' , ' forest ' , ' freeway ' , ' golfcourse ' , ' harbor ' , ' intersection ' , ' mediumresidential ' , ' mobilehomepark ' , ' overpass ' , ' parkinglot ' , ' river ' , ' runway ' , ' sparseresidential ' , ' storagetanks ' , ' tenniscourt ' ] def testing(sess , path , classes = classes , save_dir = "" save/"",save_file = "" data "" ) : data = cv2.imread(path ) labels = np.zeros((1,len(classes ) ) ) # get image height , width , channels height , width , channels = data.shape data = np.array([data ] ) print(data.shape ) print(""input image size : "" , height , width , channels ) if not path.isdir(save_dir ) : if makedirs(save_dir ) : print(save_dir,""is created "" ) with sess : if path.isdir(save_dir ) and path.isfile(save_dir+save_file+"".meta "" ) and path.isfile(save_dir+""checkpoint "" ) : print(""files are exist "" ) saver = tf.train.import_meta_graph(save_dir+save_file+"".meta "" ) saver.restore(sess , tf.train.latest_checkpoint(""save/ "" ) ) print(""data are restored "" ) graph = tf.get_default_graph ( ) x = graph.get_tensor_by_name(""t_picture:0"")#vrai y = graph.get_tensor_by_name(""t_labels:0"")#vrai # train = tf.get_collection('train_op')#vrai # loss = tf.get_collection('loss_op')#vrai logists = tf.get_collection('logits_op')#vrai # errors = tf.get_collection('errors')#vrai else : exit(""data are not exist "" ) print(""start "" ) curr_logists = sess.run([logists ] , { x : data , y : labels } ) curr_logists = np.array(curr_logists)[0,0,0 ] softmax = sess.run(tf.nn.softmax(curr_logists ) ) print(""logists : "" , curr_logists ) print(""softmax : "" , softmax ) print(""class : "" , classes[np.argmax(softmax ) ] ) # print(""loss:\n%s "" % ( curr_loss ) ) print(""test is finished "" ) sess.close ( ) if _ _ name _ _ = = ' _ _ main _ _ ' : # path=""ucmerced_landuse / images/ "" path=""runway.tif "" save_dir = "" save/ "" save_file = "" datasaved "" testing(tf.session(),path , classes , save_dir , save_file )",12387,,,2018-02-24T21:15:45.637,my network does always predict always the first right class,training tensorflow python,0,4,
1335,5422,1,5424,2018-02-24T21:55:33.637,0,67,"if possible consider the relationship between implementation difficulty and accuracy in voice examples or simply chat conversations . and currently , what are the directions on algorithms like deep learning or others to solve this .",12958,,,2018-02-24T22:58:21.840,what is easier or more efficient to summarize voice or text ? [ dp / rn ],neural-networks deep-learning natural-language-processing voice-recognition text-summarization,2,0,1
1336,5426,1,,2018-02-25T10:35:35.410,1,32,"i am trying to produce decision tree from feed forward neural network . the input to the feed forward neural network is condition action statement for example , if airthrusthold > 90 , power up the engine else rotate shaft 5 degree wide above statement is the input to the ffnn . how do i feed the statement ? either converting into word2vec ? ( or ) there is any other format to do ? and i need to produce decision tree from the outcome of neural network can we do this using reinforcement learning using markov decision process ? thanks !",12963,,,2018-02-25T10:35:35.410,condition action statement - feed forward neural network,neural-networks decision-theory,0,2,1
1337,5427,1,5440,2018-02-25T10:45:11.593,1,28,i 've been working with vanilla feed forward neural networks and have been researching the convolutional neural network literature . thus far i 've have not encountered how often the model is executed in order to classify objects . for example if a camera is capturing video at a rate of 15 frames per second is the classification model being trained / executed iteratively in order to maintain non time delayed classifications ?,12964,,,2018-02-26T11:29:24.153,executing trained image classification model for video,deep-learning image-recognition computer-vision,1,0,
1338,5428,1,,2018-02-25T11:22:56.363,4,511,"is there a way for people outside of core research community of agi to contribute to the cause ? there are a lot of people interested in supporting the field , but there is no clear way to do that . is there something like boinc for agi researches , or open projects where random experts can provide some input ? maybe kickstarter for crazy ai projects ?",12935,,,2019-04-01T18:05:53.573,how can people contribute to agi research ?,research agi superintelligence,2,7,2
1339,5432,1,5434,2018-02-25T16:28:49.717,3,199,"if the game had a variable speed and was essential in evolution / gaining score(idk ai terminologies ) . would the ai be able to figure out when to slow down and speed up ? if it is able to solve the problem or complete the level , will it have an equation to relating acceleration , or perhaps a number on when to speed up and down . what if the game environment was dynamic ? can you even teach math to an ai ? ps : i 'm not sure if i should ask separate question ?",8433,,,2018-09-10T11:11:24.163,can a game ai learn the concept of acceleration ?,machine-learning game-ai,2,0,
1340,5438,1,5439,2018-02-26T05:55:44.750,0,1185,"in the facenet paper , under section 3.2 the authors mention that : the embedding is represented by f(x ) ∈ r d . it embeds an image x into a d - dimensional euclidean space . additionally , we constrain this embedding to live on the d - dimensional hypersphere , i.e. ||f(x)|| 2 = 1 . i do n't quite understand how the above equation holds ! as far as i understand l2 norm is same as euclidean distance but i do n't quite understand how this imposes ||f(x)|| 2 = 1 criteria .",8418,,,2018-02-26T09:27:19.000,l2 normalization in facenet paper acting as a constraint ?,tensorflow keras,1,2,
1341,5443,1,,2018-02-26T16:33:11.623,1,30,"many of you have probably seen the turtle from labsix that gets mistaken for a rifle in google 's inceptionv3 image classifier . i read the paper and i understand how they apply eot to 2d images and on the individual pixel values , but i am still unsure how they implement the eot algorithm to the 3d model . are they using eot to perturb the individual coordinates in the 3d model 's mesh ? or are they perturbing images of a turtle and then printing the turtle from the images ? how do they check the inceptionv3 output iteratively without having to 3d print the object each time and check the probabilities given ? any examples that someone can point to would also be very helpful .",12983,12983,2018-02-26T16:52:14.900,2018-02-26T16:52:14.900,how to apply eot algorithm to 3d model,neural-networks image-recognition,0,0,
1342,5448,1,,2018-02-26T21:35:59.397,1,121,"recently my friend asked me a question : having two input matrices x and y ( each size nxd ) where d > > n , and ground truth matrix z of size dxd , what deep architecture shall i use to learn a deep model of this representation ? n ~ is in the order of tens d ~ is in the order of tens of thousands the problem is located in the domain of bioinformatics , however , this is more of an architectural problem . all matrices contain floats . i tried first a simple model based on a cnn model in keras . i 've stacked input x and y into an input matrix of size ( number of training examples , n , d , 2 ) . outputs are of size ( number of training examples , d , d , 1 ) conv2d layer leaky relu conv2d layer leaky relu dropout flattening layer dense ( fully connected ) of size d leaky relu droout dense ( fully connected ) of size d**2 ( d squared ) leaky relu droout reshaping output into ( d , d,1 ) ( for single training set ) however , this model is untrainable . it has over billion parameters for emulated data . ( exactly 1,321,005,944 for my randomly emulated dataset ) do you find this problem solvable ? what other architectures i might try to solve this problem ? best .",2655,2655,2018-02-27T21:56:21.577,2018-02-27T21:56:21.577,deep nn architecture for predicting a matrix from two matrices,machine-learning deep-learning convolutional-neural-networks keras data-science,0,7,3
1343,5451,1,,2018-02-27T10:37:24.430,1,278,"i recently came across a quora post , where i saw the term "" imagination learning "" . it seems to be based on something called "" imagination machines "" ( the link is based on a guy 's work profile as of now ; subject to change ) . the only thing that i could find on internet about it is this paper : imagination - augmented agents for deep reinforcement learning . ( but i 'm not sure if it 's related to that concept . ) any ideas on this would be appreciated .",12574,12574,2018-02-27T10:57:53.280,2018-03-03T09:15:33.063,what is imagination learning and imagination machines ?,reinforcement-learning,1,0,1
1344,5452,1,,2018-02-27T10:48:50.593,5,2898,for a multi ( 4xtitan xp ) gpu deep learning setup what kind of cpu is preferable ? specifically i am comparing : intel xeon e5 - 2620 with 8x2.1ghz 20 mb l3 cache intel xeon e5 - 1620k with 4x3.5ghz 10 mb l3 cache intel xeon e5 - 1650k with 6x3.6ghz 15 mb l3 cache intel i7 - 6850k with 6x3.6ghz 15 mb l3 cache i wonder if the higher clock rates are important or is it better to have more number of cores in this use case .,13004,1641,2018-08-12T09:27:36.597,2018-08-12T09:27:36.597,cpu preferences and specifications for a multi gpu deep - learning setup,deep-learning hardware hardware-evaluation,3,4,1
1345,5453,1,,2018-02-27T12:24:39.213,3,74,"i have a data - set with m observations and p categorical variables ( nominal ) , each variable x1,x2 ... xp has several different possible values . ultimately , i am looking for a way to find anomalies i.e to identify rows for which the combination of values seems incorrect with respect to the data i saw so far . so far , i was thinking about building a model to predict the value for each column and then build some metric to evaluate how different the actual row is from the predicted row . i would greatly appreciate any help !",13001,1671,2018-02-28T18:59:24.527,2018-06-28T17:43:16.083,find anomalies from records of categorical data,machine-learning datasets data-science,1,2,
1346,5454,1,,2018-02-27T13:27:26.843,2,101,"the situation i am referring to the paper t. p. lillicrap et al , "" continuous control with deep reinforcement learning "" where they discuss deep learning in the context of continuous action spaces ( "" deep deterministic policy gradient "" ) . based on the dpg approach ( "" deterministic policy gradient "" , see d. silver et al , "" deterministic policy gradient algorithms "" ) , which employs two neural networks to approximate the actor function mu(s ) and the critic function q(s , a ) , they use a similar structure . however one characteristic they found is that in order to make the learning converge it is necessary to have two additional "" target "" networks mu'(s ) and q'(s , a ) which are used to calculate the target ( "" true "" ) value of the reward : y_t = r(s_t , a ) + gamma * q'(s_t1 , mu'(s_t1 ) ) then after each training step a "" soft "" update of the target weights w_mu ' , w_q ' with the actual weights w_mu , w_q is performed : w ' = ( 1 - tau)*w ' + tau*w where tau & lt;&lt ; 1 . according to the paper this means that the target values are constrained to change slowly , greatly improving the stability of learning . so the target networks mu ' and q ' are used to predict the "" true "" ( target ) value of the expected reward which the other two networks try to approximate during the learning phase . they sketch the training procedure as follows : the question so my question now is , after the training is complete , which of the two networks mu or mu ' should be used for making predictions ? equivalently to the training phase i suppose that mu should be used without the exploration noise but since it is mu ' that is used during the training for predicting the "" true "" ( unnoisy ) action for the reward computation , i 'm apt to use mu ' . or does this even matter ? if the training was to last long enough should n't both versions of the actor have converged to the same state ?",13008,,,2018-10-10T22:00:20.993,should the actor or actor - target model be used to make predictions after training is complete ( ddpg ) ?,deep-learning reinforcement-learning deep-network q-learning,2,0,
1347,5457,1,,2018-02-27T17:31:41.933,1,108,"i have a cancer patient database from mass spectrometry on patients which consists of more than half million features . my task is to apply a feature selection algorithm to extract the most relevant features from it . my question is , which feature selection model would be the most appropriate in this case ? any suggestion from practical experience for these types of data is appreciated .",13011,9947,2018-02-28T18:46:40.550,2019-05-30T12:02:10.270,feature selection algorithm for a high featured data,machine-learning classification,3,1,
1348,5458,1,11306,2018-02-27T19:04:46.527,3,136,the region proposal network ( rpn ) in faster - rcnn models contains a classifier and a regressor network . why does the classifier network output two scores ( object and background ) for each anchor instead of just a single probability ? are n't the two classes considered exclusive ? source : figure 3 of the original faster - rcnn paper,13015,2444,2019-03-18T20:45:19.237,2019-03-18T20:45:19.237,why does the classifier network in rpn output two scores ?,machine-learning convolutional-neural-networks object-recognition,1,0,1
1349,5460,1,,2018-02-28T10:37:55.530,1,38,"neural networks have the problem , that they are not turing - complete . that means , it is not possible to express any function with it . instead , logicgate networks which are consisting of and , or , not are turing complete . it is possible to implement a primenumber generator with boolean algebra : a circuit to find prime numbers in the hope that logicgates can be trained like a neural network , i implemented a prototype : / * i0i1| a b c d 0 0 | 0 0 0 0 0 1 | 0 1 1 1 1 0 | 0 1 0 1 1 1 | 1 1 1 1 a and b or c i1 or a d i0 or c * / class logicgate { public : void addrandomgate ( ) { int id1 = std::rand ( ) % mylogicgate.size ( ) ; // 0 .. mylogicgate.size ( ) int id2 = std::rand ( ) % mylogicgate.size ( ) ; int relation = std::rand ( ) % 3 ; // 0 = and , 1 = or , 2 = not bool value=0 ; mylogicgate.push_back({id1,id2,relation,value } ) ; } void calc ( ) { for ( auto i = numberinput;i&lt;mylogicgate.size();i++ ) { // get value bool temp1 = mylogicgate[mylogicgate[i].id1].value ; bool temp2 = mylogicgate[mylogicgate[i].id2].value ; // logic operation if ( mylogicgate[i].relation==0 ) mylogicgate[i].value = temp1 & amp;&amp ; temp2 ; if ( mylogicgate[i].relation==1 ) mylogicgate[i].value = temp1 || temp2 ; if ( mylogicgate[i].relation==2 ) mylogicgate[i].value = ! temp1 ; } } void train ( ) { for ( auto trial=0;trial&lt;100000000;trial++ ) { if ( error&lt;minerror ) std::cout&lt;&lt;""trial "" & lt;&lt;trial&lt;&lt ; "" error "" & lt;&lt;error&lt;&lt;""\n "" ; } } } ; the prototype works fine . he calculates for the input the output , and it is possible to set the logicgates via a random - generator . the idea is to use a brute - force - solver for testing out all possible logicgates and find a mapping from input to output . in the literature this concept is called binary decision tree . but i found a bug . the same problem is happening like in neural networks learning too . the cpu consumption in testing out all possibilities is very high , but the solver do n't find the correct weight . weight means here , the boolean algebra which is the program of the network . how can i improve the performance of the solver ? update mcculloch pitts neuron is the correct term . it describes a neuron which is equal to a logicgate . the idea was to use some kind of genetic programming on that model , but the state - space of all possible connections seems heavy large . for example , if 10 neurons are possible , that means every neuron can connected with two other neurons : 10x10 and has either the and , or , not function . so the overall number of possibilities is ( 10 * 10 * 3)^10=3.4867844e+49 ?",11571,11571,2018-02-28T14:29:32.497,2018-02-28T14:29:32.497,training of a logicgate network,machine-learning,0,1,
1350,5461,1,7778,2018-02-28T11:48:37.810,2,105,"i 'd like to generate subtitles for a silent film . is there an open source project out there capable of creating captions based on a series of images ( such as a scene from a movie ) ? edit : thanks for the comments below . to clarify , what i 'm looking for is an algorithm which can generate a caption for a sequences of images within a movie describing what happens in the sequence . this is for preliminary research , so accuracy is less important .",13030,13030,2018-03-01T22:17:35.500,2018-12-30T06:03:00.663,create captions based on a series of images,image-recognition,3,5,
1351,5462,1,5489,2018-02-28T14:52:02.613,2,715,"usually , in binary classification problems , we use sigmoid as activation function of last layer plus the binary cross - entropy as cost function . however , i have already experienced ( more than once ) that tanh as activation function of last layer + mse as cost function worked slightly better for binary classification problems . using a binary image segmentation problem as an example , we have the two scenarios : sigmoid ( last layer ) + cross entropy : the output of the network will be a probability for each pixel and we want to maximize it according to the correct class . tanh ( last layer ) + mse : the output of the network will be a normalized pixel value [ -1 , 1 ] and we want to make it as close as possible the original value ( normalized too ) . we all know the problems associated with sigmoid ( vanish of gradients ) and the benefits of cross - entropy cost function . we also know tanh is slightly better than sigmoid ( zero - centered and little less prone to gradient vanishing ) , but when we use mse as the cost function , we are trying to minimize a completely different problem - regression instead of classification . anybody else has already faced the same results ? is there any intuition about why "" tanh + mse "" worked better than "" sigmoid + cross entropy "" ?",13036,,,2018-03-02T17:17:01.980,binary classification with tanh + mse,neural-networks deep-learning convolutional-neural-networks,1,0,
1352,5466,1,,2018-02-28T17:22:07.780,1,17,"i am looking for an algorithm to transform an input data to a goal data using a series of operations . the shorter the series the better . the following is known : the input data the goal data input and goal data does not stand in any correlation operations ( and there impact to the current data ) which can be endless combined different input data for the same goal data could have same , similar or totally different operation series for some data states not all operations are possible i thought of a pathfinding algorithm , since i can calculate the distance between current data and goal data . so each edge would be an operator and each node the current data . but i am unsure about variety and combination amount of possible operations . what approach could i try ?",13026,,,2018-02-28T17:22:07.780,approach for data transformation needed,ai-basics heuristics pathfinding,0,0,
1353,5468,1,,2018-02-28T23:23:59.250,1,77,"i want to plot a schedule of races based on rules . rules like "" each team needs at least 2 races between their next race "" and some teams ( e.g. collegiate ) need to be clumped near each other . what would be the best algorithm to approach this ? so far , all i 've found is genetic algorithm . are there any other alternatives i could look into ?",13042,,,2018-03-01T12:25:11.790,which algorithm for scheduling race grid ?,algorithm genetic-algorithms,1,4,
1354,5471,1,,2018-03-01T07:24:56.000,2,299,"i am currently trying to solve a regression problem using neural networks . i want to detect movement patterns in images over time ( video ) and output a continuous value . during the training process i noticed a strange behaviour for the validation loss curve and i was wondering if anyone has noticed this kind of periodic pattern on some of their own work . what might cause this ? the model looks like the following : - timedistributed(conv2d(32 , ( 3,3 ) ) ) - timedistributed(conv2d(16 , ( 3,3 ) ) ) - timedistributed(flatten ( ) ) - gru(64 , stateful = true ) - dropout(0.5 ) - dense(64 , activation='relu ' ) - dense(1 ) i trained the model using the mean squared error as the loss function , a batch size of 1 and the adamoptimizer with an initial learning rate of 10^(-6 ) . obviously , the loss curve for the training data is not very good , but i am currently just wondering about the pattern of the val_loss . the plots below represent the loss of 65 epochs . thanks ! edit : the way i try to solve my task relies on a sliding window approach where i try to predict a continuous value for the next second based on the last 20 seconds ( 400 frames ) of the time - series input data . but i do n't think this information is needed to solve my initial question since the periodic patterns appear over several epochs ( one "" peak "" for about every 15 epochs ) which is strange . although the stateful - version of the gru is used ( btw : using tensorflow and keras ) , the internal state of the gru is reset after every epoch to maintain a clean start . the stateful keyword is used to indicate a dependency between batches .",13054,13054,2018-03-01T14:55:29.460,2019-04-28T10:02:10.617,periodic pattern in validation loss curve,deep-learning training keras,1,2,
1355,5474,1,,2018-03-01T12:50:55.907,4,163,"i am doing a project on visual place recognition in changing environments . the cnn used here is mostly alexnet , and a feature vector is constructed from layer 3 . does anyone know of similar work using other cnn 's e.g. vggnet ( which i am trying to use ) and the corresponding layers please ? i have been trying out the different layers of vggnet-16 . i am trying to get the nearest correspondence to the query image by using the cosine difference between query image and database images . so far no good results . thanks .",13058,,,2019-04-28T02:01:15.427,other deep learning networks for visual place recognition ?,machine-learning convolutional-neural-networks computer-vision,1,1,1
1356,5475,1,,2018-03-01T13:26:29.477,4,279,"as discussed in this thread , you can handle invalid moves in reinforced learning by re - setting the probabilities of all illegal moves to zero and renormalising the output vector . in back - propagation , which probability matrix should we use ? the raw output probabilities , or the post - processed vector ?",13060,2193,2018-06-13T16:23:09.920,2018-10-11T20:00:30.850,how to deal with back - propagation when dealing with invalid moves in reinforced learning ?,reinforcement-learning backpropagation,1,0,
1357,5477,1,,2018-03-01T15:21:52.770,2,348,"recently i came across this website which is a year old : https://affinelayer.com/pixsrv/ on desktop we can draw and download the trained model in browser and see the corresponding image generated on the right side . wondering how it works and with some investigation i have the following 2 questions : why are we only downloading one file per “ showcase ” ? because gan should need 1 model for converting the image and 1 model for verifying the image from what i have read . why are the models in pict extension ? which framework did the author use to create the pict files ? in linked repositories i have found tensorflow , pytorch etc which none of them produces pict files ... thanks in advance . i am very curious why such javascript client side demo is lacking on the internet ... or i just have n’t found the correct keyword to find them out .",12809,,,2018-07-16T14:59:05.823,javascript client side gan implementation,deep-learning reinforcement-learning computer-vision,1,0,
1358,5482,1,5484,2018-03-01T18:14:17.167,2,88,"as far as i understand , neural networks are n't good at classifying ' unknowns ' , i.e. objects that do not belong to a learned class . but how do face detection / recognition approaches usually determine that no face is detected / recognised in a region ? is the predicted probability somehow thresholded ? i 'm asking because my application will involve identifying unknown objects . in fact , most of the input objects are unknown and only a fraction is known .",13068,,,2018-03-01T18:34:42.377,facial recognition and classifying unknowns with neural networks,neural-networks machine-learning deep-learning classification,1,0,
1359,5485,1,,2018-03-02T02:38:51.567,2,81,"i was reading an interesting book about the role of ai in cybersecurity , and the author mentioned there being 3 - 4 types . each one is dependent on its abilities and understanding . for example , a ‘ narrow artificial intelligence ’ is only capable of one such ability . what are these “ levels ” and how are they gauged ?",13074,1847,2018-03-05T16:26:41.110,2018-03-05T16:26:41.110,what are the types of artificial intelligence and how are they measured ?,ultraintelligent-machine,1,4,
1360,5486,1,5689,2018-03-02T07:26:01.590,2,776,what will be the difference when used for video classification ? will they yield different results or are they the same fundamentally ?,14633,,,2018-03-15T08:54:17.103,what is the difference between convlstm and cnn lstm ?,convolutional-neural-networks lstm,1,0,
1361,5488,1,,2018-03-02T13:35:26.667,1,27,"in order to model a card game as an exercise i was thinking an elementary setting as a multiarmed bandit , each lever being the distribution of expected rewards of an specific card . but of course the player only have some cards in the hand each round , or equivalently for a given round it has available a number $ n$ of arms randomly selected from the total number $ n$ of levers . is this just a "" contextual bandit "" or has it some specific , narrower , name that i could use to look up in the literature ?",13080,,,2018-03-02T13:35:26.667,name of a multiarmed bandit with only some levers available,reinforcement-learning ai-basics,0,2,
1362,5493,1,5521,2018-03-02T21:27:24.353,9,4383,it is said that activation functions in neural networks help introduce non - linearity . what does this mean ? what does non - linearity mean in this context ? how does introduction of this non - linearity help ? are there any other purposes of activation functions ?,12957,,,2018-12-11T11:58:37.753,what is the purpose of an activation function in neural networks ?,neural-networks deep-learning,6,0,6
1363,5496,1,,2018-03-03T00:52:34.247,5,423,"does neat requires only connection genes to be marked with a global innovation number ? from the neat paper whenever a new gene appears ( through structural mutation ) , a global innovation number is incremented and assigned to that gene . it seems that any gene ( both node genes and connection genes ) requires an innovation number , however i was wondering what was the node gene innovation number for . is it to provide the same node i d across all elements of the population ? is n't the connection gene innovation number sufficient ? besides , the neat paper includes the following image which does n't show any innovation number on node genes .",13087,1671,2018-03-05T18:56:55.170,2018-05-23T14:44:01.213,neat - innovation for connection genes only ?,neural-networks genetic-algorithms neat,2,2,1
1364,5497,1,5498,2018-03-03T01:48:56.240,2,179,"in the add node mutation , the connection between two chosen nodes ( e.g a and b ) is first disabled and then a new node is created between a and b with their respective two connections . i guess that the former a - b connection can be re - enabled via crossover ( is it right ? ) . can the former a - b connection also be re - enabled via mutation ( e.g. "" add connection "" ) ?",13087,1671,2018-03-05T19:09:17.740,2019-02-07T08:55:21.403,neat - can mutation enable a disabled connection ?,neural-networks genetic-algorithms neat,2,0,1
1365,5502,1,,2018-03-03T11:40:23.863,0,34,can you please point me to some good tutorials / examples on the application of incremental ( online ) learning novelty detection algorithms ?,12965,2444,2019-05-01T17:17:51.247,2019-05-01T17:17:51.247,resources on incremental and online learning novelty detection algorithms,machine-learning applications reference-request incremental-learning online-learning,1,0,
1366,5506,1,5508,2018-03-03T21:59:41.030,2,63,"is there any rule to follow when it comes to total amount of nn parameters ( weights and biases ) taking into account the amount of training data ? is there any recommended ratio between the two , for example 10 training vectors for each nn parameter ?",12327,,,2018-03-04T06:49:44.633,nn robustness in relation to quantity of training data,neural-networks,2,1,
1367,5509,1,,2018-03-04T10:38:58.867,3,153,"i have a general question about the updating of the network / model in the ppo algorithm . if i understand it correctly , there are multiple iterations of weight updates done on the model with data that is created from the environment ( with the model before the update ) . now , i think that the updates of the model weights are not correct anymore after the first iteration / optimization step , because the model weights changed and therefore the training data is outdated ( since the model would now give different actions in the environment and therefore different rewards ) . basically , in the pseudo code of the algorithm i do n't understand the line "" optimize surrogate l ... with k epochs ... "" . if the update is done for multiple epochs the data that is learned on is outdated already after the first iteration of optimization since the model weights changed . in other algorithms like a2c there is only one opimization step done instead of k epochs . is this some form of approximation or augmentation on the data by using the data that was created by an older model for multiple iterations or am i missing something here ? if yes , where was this idea first introduced or better described ? and where is a ( empirical ) proof that this still leads to a correct weight updating ?",13104,2444,2019-04-12T21:55:44.610,2019-04-12T21:55:44.610,understanding multi iteration update of model in policy gradient ppo algorithm,neural-networks deep-learning reinforcement-learning deep-rl actor-critic,0,0,2
1368,5511,1,,2018-03-04T12:55:06.937,2,428,are mathematical models sufficient to create general artificial intelligence ? i am not sure if it is possible to represent e.g. emotions or intuition using mathematical model . do we need a new approach in order to solve this problem ?,13107,2444,2019-04-19T15:16:41.987,2019-04-19T15:16:41.987,are mathematical models sufficient to create general artificial intelligence ?,philosophy models agi,2,0,3
1369,5512,1,,2018-03-04T13:35:19.877,0,812,i have installed jfuzzylite and now i want to use it in java but i could not find any documentation or examples to learn how to use this package please could you kindly tell me what should i do ?,13108,,,2018-03-06T17:20:57.133,fuzzy logic using java,fuzzy-logic,1,4,
1370,5517,1,,2018-03-04T22:39:31.653,2,225,"i am seeking the information for this kind of chatbot architecture : there are two chatbots . one plays the role of teacher , and another is a student who is learning . the goal is to test the student 's quality , and to improve the student 's ability . i did n't find much reference . there are : bottester : testing conversational systems with simulated users and the parlai , a python - based platform for enabling dialog ai research has the notion of "" teacher agent "" , which seems to be what i am looking for . of course , we also have deep reinforcement learning which might be related . i prefer to have some classical references for this approach to chatbots . currently , reinforcement learning is not in my consideration . constructing two chatbots talking to each other , like what facebook did , is not what i want . because in this case , both of them are student agents .",13118,,,2018-05-17T07:25:12.400,two chatbots - one teaches another,natural-language-processing chat-bots,2,3,2
1371,5519,1,5522,2018-03-05T09:16:39.143,1,210,"what is supposed to happen first : strong ai or technological singularity ? meaning which option is more likely , that the strong ai that will bring as to the state of technological singularity or achieving technological singularity will allow us to construct strong ai ?",12935,,,2018-03-05T14:36:30.103,strong ai vs singularity - which should happen first ?,agi singularity superintelligence,1,1,
1372,5526,1,5537,2018-03-05T19:27:28.177,4,71,"summary : i am teaching bots to pick food on a playing field . some food is poisonous and some is good . food details : poisonous food subtracts score points and good food adds . food points vary based on it 's size . there is about 9:1 ratio of poisonous food to good food , so a lot more chances to end up in negative numbers . food grows in points overtime . food spoils after some predetermined size becoming poisonous . fitness function : the fitness function i use is simply counting points by the end of iterations . bot 's might choose to eat it or skip it . the problem : the problem i am having is that first generation , most bots eat a lot of bad crap and the curious ones end up in negative numbers . so mostly the ones that make it are the ones that are lazy and did n't eat or did n't head towards the food and most of the time the fittest for first few generations comes out with 0 points and 0 eats of any kind of food . when trained for long time they just end up waiting for the food instead of eating multiple times . often while they wait food goes bad and they just end up going to another food . this way in the end of the iteration , i have some winners but they are nowhere near the potential they could have been at . question : i somehow need to weight the importance of eating food . i want them to eventually learn to eat . so i thought of this : brain.score + = foodvalue * numtimestheyatesofar but this blows up the score too much and now the food quality is not respected and they just gulp on anything slightly above 0 . please help .",13138,,,2018-03-06T11:10:05.103,help with dual parameter fitness function,neat fitness-functions,2,2,
1373,5527,1,,2018-03-05T20:22:24.687,5,449,"i 'm trying to implement a custom version of yolo neural network . originally it was described in this paper . i have some problems understanding the loss function they used . basic information : an input image is divided into s by s grid ( that gives the total of s^2 cells ) and each cell predicts b bounding boxes and c conditional class probabilities . each bbox predicts 5 values : x , y , w , h , c ( center of bbox , width and height and confidence score ) . this makes the output of yolo a sxsx(5b*c ) tensor . the ( x , y ) coordinates are calculated relative to the bounds of the cell and ( w , h ) is relative to the whole image . i understand that the first term penalizes the wrong prediction of the center of a bbox ; 2-nd term penalizes wrong width and height prediction , 3-rd term - wrong confidence prediction , 4-th is responsible for pushing confidence to zero when there is no object in a cell ; the last term penalizes wrong class prediction . my problem : i do n't understand when 1^obj_ij should be 1 or 0 . in the paper , they write : "" 1^obj_ij denotes that the j - th bbox predictor in i - th cell is responsible for that prediction "" and also "" loss function only penalizes bounding box coordinate error if that predictor is responsible for ground truth box "" . so is it right that for every object in the image there should be exactly one pair of ij such taht 1^obj_ij=1 ? and if this is correct , this means that the center of the ground truth bbox should fall into i - th cell , right ? if this is not the case , what are other possibilities when 1^obj_ij=1 and what ground truth labels x_i and y_i should be in these cases ? also , i assume that ground truth p_i(c ) should be 1 if there is an object of class c in the cell i , but what ground truth p_i(c ) should be equal to in case there are several objects of different classes in the cell ?",13102,,,2018-08-05T19:01:47.577,understanding the loss function of you only look once(yolo ) network,deep-learning convolutional-neural-networks object-recognition,1,0,
1374,5529,1,,2018-03-05T23:18:44.650,4,756,"i 'm new to this ai / machine learning and was playing around with openai gym a bit . when looking through the environments i came across "" blackjack - v0 "" which is a basic implementation of the game where the state is the hand count of the player and the dealer and if the player has an useable ace . the actions are only hit or stand and the possible rewards 1 if the player wins , -1 if the player loses and 0 when draw . so that got me thinking what a more realistic environment / model for this game would look like , taking into account the current balance and other factors and has multiple actions like betting 1 - 10 € and hit or stand . this brings me to my actual question : as far as i understand neural networks ( and i do not very well yet , i guess ) the input will be the state and the output the possible actions and how good the network thinks they are / will be . but now there are two different action spaces which apply to different states of the game ( betting or playing ) , so some of the actions are useless . how would be the right way to approach this scenario ? i 'm guessing one answer would be to give some kind of negative reward if the network guesses an useless action but in this case i think the reward should be the actual stake ( negative reward ) and the actual win if any . therefor this would cause some bias in how the game proceeds as it should start with some amount of balance and end if the balance is 0 or after a specified amount of rounds . limiting timesteps would n't be an option either i guess because it should be limited to rounds so it wo n't end after a betting step e.g. therefore , for a useless step the reward would be 0 and the state would stay the same but for the network it does n't matter how many useless steps it takes because it 'll make no difference to the actual outcome . corollary question : should be split up into two neural networks ? one for betting and one for playing ?",13141,1671,2018-03-06T17:29:36.500,2018-12-03T20:02:20.447,different action spaces for different states of the environment,neural-networks machine-learning reinforcement-learning ai-basics python,2,3,
1375,5531,1,5534,2018-03-06T06:35:02.250,2,185,"in the diagram below , although the flow of information happens from the input to output layer , the labeling of weights appears reverse . eg : for the arrow flowing from x3 to the fourth hidden layer node has the weight labeled as w(1,0 ) and w(4,3 ) instead of w(0,1 ) and w(3,4 ) which would indicate data flowing from the 3rd node of the 0'th layer to the 4th node of the 1st layer . one of my neural networks teachers did not emphasize on this convention at all . another teacher made it a point to emphasize on it . is there a reason there is such an un - intuitive convention and is there really a convention ?",9268,,,2018-03-07T16:36:08.127,is there a naming convention for network weights for multilayer networks ?,neural-networks,1,11,
1376,5536,1,7554,2018-03-06T10:35:34.890,3,688,"i 'm training seq2seq model on opensubtitles dialogs - cornell - movie - dialogs - corpus . my work based on the following papers ( but currently i 'm not implemented attention yet ) : sequence to sequence learning with neural networks , sutskever et al . 2014 a neural conversational model , vinyals , le , 2015 the loss i received is quite high and sucked in variation ~6.4 after 3 epoches . the model predicts the most common words with some times other not significant words ( but 99.99 % is just ' you ' ) : i ’ve experimented with 128 - 2048 hidden units and with 1 or 2 or 3 lstm layers per encoder and decoder . the outcomes are more or less the same . seq1 : yeah man it means love respect community and the dollars too the package the unk end seq2 : but how did you get unk 82 end prediction : promoting 16th dashboard be of the the the you you you you you you you you you you you you you you you you you you you you you you you you i 'm using here greedy prediction , meaning - after i receive logit i do argmax ( .. ) on all its value for first-3 mini - batch - elements ( here i present only first element ) . for convenient - seq1 and seq2 are also printed - to know the actual dialog which was presented to the model . the pseudo - code of my architecture looks like this ( i 'm using tensorflow 1.5 ) : seq1 = tf.placeholder ( ... ) seq2 = tf.placeholder ( ... ) embeddings = tf.variable(tf.random_uniform([vocab_size , 100],-1,1 ) ) seq1_emb = tf.nn.embedding_lookup(embeddings , seq1 ) seq2_emb = tf.nn.embedding_lookup(embeddings , seq1 ) encoder_out , state1 = tf.nn.static_rnn(basiclstmcell ( ) , seq1_emb ) decoder_out , state2 = tf.nn.static_rnn(basiclstmcell ( ) , seq2_emb , initial_state = state_1 ) logit = dense(decoder_out , use_bias = false ) crossent = tf.nn.saparse_softmax_cross_entropy_with_logits(logits=logit , labels = target ) crossent = mask_padded_zeros(crossent ) loss = tf.reduce_sum(crossent ) / number_of_words_in_batch train = tf.train.adamoptimizer(learning_rate=0.00002).minimize(loss ) i 'm also wonder if i pass well state1 to decoder , which in general looks like this : # reshape in pseudocode : state1 = state[1 : ] new_state1 = [ ] for lstm in state1 : new_lstm = [ ] for gate in lstm : new_lstm.append(gate[1 : ] ) new_state1.append(tuple(new_lstm ) ) state1 = tuple(new_state1 ) should i use some projection layer between states of encoder and decoder ? so if seq1 has 32 words , seq2 has 31 ( since we will not predict nothing after the last word , which is the tag & lt;end&gt ; ) .",12691,4302,2018-10-08T12:15:27.360,2018-10-08T12:15:27.360,seq2seq dialogs predicts only most common words like ` you ` after couple of epoches,neural-networks natural-language-processing tensorflow recurrent-neural-networks lstm,1,0,1
1377,5538,1,,2018-03-06T13:24:53.627,3,605,"google analytics allows me to collect data about every web - session . for simplicity , let 's assume for each user , we collect the number of pages and time spent on site for each session : user_id visit_id page_views time_spent result 1 1 10 100 0 1 2 31 510 0 1 3 1 10 1 how would you model this data ? what i would like the ml algorithm to do : extract as much information as possible have a flexible number of inputs ( e.g. the number of sessions can go to infinity ) what i can think of : aggregate the data per user e.g. average page_views or total page_views and feed it into a general algorithm e.g. random forrest ( but i lose information with aggregation ) use lstm and feed at most last 3 visits ( will also lose information , but would this perform better than aggregation ? ) goal : to build a predictive model to analyse all user sessions and make a prediction whether the person will convert or not .",13148,,,2018-07-08T19:32:51.770,ml model that is most suited to analyse google analytics data,machine-learning deep-learning reinforcement-learning,2,1,2
1378,5539,1,,2018-03-06T14:46:16.433,3,247,"there are a lot of papers that show that neural networks can approximate a wide variety of functions . however , i ca n't find papers that show the limitations of nns . what are the limitations of neural networks ? which functions ca n't neural networks learn efficiently ( or using gradient - descent ) ? i am looking also for links to papers that describe these limitations .",13067,2444,2019-05-10T14:45:18.307,2019-05-10T14:45:18.307,which functions ca n't neural networks learn efficiently ?,neural-networks machine-learning reference-request function-approximation,3,2,1
1379,5546,1,,2018-03-06T21:01:55.610,14,19099,"i 've seen these terms thrown around this site a lot , specifically in the tags convolutional - neural - networks and neural - networks . i know that a neural network is a system based loosely on the human brain . but what 's the difference between a convolutional neural network and a regular neural network ? is one just a lot more complicated and , ahem , convoluted than the other ?",145,,,2018-11-03T18:17:02.057,what is the difference between a convolutional neural network and a regular neural network ?,neural-networks convolutional-neural-networks definitions terminology,3,0,8
1380,5548,1,5549,2018-03-06T22:29:47.460,2,408,"i am reading through the neat paper here . on page 14 of the pdf , there is this quote about mutation : there was an 80 % chance of a genome having its connection weights mutated , in which case each weight had a 90 % chance of being uniformly perturbed and a 10 % chance of being assigned a new random value . what exactly does it mean to perturb weights ? what is uniform vs. nonuniform perturbation ? is there an established method to do this ? i am imagining the process as multiplying each connection weight by a random number , but i 'm unfamiliar with the term .",13160,,,2018-03-07T16:19:27.323,"how are connection weights "" perturbed "" ?",neural-networks evolutionary-algorithms neat,1,0,1
1381,5551,1,5562,2018-03-07T04:23:28.307,0,47,"let us for these purposes say with are working with any feed forward neural network . let us also say , that we know beforehand that certain portion of our dataset arsignificantly more impactful or important to our underlying representation . is there anyway to add that “ weighting ” to our data ?",9608,,,2018-03-09T09:29:08.607,a way to give more weight to particular data ?,training datasets,2,3,
1382,5553,1,5581,2018-03-07T06:17:34.207,3,491,"as we all know , there has been tons of gan variants featuring different aspects of the image generation task such as stability , resolution or the ability to manipulate images . however , it is still confusing to me that how do we determine that images generated by one network are more plausible than images generated by another ? ps : could someone with higher reputation create more tags like image generation ?",13165,1847,2018-03-07T08:38:11.113,2018-03-08T20:33:55.370,how to evaluate the goodness of images generated by gans ?,deep-learning generative-model,1,0,1
1383,5558,1,,2018-03-07T07:32:33.493,3,54,i have a fully connected network that takes in a variable length input padded with 0 . however the network does n't seem to be learning and i am guessing that the high number of zeros in the input might have something to do with that . are there solution for dealing with padded input in fully connected layers or should i consider a different architecture ? update ( to provide more details ) : the goal of the network if to clean full file paths : i.g . : /my document / some folder / a file name.txt & gt ; a file name /hard drive / book / deeplearning/1.txt & gt ; deeplearning the constraint is that the training data labels have been generated using a regex on the file name itself so it 's not very accurate . i am hopping that by treating every word equally ( without sequential information ) the network would be able to generalize as to which type of word is usually kept and which is usually discarded . then network takes in a sequence of word embedding trained on paths data and output a logits that corresponds to probabilities of each word to be kept or not .,12931,12931,2018-03-07T08:44:09.773,2019-05-20T04:00:36.213,how to deal with padded inputs in a fully connected feed forward network ?,deep-learning mlp,1,2,
1384,5559,1,,2018-03-07T08:41:55.487,0,87,"i have an idea about how to use neural networks but i 'm not sure if it is possible or not . in supervised learning we have a set of attributes labeled with an output value . i can use these set to train my network . now i have a network trained to get an output value from an random set of attributes but , can i use this trained network to get the input attributes using only the desired output ? i will have n input values and only 1 output value . i 've thought that i can use the weights for that network into a new one with 1 input value and n output values but i 'm not sure if i can do that .",4920,4920,2018-03-07T12:58:38.843,2018-03-09T18:09:01.630,neural network to get input attributes using only the output value,neural-networks generative-model,2,0,
1385,5568,1,,2018-03-08T00:38:21.927,0,146,"i used to work with ' traditional ' layered neural network and i evaluated the output given certain inputs by processing layer - by - layer . with neat , a neural network may assume any topology and they are no longer layered , so how to evaluate the output ? i understand time - steps must be taken into account , but how ? should i keep the inputs until all hidden neurons are processed and output is produced ? should i wait for output to stabilize ?",13087,4302,2018-07-23T15:38:08.340,2018-07-23T15:38:08.340,how to evaluate output of unlayered nn ?,neural-networks neat topology,1,0,1
1386,5570,1,,2018-03-08T06:10:00.340,2,286,"in two player games , the exact value of the evaluation function does n't matter as long as it 's bigger for better positions . however , for learning , it 's customary when it does change when the best move gets made . this way , the learning can minimize the difference between the directly computed value f(0 , p ) of a position p and the value obtained from n step minimax f(n , p ) . what i 'm missing here is a way to direct the evaluation function to actually winning . for example , a perfect evaluation function for a won position in chess would always return +1 without any hint how to progress towards checkmate . in a chess variant without the fifty - move limit , it could play useless turns forever . i guess , this is a rather theoretical problem as we wo n't ever have such a good function , but i wonder if there 's a way to avoid it ?",12053,,,2018-08-05T11:54:31.857,game ai evaluation function and making progress towards winning,reinforcement-learning gaming,2,0,
1387,5571,1,,2018-03-08T06:46:48.417,4,91,how does one prove the uniqueness of the value function obtained from value iteration in the case of bounded and undiscounted rewards ? i know that this can be proven for the discounted case pretty easily using the banach fixed point theorem .,13185,1847,2018-03-09T18:16:31.120,2018-08-13T01:00:26.170,proof of uniqueness of value function for mdps with undiscounted rewards,reinforcement-learning,1,0,2
1388,5577,1,,2018-03-08T12:54:00.040,4,426,"i am working on a js library which focuses on error handling . a part of the lib is a stack parser which i 'd like to work in most of the environments . the hard part that there is no standard way to represent the stack , so every environment has its own stack string format . the variable parts are message , type and frames . a frame usually consists of called function , file , line , column . in some of the environments there are additional variable regions on the string , in others some of the variables are not present . i can run automated tests only in the 5 most common environments , but there are a lot more environments i 'd like the parser to work in . my goal is to write an adaptive parser , which learns the stack string format of the actual environment on the fly , and after that it can parse the stack of any exception of that environment . i already have a plan how to solve this in the traditional way , but i am curious , is there any machine learning tool ( probably in the topic of unsupervised learning ) i could use to solve this problem ? according to the comments i need to clarify the terms "" stack string format "" and "" stack parser "" . i think it is better to write 2 examples from different environments : a. ) example stack string : statement on line 44 : type mismatch ( usually a non - object value used where an object is required ) backtrace : line 44 of linked script file://localhost / g:/js / stacktrace.js this.undef ( ) ; line 31 of linked script file://localhost / g:/js / stacktrace.js ex = ex || this.createexception ( ) ; line 18 of linked script file://localhost / g:/js / stacktrace.js var p = new printstacktrace.implementation ( ) , result = p.run(ex ) ; line 4 of inline#1 script in file://localhost / g:/js / test / functional / testcase1.html printtrace(printstacktrace ( ) ) ; line 7 of inline#1 script in file://localhost / g:/js / test / functional / testcase1.html bar(n - 1 ) ; line 11 of inline#1 script in file://localhost / g:/js / test / functional / testcase1.html bar(2 ) ; line 15 of inline#1 script in file://localhost / g:/js / test / functional / testcase1.html foo ( ) ; stack string format ( template ) : statement on line { frames[0].location.line } : { message } backtrace : { foreach frames as frame } line { frame.location.line } of { frame.unknown[0 ] } { frame.location.path } { frame.calledfunction } { /foreach } extracted information ( json ) : { message : "" type mismatch ( usually a non - object value used where an object is required ) "" , frames : [ { calledfunction : "" this.undef ( ) ; "" , location : { path : "" file://localhost / g:/js / stacktrace.js "" , line : 44 } , unknown : [ "" linked script "" ] } , { calledfunction : "" ex = ex || this.createexception ( ) ; "" , location : { path : "" file://localhost / g:/js / stacktrace.js "" , line : 31 } , unknown : [ "" inline#1 script in "" ] } , ... ] } b. ) example stack string : referenceerror : x is not defined at repl:1:5 at replserver.self.eval ( repl.js:110:21 ) at repl.js:249:20 at replserver.self.eval ( repl.js:122:7 ) at interface.&lt;anonymous&gt ; ( repl.js:239:12 ) at interface.eventemitter.emit ( events.js:95:17 ) at interface._online ( readline.js:202:10 ) at interface._line ( readline.js:531:8 ) at interface._ttywrite ( readline.js:760:14 ) at readstream.onkeypress ( readline.js:99:10 ) stack string format ( template ) : { type } : { message } { foreach frames as frame } { if frame.calledfunction is undefined } at { frame.location.path}:{frame.location.line}:{frame.location.column } { else } at { frame.calledfunction } ( { frame.location.path}:{frame.location.line}:{frame.location.column } ) { /if } { /foreach } extracted information ( json ) : { message : "" x is not defined "" , type : "" referenceerror "" , frames : [ { location : { path : "" repl "" , line : 1 , column : 5 } } , { calledfunction : "" replserver.self.eval "" , location : { path : "" repl.js "" , line : 110 , column : 21 } } , ... ] } the parser should process the stack strings and return the extracted information . the stack string format and the variables are environment dependent , the library should figure out on the fly how to parse the stack strings of the actual environment . i can probe the actual environment by throwing exceptions with well known stacks and check the differences of the stack strings . for example if i add a whitespace indentation to the line that throws the exception , then the column and probably the called function variables will change . if i detect a number change somewhere , then i can be sure that we are talking about the column variable . i can add line breaks too , which will cause line number change and so on ... i can probe for every important variables , but i can not be sure that the actual string does not contain additional unknown variables and i can not be sure that all of the known variables will be added to it . for example the frame strings of the "" a "" example contain an unknown variable and do not contain the column variable , while the frame strings of the "" b "" example do not always contain the called function variable .",13192,13192,2018-03-10T12:32:28.127,2019-01-21T09:30:27.940,is it possible to write an adaptive parser ?,machine-learning unsupervised-learning javascript,2,8,
1389,5580,1,5620,2018-03-08T16:22:55.040,6,4260,"i am trying to understand backpropagation . i used a simple neural network with one input x , one hidden layer h and one output layer y , with weight w1 connecting x to h , and w2 connecting h to y . x--[w1]-- > h --[w2]-->y in my understanding these are the steps happening while we train a neural network : i understood most parts of backpropogation , but how do we get the gradients for the middle layer weights dl / dw1 ? edit latex \\ feed \ forwarding \\ h=\sigma ( xw_{1}+b ) \\ { y}'=\sigma ( hw_{2}+b ) \\ \\ loss \ function \\ l=\frac{1}{2}\sum(y-{y}')^{2 } \\ \\ gradient \ calculation \\ \\ \frac{\partial l}{\partial w_{2}}=\frac{\partial { y}'}{\partial w_{2}}\frac{\partial l } { \partial { y } ' } \\ \\ \frac{\partial l}{\partial w_{1}}= \frac{\partial h}{\partial w_{1 } } \frac{\partial { y}'}{\partial h } \frac{\partial l}{\partial { y } ' } \\ \\ % duttaa 's solution weight \ update \\ w_{i}^{t+1 } \leftarrow w_{i}^{t}-\alpha \frac{\partial l}{\partial w_{i } } how should we calculate gradient of a network similar to this ? is this the correct equation ? latex format \frac{\partial l}{\partial w_{1}}=\frac{\partial h_{1}}{\partial w_{1}}\frac{\partial w_7}{\partial h_{1}}\frac{\partial o_2}{\partial w_{7}}\frac{\partial l}{\partial o_{2 } } + \frac{\partial h_{1}}{\partial w_{1}}\frac{\partial w_5}{\partial h_{1}}\frac{\partial o_1}{\partial w_{5}}\frac{\partial l}{\partial o_{1 } }",39,39,2018-03-10T16:15:33.413,2018-03-10T16:15:33.413,how is gradient calculated for middle layer weights ?,neural-networks backpropagation gradient-descent,2,5,1
1390,5582,1,,2018-03-08T21:13:19.673,0,53,"in multivariate linear regression ( linear regression with more than one variable ) the model is $ yi = b_0 + b_1x_{1i } + b_2x_{2i } + ... $ , and so on . but how is the $ b_n$ value calculated iteratively ? can it be calculated non - iteratively ? what is the intuition behind using that method to calculate $ b_2 $ ?",13202,2444,2019-04-01T13:43:29.063,2019-04-02T18:31:08.577,"in the multi - linear regression , how is the value of weight $ b_2 $ calculated ?",machine-learning learning-algorithms linear-regression,2,0,
1391,5586,1,,2018-03-08T23:48:42.083,2,144,"i am wondering , when one uses neat to evolve best fitting network for the job , does training take place in each epoch as well ? if i understand correctly , training is adjustment of weights in the gates via back propagation process . during neat say a generation runs through 1000 iterations . during that time , is there any training involved , or does each genome randomly poke around and the winner takes it to the next stage ? i am wondering because i use neat and somehow not training networks in the process is not logical to me , but at the same time i ca n't find any code in my framework(neataptic.js ) that would train the generation during the epoch .",13138,,,2018-03-09T19:53:14.837,does training happen during neat ?,neat,1,0,1
1392,5589,1,5598,2018-03-09T06:19:15.997,-1,156,"i know that this question is very common but i am very confused about where to start ? first let me introduce my self i am php developer and i have 3 years of experience in programming now i want to move my career into ai for that i have also learned basic of python and also started learning linear algebra . i have also research many posts to start career in ai but most of posts path are not same like : -in 1 post i have found that i should learn python first , then linear algebra , probability , statistics and then i need to take any course of ml from udemy or any other study platform . -in another post i found that math is really no required at beginning phase so i need to jump directly to ml course . different people to different views that 's common thing i know they all are right for different perspective . can anyone give a guideline from where i should start because i am a programmer i do n't need to learn analytics and all other non programming field . thanks .",13209,,,2018-03-09T18:11:23.983,where and how to start machine learning journey ?,machine-learning getting-started,1,2,1
1393,5593,1,5603,2018-03-09T07:29:44.073,6,424,"what are the advantages/ strengths and disadvantages / weakness of programming languages like common lisp , python and prolog ? why are these languages used in the domain of artificial intelligence ? what type of problems related to ai are solved using these languages ? please , give me link to papers or books regarding the mentioned topic .",12803,2444,2019-03-01T20:46:15.730,2019-03-01T20:46:15.730,"why is common lisp , python and prolog used in artificial intelligence ?",programming-languages,3,3,5
1394,5601,1,5621,2018-03-09T10:45:31.297,8,555,"so , currently the most commonly used activation functions are re - lu 's . so i answered this question what is the purpose of an activation function in neural networks ? and while writing the answer it struck me , how exactly can re - lu 's approximate non - linear function ? by pure mathematical definition , sure , its a non - linear function due to the sharp bend , but if we confine ourselves to the positive or the negative portion of the x - axis only , then its linear in those regions . let 's say we take the whole x - axis also , then also its kinda linear ( not in strict mathematical sense ) in the sense that it can not satisfactorily approximate curvaceous functions like sine wave ( 0 --&gt ; 90 ) with a single node hidden layer as is possible by a sigmoid activation function . so what is the intuition behind the fact that re - lu 's are used in nn 's , giving satisfactory performance ( i am not asking the purpose of re - lu 's ) even though they are kind of linear ? or are non linear functions like sigmoid and tanh thrown in the middle of the network sometimes ? edit : as per @eka 's comment re - lu derives its capability from discontinuity acting in the deep layers of neural net . does this mean that re - lu 's are good as long as we use it in deep nn 's and not a shallow nn ?",9947,9947,2018-03-10T03:34:28.620,2018-03-15T23:32:45.080,mathematical intuition for the use of re - lu 's in machine learning,neural-networks machine-learning,1,3,1
1395,5602,1,5648,2018-03-09T12:46:33.607,1,165,"i 'm a software developer who keeps trying ( and failing ) to get my head around ai and neural networks . there is one area that sparked my interest recently - simulating a mouse "" homing in "" on a piece of cheese by following the smell . based on the rule that moving closer to the cheese = stronger smell = good , then it feels like it should be quite a simple problem to solve - in theory at least ! my thought process was to start by placing the mouse and cheese in random positions on the screen . i would then move the mouse one step in a random direction and measure its distance to the cheese , and if it 's closer than before ( stronger smell ) then that 's good . this is where i come unstuck on the theory - this "" feedback "" somehow needs to modify the mechanism used to move the mouse , gradually refining it until the mouse is able to head straight towards the cheese . once "" trained "" , i should be able to reposition the cheese and expect the mouse to travel to it more quickly . note i 'm also keeping things simple by not having obstacles for the mouse to negotiate around . how on earth would this be implemented with a nn ? i understand the basic concepts , but i find that things unravel once i start looking at real code ! the examples i 've seen typically start by training the nn from a data set , but this does n't seem to apply here as it feels like the only training available is "" on the fly "" as the mouse moves around ( i.e. closer = good , further away = bad ) . i 'm assuming the brain has some kind of "" reward mechanism "" triggered by a stronger smell of cheese . am i barking up the wrong tree - either with my thought process , or nn not being a good fit for this problem ? this is n't homework btw , just something that i 've been puzzling over in the back of my mind .",13216,1671,2018-03-09T18:38:37.207,2018-03-12T16:59:26.180,"neural network to control movement and "" home in "" on a target",neural-networks getting-started,1,1,1
1396,5606,1,5607,2018-03-09T19:41:52.757,4,149,"i 'm having trouble wrapping my head around some details of neural nets and back prop . for example 's sake , consider the following net , where i have separated the ' neurons ' into linear nodes plus activation ( in this case sigmoid ) nodes , more like a general computation graph . l2 is the squared loss function . this is a 3 part question ( parts 1 and 2 disregarding linear algebra / vectorization ) . 1 . i want to confirm that i can not just apply chain rule all the way from the l2 to the inputs and get the derivatives for weights at different layers . for example : but for w5 , at o1 i took the derivative with respect to w5 . but for w1 , at o1 i had to instead take the derivative with respect to ah1 to keep moving backwards . 2 . i want to confirm that at any node that branches into more than one node at forward time ( i.e. ah1 ) , when i back prop , i have to add the derivatives of all it 's branches : in this case do1 / dah1 and do2 / ah1 . 3 . if these 2 things are as i say , then how can i implement backprop in vectorized + linear algebra way without branching or using conditional logic . the forward pass is easy : x1 + x2 are a single input vector . h1 + h2 are a matrix of 2 rows ( number of output / units ) and 2 columns ( number of inputs ) . ah1 + ah2 are a single function that can operate on vectors ( i.e. 1/(1 + np.exp ( ) ) ) the same for the next layer . so to do a forward pass , i just do : l2(y , a_o(w_o @ a_h(w_h @ x ) ) ) ( @ = matrix mul ) but for the back pass not so much , since i 'd have to check which derivative to use at o1 and i 'd have to sum o1 and o2 at ah1 and ah2 . http://neuralnetworksanddeeplearning.com/chap2.html this shows that in theory it can be done by always transposing the previous weight matrix , but i have n't fully understood it yet .",12773,12773,2018-03-10T06:23:58.170,2018-03-10T06:23:58.170,a few doubts on back propagation,neural-networks backpropagation,1,0,1
1397,5614,1,,2018-03-10T07:06:25.170,0,65,"i was trying to build a prediction system where i have the input data arranged in multiple columns . the input data would be of the type where i have weather , service type ( bronze , silver , gold ) , size(xs , s , m , l , xl , xxl ) , time , availability , pin code and the result ( target ) . each of the data types is arranged in columns with a specific code . i have read this , this , this , this , and this . they are helpful but do not give me a clear picture . i would like to achieve multi - vs - one prediction . most of the schemes available are one - vs - one where the data is a 1 * 1 entity . here is a sample code that i was working with : regressionmodel = linear_model.linearregression ( ) "" "" "" 3 . processing is not necessary for current concept "" "" "" y = pd.dataframe(modifieddfset['code ' ] ) print(y.shape ) drop2 = [ ' code ' ] x = modifieddfset.drop(drop2 ) print(x.shape ) "" "" "" 4 . data scaling , data imputation is not necessary . training and test data is prepared using train - test - split "" "" "" train_data , test_data = train_test_split(x , test_size=0.20 , random_state=42 ) "" "" "" 5 . the regression model "" "" "" # h = .02 # step size in the mesh # logreg = linear_model.linearregression ( ) # we create an instance of neighbours classifier and fit the data . regressionmodel.fit(x , y ) d_predictions = regressionmodel.predict(y ) x.shape and y.shape would yield ( 500 , 6 ) & amp ; ( 500 , 1 ) respectively . which would obviously cause a dimensional error in the d_predictions meaning the regression model does not take multiple column inputs . i have a hypothesis that i can create a scoring scheme that will take into account the importance of each of the columns and create a scheme that creates a score and the end result would be a one - vs - one regression problem . looking for some direction with respect to my hypothesis . is it correct , wrong or halfway ? any help and i am grateful already . cheers people .",8215,,,2019-05-04T16:03:00.603,multi vs one prediction using regression,machine-learning linear-regression data-science,1,2,
1398,5617,1,,2018-03-10T11:26:54.217,-1,60,"i have hundreds of .txt files . i need to get into each one of them and remove certain paragraphs that start with specific words but as a whole , are not exactly the same every time . is there an automatic way that can help me clean these parts out ? if yes , what is it ? if not , is it easy / quick to create my own ai tool for this job ? assuming that i need to get this done very soon , does it take a lot of time to learn how to create an ai tool to get the job done for me ? thanks in advance !",13235,9947,2018-03-11T02:00:29.547,2018-03-12T18:45:17.783,automated way to clean lots of .txt files ?,deep-learning automation,1,4,
1399,5619,1,,2018-03-10T13:03:40.733,1,41,"i 'm developing a millitary game and i want soldiers to place themselves in specific order , when commander orders . how can i do this ? should agents learn somehow how to achieve given order or there exist other way ? i 'm new in ai , so i do not really know how can i accomplish my goal . thanks for every advice .",13237,,,2018-03-11T13:11:08.140,placing agents in a specific order on demand,machine-learning game-ai,2,3,
1400,5622,1,,2018-03-10T21:46:12.247,3,77,"i 'm trying to design an orbital rendezvous program for kerbal space program . i 'm focusing on when to launch my spacecraft so that i can end up in the general vicinity of the target . if i can control the ascent profile , the remaining dependent variables are the ship 's twr and the target 's altitude . i want to try a computer learning solution . what is the best way to formulate the problem of learning the time to launch based on some twr ? how can i make an algorithm to compute the general equation of launch time to a altitude based on my ability to accelerate ? what type of learning problem could this be classified as ? what are some approaches to solve problems with known dependent variables ? this may be an obvious question , i kind of expect the answer is regression ? but it seemed a general enough question to be sure about to get a solid foothold in computer learning with this type of problem , which seems to come up a lot .",13243,13243,2018-03-11T11:06:45.627,2019-05-05T18:02:35.523,how can i have a computer learn the equation with known dependent variables ?,machine-learning,1,10,
1401,5625,1,,2018-03-11T07:58:43.857,2,47,"it 's an academic point but according to the definition of a fully observable environment in russell & amp ; norvig , aima ( 2nd ed ) p41 - 44 , an environment is only fully observable if it requires zero memory for an agent to perform optimally . i.e. all relevant information is immediately available from sensing the environment . from this definition and from the definition of a "" episodic "" environment in the same book , it 's implied that all fully observable environments are in - fact episodic or can be treated as episodic . which does n't seem intuitive , but logically follows from the definitions . also no stochastic environment can be fully observable , even if the entire state space at a given point in time can be observed because rational action may depend on previous observation that must be remembered . am i wrong ?",13250,,,2018-03-11T08:50:45.220,all fully observable environments are episodic according to russell norvig 2nd ed,intelligent-agent russell-norvig,1,0,
1402,5629,1,5647,2018-03-11T13:13:01.843,2,500,"came across this line while reading the original paper on spatial transformers by deepmind in the last paragraph of sec 3.1 : the localisation network function floc ( ) can take any form , such as a fully - connected network or a convolutional network , but should include a final regression layer to produce the transformation parameters θ . i understand what regression is but what is meant by a regression layer ?",13255,,,2018-03-12T13:55:03.750,what is regression layer in a spatial transformer ?,neural-networks deep-learning deepmind,1,0,
1403,5638,1,5650,2018-03-12T01:49:01.093,2,204,"i used to treat back propagation as a black box but lately i want to understand more about it . i have used mattmuzr 's and dutta 's explanaiton as a guide to hand compute a simple neural network . i have computed feed forward and back propagation to a network similar to this one with one input , one hidden and one output here are my computations is my computations correct ? * full latex code",39,39,2019-04-05T05:43:07.250,2019-04-05T05:43:07.250,hand computing feed forward and back propagation of neural network,neural-networks backpropagation,1,12,
1404,5645,1,,2018-03-12T10:23:44.230,2,221,"i 'm working on a q - learning project that involves a "" robot "" solving a maze , and there is a problem with how i update the q values ( every time the robot ends up switching between two squares instead of actually learning ) but i 'm not sure where : i am at my wits end . any pointers are welcome , here is the minimal viable example ( i really ca n't condense it much more ) .. thanks ! from enum import enum import numpy as np from random import randrange import string import random class direction(enum ) : up=0 down=1 left=2 right=3 stepstaken=0 nbmaxsteps=500 q = { } gamma=0.95 strat=1 epsilon=0.99 maze= [ ] penalty=0 # values of each movement step=-1 steptrap=-20 stepexit=500 stepwall=-100 # current position of the robot position=[0 , 0 ] # funciton that checks if a certain place in the q matrix is empty , returns 1 if it is def currentqempty ( ) : global q global position moves= [ ] if ( position[0]!=0 ) : moves.append(direction.left ) if ( position[0]!=cols-1 ) : moves.append(direction.right ) if ( position[1]!=0 ) : moves.append(direction.down ) if ( position[1]!=rows-1 ) : moves.append(direction.up ) for d in moves : if ( q.get((position[0],position[1],d),'a')=='a ' ) : return 1 return 0 # intialise the q matrix cols=10 rows=10 values = np.zeros((rows , cols ) ) for x in range(rows ) : for y in range(cols ) : for dir in direction : q[(x , y , dir ) ] = 0 # fills the q matrix ( replaces empty values only ) def qfill(moves ) : global maze global position global q global step global steptrap global stepwall global stepexit global gamma for d in moves : reward=0 newpos = position if d==direction.up : newpos=[position[0 ] , position[1]+1 ] if d==direction.down : newpos=[position[0 ] , position[1]-1 ] if d==direction.left : newpos=[position[0]-1 , position[1 ] ] if d==direction.right : newpos=[position[0]+1 , position[1 ] ] reward = reward+values[newpos[0],newpos[1 ] ] if(q.get((position[0],position[1],d),0)==0 ) : q[position[0],position[1],d]=reward # qmove : decides which move to make depending on current q values # this is where the issue is ! def qmove(moves ) : global position global q global step global steptrap global stepwall global stepexit global gamma bestd=0 newd = moves[random.randint(0,len(moves)-1 ) ] for d in moves : newpos = position if d==direction.up : newpos=[position[0 ] , position[1]+1 ] if d==direction.down : newpos=[position[0 ] , position[1]-1 ] if d==direction.left : newpos=[position[0]-1 , position[1 ] ] if d==direction.right : newpos=[position[0]+1 , position[1 ] ] # update value to best value of new position if q.get((newpos[0],newpos[1],d),0)&gt;=q.get((newpos[0],newpos[1],bestd),0 ) : bestd = d q[position[0],position[1],d]=q.get((position[0],position[1],d),0)+ ( values[newpos[0]][newpos[1 ] ] + gamma * q.get((newpos[0],newpos[1],bestd),1 ) - q.get((position[0],position[1],d),0 ) ) # update arrow if q.get((position[0],position[1],d),0)&gt;q.get((position[0],position[1],newd),0 ) : newd = d return newd # create maze ch=['0 ' , ' 1 ' , ' 3 ' ] for i in range(cols ) : maze.append([0]*(cols ) ) for j in range(cols ) : random_index = randrange(0,len(ch ) ) maze[i][j]=ch[random_index ] if i==cols-1 and j==cols-1 : maze[i][j]='5 ' if i==0 and j==0 : maze[i][j]='0 ' if(maze[i][j]==""1 "" ) : values[i][j]=step elif(maze[i][j]==""0 "" ) : values[i][j]=stepwall elif(maze[i][j]==""3 "" ) : values[i][j]=steptrap else : values[i][j]=stepexit # move while(stepstaken&lt;nbmaxsteps ) : moves= [ ] # if he finishes he starts over if(position[0]==rows-1 and position[1]==cols-1 ) : position[0]=0 position[1]=0 penalty=0 # identify the moves he can legally make if ( position[0]!=0 ) : moves.append(direction.left ) if ( position[0]!=cols-1 ) : moves.append(direction.right ) d = moves[0 ] if ( position[1]!=0 ) : moves.append(direction.down ) if ( position[1]!=rows-1 ) : moves.append(direction.up ) dest= [ ] # choose epsilon value rand = random.uniform(0 , 1 ) if(rand&lt;epsilon**stepstaken ) : strat=1 # explore else : strat=2 # exploit # print(epsilon**stepstaken ) if(currentqempty ( ) or strat==1 ) : qfill(moves ) d = moves[random.randint(0,len(moves)-1)]#how and why he moves print('dumb ' ) else : d = qmove(moves ) print('smart ' ) if(d==direction.left ) : dest.append(position[0]-1 ) # x decreases by 1 place dest.append(position[1 ] ) # y does not change if(d==direction.right ) : dest.append(position[0]+1 ) # x increases by 1 place dest.append(position[1 ] ) # y does not change if(d==direction.up ) : dest.append(position[0 ] ) # x does not change&amp;&amp ; dest.append(position[1]+1 ) # y increases by 1 if(d==direction.down ) : dest.append(position[0 ] ) # x does not change dest.append(position[1]-1 ) # y decreases by 1 # penalty is calculated penalty = penalty+values[dest[0]][dest[1 ] ] if(maze[dest[0]][dest[1]]!='0 ' ) : # not a wall position = dest stepstaken = stepstaken+1 # show q matrix x = position[0 ] y = position[1 ] print(""x:"",x , "" y:"",y ) print ( "" up:%s "" % q.get((x,y , direction.up ) ) ) print ( "" down:%s "" % q.get((x,y , direction.down ) ) ) print ( "" left:%s "" % q.get((x,y , direction.left ) ) ) print ( "" right:%s\n "" % q.get((x,y , direction.right ) ) )",12940,12940,2018-03-12T12:37:53.193,2019-04-04T18:36:45.567,q - learning in python,python q-learning,1,6,1
1405,5646,1,,2018-03-12T13:09:09.390,2,22,"how could we express the following as strips operators ? a computer has memory cells m1 , ... , mn and registers r1 , ... , rm . two computer instructions could be : load(m , r ) ( copy contents of m into r , overwriting what ’s there ) add(m , r ) ( add contents of m to contents of r , leaving result in r )",13269,,,2018-03-12T13:09:09.390,how can one express these instructions as strips operators ?,path-planning,0,4,
1406,5652,1,,2018-03-12T18:50:49.307,2,75,"i 'm new to ai development and am looking for a quality algorithm ( potentially nlp ? ) implementation proved against us legal texts . obviously some training would need to be done , but i 've found little to no online references to go on when it comes to running assessment against us legal documents . my goal is to use an algorithm to discover potential issues in long and complex legal texts , or associated ( groups ) of legal texts which bind one or more related entities ( people or corporations ) to potentially conflicting clauses . just a pointer in some kind of direction would be helpful .",13043,,,2018-03-12T18:50:49.307,nlp proved against us legal texts,neural-networks ai-design natural-language-processing legal,0,4,1
1407,5656,1,5657,2018-03-13T03:57:18.043,0,77,"i am currently learning c # any advice with this language about courses or should use another programming language , preferred languages ​​spanish or english",12179,,,2018-03-13T06:43:31.920,it is advisable to use c # to start in the world of ai,programming-languages spanish-language,1,3,0
1408,5658,1,5668,2018-03-13T10:04:26.173,3,47,"this is a theoretical question . i am newbie to artificial intelligence and machine learning , and the more i read the more i like this . so far , i have been reading about evaluation of language models ( i am focused on asr ) , but i still do n't get the concept of development test . the clearest explanation i have come across is the following "" sometimes we use a particular test set so often that we implicitly tune to its characteristics . we then need a fresh test set that is truly unseen . in such cases , we call the initial test set the development test set or , devset "" nevetheless i have not found sense as for why an additional test has to be used . in other words , why are n't training and test sets enough ? thanks in advance !",13291,4302,2018-10-08T12:48:54.253,2018-10-08T12:48:54.253,what are development tests used for ?,natural-language-processing intelligence-testing voice-recognition,1,1,
1409,5665,1,,2018-03-13T14:41:45.543,3,94,interested to know if there was any use or interest in activation functions with more than one output value to the next column instead of single firing . i 'm interested to know if this would have any use or would be almost identical to a single value firing .,11893,4302,2018-08-12T02:54:24.493,2018-08-15T20:32:34.323,anns with multiple activation outputs,neural-networks machine-learning activation-function,2,3,
1410,5666,1,5667,2018-03-13T14:44:20.237,0,245,"i want to learn a policy network for a domineering game for each position i have to recall the input ( i.e. the board , the flipped board , and the turn ) and the output of the same size as the board with only the best move found by the monte carlo evaluation marked as 1 . for instance csv lines for a 2x2 board : 0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0 which corresponds to board , flipped board , player plane , move to learn corresponding input tensor : 0 0 1 1 1 1 0 0 1 1 1 1 corresponding output tensor : 1 0 0 0 from here i have a 8 * 8 board games database . with this tutorial i already developed a failing neural network . indeed i did 12 nodes for 128 inputs there seems to be a problem with the first layer model.add(dense(12 , input_dim=128 , activation='relu ' ) ) . # model construction model = sequential ( ) model.add(dense(12 , input_dim=128 , activation='relu ' ) ) model.add(dense(128 , activation='relu ' ) ) model.add(dense(128 , activation='sigmoid ' ) ) print(""compile "" ) model.compile(loss='binary_crossentropy ' , optimizer='adam ' , metrics=['accuracy ' ] ) print(""fit "" ) model.fit(x_test , y_test , epochs=3 , batch_size=10 ) in effect it gives me the following error value : valueerror : error when checking target : expected dense_27 to have shape ( 128 , ) but got array with shape ( 127 , ) and when i try to replace with 127 , just to see , it says : valueerror : error when checking input : expected dense_28_input to have shape ( 127 , ) but got array with shape ( 128 , ) here is the entire code , which you can get on github as well . it 's the ipython notebook . # ! /usr / bin / env python3 from timeit import default_timer as timer import csv import pandas as pd import numpy as np import matplotlib.pyplot as plt plt.style.use(""ggplot "" ) from keras.models import sequential from keras.layers import dense # we divide data.csv into train and tests with open(""data.csv "" , ' r ' ) as f : plays = np.array(list(csv.reader(f , delimiter= "" , "" ) ) ) print(plays.shape ) # we take the 126 first columns as input df = pd.dataframe(data=plays[0:28961,1:256 ] ) # we take the 126 last columns as output y = pd.dataframe(data=plays[0:28961,129:256 ] ) # plays.reshape((64,64 ) ) # board = np.reshape(plays , ( 8 , 8) ) df['split ' ] = np.random.randn(df.shape[0 ] , 1 ) msk = np.random.rand(len(df ) ) & lt;= 0.7 train_df = df[msk].fillna(""sterby "" ) test_df = df[~msk].fillna(""sterby "" ) # we take the 128 first columns has input x_train = train_df.iloc[:,0:128].values # we take the 128 last columns has input y_train = train_df.iloc[:,129:].values x_test = test_df.iloc[:,0:128].values y_test = test_df.iloc[:,129:].values # necesary keras importations from keras.preprocessing import sequence from keras.models import model , input from keras.layers import dense , embedding , globalmaxpooling1d from keras.preprocessing.text import tokenizer from keras.optimizers import adam # model construction model = sequential ( ) model.add(dense(12 , input_dim=128 , activation='relu ' ) ) model.add(dense(128 , activation='relu ' ) ) model.add(dense(128 , activation='sigmoid ' ) ) print(""compile "" ) model.compile(loss='binary_crossentropy ' , optimizer='adam ' , metrics=['accuracy ' ] ) print(""fit "" ) model.fit(x_test , y_test , epochs=3 , batch_size=10 ) print(""evaluate "" ) # evaluate the model scores = model.evaluate(x_test , y ) print(""\n%s : % .2f%% "" % ( model.metrics_names[1 ] , scores[1]*100 ) ) and the error : compile fit --------------------------------------------------------------------------- valueerror traceback ( most recent call last ) & lt;ipython - input-60 - 8b0dc569bd70&gt ; in & lt;module&gt ; ( ) 3 4 print(""fit\n "" ) ----&gt ; 5 model.fit(x_test , y_test , epochs=3 , batch_size=10 ) 6 7 # evaluate the model /usr / local / lib / python3.5 / dist - packages / keras / models.py in fit(self , x , y , batch_size , epochs , verbose , callbacks , validation_split , validation_data , shuffle , class_weight , sample_weight , initial_epoch , steps_per_epoch , validation_steps , * * kwargs ) 961 initial_epoch = initial_epoch , 962 steps_per_epoch = steps_per_epoch , --&gt ; 963 validation_steps = validation_steps ) 964 965 def evaluate(self , x = none , y = none , /usr / local / lib / python3.5 / dist - packages / keras / engine / training.py in fit(self , x , y , batch_size , epochs , verbose , callbacks , validation_split , validation_data , shuffle , class_weight , sample_weight , initial_epoch , steps_per_epoch , validation_steps , * * kwargs ) 1628 sample_weight = sample_weight , 1629 class_weight = class_weight , -&gt ; 1630 batch_size = batch_size ) 1631 # prepare validation data . 1632 do_validation = false /usr / local / lib / python3.5 / dist - packages / keras / engine / training.py in _ standardize_user_data(self , x , y , sample_weight , class_weight , check_array_lengths , batch_size ) 1478 output_shapes , 1479 check_batch_axis = false , -&gt ; 1480 exception_prefix='target ' ) 1481 sample_weights = _ standardize_sample_weights(sample_weight , 1482 self._feed_output_names ) /usr / local / lib / python3.5 / dist - packages / keras / engine / training.py in _ standardize_input_data(data , names , shapes , check_batch_axis , exception_prefix ) 121 ' : expected ' + names[i ] + ' to have shape ' + 122 str(shape ) + ' but got array with shape ' + --&gt ; 123 str(data_shape ) ) 124 return data 125 valueerror : error when checking target : expected dense_12 to have shape ( 128 , ) but got array with shape ( 127 , )",4738,,,2018-03-13T16:02:18.313,what dimension to give to a neural network intputs ?,neural-networks keras python,1,1,
1411,5670,1,5671,2018-03-14T06:11:01.467,2,181,"the result of gradient descent algorithm is a vector . so how does this algorithm decide the direction for weight change ? we give hyperparameters for step size . but how is the vector direction for weight change , for the purpose of reducing the loss function in a linear regression model , determined by this algorithm ?",13169,1671,2018-03-15T21:44:58.607,2018-03-16T10:39:42.203,how is direction of weight change determined by gradient descent algorithm,machine-learning linear-regression gradient-descent,2,1,2
1412,5672,1,,2018-03-14T08:31:11.863,2,103,the lithographs of dutch artist m.c . escher have been used in the study of artificial intelligence . how can the human mind incorporate these optical illusions into abstract thought ? is this reverse artificial intelligence ?,13302,,,2018-07-01T21:38:57.317,m.c.escher and abstract thought,machine-learning reinforcement-learning,1,3,2
1413,5682,1,5683,2018-03-14T13:13:25.470,2,266,"i am trying to understand the difference between the workings biological evolution and artificial evolution . if we look at it in terms of genetics , in both of them , selection is the key term , natural selection in biological way and selection as in genetic algorithms than what 's the difference in between artificial and biological evolution ?",12806,9947,2018-03-14T13:42:40.997,2018-03-14T17:39:48.160,what 's the difference in between biological evolution and artificial evolution ?,genetic-algorithms evolutionary-algorithms biology,2,4,
1414,5688,1,,2018-03-15T08:26:48.680,3,65,"i am not clear with the concept that an unsupervised model learns . we are giving an input and output to the supervised model so that it can generate a particular value , pattern or something out of it which can be used to categorize something in the future . by contrast , in unsupervised learning we are clustering and all so why do we need learning ? can anyone detail me with some real world examples ?",13326,1671,2018-03-15T21:44:00.617,2018-03-15T21:44:00.617,learning in unsupervised learning,unsupervised-learning,2,0,1
1415,5694,1,5695,2018-03-15T09:42:21.513,3,76,"i asked my self this simple question while reading "" comment abuse classification with deep learning "" by chu and jue . indeed , they say at the end of the that it is clear that rnns , specifically lstms , and cnns are state - of - the - art architectures for sentiment analysis to my mind cnns were only neurons arranged so that they correspond to overlapping regions when paving the input field . it was n't that recurrent at all .",4738,4738,2018-03-15T09:50:53.693,2018-03-16T18:54:33.520,are convolutional neural networks recurrent neural networks ?,convolutional-neural-networks recurrent-neural-networks,2,2,2
1416,5703,1,5704,2018-03-15T23:53:52.083,1,78,"i 'm reading the alexnet paper . in the section 4 where the authors explain how they prevent overfitting , they mention "" although the 1000 classes of ilsvrc make each training example impose 10 bits of constraint on the mapping from image to label , ~ "" . what does this mean ?",4656,,,2018-03-16T03:11:44.243,question about the alexnet paper,machine-learning deep-learning,2,0,
1417,5708,1,5710,2018-03-16T14:44:31.967,1,94,"i was reading ai for humans vol . 1 by jeff heaton when i came across the terms "" equilateral encoding "" and "" one - of - n encoding . "" the explanations unfortunately made no sense to me and the reddit threads on the web are blocked by my internet provider ( i use a high - school machine ) . is anyone here able to provide basic explanations regarding the two procedures for me ? thanks in advance .",12950,,,2018-03-16T15:23:38.620,equilateral and one - of - n encoding,ai-basics,1,0,
1418,5709,1,,2018-03-16T15:14:20.827,2,73,"update : the tables look messed up so i put them on pastebin for better visibility . https://pastebin.com/gdx28uvf i am using neural network with different learning types ( for example standard backpropagation ) to classify trends in time series . as stated in several papers , data normalization is a very important factor for successful / efficient learning . i am trying to be clear and precise as possible in the description . problem / learning goal : the network gets trained with time series and 2 indicators to predict a specific cluster . here is a very simple ( madeup ) example of raw data to understand the problem : example raw data timestamp;densityx;wavelengthy;temperature ( k ) 1;0.1;2;200 2;0.9;3;150 3;-0.5;1;175 4;0;6;154 5;1;8;155 6;1.3;1.5;220 7;-0.5;3.4;250 8;0.2;2;255 9;0.1;1;180 see https://pastebin.com/gdx28uvf for better visual i use the following process to generate suitable sample data for training : the neural network receives n time slices with the indicators and tries to check if a future trend in the temperature occurs ( for x future time slices ) . for example n = 2 ; x=3 . the input and output are defined as follows : input vector : in1 = density_(t-2 ) in2 = wavelength_(t-2 ) in3 = density_(t-1 ) in4 = wavelength_(t-1 ) output vector : output vector is a classification encoded by effects encoding or dummy encoding ( details in “ neural networks using c # succinctly ” ) calculation : classification “ down ” : temperature drops 3 times in a row ( encoded as 0;1 ) classification : “ stable ” : temperature does neither drop nor raises 3 times in a row ( 1;0 ) classification : “ up ” : temperature raises 3 times in a row . ( -1;-1 ; ) so the “ processed ” training sample would look like this : processed data pattern;i1;i2;i3;i4;o1;o2;class;used ts 1;0.1;2;0.9;3;0;1;down;1 to 5 2;0.9;3;-0.5;1;-1;-1;up;2 to 6 3;-0.5;1;0;6;-1;-1;up;3 to 7 4;0;6;1;8;1;0;stable;4 to 8 5;1;8;1.3;1.5;1;0;stable;5 to 9 see https://pastebin.com/gdx28uvf for better visual as you can see due to the different indicators ranges i want to normalize the data . basically i found the following propositions in literature and research : min / max normalization requires the following values to calculate - datahigh : the highest unnormalized observation . datalow : the lowest unnormalized observation . normalizedhigh : the high end of the range to which the data will be normalized . normalizedlow : the low end of the range to which the data will be normalized . reciprocal normalization every value is processed to its reciprocal ( x=1 / x ) . calculated values for density x would be : timestamp;reciprocal density 1;10 2;1.111111111 3;-2 4;#div/0 ! 5;1 6;0.769230769 7;-2 8;5 9;10 see https://pastebin.com/gdx28uvf for better visual percentage normalization the percentual delta is calculated using the value from the previous time stamp . the starting point was timestamp 1 where the delta equals 0 . for each timestamp the delta percentage is calculated evaluating the previous value . so calculating the time series delta percentages would turn out to : timestamp;""delta density x "" 1;0 2;0.9 3;-0.555555556 4;0 5;#div/0 ! 6;1.3 7;-0.384615385 8;-0.4 9;0.5 see https://pastebin.com/gdx28uvf for better visual as you can see there are errors with handling zero values and the range is still a problem in my opinion . the min / max approach is generally leads to a good normalization but i think there is a problem as well , because live data may breach the max and min values of the training set . my questions are : what are your thoughts about the general idea how i process the raw data ? how would you normalize the given data – if at all ? a ) does it make sense for minmax normalization to propose a min max value which will include live data ( and throw some error in case it happens ) b ) how to handle 0 values ( maybe convert it to a small positive or negative value ? are there other ideas or concepts to conduct this problem ? i am looking forward to your input . everything is appreciated . thanks in advance ! i also apologize for errors in the example values . anyways , thanks for your time . cheers , hob .",13352,13352,2018-03-17T22:23:10.093,2018-03-17T22:23:10.093,classification learning - normalization of time series and live usage,neural-networks training datasets,0,4,
1419,5713,1,,2018-03-16T19:16:50.503,0,96,could you implement code into an ai that ca n't be modified ? like if you place a code that shuts - down the program / machine would they be able to rewrite/ reinterpret the ideas ?,7801,,,2018-03-18T21:24:49.933,ai self - destruct button,ai-design,1,1,
1420,5715,1,,2018-03-17T03:55:06.380,6,193,"i 'm new to neural network , i study electrical engineering , and i just started working with adalines . i use matlab , and in their documentation they cite : however , here the lms ( least mean squares ) learning rule , which is much more powerful than the perceptron learning rule , is used . the lms , or widrow - hoff , learning rule minimizes the mean square error and thus moves the decision boundaries as far as it can from the training patterns . the lms algorithm is the default learning rule to linear neural network in matlab , but few days later i came across another algorithm which is : recursive least squares ( rls ) in a 2017 research article by sachin devassy and bhim singh in the journal : iet renewable power generation , under the title : performance analysis of proportional resonant and adaline - based solar photovoltaic - integrated unified active power filter where they state : adaline - based approach is an efficient method for extracting fundamental component of load active current as no additional transformation and inverse transformations are required . the various adaptation algorithms include least mean square , recursive least squares etc . my questions are : is rls just like lms ( i mean can it be used as a learning algorithm too ) ? if yes , how can i customize my adaline to use rls instead of lms as a learning algorithm ( preferably in matlab , if not in python ) because i want to do a comparative study between the two algorithm !",13361,13361,2018-03-17T04:21:14.493,2018-03-17T04:21:14.493,can we use the recursive least squares as a learning algorithm to an adaline ?,learning-algorithms linear-regression,0,4,
1421,5716,1,,2018-03-17T04:20:21.737,1,228,"i want to use a custom loss function which is a weighted combination of l1 and dssim losses . the dssim loss is limited between 0 and 0.5 where as the l1 loss can be orders of magnitude greater and is so in my case . how does backpropagation work in this case ? for a small change in weights , the change of the l1 component would obviously always be far greater than the ssim component . so , it seems that only l1 part will affect the learning and the ssim part would almost have no role to play . is this correct ? or i am missing something here . i think i am , because in the dssim implementation of keras - contrib , it is mentioned that we should add a regularization term like a l2 loss in addition to dssim ( https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/dssim.py ) ; but i am unable to understand how it would work and how the ssim would affect the backpropagation being totally overshadowed by the large magnitude of the other component . it will be a great help if someone can explain this . thanks .",12754,,,2018-03-17T04:20:21.737,how does backpropagation work on a custom loss function whose components have magnitudes of different orders ?,convolutional-neural-networks backpropagation keras,0,0,
1422,5717,1,5726,2018-03-17T05:46:34.397,1,118,"i am ( planning on ) using the microsoft bot framework to build a chatbot i am working on . however i simply can not wrap my head around why the chatbot needs to be registered with azure . my line of expectation going into this was the same as any other .net project . i build it , i deploy it to my own server and i live happily ever after . i have bought two books on the subject and yet after going through online videos , the books and articles i still can not see a good reason for why one is forced to register the bot . i feel like lisa simpson in that episode of the simpsons where there is a puzzle that is super easy and everyone understands it but her . i know i am being blinded by my own assumptions and expectations going into this . if anyone can explain the ' why ' of the registration that will really help . i am also wondering if azure went down like aws did if that would stop the bot working ? my backup plan is just to write the bot using asp.net web api and then connect to it from a web container and facebook . neither of which needs the registration . thanks , -mike",12788,,,2018-03-18T06:39:46.347,why do you have to register a bot with azure ?,ai-basics chat-bots,1,0,
1423,5720,1,5722,2018-03-17T17:00:51.140,4,1058,"i 've seen numerous mathematical explanations of reward , v(s ) value functions , and returns functions . the reward provides an immediate return for being in a specific state . the better the reward , the better the state . as i understand it , it can be better to be in a low - reward state sometimes because we can accumulate more long term which is where the expected returns function comes in . an expected return , return or cummulative reward function effectively adds up the rewards from the current state to the goal state . this implies it 's model - based . however it seems a value function does exactly the same ? is a value function a return function ? or are they different ?",12726,12726,2019-02-15T10:25:11.403,2019-02-15T10:25:11.403,difference between expected return and value function in reinforcement learning,reinforcement-learning,1,9,
1424,5728,1,5730,2018-03-18T11:26:55.320,12,5724,"suppose that a nn contains $ n$ hidden layers , $ m$ training examples , $ x$ features , and $ n_i$ nodes in each layer . what is the time complexity to train this nn using back - propagation ? i have a basic idea about how they find the time complexity of algorithms , but here there are 4 different factors to consider here i.e. iterations , layers , nodes in each layer , training examples , and maybe more factors . i found an answer here but it was not clear enough . are there other factors , apart from those i mentioned above , that influence the time complexity of the training algorithm of a nn ?",9947,2444,2019-02-21T16:26:59.627,2019-02-21T20:00:14.367,what is the time complexity for training a neural network using back - propagation ?,neural-networks machine-learning backpropagation time-complexity,2,0,6
1425,5729,1,,2018-03-18T13:54:39.473,-1,37,"i have seen weka j48 classifier , i want to build a classifier similar to it but i do n't know how to go about it . can anyone advice me on how to create a classifier algorithm for decision tree ?",14382,,,2018-03-18T18:28:34.617,decision tree classifier,classification,1,4,
1426,5738,1,5744,2018-03-18T21:59:09.460,3,212,"in a convolutional neural network ( cnn ) , since the rgb values get multiplied in the first convolutional layer , does this mean that color is essentially only extracted in the very first layer ? snippets from stanford cs231n chapter on cnn : [ ... ] one dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs , which can indicate dead filters , and can be a symptom of high learning rates [ ... ] typical - looking activations on the first conv layer ( left ) , and the 5th conv layer ( right ) of a trained alexnet looking at a picture of a cat . every box shows an activation map corresponding to some filter . notice that the activations are sparse ( most values are zero , in this visualization shown in black ) and mostly local .",14389,1671,2018-03-19T20:35:50.793,2018-03-19T20:41:58.623,is color information only extracted in the first input layer of a convolutional neural network ?,neural-networks deep-learning convolutional-neural-networks image-recognition ai-basics,1,2,
1427,5739,1,,2018-03-19T01:35:47.950,0,34,"say you follow a tutorial on the tensorflow website for a wide and deep model ( https://www.tensorflow.org/tutorials/wide_and_deep ) i create a model based on the us census data to predict whether or not an individual will make more or less than \$50k given a number of features like age , education , profession , etc . i 've been able to create the model as well as create a predictor that uses the model just fine . but is there a way to see what features tensorflow is "" weighing "" more than others ? for instance , does it weigh a higher education more than someones age ? ( i.e. if someone has a phd and is 26 years old , is the model more likely to say they make more than \$50k vs someone who has an associates degree and is 55 years old ? ) i 'm using a dnnlinearcombinedclassifier if it matters to this question .",11667,21109,2019-02-01T13:33:33.053,2019-02-01T13:33:33.053,"being able to see how tensorflow "" weighs "" features in classifier",neural-networks deep-learning tensorflow,0,2,
1428,5741,1,5745,2018-03-19T07:24:40.713,3,382,"given a dataset with no noisy examples ( i.e. , it is never the case that for 2 examples , the attribute values match but the class value does not ) , is the training error for the id3 algorithm is always equal to 0 ?",12519,,,2018-03-21T08:17:25.870,error in decision trees,machine-learning,1,1,1
1429,5755,1,,2018-03-20T13:36:14.327,0,64,right now i 'm planning to make a dnn for classifying taste of crystals with their molecular structure which include information like no of atoms mass of each atoms atomic no of atoms . how should i make a data set for training testing and validation ?,14426,,,2018-03-20T15:03:28.107,how to make training data for ai,classification research datasets ai-community,1,0,
1430,5757,1,,2018-03-20T17:21:26.060,2,99,"i 'm new to the world of machine learning . my question is how can i determine the size of the biases in a neural network ( with backpropagation algorithm ) ? currently , i have a 2-layer neural network ( 1 hidden and 1 output layer ) . here 's the code : import numpy as np from matplotlib import pyplot as plt sigmoid = lambda x : 1 / ( 1 + np.exp(-x ) ) dsigmoid = lambda y : y * ( 1 - sigmoid(y ) ) # this function performs the given function ( func ) to the whole numpy array def mapfunc(array , func ) : newarray = array.copy ( ) for element in np.nditer(newarray , op_flags=['readwrite ' ] ) : element [ ... ] = func(element ) return newarray class neuralnetwork : def _ _ init__(self , input_nodes , hidden_nodes , output_nodes ) : self.input_nodes = input_nodes self.hidden_nodes = hidden_nodes self.output_nodes = output_nodes self.w_ih = np.random.rand(hidden_nodes , input_nodes ) self.w_ho = np.random.rand(output_nodes , hidden_nodes ) self.b_ih = np.random.rand(hidden_nodes , 1 ) self.b_ho = np.random.rand(output_nodes , 1 ) self.learningrate = 0.1 def predict(self , inputs ) : # calculate hidden 's output h_output = np.dot(self.w_ih , inputs ) h_output + = self.b_ih h_output = mapfunc(h_output , sigmoid ) # activation # calculate output 's output o_output = np.dot(self.w_ho , h_output ) o_output + = self.b_ho o_output = mapfunc(o_output , sigmoid ) # activation return o_output def train(self , inputs , target ) : # calculate hidden 's output h_output = np.dot(self.w_ih , inputs ) h_output + = self.b_ih h_output = mapfunc(h_output , sigmoid ) # activation # calculate output 's output o_output = np.dot(self.w_ho , h_output ) o_output + = self.b_ho o_output = mapfunc(o_output , sigmoid ) # activation # calculate output error : o_error = o_output - target # calculate output delta o_gradient = mapfunc(o_output , dsigmoid ) o_gradient = np.dot(o_gradient , np.transpose(o_error ) ) * self.learningrate w_ho_delta = np.dot(o_gradient , np.transpose(h_output ) ) self.w_ho -= w_ho_delta self.b_ho -= o_gradient # calculate hidden error : w_ho_t = np.transpose(self.w_ho ) h_error = np.dot(w_ho_t , o_error ) # calculate hidden delta : h_gradient = mapfunc(h_output , dsigmoid ) h_gradient = np.dot(h_gradient , np.transpose(h_error ) ) * self.learningrate w_ih_delta = np.dot(h_gradient , inputs ) self.w_ih -= w_ih_delta self.b_ih + = h_gradient return o_output n = neuralnetwork(2 , 2 , 1 ) inputs = np.matrix([[1 ] , [ 0 ] , [ 1 ] , [ 1 ] , [ 0 ] , [ 1 ] , [ 0 ] , [ 0 ] ] ) input_list = [ ] input_list.append([[1 ] , [ 0 ] ] ) input_list.append([[0 ] , [ 1 ] ] ) input_list.append([[1 ] , [ 1 ] ] ) input_list.append([[0 ] , [ 0 ] ] ) target = np.matrix([[0 ] , [ 0 ] , [ 1 ] , [ 1 ] ] ) outputs = [ ] for i in range(50000 ) : ind = np.random.randint(len(input_list ) ) inp = input_list[ind ] out = n.train(inp , target[ind]).tolist ( ) outputs.append(out[0][0 ] ) print outputs plt.plot(outputs ) plt.show ( ) newinput = [ [ 1 ] , [ 1 ] ] print ( n.predict(newinput ) ) in the train function , the line self.b_ih + = h_gradient throws me an error about their sizes not being equal . i even tried to make the biases only a single number but that did n't help as it gets changed by h_gradient to a matrix . so , is there something wrong in the bias itself or i did some other step(s ) wrong ?",14433,,,2018-07-19T19:06:09.487,how to determine the size of biases ?,neural-networks backpropagation,1,0,
1431,5762,1,,2018-03-21T13:25:14.100,5,167,my goal is to take an image and return another image that looks as if the scene was viewed from another angle . the difference in angle can be small — let 's say as if the hand holding the camera moved slightly sideways .,14450,9203,2018-03-22T15:54:56.330,2018-03-22T15:54:56.330,algorithms for scene rotation,deep-learning computer-vision,2,0,
1432,5763,1,7383,2018-03-21T14:45:23.513,8,4717,"i did my master 's thesis on deep generative models and i 'm currently looking for a new subject . q : what are the "" hottest "" research topics that are taking a lot of attention of the deep learning community lately ? a few clarifications : i did look through similar questions and none of them answered my question . i come from a pure mathematical background , i only transitioned into deep learning a year ago , and my research on generative models was mostly theoretical . which means , most of my work revolved around structured probabilistic models , and approximate inference . that said , i have yet to explore real world applications of deep learning . i did my homework before posing the question . my goal was to get ai se 's input on the matter and see what people are working on .",12672,1671,2018-03-23T20:26:44.260,2018-10-11T06:35:12.190,what are the latest ' hot ' research topics for deep learning and ai ?,deep-learning ai-field,2,2,8
1433,5768,1,,2018-03-21T20:42:49.653,1,41,"how would you design and handle training nn where the label is 1000++ bit binary ( 50 % ones , 50 % zeros ) . the number of labels can be small or big in different situations . my question : is such scenario well suited for nn ? if that seem like ok how will you design it . for the sake of example let say i 'm training the nn with minst db . i.e. 10 labels . again the important thing is that the label is with the specification i described . i can drop 50 % ones , 50 % zeros requirement if absolutely necessary , but the size of the binary have to stay .",14457,14457,2018-03-22T20:25:51.970,2018-03-22T20:25:51.970,training nn with 1000 + bits binary labels ?,neural-networks ai-basics python,0,0,
1434,5769,1,,2018-03-22T02:36:20.950,24,20741,"my understanding is that the convolutional layer of a convolutional neural network has four dimensions : input_channels , filter_height , filter_width , number_of_filters . furthermore , it is my understanding that each new filter just gets convoluted over all of the input_channels ( or feature / activation maps from the previous layer ) . however , the graphic below from cs231 shows each filter ( in red ) being applied to a single channel , rather than the same filter being used across channels . this seems to indicate that there is a separate filter for each channel ( in this case i 'm assuming they 're the three color channels of an input image , but the same would apply for all input channels ) . this is confusing - is there a different unique filter for each input channel ? source : http://cs231n.github.io/convolutional-networks/ the above image seems contradictory to an excerpt from o'reilly 's "" fundamentals of deep learning "" : "" ... filters do n't just operate on a single feature map . they operate on the entire volume of feature maps that have been generated at a particular layer ... as a result , feature maps must be able to operate over volumes , not just areas "" ... also , it is my understanding that these images below are indicating a the same filter is just convolved over all three input channels ( contradictory to what 's shown in the cs231 graphic above ) :",14389,14389,2018-03-22T23:37:10.023,2019-03-03T17:57:20.323,"in a cnn , does each new filter have different weights for each input channel , or are the same weights of each filter used across input channels ?",deep-learning convolutional-neural-networks image-recognition,7,1,15
1435,5770,1,5780,2018-03-22T05:02:26.713,1,667,"i wanted to start developing a project with image recognition . i want to know the difference between the intel movidius neural compute stick and tensorflow to develop this project any help would greatly be appreciated . what is the difference between them and what is better for accuracy ? thanks , aditya",14462,5763,2018-03-26T21:44:35.460,2018-03-26T21:44:35.460,intel movidius neural stick vs tensorflow,neural-networks image-recognition getting-started software-evaluation,1,2,
1436,5773,1,,2018-03-22T10:22:37.747,1,66,"when recording audio for screencasts or similar , very often the keyboard is clearly visible and can start to annoy listeners after a while . nn are quiet good at recognizing patterns . image classification is all over the place these days . there is also some work on audio , so that seems to work as well . could the following approach therefore work to eliminate ( or greatly reduce ) the sounds of the keyboard in a recording whilst leaving the voice quality largely untouched ? train a nn to recognize the clicking sounds of the keys . lots of labeled data can be created by just recording and tracking key clicks in the millisecond range . that way markers can be placed on the recording automatically that "" label "" clicks from non clicks . let 's say a click has on average a 10ms range in the audio , the audio feed could be cut into snippets of 10ms and those that have a click sound in it are labelled as such . a adversarial network is trained to modify an input stream so as to fool the first one into thinking there are no clicks while also being punished for large changes in the stream data . so the better it removes the clicks sounds the better but if it just gives out nothing ( technically no clicks then ) , it 's of course bad so there needs to be some reward for being "" close to input "" would this be a good approach ? are there other ways to filter this ? i know there is an "" ehm detector "" that uses mdp to warn speakers whenever they are likely to say "" ehm "" . this would n't apply to this though , because it 's not that i want to guess when the next click comes but rather i want to manipulate the input stream without running a constant filter on the entire stream such as a lowpass filter for removing unwanted constant noise . so ideally the algorithm would learn to apply a "" correction stamp "" whenever a click is detected to remove a range of frequencies during small windows in the overall recording but leaving most of it untouched .",11429,,,2018-03-22T10:22:37.747,learning algorithm that filters keyboard clicking in audio feeds,neural-networks machine-learning,0,0,1
1437,5774,1,,2018-03-22T10:48:09.347,5,434,"i read that to compute the derivative of the error with respect to the input of a convolution layer is the same to make of a convolution between deltas of the next layer and the weight matrix rotated by $ 180 ° $ , i.e. something like $ $ with $ * $ convolution operator ; this is valid with $ stride=1 $ ; but what happens when stride is greater than $ 1 $ ? is still a convolution with a kernel rotation or i ca n't make this simplification ?",2189,1641,2019-01-11T13:13:57.003,2019-05-07T21:06:39.653,cnn backpropagation with stride>1,convolutional-neural-networks backpropagation gradient-descent,3,1,1
1438,5776,1,,2018-03-22T11:06:56.577,1,24,"there are many machine learning api for scanning images but they just return a bunch of tags . https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/ { "" tags "" : [ "" train "" , "" platform "" , "" station "" , "" building "" , "" indoor "" , "" subway "" , "" track "" , "" walking "" , "" waiting "" , "" pulling "" , "" board "" , "" people "" , "" man "" , "" luggage "" , "" standing "" , "" holding "" , "" large "" , "" woman "" , "" yellow "" , "" suitcase "" ] , "" confidence "" : 0.833099365 } ] } are there any apis for combining these into a sentence ? ms cognitive vision is the only one that produces a full caption "" captions "" : [ { "" text "" : "" people waiting at a train station "" , google sentiment analysis can split a sentence into grammar parts but is there any api that does the reverse ? https://cloud.google.com/natural-language/docs/basics input : "" train "" , "" platform "" , "" station "" , "" building "" , "" indoor "" , "" subway "" , "" track "" , "" walking "" , "" waiting "" , "" pulling "" , "" board "" , "" people "" , "" man "" , "" luggage "" , "" standing "" , "" holding "" , "" large "" , "" woman "" , "" yellow "" , "" suitcase "" output : "" people waiting at a train station """,14472,4302,2018-09-24T06:30:39.720,2018-09-24T06:30:39.720,commercial api q : is there an api for converting vision tags into a caption ?,machine-learning natural-language-processing computer-vision software-evaluation,0,5,
1439,5781,1,,2018-03-23T12:52:09.840,1,47,"i have a large set of simulation logs for a market simulation of which i want to learn from . the market includes : customers products ( subscriptions ) the customers choose products and then stick with them until they decide on a different one . examples could be phone , electricity or insurance contracts . for every simulation i get the data about the customers ( some classes and metadata ) and then for each round i get signups / withdrawals and charges for the use of the service . i am trying to learn a few things competitiveness of an offering ( in relation to the environment / competition ) usage patterns of customers ( the underlying model is a statistical simulation ) depending on their chosen tariff , time of day and their metadata + historical usage ability to forecast customer numbers for each product the use cases are all very applicable to real world data although my case is all a ( rather large ) simulation . my problem is this : what kind of learning is this ? supervised ? unsupervised ? i have come up with various hypotheses and can not find a definite answer for either . pro supervised : for the usage patterns of the customers i have historical data of actual usage so i can do something similar to time - series forecasting . however , i do n’t want to forecast simply off of their previous usage but also off of their metadata and their tariff choice ( so also metadata in a way ) . pro unsupervised : the forecasting of the “ competitiveness ” of a randomly chosen product configuration is hard to label even with historical data . the exact reason why a product has performed in a certain way is very high - dimensional . i do get subscription records about every product for every time slot though , so i guess some “ feedback ” could be generated . this might also be a rl problem though ? so obviously i need help pulling these different concepts apart so as to map them on this kind of problem which is not the classical “ dog or cat ” problem or the classical “ historical data here , please forecast ” timeseries issue . it ’s also not a “ learn how to walk ” reinforcement problem as it ’s based on historical data . the end goal is however to write an agent that generates these products and competes in the market so that will be a reinforcement problem .",11429,,,2018-03-23T12:52:09.840,"learning from events . supervised , unsupervised or mdp ?",machine-learning reinforcement-learning classification unsupervised-learning lstm,0,0,
1440,5782,1,,2018-03-23T16:38:40.500,4,765,"in section 1.1 of artificial intelligence : a modern approach , it is stated that a computer which passes the turing test would need 4 capabilities , and that these 4 capabilities comprise most of the field of artificial intelligence : natural language processing : to enable it to communicate successfully in english knowledge representation : to store what it knows and hears automated reasoning : to use the stored information to answer questions and to draw new conclusions machine learning : to adapt to new circumstances and to detect and extrapolate patterns did alan turing discern the requirements for the field of artificial intelligence ( the necessary subfields ) and purposefully design a test around these requirements , or did he simply design a test that is so general that the subfields which developed within artificial intelligence happen to be what is required to solve it ? that is , was he prescient or lucky ? are these turing 's subdivisions , or peter norvig 's and stuart russell 's ? if turing did foresee these 4 requirements , what did he base them on ? what principles of intelligence allowed him to predict the requirements for the field ?",2897,1671,2018-03-23T19:57:41.970,2018-03-24T23:59:48.257,"were the requirements to solve the turing test in "" ai : a modern approach "" foreseen by alan turing , or backfilled by peter norvig and stuart russell ?",machine-learning strong-ai turing-test norvig-russell,1,4,1
1441,5790,1,,2018-03-24T04:01:21.783,2,187,i have the following question .,14514,,,2018-03-24T16:31:43.070,how to solve constraint satisfaction problem of queen and knights,algorithm,1,0,
1442,5792,1,6516,2018-03-24T19:22:30.227,0,39,"i 've just started learning grammatical evolution and i 'm reading the paper grammatical evolution from michael o’neill . on page three said : during the genotype - to - phenotype mapping process it is possible for individuals to run out of codons and in this case , we wrap the individual and reuse the codons . i 'm not english speaker and i do n't understand the meaning of the word wrap here . what does it mean ? i understood that if not of the symbols are terminals , we have to start from the begging of the genotype again and replace the nonterminal symbols until we have only terminal symbols . but , if i 'm correct , when i have to stop ? in the paper said also about non valid individuals .",4920,,,2018-05-26T14:18:35.683,what 's wrapping in grammatical evolution ?,genetic-programming,1,0,
1443,5793,1,,2018-03-24T20:01:35.333,2,145,i have following planning problem,14514,,,2018-07-13T17:19:40.117,how to solve planning in artificial intelligence using strips,prediction,1,1,
1444,5794,1,,2018-03-24T20:12:28.483,1,109,i have implemented multiple mcts based ai players for the love letter game ( rules ) . it is a 2 - 4 players zero sum card game where players make alternating moves . i am struggling with how to properly conduct experiments for estimating ai player strength against human players : in 2 player game where one of the players is ai bot in 4 player game where one ( or multiple ) of players is ai bot,14534,,,2018-03-26T18:50:41.867,how to estimate the ai player 's strength in multiplayer game ?,game-ai monte-carlo-tree-search,1,3,
1445,5797,1,,2018-03-25T09:33:07.463,4,144,"basically , i want a system that reads blobs of text , parses it and stores related chunks and when the user asks a question , it returns with an in - context answer which groups together a bunch of facts . the topic and the initial blob set is hardcoded . i am noober than a newbie and i do n't understand how the chunks would form relationships amongst themselves by logic and not by rule when the inference engine would run and also , how they would be stored together . also , if i am trying to reinvent the wheel , i 'd be grateful if you could point me to open - source systems that already exist which just take in data and produce answers to questions after having performed some logical operations on it . thanks in advance .",14549,,,2018-04-10T13:57:12.690,logic for a knowledge based expert system,knowledge-representation expert-system,1,2,
1446,5800,1,5820,2018-03-26T08:57:31.797,1,42,"i have started reading fundamentals of deep learning by nikhil buduma and i have a question regarding tanh neurons . in the book , it is stated : "" when s - shaped nonlinearities are used , the tanh neuron is often preferred over the sigmoid neuron because it is zero - centered . "" can anyone explain me why exactly ? ?",14568,1671,2018-03-26T21:53:08.177,2018-03-28T10:25:21.237,s - shaped nonlinearities in tanh neurons,neural-networks machine-learning,1,1,
1447,5801,1,5813,2018-03-26T09:29:22.130,3,101,"suppose one trains a cnn to determine if something was either a cat / dog or neither ( 2 classes ) , would it be a good idea to assign all cats and dogs to one class and everything else to another ? or would it be better to have a class for cat , a class for dog and a class for everything else ( 3 classes ) ? my colleague argues for 3 classes because dogs and cats have different features , but i wonder if he 's right .",13068,13068,2018-03-26T10:10:09.617,2018-05-06T05:20:52.640,good idea to assign different objects to same class ?,neural-networks convolutional-neural-networks training datasets,3,2,
1448,5810,1,,2018-03-27T13:19:29.650,2,739,"i 'm currently having troubles to win against a random bot playing the schieber jass game . it is a imperfect card information game . ( famous in switzerland https://www.schieber.ch/ ) the environement i 'm using is on github https://github.com/murthy10/pyschieber to get a brief overview of the schieber jass i will describe the main characteristics of the game . the schieber jass consists of four players building two teams . at the beginning every player gets randomly nine cards ( there are 36 cards ) . now there are nine rounds and every player has to chose one card every round . related to the rules of the game the "" highest card "" wins and the team gets the points . hence the goal is to get more points then your opponent team . there are several more rules but i think you can image how the game should roughly work . now i 'm trying to apply a dqn approach at the game . to my attempts : i let two independent reinforcement player play against two random players i design the input state as a vector ( one hot encoded ) with 36 "" bits "" for every player and repeated this nine times for every card you can play during a game . the output is a vector of 36 "" bits "" for every possible card . if the greedy output of the network suggest an invalid action i take the action with the highest probability of the allowed actions the reward is +1 for winning , -1 for losing , -0.1 for a invalid action and 0 for an action which does n't lead to a terminal state my question : would it be helpful to use a lstm and reduce the input state ? how to handle invalid moves ? do you have some good ideas for improvements ? ( like neural - fictitious self - play or something similar ) or is this the whole approach absolute nonsense ?",14587,14587,2018-06-18T14:32:08.663,2018-06-18T14:32:08.663,how to use dqn to handle an imperfect but complete information game ?,reinforcement-learning game-ai q-learning imperfect-information,1,1,1
1449,5814,1,,2018-03-27T19:14:04.540,4,645,"suppose a cnn is trained to detect bounding box of a certain type of object ( people / cars / houses / etc . ) if each image in the training set contains just one object ( and its corresponding bounding box , ) how well can a cnn generalise to pick up all objects if the input for prediction contains multiple objects ? should the training images be downsampled in order for the cnn to pick out multiple objects in the prediction ? edit : i do n't have a specific one in mind . i was just curious about the general behaviour .",14570,13068,2018-03-29T01:52:48.133,2018-09-26T22:01:51.420,how well can cnn for bounding box detection generalise ?,neural-networks convolutional-neural-networks software-evaluation,1,4,
1450,5818,1,,2018-03-28T09:36:33.223,1,77,"i am new to deep learning and computer vision . i have a problem where i use yolo algorithm ( https://pjreddie.com/ ) to detect objects . in the original paper , they define the output to recognize 80 classes , but for my problem i just want to recognize human only . so i change the final layer to only 1 neuron , and do the training process with transfer learning techniques ( used pretrained weights for the cases of 80 classes , of course not use the final layer weights and these weights becomes random number for my problems ) . i feed only human data to the algorithm . however , i realize that after longer training , the model becomes worse . it starts to recognize other objects as human . i would like to hear any advice from you guys , should i also feed non - human data to the model . thanks",14613,14612,2018-03-28T16:27:34.037,2018-03-31T09:24:56.200,extracting one class from a pretrained convolutional neural network,deep-learning convolutional-neural-networks image-recognition computer-vision,1,0,0
1451,5821,1,,2018-03-28T14:56:34.707,4,32,i need to efficiently align characters vertically using multi objective pso . alignment is achieved by adding spaces in between a given set of characters . a b c d e f b b d h g c a b f might be - a b - c d e f - - - - b b - d - - h g c a b - - - - f - - now this is a multi objective solution . i need to maximize the characters that get aligned vertically and minimize the amount of spaces in between the characters . i wanted to focus firstly on how to get a set of characters to represent a position of a particle . this would mean that i need to somehow transform a possible set of characters into a position of a particle . if i can somehow achieve this then the rest should fall into place . how do i transform these set of characters into a position of a particle ? also is this the best approach or are there better ways to approach this problem ?,14621,1671,2018-03-28T21:15:30.273,2019-05-05T15:01:15.360,how to use mopso to align characters vertically ?,algorithm optimization combinatorics,1,1,
1452,5822,1,,2018-03-28T18:05:05.123,-1,311,"is there ai open source software or service that can blur things like license plate number , house numbers and smears people faces ( or better yet remove people and fills the background ) automatically from image and video of outdoor shooting ?",14627,1671,2018-03-28T21:11:45.610,2018-03-29T10:03:19.207,open source ai software to remove / obscure elements in video or still images ?,ai-design ai-basics open-source,1,2,
1453,5825,1,6093,2018-03-28T21:32:52.760,1,45,"an exponential linear unit ( as proposed by clevert et al . ) uses the function : ( sorry -- would have used mathjax were it available . ) elu α ( x ) = α(e x - 1 ) ( if x&lt;0 ) , or x ( if x≥0 ) now , this is continuous at x=0 , which is great . it 's differentiable there too if α=1 , which is the value that the paper used to test elu units . but if α≠1 ( as in the above diagram ) , then it 's no longer differentiable at x=0 . it has a crook in it , which seems weird to me . having your function be differentiable at all points seems advantageous . further , it seems that if you just make the linear portion evaluate to αx rather than x , that it would be differentiable there . is there a reason that the function was n't defined to do this ? or did they not bother , because α=1 is definitely the hyperparameter to use ?",14628,9647,2018-04-16T21:05:50.683,2018-04-17T18:02:50.993,why do n't elus multiply the linear portion by α ?,neural-networks deep-learning,1,0,
1454,5833,1,5834,2018-03-29T04:16:23.857,5,246,"when google researchers created alphago , how did they simulate the game of go ? if i wanted to take the same approach to other games , like risk , how would i go about coding the rules of the game ? is there a programming package , book , or general technique for coding the rules of a game for deep learning ?",14631,,,2018-04-06T06:04:48.497,how does one code the rules of a boardgame for deep learning ?,deep-learning gaming alphago,1,0,1
1455,5835,1,,2018-03-29T09:57:12.370,3,423,"the q function uses the ( current and future ) states to determine the action that gets the highest reward . however , in a stochastic environment , the current action ( at the current state ) does not determine the next state . how does q learning handle this ? is the q function only used during the training process , where the future states are known ? and is the q function still used afterwards , if that is the case ?",14638,2444,2019-02-22T17:24:31.600,2019-02-22T17:24:31.600,how does q - learning work in stochastic environments ?,reinforcement-learning q-learning environment,1,3,
1456,5836,1,,2018-03-29T19:52:37.683,1,952,"what is the output value of the network for these inputs respectively , and why ? ( linear activation function is fine . ) [ 2 , 3][-1 , 2][1 , 0][3 , 4 ] my main question is how you take the ' backwards ' directed paths into account .",14650,,,2018-06-28T22:17:41.510,how to calculate the output of this neural network ?,neural-networks recurrent-neural-networks,1,0,
1457,5837,1,,2018-03-30T12:28:12.523,1,137,"let us suppose i have a nxn matrix and i want to classify in m classes each entry of the matrix using a fuzzy classifier . the output of my classifier will be , for each matrix entry , an m -dimensional vector containing the probabilities for the entry to be classified in each class . a naive way to build a confusion matrix would be to select the highest probability in each vector and use it as a crips classification . however , i would like to take into account all the probabilities associated to each entry and compute a "" fuzzy "" confusion matrix . is this possible ?",14661,,,2018-03-30T12:28:12.523,fuzzy confusion matrix for fuzzy classifier,classification fuzzy-logic,0,0,
1458,5838,1,7104,2018-03-30T13:12:22.373,3,170,"how can i train a neural network to recognize sub - sequences in a sequence flow ? for example : given the sequence 111100002222 as an input sample from a stream , the neural network would recognize that 1111 , 0000 , 2222 are sub sequences ( so 111100 would not be a valid subsequence ) and so on for ~ 50 to 100 different subsequences . there is no particular order in which the subsequence would appear in the flow . no network architecture restriction . subsequences are of variable length . general concepts , ideas , and theory are welcome .",13038,5210,2018-04-02T16:57:41.827,2018-07-10T13:38:29.800,ideas on how to make a neural net learn how to split sequence into sub sequences,neural-networks algorithm,5,7,0
1459,5839,1,5849,2018-03-30T18:29:55.530,4,1554,"i 've recently read the original paper about neuroevolution of augmenting topologies by kenneth o. stanley and am now trying to prototype it myself in javascript . i stumbled across a few questions i ca n't answer . my questions : what is the definition of "" structural innovation "" , and how do i store these so i can check if an innovation has already happened before ? however , by keeping a list of the innovations that occurred in the current generation , it is possible to ensure that when the same structure arises more than once through independent mutations in the same generation , each identical mutation is assigned the same innovation number is there a reason for storing the type of a node ( input , hidden , output ) ? in the original paper , only connections have an innovation number , but in other sources , nodes do as well . is this necessary for crossover ? ( this has already been asked here . ) how could i limit the mutation functions to not add recurrent connections ? i think that 's it for now . all help is appreciated . the relevant parts of my code : genome class genome { constructor(inputs , outputs ) { this.inputs = inputs ; this.outputs = outputs ; this.nodes = [ ] ; this.connections = [ ] ; for ( let i = 0 ; i & lt ; inputs + outputs ; i++ ) { this.nodes.push(new node ( ) ) ; } for ( let i = 0 ; i & lt ; inputs ; i++ ) { for ( let o = 0 ; o & lt ; outputs ; o++ ) { let c = new connection(this.nodes[i ] , this.nodes[inputs + o ] , outputs * i + o ) ; this.connections.push(c ) ; } } innovation = inputs * outputs ; } weightmutateperturb ( ) { let w = this.connections[math.floor(random(this.connections.length))].weight ; w + = random(-0.5 , 0.5 ) ; } weightmutatecreate ( ) { this.connections[math.floor(random(this.connections.length))].weight = random(-2 , 2 ) ; } connectionmutate ( ) { let i = this.nodes[math.floor(random(this.nodes.length ) ) ] ; let o = this.nodes[math.floor(random(this.inputs , this.nodes.length ) ) ] ; let c = connection.exists(this.connections , i , o ) ; if ( c ) { c.enabled = true ; } else { this.connections.push(new connection(i , o , innovation ) ) ; innovation++ ; } } nodemutate ( ) { let oldcon = this.connections[math.floor(math.random(this.connections.length ) ) ] ; oldcon.enabled = false ; let newnode = new node ( ) ; this.nodes.push(newnode ) ; this.connections.push(new connection(oldcon.input , newnode , innovation , 1 ) ) ; innovation++ ; this.connections.push(new connection(newnode , oldcon.output , innovation , oldcon.weight ) ) ; innovation++ ; } } node class node { constructor ( ) { this.value = 0 ; this.previousvalue = 0 ; } } connection class connection { constructor(input , output , innov , weight ) { this.input = input ; this.output = output ; this.innov = innov ; this.weight = weight ? weight : random(-2 , 2 ) ; this.enabled = true ; } static exists(connections , i , o ) { for ( let c = 0 ; c & lt ; connections.length ; c++ ) { if ( connections[c].input = = = i & amp;&amp ; connections[c].output = = = o ) { return connections[c ] ; } } return false ; } }",14625,14625,2018-03-31T19:51:30.057,2018-03-31T19:51:30.057,"implementing the "" original "" neat algorithm in javascript",neural-networks genetic-algorithms deep-network evolutionary-algorithms neat,1,4,2
1460,5840,1,,2018-03-30T19:07:54.197,4,180,"so i came across these 2 questions : ideas on how to make a neural net learn how to split sequence into sub sequences search minimum value with learning machine algorithm for me both problems could be solved easily using traditional algorithmic techniques ( as in coding in your typical programming language . i assume that training a nn ( or any other machine learning technique ) for such sorts of problems will be more time consuming , resource intensive and pointless . my question is : if i want to solve a problem , how to decide whether it is better to solve algorithmically or by using nn / ml techniques ? what are the pros and cons ? how can this be done in a systematic way ? and if i have to answer someone why i chose a particular domain , how should i answer ? summary : choosing between normal computational approach vs abstract approach used in neural nets or ml or ai . example problems are appreciated :)",9947,4880,2018-04-10T13:57:09.760,2018-04-10T21:09:39.480,definitive methods vs neural networks and artificial intelligence,neural-networks machine-learning algorithm,3,14,2
1461,5855,1,5858,2018-04-01T11:48:16.067,2,727,"i have a neural network with 2 inputs and one output , like so : input | output _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ a | b | c 5.15 |3.17 | 0.0607 4.61 |2.91 | 0.1551 etc .. i have 75 samples and i am using 50 for training and 25 for testing . however , i feel that the training samples are not enough . because i ca n't provide more real samples ( due to time limitation ) , i would like to train the network using fake data : for example , i know that the range for the a parameter is from 3 to 14 , and that the b parameter is ~65 % of the a parameter . i also know that c is a number between 0 and 1 and that it increases when a & amp ; b increase . so , what i would like to do is to generate some data using the above restrictions ( about 20 samples ) . for example , assume a = 13 , b = 8 and c= 0.95 , and train the network with these samples before training it with the real samples . has anybody studied the effect of doing this on the neural network ? is it possible to know if the effect will be better or worse on the networks ? and are there any recommendations / guidelines if i want to do this ?",14701,,,2018-04-01T18:42:25.277,training neural network with ' fake ' data ?,neural-networks reasoning,2,5,
1462,5857,1,,2018-04-01T13:17:46.877,1,20,"i created and operate a social network for meeting new people . as a result of the recent fosta legislation , it ’s imperative that i implement an automated system to prevent users from posting advertisements relating to prostitution . i do not have much expierence with ai / machine learning . what library , algorithm , method should i look into to solve this problem ?",14705,,,2018-04-01T13:17:46.877,social network filtering for specific topic,machine-learning ai-design,0,3,
1463,5859,1,,2018-04-01T17:44:56.880,1,62,"i am looking to learn ai / machine learning during my spare and need advice on tools used / what to read / how one can integrate machine learning with a simple website for user feedback scenario : .net web application in c # a simple website that a user can use to to track their fitness information ... i.e one inputs weight ( at the end of the day ) , diet ( food eaten , calorie counting ) , exercises done for the day ( calories burned ) . kind of like myfitnesspal.com in a way . from then on wards a user can view charts of their weight tracking to check / analyse progress of weight gain / weight loss etc . for example on user creation , one will insert a target goal to reach target - 69 kg current - 85 kg so they most likely want to track how far they are from their target weight loss on a day to day / weekly / monthly basis . so chart info in user 's dashboard will show progress towards that goal . you get the idea ... simple enough right ? expansion : now say i want to attach a more intelligent hands off system that can analyse the user 's data and provide feedback / advice on how improve or change habits from all the user 's input over a period . determine positive consistent patterns ( or negative consistent patterns ) with regards to them achieving their desired goal . be it weight loss / bulking up / weight maintenance . python seems to be coming up a lot when i google this , but none specific information on how to exactly go about this as in to attach python data analysis script / application to the .net site ( could be php site or whatever ) . what are my options here ? am i looking for natural language processing , machine learning or data analysis . what s my starting point or more popular tools to use with plenty of resources that one can have a look at . creating the site is n't an issue .. its the more intelligent data analysis side that i want to dive into . f # has been mentioned here and there , but more posts seem to point out that there 's more support or people using python which keeps popping up again and again . at the most basic level , what i hope to learn / achieve if for the intelligent side of the system to give feedback like machine response : ( from weekly / monthly view ) "" your most efficient days of losing weight were on day x when you did exercise a and b , but you could maximize your progress if you ate food x from day c. "" i do realize there there would be some data science involved to let the application know about healthy habits / healthy foods for weight loss etc . so in general , if one has a .net site and learns machine learning / ai .. how do i attach this to my site . what tools are mostly used ? do i learn machine learning in python and roll with the flow ? there does n't seem to be information that can guide a learner diving into this sort of implementation . any advice on the path to take will be greatly appreciated . help me obi - wan - stackexchange , you are my only hope .",14708,14708,2018-04-04T13:39:47.297,2018-04-04T13:39:47.297,simple website and attaching data analysis to user information for feedback,machine-learning ai-basics,0,1,
1464,5861,1,,2018-04-01T19:50:28.670,3,148,deep networks notoriously take a long time to train . what is the most time consuming aspect of training them ? is it the matrix multiplications ? is it the forward pass ? is it some component of the backward pass ?,11566,,,2018-09-26T19:31:39.587,what is the most time consuming part of training deep networks ?,deep-learning deep-network,3,9,1
1465,5862,1,,2018-04-01T20:22:48.363,4,291,"for example , hidden layer 1 's outputs would be fed to the perceptrons in layer 2 , 3 , 4 , ... etc . beyond computational power considerations , would n't this be better than only connecting layers 1 and 2 , 3 and 4 , etc ? my intuition is that humans combine simple decisions with more complex ones to form an answer . also , would n't this solve the vanishing gradient problem ? if computational power is the concern , perhaps you could connect layer 1 only to the next n layers .",14710,,,2018-04-02T09:28:51.013,"in feed forward neural networks , why do n't we connect the output of each layer to all proceeding layers ?",neural-networks hidden-layers,2,1,
1466,5869,1,,2018-04-02T18:33:30.800,-1,605,"i have absolutely no experience with any kind of ai and really want to create this : a program that can train on a set of images to determine if an image is showing a fire flame or not ( for fire detection ) . it should train on a set of "" flame images "" . i heard about the keras python library which apparently allows to do this in 11 lines of code ( http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/ ) ... can anyone explain how i can make this work ? thank you very much in advance !",14731,,,2018-04-10T14:22:12.333,simple image processing ai for fire detection,keras python,1,4,1
1467,5874,1,,2018-04-03T16:33:54.950,4,118,"can we define the feeling of the human through conversations with an ai ? something like a "" confessional , "" disregarding human possibilities to lie . below , i have the categories joyful , sadness , anger , fear and affection . for each category , there are several words that can be in the texts that refer to it . joy : ( cheerful , happy , confident , happy , satisfied , excited , interested , dazzled , optimistic , relieved , euphoric , drunk , witty , good ) sadness : ( sad , desperate , displeased , depressed , bored , lonely , hurt , desolate , meditative , defrauded , withdrawn , pitying , concentrated , depressed , melancholic , nostalgic ) anger : ( aggressive , critical , angry , hysterical , envious , grumpy , disappointed , shocked , exasperated , frustrated , arrogant , jealous , agonized , hostile , vengeful ) fear : ( shy , frightened , fearful , horrified , suspicious , disbelieving , embarrassed , embarrassed , shaken , surprised , guilty , anxious , cautious , indecisive , embarrassed , modest ) affection : ( loving , passionate , supportive , malicious , dazzled , glazed , homesick , embarrassed , indifferent , curious , tender , moved , hopeful ) flow example phrase 1 : "" i 'm very happy ! it concludes college . "" categorization 1 : - joy ( +1 ) - sadness ( -1 ) phrase 2 : "" i 'm sad , my mother passed away . "" categorization 2 : - sadness ( +1 ) - joy ( -1 ) phrase 3 : "" i met a girl , but i was ashamed . "" categorization 3 : - fear ( +1 ) is this a clever way to follow and / or improve , or am i completely out of the way ? i see that there is a google product that creates parsing according to the phrases . i do not know how it works , because i like to recreate the way i think it would work . remembering that this would not be the only way to categorize the phrase . this would be the first phase of the analysis . i can also identify the subject of the sentence , so we would know if the sadness is from the creator of the message or from a third party , in most cases . nltk sentiment analysis python example",7800,7800,2018-04-05T18:53:30.830,2018-04-18T09:38:43.747,sentiment analysis,natural-language-processing sentiment-analysis,3,7,3
1468,5875,1,,2018-04-03T16:38:30.767,1,71,"i 'm working with acoustic data ( filterbank features ) and i want to build a neural network to detect claps using an lstm ( or a gru ) with a binary output ( present / abscent ) , and i 'm wondering about how i should prepare my data before feeding them to the rnn . if i have 20 seconds of claps ( separate claps separated by ~ 0.1 seconds ) what is the difference between : feeding the network a series of n claps as one example ( with variable n : 1 , 2 , .. , 10 , .. ) + padding with zeros to fit the longest sequence . feeding the network multiple examples of 1 clap . my problem is not restricted to claps but covers patterns that can be observed as separated occurrences , periodic sequence of occurrences , variable - length - period "" quasi - periodic "" sequence of occurrences , etc . unlike an ergodic hmm , an rnn does n't have any loops to "" jump back "" to a previous "" acoustic state "" , so what should - i do with this kind of data ?",14751,,,2018-04-03T16:38:30.767,best practices to classify recurring patterns using an lstm or gru,recurrent-neural-networks,0,0,
1469,5885,1,,2018-04-04T01:52:15.150,7,276,"a question about swarm intelligence as a potential method of strong general ai came up recently , and yielded some useful answers and clarifications regarding the nature of swarm intelligence . but it got me thinking about group intelligence in general . here organism is synonymous with algorithm , so a complex organism is an algorithm made up of component algorithms , based on a set of instructions in the form of a string . now consider the portuguese man o ' war , not a single animal , but a colonial organism . in this case that means a set of animals connected for mutual benefit . and physalia physalis are pretty smart as a species in that they 've been around for a while , i 'm not finding them on any endangered lists , and based on their habitat it looks like global warming will be a jackpot for them . and they do n't even have brains . each component of the physalia has a narrow function , colony organism organism itself has a more generalized function , which is the set of functions necessary for maintenance and reproduction . { man o ' war } ⊇ { { pneumatophore } , { gonophores , siphosomal nectophores , vestigial siphosomal nectophores } , { free gastrozooids , tentacled gastrozooids , gonozooids , gonopalpons } , { dactylozooids } , { gonozooids } , { gastrozooids } } what types of applications qualify as "" compound intelligences "" ? what is the thinking on groups of neural networks comprising generally stronger or simply more generalized intelligence ? i recognize the underlying problem is ultimately complexity and that "" strong narrow ai "" is , by definition , limited , so i use "" generalized "" and omit "" strong "" because because human - like and superintelligence are not conditions . compound intelligence is defined as a colony of dependent intelligences . * utility software is often a form of expert system that manages a set of functions of varying degrees of complexity . there 's currently a great deal of focus on autonomous vehicles , which would seem to require sets of functions . links to research papers on this or related subjects would be ideal . portuguese man o ' war ( oceana.org ) the bugs of the world could squish us all",1671,1671,2018-04-04T02:09:42.727,2018-11-06T23:09:11.420,compound intelligence ?,machine-learning theory biology architecture emergence,3,3,1
1470,5889,1,,2018-04-04T06:11:51.267,2,35,"it is possible to use case - based reasoning for forward simulation in mario ai , this is explained by game engine learning from video they are using features : distance , velocity and position to predict following game - frames like in a physics engine . what my problem with the paper is , that the “ learned game engine ” seems to be working autonomously . that means , the human operator is out of the loop , he is not changing the cases manually and he is not involved in predicting the future . is it possible to make an interactive forward simulation ? for example , the game - engine asks the operator what will happen if mario jumps over the wall . what i do not understand is how to design the gui - interface in such a situation , because the number of possible reactions of the game - engine to a situation is endless , and to make the simulation interactive the user would click very fast on some buttons . for example , if the desired framerate is 30fps , should the user define all the parameters in a timestep of 1/30 seconds ?",11571,,,2018-04-04T06:11:51.267,interactive forward simulation,game-ai,0,1,
1471,5890,1,,2018-04-04T09:13:15.360,3,251,i am trying to find out what are some good learning strategies for deep q - network with opponents . let 's consider the well known game tic - tac - toe as an example : how should an opponent be implemented to get good and fast improvements ? is it better to play against a random player or a perfect player or should the opponent be a dqn player as well ?,14587,1671,2018-04-04T17:36:15.333,2018-04-04T17:36:15.333,what are good learning strategies for deep q - network with opponents ?,reinforcement-learning game-ai q-learning self-play perfect-play,1,1,3
1472,5891,1,5892,2018-04-04T12:36:54.207,4,1980,"to provide a bit of context , i 'm a software engineer & amp ; game enthusiast ( card games specially ) . the thing is i 've always been interested in ai oriented to games . in college , i programmed my own gomoku ai so i 'm a bit familiar with the basic concepts of ai game oriented and have read books & amp ; articles about game theory as well . my issue comes when i try to analyze ai 's for imperfect information games like ( poker , magic the gathering , hearthstone , etc ) . in most cases when i found an ai for hearthstone , it was either some sort of monte carlo or minmax strategy . i honestly think although it might even provide some decent results it will still be always quite flat and linear since it does n't take into account what deck the opponent is playing and almost always tries to follow the same game - plan , since it will not change based on tells your opponent might give away via cards played ( hint that a human would catch ) . i would like to know if using neural networks would be more better than just using a raw evaluation of board state + hands + hp each turn without taking into account learning about possible threads the opponent might have , how to deny the opponent the best plays he could make , etc . my intuition tells me that this is way harder and far more complex . is that the only reason the nn method is not used?has there been any research to prove how much efficiency edge would be between those 2 approaches ?",14769,9947,2018-04-04T12:58:23.100,2018-04-04T13:35:42.850,why most imperfect information games usually use non machine learning ai ?,machine-learning game-ai imperfect-information,1,1,3
1473,5898,1,8053,2018-04-04T17:45:39.967,4,962,"i found this nice - ish - looking diagram , but it has a wholly inadequate descriptions for each of the cell types , aside from including names . what is the definition / description of each of these cell types ? input cell backfed input cell noisy input cell hidden cell probablistic hidden cell spiking hidden cell output cell match input output cell recurrent cell memory cell different memory cell kernel convolution or pool",14776,1671,2018-04-04T17:51:34.113,2018-12-10T19:11:11.530,neural network cell ( node ) types,neural-networks ai-basics terminology,1,4,3
1474,5899,1,,2018-04-04T18:54:43.797,5,526,"in the search tree below , there are 11 nodes , 5 of which are leaves . there are 10 branches . is the average branching factor given by 10/6 , or 10/11 ? are leaves included in the calculation ? intuitively , i would think not , since we are interested in nodes with branches . however , a definition given to me by my professor was "" the average number of branches of all nodes in the tree "" , which would imply leaves are included .",14777,1671,2018-04-04T19:24:21.660,2018-09-07T17:01:12.283,are leaf nodes included in the calculation of average branching factor for search trees ?,ai-basics search branching-factors,2,1,2
1475,5904,1,5926,2018-04-05T08:59:43.157,7,185,"in a nutshell : i want to understand why a one hidden layer neural network converges to a good minimum more reliably when a larger number of hidden neurons is used . below a more detailed explanation of my experiment : i am working on a simple 2d xor - like classification example to understand the effects of neural network initialization better . here 's a visualisation of the data and the desired decision boundary : each blob consists of 5000 data points . the minimal complexity neural network to solve this problem is a one - hidden layer network with 2 hidden neurons . since this architecture has the minimum number of parameters possible to solve this problem ( with a nn ) i would naively expect that this is also the easiest to optimise . however , this is not the case . i found that with random initialization this architecture converges around half of the time , where convergence depends on the signs of the weights . specifically , i observed the following behaviour : w1 = [ [ 1,-1],[-1,1 ] ] , w2 = [ 1,1 ] --&gt ; converges w1 = [ [ 1,1],[1,1 ] ] , w2 = [ 1,-1 ] --&gt ; converges w1 = [ [ 1,1],[1,1 ] ] , w2 = [ 1,1 ] --&gt ; finds only linear separation w1 = [ [ 1,-1],[-1,1 ] ] , w2 = [ 1,-1 ] --&gt ; finds only linear separation this makes sense to me . in the latter two cases the optimisation gets stuck in suboptimal local minima . however , when increasing the number of hidden neurons to values greater than 2 , the network develops a robustness to initialisation and starts to reliably converge for random values of w1 and w2 . you can still find pathological examples , but with 4 hidden neurons the chance that one "" path way "" through the network will have non - pathological weights is larger . but happens to the rest of the network , is it just not used then ? does anybody understand better where this robustness comes from or perhaps can offer some literature discussing this issue ? some more information : this occurs in all training settings / architecture configurations i have investigated . for instance , activations = relu , final_activation = sigmoid , optimizer = adam , learning_rate=0.1 , cost_function = cross_entropy , biases were used in both layers .",14789,14789,2018-04-05T14:33:55.940,2018-04-06T10:17:42.757,why does a one - layer hidden network get more robust to poor initialization with growing number of hidden neurons ?,neural-networks optimization,1,15,1
1476,5906,1,5917,2018-04-05T13:31:18.120,3,100,"i have recently gone about and made a simple ai , one that gives responses to an input ( albeit completely irrelevant and nonsensical ones ) , using synaptic.js . unfortunately , this is not the type of text generation i am looking for . what i am looking for would be a way to get connections between words and generate text from that . ( what would be preferable would be to also generate at least semi - sensible answers also . ) this is part of project raphiel , and can be checked out in the room associated with this site . what i want to know is what layer combination would i use for text generation ? i have been told to avoid retrieval - based bots . i have the method to send and receive messages , i just need to figure out what combination of layers would be the best . unless i have the numbers wrong , this will be se 's second nn chatbot .",14723,14723,2018-04-10T18:32:56.733,2018-04-10T18:32:56.733,how would one go about generating * sensible * responses to chat ?,ai-design text-summarization,1,2,0
1477,5916,1,,2018-04-06T00:09:08.880,4,29,"i 'm looking to perform two tasks : train a classifier to classify code as serial or parallel train a generative algorithm to generate parallel code from serial for the first task a simple scraper can scrape random c and c++ code from git , however for the second step i would need a decently large source of examples of serial to parallel code . any ideas or pointers for existing or creating this type of dataset would be greatly appreciated .",14809,,,2018-06-26T15:08:46.250,"looking to build , compile , and/or find dataset for serial - parallelized code examples",classification datasets generative-model,1,0,2
1478,5919,1,,2018-04-06T06:09:29.033,1,356,"when training on large neural network , how to deal with the case that the gradients are too small to have any impact ? fyi , i have an rnn , which has multiple lstm cells and each cell has hundreds of neurons . each training data has thousands of steps , so the rnn would unroll thousands of times . when i print out all gradients , they are very small , like e-20 of the variable values . therefore the training does not change the variable values at all . btw , i think this is not an issue of vanishing gradients . note that the gradients are uniformly small from the beginning to the end . any suggestion to overcome this issue ? thank !",14816,,,2018-08-12T03:01:46.073,too small gradient on large neural network,gaming,2,3,
1479,5927,1,,2018-04-06T14:02:14.460,1,28,"i 'm detecting objects on images . i want to detect up to 10 objects , however , i 'm not sure how to deal with the situation , where only one object is present . should i fill the remaining spaces in the label input data with vectors filled with 0 ? e.g : [ [ xmin , ymin , xmax , ymax],[0,0,0,0 ] ... ] or is there any better way ? thanks for help !",4695,14723,2018-04-10T14:41:51.527,2018-04-10T14:41:51.527,filling space with empty bounding box,convolutional-neural-networks object-recognition,0,0,1
1480,5928,1,5932,2018-04-06T15:49:15.430,4,66,"i am trying to create a fixed - topology mlp network from scratch ( c # ) which can classify some simple problems such as xor and mnist ( handwriting ) . the network will be trained purely with genetic algorithms instead of backprop . here are the details : population size : 50 activation function : sigmoid fixed topology xor : 2 inputs , 1 output . tested w/ different numbers of hidden layers / nodes . mnist : 28 * 28=784 inputs for all pixels , will be either on(1 ) or off(0 ) . 10 outputs to represent digits 0 - 9 initial population will be given random weights between 0 and 1 10 "" fittest "" networks survive each iteration , and performs crossover to reproduce 40 offspring for all weights , mutation occurs to add a random value between -1 to 1 , with a 5 % chance with 2 hidden layers of 4 and 3 neurons respectively , xor managed to achieve 97 - 99.9 % accuracy in around 100 generations . biases were not used here . however , trying out mnist revealed a pretty glaring issue - the 784 inputs ; a large increase of nodes compared to xor , multiplied with weights and added up results in huge values of 50 to even 100 , way beyond the typical domain range of the activation function ( sigmoid ) . this just renders all layers ' outputs as 1 or 0.99999-something , which breaks the entire network . also , since this makes all individuals in a population extremely similar to one other , the genetic algorithm seems to have no clue on how to improve . crossover will produce an offspring almost identical to its parents , and some lucky mutations are simply ignored my the sheer amount of other neurons ! what can be a viable solution to this ? it 's my first time studying nns , and this is really ... challenging . please help !",14833,,,2018-04-07T08:23:03.840,summed weights are too big for activation function [ gann ],neural-networks machine-learning deep-learning genetic-algorithms,1,0,1
1481,5933,1,,2018-04-07T10:22:35.037,1,161,"i have been searching for information regarding industry accepted certifications in the area of artificial intelligence , neural networks , machine learning , nlp , deep learning . i could n't find much information after searching . could anyone please share , if you are aware of any certifications in these areas . p.s : i am not looking for course certifications but rather something along the lines of aws certifications , ocjp , etc .",14840,8,2018-04-10T13:56:42.693,2019-05-22T07:25:19.143,industry accepted certifications in the field of artificial intelligence,machine-learning ai-basics ai-community,3,0,1
1482,5939,1,,2018-04-07T17:29:52.477,9,136,"with the growing ability to cheaply create fake pictures , fake soundbites , and fake video there becomes an increasing problem with recognizing what is real and what is n't . even now we see a number of examples of applications that create fake media for little cost ( see deepfake , faceapp , etc . ) . obviously , if these applications are used in the wrong way they could be used to tarnish another person 's image . deepfake could be used to make a person look unfaithful to their partner . another application could be used to make it seem like a politician said something controversial . what are some techniques that can be used to recognize and protect against artificially made media ?",13088,1581,2018-10-31T19:56:24.773,2019-03-02T20:01:10.240,what are some tactics for recognizing artificially made media ?,machine-learning image-recognition pattern-recognition,2,5,1
1483,5941,1,,2018-04-07T21:06:37.043,3,193,"i am working on a project which maps to a variant of path finding problem . i am new to this area and i would be very grateful if you could give suggestions/ point to libraries for relevant algorithms . a simplified version of my problem statement is as follows- goal : on a 2d grid , starting from a fixed point reach the destination in exactly n steps . allowed actions : 1 . at every position , you have a choice of up to three moves ( i.e. straight , curve left , curve right ) . 2 . you can not collide with the path traveled so far ( just like in the snake game ) . dimension of the grid : n x n where n is between 100 - 1000 scalable : later on , the problem will be scaled to have multiple such snakes going between different pairs of points on the grid . the ultimate goal is to get all snakes to reach their respective destinations in a fixed number of steps without any collisions . tl;dr : essentially i have to find a fixed length path on a dynamically generated directed graph . is there a better choice than a a * / greedy heuristic ? is it worth taking a q- learning approach ? a rudimentary one snake version written in python can be found here - github link thanks in advance !",14851,14851,2018-04-07T22:42:18.823,2018-04-08T08:11:57.787,snake path finding variant : algorithm choice,reinforcement-learning q-learning heuristics path-planning pathfinding,1,2,
1484,5942,1,,2018-04-07T22:43:16.867,5,62,"i have two classes in the training set : one that has images with a feature and the other of images without that feature . can there be a lot more images with "" no feature "" so i can fit in all possible false positives ?",14731,,,2019-05-04T22:02:45.737,"two data classes for a convolutional neural network , can one have a lot more images for training than the other ?",ai-design classification keras,2,1,
1485,5943,1,,2018-04-08T00:54:44.760,1,94,"in time series prediction , we have a stream of vectors . there are different approaches for accounting for the temporal patterns between these vectors . there 's two that i 'm considering . an lstm or augmenting the feature space . what 's the difference between the two ? the most obvious to me is that an lstm is more expressive and can get superior accuracy if modelled properly .",11566,,,2018-04-09T16:11:13.527,time series : lstm or augmented vector space ?,machine-learning recurrent-neural-networks lstm,2,0,
1486,5949,1,5951,2018-04-08T14:38:12.313,2,2860,"i encountered the algorithm below , which is the general tree search algorithm . function tree - search(problem , fringe ) returns a solution or failure fringe & lt;-- insert(make - node(initial - state[problem ] ) , fringe ) loop do if fringe is empty then return failure node & lt;- remove - front(fringe ) if goal - test[problem](state[node ] ) then return solution(node ) fringe & lt;- insertall(expand(node , problem ) , fringe ) function expand(node , problem ) returns a set of nodes successors & lt;- the empty set for each action , result in successor - fn[problem](state[node ] ) do s & lt;- a new node parent - node[s ] & lt;- node ; action[s ] & lt;- action ; state[s ] & lt;- result path - cost[s ] & lt;- path - cost[node ] + step - cost(node , action , s ) depth[s ] & lt;- depth[node ] + 1 add s to successors return successors what is the "" fringe "" in the context of search algorithms ?",14862,2444,2018-10-29T21:50:49.267,2018-10-29T21:50:49.267,"what is the "" fringe "" in the context of search algorithms ?",ai-basics terminology definitions search,1,0,
1487,5955,1,,2018-04-08T21:34:26.637,11,1117,"i 've heard multiple times that "" neural networks are the best approximation we have to model the human brain "" , and i think it is commonly known that neural networks are modelled after our brain . i strongly suspect that this model has been simplified , but how much ? how much does , say , the vanilla nn differ from what we know about the human brain ? do we even know ?",14612,4302,2018-07-23T15:39:15.823,2019-01-11T15:40:00.573,how are artificial neural networks and the biological neural networks similar and different ?,neural-networks brain topology,3,2,4
1488,5958,1,5968,2018-04-09T10:48:20.803,-3,148,"i want to train my computer to recognize texts from websites , copy them , insert them somewhere else , and other things like this , but i do nt even know where to start my research and what tools should i use . can you give me an advice ?",14872,14723,2018-04-11T16:40:07.913,2018-04-16T12:19:26.683,how can i train my computer work for me ?,ai-design training ai-basics getting-started,1,4,
1489,5960,1,5961,2018-04-09T11:37:05.947,1,310,"the problem to solve is non - linear regression of a non - linear function . my actual problem is to model the function "" find the max over many quadratic forms "" : max(w.h.t q w ) , but to get started and to learn more about neural networks , i created a toy example for a non - linear regression task , using pytorch . the problem is that the network never learns the function in a satisfactory way , even though my model is quite large with multiple layers ( see below ) . or is it not large enough or too large ? how can the network be improved or may be even simplified to get a much smaller training error ? i experimented with different network architectures , but the result is never satisfactory . usually the error is quite small within the input interval around around 0 , but the network is not able to get good weights for the regions at the boundary of the interval ( see plots below ) . the loss does not improve after a certain number of epochs . i could generate even more training data , but i have not yet understood completely , how the training can be improved ( tuning parameters such as batch size , amount of data , number of layers , normalizing input ( output ? ) data , number of neurons , epochs etc . ) my neural network has 8 layers with the following number of neurons : 1 , 80,70,60 , 40,40,20 , 1 for the moment i do not care too much about overfitting , my goal is to understand , why a certain network architecture / certain hyper parameters need to be chosen . of course , avoiding overfitting at the same time would be a bonus . i am especially interested in using neural networks for regression tasks or as function approximators . in principle my problem should be able to be approximated to arbitrary accuracy by a single layer neural network , according to the universal approximation theorem , is n’t this correct ?",14873,14873,2018-04-09T18:39:15.640,2018-04-09T18:39:15.640,non - linear regression . universal function approximation with deep neural networks,neural-networks machine-learning deep-learning python,1,3,
1490,5962,1,,2018-04-09T13:23:36.900,5,52,"we store supplier information for many companies . each supplier database is logically separated , making it possible for a supplier to be common across logical separations . at the moment , we have analysts go through the system manually to try identify common suppliers by name and address . is this a problem for a certain domain of ai ? if so , what is the right hammer for this nail ?",14875,,,2019-05-30T16:03:10.677,what s the correct technique / technology to identify duplicate data,ai-design ai-basics,1,4,
1491,5964,1,6043,2018-04-09T16:10:35.577,6,249,"youtube has a huge amount of videos , many of which also containing various spoken languages . this would presumably provide something like the data that a "" challenged "" baby would experience - "" challenged "" meaning a baby without arms or legs ( unfortunately many people are born that way ) . would this not allow unsupervised learning in a deep learning system that has both vision and audio capabilities ? the neural network would presumably learn correlations between words and images , and could perhaps even learn rudimentary language skills , all without human supervision . i believe that the individual components to do this already exist . has this been tried , and if not , why ?",5911,47,2018-04-15T00:46:58.770,2018-04-15T00:46:58.770,has anybody tried unsupervised deep learning from youtube videos ?,deep-learning deep-network computer-vision unsupervised-learning datasets,3,5,3
1492,5966,1,,2018-04-09T16:18:15.580,1,97,i wanted to use the visualization of the activation maximization of the filters that is described in the following keras tutorial / blog : https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html i 'd like to know what is the intention behind the decision that filters that produce a loss & lt;= 0 are skipped . i know for 0 that would be reasonable since their would be no gradient flowing then ( i think ) but what about negative values ? and is it also reasonable to use the mean of the outputs of the filters as a loss ? what if there are weights of a filter that have high negative and positive numbers . would that be a problem ?,14881,14881,2018-04-09T18:53:50.353,2018-04-09T18:53:50.353,questions regarding keras activation maximization visualization,convolutional-neural-networks keras,0,0,
1493,5969,1,,2018-04-09T20:06:29.793,1,30,"are decision trees able to be used with time - related data ? i 've read that decision trees are based on matrices and that arrays of input matrices can be used to factor in time however i ca n't find an example of this . say for example , i 'm monitoring the progress of students taking exams . each day i ask them questions related to their mental state ( fatigued , positivity , ability to concentrate , expectations for coming exam , confidence , etc ) . i have twenty days worth of questions . day 1 for student a may see them studying for an exam the following day , while day 1 for student b may see them actually doing the exam . there will be a relation between student 's fatigue ( for example ) and the results they give the following day . the examples when provided as input to a matrix will be used to show that if on any given day , the student has an exam , and has breakfast , and does x , y , z that day then the outcome will be y. however , short of encoding "" had exam previous day "" and "" had exam two days ago "" for each day , i ca n't see how i can include time dependency in decision trees .",12726,,,2018-04-09T20:06:29.793,how to factor time into decision trees ?,machine-learning,0,0,
1494,5970,1,,2018-04-09T21:02:23.180,6,2046,"i 'm studying reinforcement learning . it seems that "" state "" and "" observation "" mean exactly the same thing . they both capture the current state of the game . is there a difference between the two terms ? is the observation maybe the state after the action has been taken ?",11566,2444,2018-11-16T20:11:53.853,2018-11-16T20:11:53.853,what is the difference between an observation and a state in reinforcement learning ?,reinforcement-learning terminology,1,2,3
1495,5979,1,6562,2018-04-10T13:33:34.427,2,579,"i am trying to do 3d image deconvolution using convolution neural network . but i can not find many famous 3d convnets . can any one point out some for me ? background : i am using pytorch , but any language is ok . what i want to know most is the network structure . i ca n't find papers on this topic . links to research papers would be especially appreciated .",14907,14907,2018-04-11T07:59:29.103,2018-05-29T20:41:26.600,3d convolution neural nets,convolutional-neural-networks,1,6,
1496,5981,1,,2018-04-10T15:34:40.183,4,138,"for a school project , i would like to investigate a paper on either reinforcement learning or computer vision . i am particularly interested in dqn , rnns , cnns or lstms . i would eventually like to implement any of these . however , i also need to take into account the computing resources required to train and analyse any of these algorithms . i understand that , in computer vision , the data sets can be quite large , but i am not so sure regarding the resources needed to implement and train a typical state - of - the - art rl algorithm ( like dqn ) . would a "" standard pc "" be able to run any of these algorithms decently to achieve some sort of analysis / results ?",14913,2444,2019-02-15T16:18:40.473,2019-02-15T16:21:51.720,what are the minimum computing resources needed to train a machine learning algorithm ?,machine-learning reinforcement-learning computer-vision hardware-evaluation,1,3,
1497,5982,1,,2018-04-10T18:41:20.360,5,273,"i want to be able to input a block of text and then have it guess a string within a predefined range ( i.e. a string that starts with three letters and ends with five numbers like "" xxx12345 "" , etc ) . ideally , the string it will be guessing will be somewhere in the block of text , but sometimes it wo n't be . i have been struggling where to begin on this or if i am even going in the right direction for considering machine / deep learning to try to do this . help !",14918,14918,2018-04-11T13:30:59.753,2019-01-07T11:00:45.167,use machine / deep learning to guess a string,neural-networks machine-learning deep-learning lstm text-summarization,4,8,2
1498,5987,1,,2018-04-10T21:30:08.717,2,39,"i want to create a neural network and train it on some data , however i want to be able to create a new model without retraining it from the start . an example , i have 1000 data points in my training data model - trained on 0 - 99 model - trained on 1 - 100 model - trained on 2 - 101 and so forth so i 'm wondering if i can use the first model to train the second model , essentially forgetting the first data point . you can view it as a sliding window over the 1000 data points , sliding one data point to the right for each new model . does it make sense ? is there any easy way to solve this problem ?",14923,,,2018-04-10T21:30:08.717,shifting training data,neural-networks training datasets,0,9,
1499,5990,1,,2018-04-11T08:06:52.587,6,689,"there are two textbooks that i most love and am most afraid of in the world : introduction to algorithms by cormen et al . and artificial intelligence : a modern approach by norvig et al . i have started the "" ai : a modern approach "" more than once , but the book is so dense and full of theory that i get discouraged after a couple of weeks and stop . i am looking for a similar ai book but with an equal emphasis on theory and practice . some examples of what i am looking for : the elements of statistical learning by tibshirani et al . ( detailed theory ) an introduction to statistical learning : with applications in r by tibshirani et al . ( theory+practical ) digital image processing by gonzalez et al . ( detailed theory ) digital image processing using matlab by gonzalez et al . ( theory+practical )",14933,1671,2018-06-01T19:54:37.523,2018-08-21T19:46:13.970,""" artificial intelligence : a modern approach "" book alternatives",reference-request russell-norvig,3,5,2
1500,5991,1,5992,2018-04-11T08:59:05.170,0,415,"i 'm currently reading this explanation of convolutional neural networks and there 's a part around strides that i do n't quite understand . i 'm just starting with this so apologies if this is a really basic question . but i 'm trying to develop and understanding and some of these images have thrown me off . specifically in this image the stride has been increased to 2 and it 's using a 3x3 filter ( represented by the red , green and blue outline squares in the first picture ) why is the blue square below the red one and not shifted to the side of the green one ending at the edge of the 7x7 volume ? should it not move left to right then down 2 squares when it reaches the next line ? i 'm not sure if the author is just trying to show the stride moving down as it goes , but i think my confusion stems from the fact that the 1 stride image example is only moving in the horizontal direction ( as seen below ) . is there something fundamental i have n't grasped here ?",14935,,,2018-04-11T09:13:34.387,understanding strides and movement,convolutional-neural-networks,1,0,
1501,5993,1,,2018-04-11T10:08:28.350,1,179,"i use sigmoid activation function for neurons at output layer of my multi - layer perceptron also , i use cross - entropy cost function . as i know when activation functions like tanh is used in output layer it 's necessary to divide outputs of output layer neurons by sum of them like what is done for softmax , is such thing necessary for sigmoid activation function ? if it 's necessary to normalize outputs of neurons , does it affect derivations ?",10051,10051,2018-04-11T11:21:59.917,2018-04-11T11:21:59.917,sigmoid output layer and cross - entropy cost function,neural-networks mlp,0,0,
1502,5994,1,6039,2018-04-11T10:19:15.367,3,85,"i have been reading quite a few papers ( chapter 7 ) on genetic programming and its applications . unfortunately , i can not wrap my head around how one could apply genetic programming to robotics , for example , robot 's path planning or robot 's maneuvers . can anyone explain this in the most simple manner ? i know that it all depends on the fitness function .",14863,,,2018-04-13T13:56:59.293,how can genetic programming be used for robot path planning ?,deep-learning robots,1,3,0
1503,6002,1,6003,2018-04-11T15:02:29.147,1,18,"i am reading the cornerstone book , "" artifical intelligence , a modern approach "" by stuart russel , and peter norvig and there is a passage in the book on page 98 : the complexity results depend very strongly on the assumptions made about the state space . the simplest model studied is a state space that has a single goal and is essentially a tree with reversible actions . ( the 8-puzzle satisfies the first and third of these assumptions . ) what are the "" assumptions "" in that context ?",14862,,,2018-04-11T15:07:02.527,assumptions about the state space,getting-started,1,1,
1504,6006,1,,2018-04-11T20:34:18.817,1,78,"the situation i encountered here is that i have two inputs(for instance , image embedding , etc . ) into the first lstm of a series of lstms to predict the next word to generate sentence(from the second lstm , it started to predict the next word from the current input word ) . the length of each of the two inputs is 512 . merely for the first input , it increases the measurement , say , for instance , perplexity , by about 3 from no this input at all . merely for the second input , it increases the measurement , say , for instance , perplexity , by about 1 from no input at all . the problem is : is it possible to combine these two inputs into a model that can produce a result of increasement more than 3 or the larger amount of increasement of the former two inputs models ? if it is , how to build a model or what model should i build to combine them to do so ?",14947,14948,2018-04-12T02:31:23.403,2018-04-12T02:31:23.403,combine two embeddding inputs to increase more performance in lstm model,recurrent-neural-networks lstm,0,0,
1505,6009,1,,2018-04-12T05:12:18.157,1,79,"suppose i have a boolean function that maps n bits to one bit . if i understand correctly , this function will have 2 ^ 2^n possible configurations of its truth table . what is the minimum number of neurons and hidden layers i would need in order to have a multi - layer perceptron capable of learning all possible truth tables ? i found one set of lecture notes here that suggests that "" checkerboard "" shaped truth tables ( which is something like an n - input xor function ) are hard to learn , but do they represent the worst - case scenario ? the notes suggest that such tables require 3(n-1 ) neurons arranged in 2 log_2(n ) hidden layers",14955,,,2018-07-11T12:50:53.413,minimum number of perceptrons for an n - bit truth table ?,logic learning-theory perceptron,1,0,
1506,6018,1,,2018-04-12T10:25:40.547,2,18,"i 'm writing neural network based on neural gas algorithm ( university assignment ) and i remember that lecturer said that , when you generate random neuron weights at the beginning , it 's worth to generate them multiple times and choose the best set of them . the problem : i do n't know what 's the criteria of choosing the best set of weights for neurons ?",14962,,,2018-04-12T10:25:40.547,multiple centroid draw,neural-networks,0,1,
1507,6020,1,6528,2018-04-12T16:15:51.663,3,1475,i have read much about the programming languages being used in ai specially lisp and prolog but just wanted to know about the current situation of the both these languages in ai . can somebody tell me which language is being more used these days ? lisp or prolog .,11706,,,2018-05-27T17:54:49.453,which programming language is more used in ai the lisp or prolog ?,ai-basics ai-community lisp prolog,1,3,1
1508,6026,1,7477,2018-04-13T02:25:29.013,5,3311,"i understand that a heuristic is admissible if it never overestimates the true cost to reach the goal node from n. i also understand that if a heuristic is consistent then the heuristic value of n is never greater than the cost of its successor , n ' , plus the successor 's heuristic value . however , i am having a hard time understanding the theorems stated in my lecture slides in regard to them : if h(n ) is admissible , a * using tree - search is optimal if h(n ) is consistent , a * using graph - search is optimal can someone please clarify why these are the case .",14913,,,2019-01-16T21:37:11.630,admissible / consistent heuristic theorems,heuristics search,1,2,2
1509,6028,1,,2018-04-13T03:14:47.773,2,280,"i was coding a cgan model using keras along with the paper ( https://arxiv.org/pdf/1411.1784.pdf ) and i wanted to try and match the models to exactly what the paper says . i knew the models presented in the paper would be primitive but just wanted to replicate those and see . for example the generator model in the paper mentions this : in the generator net , a noise prior z with dimensionality 100 was drawn from a uniform distribution within the unit hypercube . both z and y are mapped to hidden layers with rectified linear unit ( relu ) activation [ 4 , 11 ] , with layer sizes 200 and 1000 respectively , before both being mapped to second , combined hidden relu layer of dimensionality 1200 . we then have a final sigmoid unit layer as our output for generating the 784-dimensional mnist samples . so for this i had the code like this : def build_generator(self ) : model = sequential ( ) model.add(dense(200 , input_dim = self.latent_dim ) ) model.add(activation('relu ' ) ) model.add(batchnormalization(momentum=0.8 ) ) model.add(dense(1000 ) ) model.add(activation('relu ' ) ) model.add(batchnormalization(momentum=0.8 ) ) model.add(dense(1200 , input_dim = self.latent_dim ) ) model.add(activation('relu ' ) ) model.add(batchnormalization(momentum=0.8 ) ) model.add(dropout(0.5 ) ) model.add(dense(np.prod(self.img_shape ) , activation='sigmoid ' ) ) model.add(reshape(self.img_shape ) ) model.summary ( ) noise = input(shape=(self.latent_dim , ) ) label = input(shape=(1 , ) , dtype='int32 ' ) label_embedding = flatten()(embedding(self.num_classes , self.latent_dim)(label ) ) model_input = multiply([noise , label_embedding ] ) img = model(model_input ) return model([noise , label ] , img ) but still i think this not exactly what the paper means . what i understand from the paper is the noise and labels are first fed into two different layers and then combined to one layer . does this mean that there should be three separate models inside the generator ? or am i mistaken thinking that ? would like to hear any thoughts on this .",12843,9647,2018-04-15T04:17:25.013,2018-04-15T04:17:25.013,coding cgan paper model in keras,neural-networks machine-learning deep-learning generative-model,0,0,
1510,6030,1,,2018-04-13T05:50:47.720,2,438,"i have similar architecture like in image : cnn . i do n't understand how to calculate gradient of filter f . i found these equations ( source ): gradient and delta , where first equation calculate gradient of weight f ab from filter f and second equation calculate deltas of input layer and e - total error , n - input width , m - input height , l - layer . 0 - first , l = l last x - input y - output o - activation function o ' - derivate of activation function , but i do n't understand how to calculate ∂e/∂y ij l . how can i can calculate gradient of f ?",2781,,,2018-04-13T05:50:47.720,how to calculate gradient of filter in convolution network,convolutional-neural-networks backpropagation math gradient-descent,0,0,1
1511,6032,1,6537,2018-04-13T10:12:06.770,1,43,"not sure if this is the correct forum , but i have been working with a large ( non - image ) dataset that will eventually be used to train a neural network . i have been puzzling over how to manage wide data sets . for this application "" wide "" is maybe 10,000 or 20,000 points wide . it is not really possible to store this as a row in a conventional rdms ( which are usually limited to several hundred columns ) . is it better to use a huge csv file or maybe a no - sql technology like cassandra ( the data is originally in json format ) ?",14994,,,2018-05-29T00:23:27.763,how to manage high numbers of input layer data points,neural-networks deep-learning convolutional-neural-networks recurrent-neural-networks,1,1,
1512,6037,1,,2018-04-13T11:28:36.907,1,21,"the random - mmp algorithm is used as a multi - modal planner for sampling the state space . multi - modal motion planning for a humanoid robot manipulation task the robot has different modes like walk and push which has to be combined to a higher task . the idea is to create a mode graph which stores all possibilities . in the paper a probabilistic roadmap is utilized , which is similar to the rrt algorithm ( rapidly - exploring random tree ) . what i 'm not understand is the expansion strategy . according to the rrt paradigm , at first a random point outside of the graph is generated and then the nearest node inside the graph is extended to that node . but how can we do this in a multi - modal setup ?",11571,,,2018-04-13T11:28:36.907,select node in multi - modal planning algorithm,path-planning,0,0,
1513,6038,1,,2018-04-13T12:07:15.427,2,64,"i have used a ffnn and lsm to perform the same task , namely , to predict the sentence "" how are you "" . the lsm gave me more accurate results than ffnn . however , the lsm did not produce perfect prediction and there are missing letters . more specifically , lsm produced "" hw are yo "" and the ffnn predicted "" hnw brf ypu "" . what is the difference between a ffnn and a lsm , in terms of architecture and purpose ?",14723,14723,2019-05-02T19:01:40.103,2019-05-02T19:01:40.103,what is the difference between a feed - forward neural network and a liquid state machine ?,neural-networks prediction difference feedforward liquid-state-machine,0,2,
1514,6040,1,,2018-04-13T16:15:31.080,5,82,"i have been reading a bit about networks where deep layers able to deal with a bunch of features ( be it edges , colours , whatever ) . i am wondering : how can possibly a network based on this ' specialised ' layers be fooled by adversarial images ? would n't the presence of specialised feature detectors be a barrier to this ? ( as in : this image of a gun does share one feature with ' turtles ' but it lacks 9 others so : no , it ai n't a turtle ) . thanks !",14999,,,2018-08-21T21:08:04.450,how can neural networks that extract many features be fooled by adversarial images ?,convolutional-neural-networks deep-network,1,2,1
1515,6041,1,,2018-04-13T16:15:42.577,2,128,"if we model the game ' 2048 ' using a max - min game tree , what is the maximal path from a start state to a terminal state ? ( assume the game ends only when the board is full this is one of the sub - questions that should prepare us to actually modeling the game as a max - min game tree . however i 'm failing to understand the question . is it actually the path to receiving 131072 as an endgame ?",15000,,,2018-08-08T15:22:47.210,adversarial search in the game ' 2048 ',search minimax,1,0,1
1516,6046,1,,2018-04-14T02:34:47.893,5,184,"warning : this question takes us into valis territory , but i would n't underestimate the profundity of that particular philosopher . there is a non - ai definition of intelligence which is simply "" information "" ( see definition 2.3 ) . if that information is active , in terms of utilization , i have to wonder if it might qualify as a form of algorithmic intelligence , regardless of the qualities of the information . what i 'm getting at is that fields such as recreational mathematics often produce techniques and solutions that do n't have immediate , real world applications . but there 's an adage that pure math tends to find uses . so you might have algorithms applied to a problems that outside of the problems from which it originated , or that could n't initially be implemented in a computing context . ( minimax in 1928 might be an example . ) goal orientation has been widely understood as a fundamental aspect of ai , but in the case of an algorithm designed for one problem , that it subsequently applied to a different problem , the goal may simply be a function of the algorithm 's structure . ( to understand the goal of minimax precisely , you read the algorithm . ) if you regard this form of information as intelligence , then intelligence can be general , regardless of strength in relation to a given problem . can we consider this form of codification of information to be algorithmic intelligence ? and , just for fun , if a string that encodes a cutting - edge ai is not being processed , does it still qualify as artificial intelligence ?",1671,1671,2018-04-17T02:38:22.697,2018-06-24T11:38:21.833,is a mathematical formula a form of intelligence ?,philosophy definitions,1,3,2
1517,6047,1,,2018-04-14T03:25:34.920,0,727,"i have a rather basic question about yolo for bounding box detection . my understanding is that it effectively associates each anchor box to a 8-dimension output . during testing , does yolo take each anchor box and classify on it alone ? what happens if the object is big and spans over several anchor boxes ( e.g. , covering 70 % of the image ) ? how can yolo classify and detect objects spanning over many anchor boxes ?",13068,,,2018-04-18T11:34:28.503,can yolo detect large objects ?,neural-networks convolutional-neural-networks,1,2,
1518,6049,1,,2018-04-14T11:41:27.750,2,28,"i 'm trying to predict grades within a course at my university . at the moment i manually extracting features , but i 'm curious if it 's possible to somehow use my entire dataset with a deep learning approach ? i have data from throughout the course of students solving mandatory exercises . all students uses a plug - in within the editor that takes a snapshot of code - base each time the student saves the project ( exercise ) . i also have data from when the students run the debugger . all exercises include tests which determine what score the student will receive on a given exercise . the students are free to execute the tests as many times as they like wile solving the exercise ( the final score is given when the students presents the final result to a teaching assistant ) . execution and results of these tests are also included in the data . timestamps exists for all data . i also have the final grade of each student ( which is determined 100 % by the final exam ) . does anyone know of an approach to use this kind of data with a deep learning approach ?",15010,,,2018-04-14T11:41:27.750,possible to use codebase snapshots as input in deep learning ?,deep-learning,0,0,
1519,6050,1,,2018-04-14T11:58:29.497,2,84,"which specific performance evaluation metrics are used in training , validation and testing and why ? i am thinking error metrics ( rmse , mae , mse ) are used in validation , and testing should use a wide variety of metrics ? i do n't think performance is evaluated in training , but not 100 % sure . specifically i am actually after deciding when to use ( i.e. in training , validation or testing ) correlation coefficient , rmse , mae and others for numeric data ( e.g. willmott 's index of agreement , nash - sutcliffe coefficient , etc . ) sorry about this being broad - i have actually been asked to define it generally ( i.e. not for a specific dataset ) . but datasets i have been using have all numeric continuous values with supervised learning situations . generally i am using performance evaluation for environmental data where i am using ann to model . i have continuous features and am predicting a continuous variable .",15011,1671,2018-06-04T20:53:52.250,2018-06-04T20:53:52.250,"performance evaluation metrics used in training , validation and testing",training data-science generative-model concepts soft-question,0,5,2
1520,6052,1,6079,2018-04-14T22:03:15.643,3,289,"as i 've thought about ai , and what i understand of the problems that we face in the creation of it , i 've noticed a recurring pattern : we always seem to be asking ourselves , "" how can we better simulate the brain ? "" why are we so fascinated with simulating it ? is n't our goal to create intelligence , not create intelligence in a specific medium ? is n't growing and sustaining living brains more inline with our goals , albeit a bit of an ethical controversy ? why is this exchange 's description : "" for people interested in conceptual questions about life and challenges in a world where ' cognitive ' functions can be mimicked in a purely digital environment ? "" to condense these feelings in a more concise question : why are we trying to create ai in a computer ?",15023,1671,2018-04-17T20:47:10.747,2018-09-05T08:18:14.853,"why are we asking , "" how can we simulate the brain ? """,philosophy human-like theory brain wetware,7,3,
1521,6054,1,,2018-04-14T23:48:25.667,1,65,"how would you design a neural network that generates the positions of comparators in a sorting network given a set of numbers . i 've tried to modify some already implemented networks that given a set of numbers it sorts the number . my goal is that given an unsorted sequence of numbers to generate a sorting network that will sort those numbers . i am not asking for the complete solution , just a starting point .",15024,1671,2018-04-15T04:11:09.300,2018-04-15T04:11:09.300,design neural network for generating sorting networks,neural-networks ai-design ai-basics getting-started,0,1,
1522,6066,1,,2018-04-15T12:54:08.807,1,76,"my objective is simple ... classify the given sequence of images(video ) as either moving or staying still from the perspective of the person inside the car . below is an example of the sequence of 12 images animated . 1.moving from the point of the person inside the car . 2.staying still from the point of the person inside the car . methods i tried to achieve this : simple cnn ( conv2d ) with those 12 images(greyscaled ) stacked in the channels dimension.(like deepmind 's dqn ) . input to the net is ( batch_size , 200 , 200 , 12 ) 3d cnn ( conv3d ) . input to the net is ( batch_size , 12 , 200 , 200 , 1 ) cnn+lstm ( timedistributed conv2d ) . input to the net is ( batch_size , 12 , 200 , 200 , 1 ) late fusion method , which is taking 2 frames from the sequence that are some time steps apart and passing them into 2 cnns ( with same weights ) separately and concatenating them in dense layer as mentioned in this paper . this is also like cnn+lstm without the lstm part . input to this net is ( batch_size , 2 , 200 , 200 , 1 ) - > the 2 images are first and last frames in the sequence all the methods i tried failed to achieve my objective . tried tuning various hyperparameters like learning rate , no of filters in cnn layers etc . and nothing worked . all the methods had a batch_size of 8 ( due to memory constraint ) and all images are greyscaled.used relus for activations and softmax in the last layer . no pooling layer was used . any help on why my methods are failing or any pointers to a related work many thanks",5030,,,2018-06-14T14:54:48.980,video clip classification,convolutional-neural-networks,1,0,
1523,6069,1,6075,2018-04-15T16:40:31.450,4,661,"in a neural network for chess ( or checkers ) , the output is a piece or square on the board and an end position . how would one encode this ? as far as i can see choosing a starting square is 8x8=64 outputs and an ending square is 8x8=64 outputs . so the total number of possible moves is 64x64 4096 outputs . giving a probability for every possible move . is this correct ? this seems like an awful lot of outputs !",4199,9647,2018-04-17T02:11:08.553,2018-04-17T02:11:08.553,how do you encode a chess move in a neural network ?,neural-networks chess,1,0,
1524,6071,1,6076,2018-04-15T17:42:28.743,0,52,if you have a game and you are training an ai there seems to be two ways to do it . first you take the game - state and a possible move and evaluate whether this move would be good or bad : ( 1 ) game_state + possible_move -- > good or bad ? the second is to take the game state and get probabilities of every conceivable move : ( 2 ) game_state --- > probabilities for each move it seems that both models are used . i.e. in language and rnn might use ( 2 ) to find the probabilities for each next word or letter . but alphazero might use ( 1 ) . noting also that in a game like chess game_state + possible_move = new_game_state . whereas in some games you might not know the result of your move . which do you think is the best method ? which is the best way to do ai ? or some combination of the two ?,4199,,,2018-04-16T07:17:01.807,which is best : evaluation of states or probability of moves ?,machine-learning learning-algorithms,1,3,
1525,6080,1,,2018-04-16T13:19:51.720,1,33,"using a neural network the method seems to be that you end up with a probability for each possible outcome . to predict the next frame in a monochrome movie of size 400x400 with 8 shades of gray , it seems like there seems to be : 8^(160000 ) possibilities . on the other had if you just predicted the probability for each pixel individually you would end up with some kind of image which gets progressively blurred . perhaps what you want is to generate a few possibilities that are none - the - less quite sharp . in a similar way to weather prediction ( ? ) so how would you go about designing a neural network that takes reads a movie and tries to predict the next frame ?",4199,,,2018-04-16T13:19:51.720,best way to predict future frame of movie or game ?,neural-networks image-recognition prediction,0,0,
1526,6081,1,,2018-04-16T16:49:11.807,1,25,"i am interested to see what advantages a loop network ( feed forward network that takes its output as input , i think it 's called an rnn , not sure ) . the only result i found was that it was extremely sensitive to context , but only the context behind it . other than that , i did not notice any changes . i figured this would be better for a language processing unit , or one used to make inferences based upon it . what are the shortcomings of each ? advantages ?",14723,,,2018-04-16T16:49:11.807,what s advantages does a loop network have over a feed forward network ?,ai-design recurrent-neural-networks,0,0,
1527,6082,1,,2018-04-16T17:00:14.823,4,149,"there is a popular story regarding the back - of - the - envelope calculation performed by a british physicist named g. i. taylor . he used dimensional analysis to estimate the power released by the explosion of a nuclear bomb , simply by analyzing a picture that was released in a magazine at the time . i believe many of you know some nice back - of - the - envelope calculations performed in machine learning ( more specifically neural networks ) . can you please share them ?",6252,2444,2019-04-12T21:32:26.420,2019-05-12T22:01:30.257,back - of - the - envelope machine learning ( specifically neural networks ) calculations,neural-networks machine-learning artificial-neuron,1,2,1
1528,6083,1,,2018-04-16T19:20:57.557,2,234,"i am looking to train a chatbot that can help me relieve stress and deal with my negative emotions . i would like for the chatbot to be like the ones that pass the turing test , remain professional , yet caring , and still be useful . i do not want my chatbot to become too general and unable to take the specific actions like recommending resources and providing general advice that i need it to . my two approaches are to manually train the chatbot with expected intents and expressions it would need to understand or use a custom deep learning model on conversational data . i think i have the right data to train the chatbot on , but i am not sure if this is the right approach . any help on the direction to take the chatbot in would be helpful .",15063,,,2018-11-27T12:23:13.543,ai chatbot design,deep-learning ai-design chat-bots emotional-intelligence,1,14,1
1529,6085,1,,2018-04-17T06:48:15.340,1,35,"i 'm new to cnns and am wondering if i understand the relationship between the following terms : in image analysis , receptive fields group "" input neurons "" to reduce the connection to the next layer . convolution ( using kernels or filters ) reduces the scale but produces depth in the form of feature maps which isolate shapes for example . each subsequent process reduces scale but produces greater depth . pooling is a process used further reduce data sets by ( in the case of max pooling ) taking the "" brightest "" or strongest features and generalizing . the stride is the "" resolution "" and a longer stride will reduce accuracy but could act like pooling to what extent is my understanding of the relationships accurate ? thank you",15074,,,2018-04-17T06:48:15.340,the relationship between cnn terms,convolutional-neural-networks,0,0,
1530,6088,1,,2018-04-17T13:57:21.350,-4,118,"i am searching for a publishable research project in agi . i have previous experience with machine learning(via andrew ng 's course ) , but i want to explore other aspects of intelligence apart from pattern recognition . given my undergraduate computer science background , any suggestions as to what i can work on ? no constraint on the programming language(i have comfortable with python ) . mathematical background : linear algebra , will learn probability if required .",15083,1581,2018-09-28T18:28:27.443,2018-10-10T23:07:25.910,what are some of the projects to contribute to inline with artificial general intelligence ?,cognitive-science artificial-consciousness,2,3,
1531,6089,1,,2018-04-17T14:02:49.700,5,171,"i am trying to understand how genetic programming can be used in autoencoders . i am going through a few papers ( the classic one and another ) but they do nt help me to even grasp the concept of genetic programming in autoencoders . i understand that autoencoders are supposed to reconstruct the instances of the particular classes they have been trained on . if another fed instance is not reconstructed as expected , then it could be called an anomaly . how does one use genetic programming in autoencoders ? you are still required to create a neural network just instead of feed forward you use autoencoders ? i would appreciate any tutorials or explanations .",14863,1671,2018-04-23T21:28:01.097,2018-12-07T19:03:09.033,genetic programming in autoencoder,genetic-programming autoencoders,2,1,1
1532,6091,1,,2018-04-17T15:29:02.883,2,214,"researchers at stanford university released this paper in 2012 : http://cs229.stanford.edu/proj2012/bernalfokpidaparthi-financialmarkettimeseriespredictionwithrecurrentneural.pdf it goes on to discuss how they used echo state networks to predict things such as google 's stock prices . however to do this once trained , the network 's inputs are a day 's stock price , and the output is the day 's predicted stock price . the way the paper is worded is like this could be used to predict future stock prices for example . however , to predict tomorrows stock price , you need to give the neural network tomorrows stock price ... all this paper seems to show is that the neural network is converging on a solution where it simply modifies its inputs a minimal amount , hence the output of the esn is just a small alteration of its input . here are some python implementations of the work shown in this paper : https://github.com/kimanalytics/recurrent-neural-network-to-predict-stock-prices https://github.com/europa502/rnn-based-bitcoin-value-predictor in particular , i was playing with the latter which produces the following graph : if i take the same trained network , and alter the 7th 's day 's "" real "" stock price to say something extreme like $ 0 , this is what comes out : as you can see , it basically regurgitates its inputs . so , what is the significance of this paper ? it has no use in any financial predictions like the network shown in this paper : https://arxiv.org/pdf/1603.08604.pdf",15085,,,2019-05-01T09:01:55.513,"what is the significance of this stanford university "" financial market time series prediction with rnn 's "" paper ?",recurrent-neural-networks,2,1,2
1533,6092,1,6096,2018-04-17T16:28:17.193,2,435,"i am having a difficult time translating this pseudocode into functional c++ code . at line 10 : the value function is represented as v[s ] , which has a bracket notation like arrays . is this a separate method or just a function of the value with a given state ? why is s inside the brackets ? is this supposed to be an array with as many elements as s ? at line 12 : vk would be the element in index k inside of array v ? at line 16 : i 'm interpreting this as the start of a do - while loop that ends at line 20 . line 19 : i 'm finding the action that maximizes the sum , for all states , of the equation following the sigma ? line 20 : i 'm interpreting this a the end of the do - while . but what is this condition ? am i checking if there is an s such that this condition applies ? so would i would have to loop between all states and stop if any state satisfies the condition ? ( basically a loop with a break , instead of a while )",15089,,,2018-04-18T08:48:53.563,value iteration algorithm from pseudo - code to c++,learning-algorithms c++,1,1,
1534,6094,1,,2018-04-18T00:42:14.503,4,63,"i have order data , here 's a sample : ninety - six ( 96 ) covered pans , desinated mark cutlery . 5 vovered pans by knife co. ( see schedule a for numbers ) . 757 soup pans 115 10-quart capacity pots . thirteen ( 13 ) , 30 mm thick covered pans . i have over 50k rows of data such as this . in a perfect world , the above would need to be tabulated as such : count , type 96 , covered pan 5 , covered pan 757 , soup pan 115 , pot 13 , covered pan could machine learning be the correct approach for a problem such as this ?",15099,15099,2018-04-18T00:49:36.130,2018-12-18T23:01:55.383,can machine learning help me digest asymmetrical order descriptions ?,machine-learning ai-basics datasets,3,0,
1535,6098,1,,2018-04-18T09:53:43.113,3,64,"i 'm trying to implement a tree sort for 8 numbers . i 've created 15 tree nodes agents and one manager agent . what i 'm trying to achieve is to synchronize leaves in the tree and send their generated numbers to manager node . my problem is that in my for cycle , the leaves are not synchronized so manager is waiting for next leaf but that leaf may have sent it 's number already . is there a way to synchronize these leaves , so that manager waits for each leaf and then prints out received number ? here are the agents : treesort.mas2j mas treesort { infrastructure : centralised agents : manager ; agent#15 ; } manager.asl : ! start . + ! start : true & lt;- for ( .range(i,8,15 ) ) { .concat(""agent"",i , tempagent ) ; .print(""waiting from "" , tempagent ) ; .term2string(yt , tempagent ) ; .wait(recievednum(yt , x ) ) ; .print(""from "" , yt , "" recvd "" , x ) ; } ; .println ( "" done "" ) . agent.asl ! start . @p1 + ! start : .my_name(agent8 ) | .my_name(agent9)| .my_name(agent10)|.my_name(agent11 ) | .my_name(agent12 ) | .my_name(agent13 ) | .my_name(agent14 ) |.my_name(agent15 ) & lt;- + mynum(math.round(math.random(100 ) ) ) ; ? mynum(x ) ; + iam(list ) ; .my_name(y ) ; .send(manager , tell , recievednum(y , x ) ) . @p2 + ! start : .my_name(agent7 ) | .my_name(agent6)| .my_name(agent5 ) |.my_name(agent4 ) | .my_name(agent3 ) | .my_name(agent2 ) & lt;- + iam(node ) . @p3 + ! start : .my_name(agent1 ) & lt;- + iam(root ) . this is the output of program : https://imgur.com/a/ewoauxv . as you can see , the manager is waiting for number from agent9 bu because he already got that number from him waiting for agent8 , he will now wait forever . there is managers database : https://imgur.com/a/zz35pd9 .",15105,15105,2018-04-19T17:45:59.677,2018-04-19T17:45:59.677,jason / agentspeak trying to synchronize agents,multi-agent-systems,0,2,
1536,6099,1,6104,2018-04-18T10:36:13.770,13,1080,"does the human brain use a specific activation function ? i 've tried doing some research , and as it 's a treshold for whether the signal is sent through a neuron or not , it sounds a lot like relu . however , i ca n't find a single article confirming this . or is it more like a step function ( it sends 1 if it 's above the treshold , instead of the input value ) .",15107,,,2018-08-13T00:50:50.507,what activation function does the human brain use ?,neural-networks machine-learning brain,3,1,7
1537,6102,1,,2018-04-18T12:01:49.907,10,259,"imagine a game where it is a black screen apart from a red pixel and a blue pixel . given this game to a human , they will first see that pressing the arrow keys will move the red pixel . the next thing they will try is to move the red pixel onto the blue pixel . give this game to an ai and it will randomly move the red pixel until a million tries later it accidentally moves onto the blue pixel to get a reward . if the ai had some concept of distance between the red and blue pixel it might try to minimise this distance . without actually programming in the concept of distance , if we take the pixels of the game can we calculate a number(s ) such as "" entropy "" or suchlike that would be lower when pixels are far apart than when close together ? it should work with other configurations of pixels . such as a game with three pixels where one is good and one is bad . just to give the neural network more of a sense of how the screen looks ? then give the nn a goal such as try and minimise the entropy of the board as well as try to get rewards . is there anything akin to this in current research ?",4199,10135,2018-10-18T10:44:59.717,2018-10-18T10:44:59.717,can a neural network work out the concept of distance ?,neural-networks game-ai path-planning teaching-concepts,4,2,
1538,6105,1,,2018-04-18T16:21:19.990,-2,103,"i need a word database to train from . i found a word2vec js word vector database , but i need a method to teach it which words go in which patterns . please note that i am not asking to have you recommend a database , just asking for some qualities that i should look for in a database . i want it to be able to run at a notice , and not take 10-ish minutes to set up . i am using word2vec word vectors , provided in json form by anthony liu . i am using the 1000 word list . tl;dr what are some qualities i should look for in a word database to train from ?",14723,2444,2019-04-16T22:36:15.353,2019-04-16T22:36:15.353,i need a word database ... any qualities i should look for ?,natural-language-processing word-embedding word2vec,1,6,
1539,6111,1,6113,2018-04-18T18:09:17.847,1,254,"deep learning refers to set of techniques for learning in neural nets . so does the statement "" reimplementation of deep learning algorithms replicating performance from papers "" mean implementing these algorithms using which neural networks learn their weights and measuring do they perform as well as said in the research paper ? or implementing a model that some research paper suggests and replicating its results ? context : openai machine learning fellow position demands this in an applicants profile state exactly in the following words : "" we look for candidates with one or more of the following credentials : open - source reimplementations of deep learning algorithms which replicate performance from the papers "" therefore , what is it that they are technically looking for ?",15116,,,2018-04-18T19:03:46.427,"what does "" reimplementation of deep learning algorithms replicating performance from papers "" mean ?",neural-networks machine-learning deep-learning research,2,0,
1540,6115,1,,2018-04-19T06:22:13.080,4,81,"i am working on an anti - fraud project . in the project , we are trying to predict the fraud user in the out time data set . but the fraud user has a very low ratio , only 3 % . we expect a model with a precision more than 15 % . i tried logistic regression , gbdt+lr , xgboost . all models are not good enough . step wise logistic regression performs best , which has a precision of 9 % with recall rate 6 % . is there any other models that i can use for this problem or any other advise ?",15126,12542,2018-04-19T07:57:38.437,2018-04-20T05:44:58.960,what model to use for fully unbalanced data ?,ai-design,2,2,1
1541,6116,1,,2018-04-19T08:32:55.490,3,430,"i tried to build a neural network from scratch to build a cat or dog binary classifier using a sigmoid output unit . i seem to get the output value around 0.5(+/- 0.002 ) for every input . this seems really weird to me . here 's my code , please let me know if there is a mistake in the implementation . def initialize_parameters_deep(layer_dims ) : l = len(layer_dims ) parameters= { } for l in range(1,len(layer_dims ) ) : parameters['w'+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])*0.01 parameters['b'+str(l)]=np.zeros((layer_dims[l],1 ) ) return parameters def linear_forward(a , w , b ) : z = np.dot(w , a)+b cache=(a , w , b ) return z , cache def sigmoid(z ) : a = 1/(1+np.exp(-z ) ) cache = z return a , cache def relu(z ) : a = np.maximum(0,z ) assert(a.shape = = z.shape ) cache = z return a , cache def relu_backward(da , cache ) : z = cache dz = np.array(da , copy = true ) # just converting dz to a correct object . # when z & lt;= 0 , you should set dz to 0 as well . dz[z & lt;= 0 ] = 0 assert ( dz.shape = = z.shape ) return dz def sigmoid_backward(da , cache ) : z = cache s = 1/(1+np.exp(-z ) ) dz = da * s * ( 1-s ) assert ( dz.shape = = z.shape ) return dz def linear_activation_forward(a_prev , w , b , activation ) : if(activation=='sigmoid ' ) : z , linear_cache = linear_forward(a_prev , w , b ) a , activation_cache = sigmoid(z ) elif activation=='relu ' : z , linear_cache = linear_forward(a_prev , w , b ) a , activation_cache = relu(z ) cache=(linear_cache , activation_cache ) return a , cache def l_model_forward(x , parameters ) : a = x l = len(parameters)//2 caches= [ ] for l in range(1,l ) : a , cache = linear_activation_forward(a , parameters['w'+str(l)],parameters['b'+str(l)],'relu ' ) caches.append(cache ) al , cache = linear_activation_forward(a , parameters['w'+str(l)],parameters['b'+str(l)],'sigmoid ' ) caches.append(cache ) return al , caches def compute_cost(al , y ) : m = y.shape[1 ] cost=-1 / m*np.sum(np.multiply(np.log(al),y)+np.multiply(np.log(1-al),1-y ) ) return cost def linear_backward(dz , cache ) : a_prev , w , b = cache m = a_prev.shape[1 ] dw = np.dot(dz,a_prev.t)/m db = np.sum(dz,axis=1,keepdims=true)/m da_prev = np.dot(w.t,dz ) return da_prev , dw , db def linear_activation_backward(activation , da_prev , cache ) : linear_cache , activation_cache = cache if activation=='sigmoid ' : dz = sigmoid_backward(da_prev , activation_cache ) da_prev , dw , db = linear_backward(dz , linear_cache ) if activation=='relu ' : dz = relu_backward(da_prev , activation_cache ) da_prev , dw , db = linear_backward(dz , linear_cache ) return da_prev , dw , db def l_model_backward(al , y , caches ) : l = len(caches ) m = al.shape[1 ] y = y.reshape(al.shape ) dal = - ( np.divide(y , al ) - np.divide(1 - y , 1 - al ) ) grads= { } current_cache = caches[-1 ] grads['da'+str(l-1)],grads['dw'+str(l)],grads['db'+str(l)]=linear_activation_backward('sigmoid',dal , current_cache ) for l in reversed(range(l-1 ) ) : current_cache = caches[l ] da_prev_temp , dw_temp , db_temp = linear_activation_backward('relu',grads['da'+str(l+1)],current_cache ) grads[""da "" + str(l ) ] = da_prev_temp grads[""dw "" + str(l + 1 ) ] = dw_temp grads[""db "" + str(l + 1 ) ] = db_temp return grads def grad_desc(parameters , grads , learning_rate ) : l = len(parameters)//2 for l in range(l ) : parameters['w'+str(l+1)]=parameters['w'+str(l+1)]-learning_rate*grads['dw'+str(l+1 ) ] parameters['b'+str(l+1)]=parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1 ) ] return parameters def l_layer_model(x , y , learning_rate , num_iter , layer_dims ) : parameters = initialize_parameters_deep(layer_dims ) costs= [ ] for i in range(num_iter ) : al , caches = l_model_forward(x , parameters ) cost = compute_cost(al , y ) grads = l_model_backward(al , y , caches ) parameters = grad_desc(parameters , grads , learning_rate ) if i%100==0 : print(cost ) costs.append(cost ) plt.plot(np.squeeze(costs ) ) def predict(x , parameters ) : al , caches = l_model_forward(x , parameters ) prediction=(al&gt;0.5 ) return al , prediction l_layer_model(x_train , y_train,0.0075,12000,[12288,20,7,5,1 ] ) prediction = predict(x_train , initialize_parameters_deep([12288,20,7,5,1 ] ) )",15128,15122,2018-04-19T18:54:40.377,2018-04-25T04:23:48.983,neural network returns about the same output(mean ) for every input,neural-networks deep-learning image-recognition,1,12,1
1542,6117,1,,2018-04-19T10:41:16.887,7,101,"how can we attach numerical value to the measurement of soft skill ? is there any book , site or materials that can teach me how to do this ? i want to know if there is any way we can measure soft skill as listed here and say a person has ( over 100 ) 70 % or 60 % in critical thinking ability or his critical thinking ability is over 72 % and be accurate with the score .",15132,1671,2018-10-15T17:33:52.003,2019-01-02T20:02:47.100,measuring soft skill,theory reference-request problem-solving reasoning quantification,2,3,
1543,6118,1,,2018-04-19T11:05:26.483,3,318,"one of my friends built a version of "" achtung , die kurve ! "" , or "" curve fever "" in python . i was starting to study ml and decided to tackle the game from a learning perspective - write a bot that would crush him in the game . did some research and found deep q learning . decided to go with that and after a whole lot of throwing around different hyperparameters and layers , i decided i need some help on this . i am new to deep and machine learning in general , so i may have missed things . i was kinda discouraged when i saw that deep q is so impractical currently in the field . how would you guys tackle this problem ? i need some guidance / help building it if someone is up to the task .",15133,,,2018-04-19T18:02:50.950,"how should i approach the game "" achtung , die kurve "" ( "" curve fever "" ) using ai ?",neural-networks machine-learning deep-learning python,1,10,0
1544,6125,1,6127,2018-04-20T10:49:59.647,1,72,"so i know that ' h ' and ' f ' will be pruned , but i 'm not sure about ' k ' and ' l ' . when we visit ' j ' , technically there is no need for us to visit ' k ' and ' l ' because there are 2 options : one or two of them might be higher than 8 ( ' j ' ) both of them less than 8 but no matter what , the decision of the max(root ) will not change , the max will choose the right side no matter what ' k ' and ' l ' are , because the right side will either be 8 or 9 , which is still higher than 4 ( returned value from left side ) so will alpha beta prune ' k ' and ' l ' or not ? if not , then it means alpha beta is not "" optimal "" overall right ? considering it will not prune all the unnecessary paths .",12782,12782,2018-04-20T13:57:30.063,2018-06-19T17:49:50.150,which edges of this tree will be pruned by alpha - beta pruning ?,ai-design game-ai logic,1,0,
1545,6126,1,6131,2018-04-20T12:30:53.137,4,641,"in mcts , we start at root node r. then we select some leaf node l. and we expand it by one or more child nodes and simulate from the child to end of game . image link my question is when to expand ? and when to simulate ? so why not expand 2 or 3 levels , and simulate from there , then backup values to top ? should we just expand 1 level ? why not more ? what is the criteria ?",15152,,,2018-04-21T08:22:17.980,when to expand and when to simulate in mcts ? ( monte carlo tree search ),monte-carlo-tree-search,1,0,
1546,6129,1,,2018-04-21T02:43:55.633,2,360,"i am training pre - trained ssd - inceptionv2-coco to detect the "" car "" , which is one of the classes in mscoco label . i train the model with ~50k sample from kitti , 500k iteration with batch size 2 . i followed this script to generate tfrecord file . then i test both original pre - trained model and my trained model with one video . the performance of my trained model is worse . more missing detected results . one thing i found recently is the classification_loss / localization_loss increases when avgnumgroundtruthboxesperimage increases . edit another thing i found is the more ground truth boxes per image i have , the less average num positive anchors per image i have . this bothers me because if the number of anchors generated per image is fixed , more ground truth boxes should provide more positive anchors per image . so i wonder where to find the root cause . any suggestion is welcome . thank you for precious time on my question .",15162,15162,2018-04-28T23:41:11.890,2018-04-28T23:41:11.890,getting worse performance when training a pre - trained model with the existing class,neural-networks machine-learning deep-learning tensorflow object-recognition,0,0,
1547,6130,1,,2018-04-21T03:54:58.987,1,59,"everything from facial recognition to the google home is coming equiped with a.i and it is being widely used , if autonomously connected to the internet , will a.i pose a threat to privacy or will it endanger free will if used for surveillance with facial recognition , like in the movie ' minority report '",15164,,,2018-04-21T03:54:58.987,will commercialisation and widespread use of a.i in security and surveillance and other household products threaten free will or endanger privacy ?,machine-learning prediction self-driving facial-recognition,0,5,
1548,6134,1,6169,2018-04-21T11:25:10.503,1,41,"a number has randomly been chosen from 1 to 3 . in each step we can make a guess and we will be told if our guess is equal , bigger or smaller than the chosen number . we 're trying to find the number with the least number of guesses . i need to draw the mdp model for this question with 7 states but i do n't know how the states are supposed to be defined . can anyone help ?",14603,,,2018-04-30T06:48:12.777,mdp model for binary search,machine-learning reinforcement-learning markov-chain,1,0,
1549,6135,1,6136,2018-04-21T13:51:56.900,1,144,"suppose one is using a multi - armed bandit , and one has relatively few "" pulls "" ( i.e. timesteps ) relative to the action set . for example , maybe there are 200 timesteps and 100 possible actions . however , you do have information on how similar actions are to each other . for example , i might want to rent a car , and know the model , year , and mileage of each car . ( specifically , i want to rent a car on a daily basis for each day in a 200 day period ; on each day , i can either continue with the existing car or rent a new one . there are 100 possible cars . ) how can i exploit this information to choose actions that maximize my payoff .",12656,12656,2018-04-21T14:32:57.943,2018-04-21T19:24:15.090,large action set in multi - armed bandits,reinforcement-learning,1,1,
1550,6137,1,,2018-04-22T08:17:57.023,1,41,"previously i had trained a neural network upon 20,000 character images . this neural net generally works well , it uses rgb- hue , saturation , intensity feature set for training . however , there can be certain character images which have rgb - hsi values that this neural net has not seen before . therefore i am looking forward to converting training data to grayscale and use some feature set well suited for grayscale images . so are there any good suggestion for extracting a feature set out of grayscale images .",15116,,,2018-04-22T08:17:57.023,feature set out of grayscale images for training a neural network ?,neural-networks image-recognition datasets artificial-neuron,0,0,
1551,6139,1,6967,2018-04-23T10:29:22.173,2,275,"problem given a collection of pairs ( x , y ) where x belongs to r^n and y belongs to r , find the x such that the associated y would be maximum . example given : ( x=(1 , 2 ) , y=-9 ) ( x=(-2 , 4 ) , y=-36 ) ( x=(-4 , 2 ) , y=-24 ) ... the algorithm should be able to detect that the function being applied to x is y=-(x[0]^2 + 2*(x[1]^2 ) ) and find the input that maximizes this function , in this case x=(0,0 ) because y=0 ^ 2 + 2 * 0 ^ 2=0 and 0 is the maximum possible value , as all the other values are negative . how i 've tried to solve it my first guess has been to create a neural network that predicts y given x , but , after that is done , i do n't know how to go about optimizing the input . questions is there any algorithm that would help in this situation ? also , would some other supervised learning algorithm fit better here than a neural network ?",15196,15196,2018-04-23T15:44:33.447,2018-06-30T18:39:53.747,input optimization on a supervised learning system,neural-networks ai-design optimization,2,6,
1552,6142,1,,2018-04-23T16:41:08.710,3,209,"this is ai : a modern approach , 3.17c . the solution manual gives the answer as , where $ d$ is the depth of the shallowest goal node . iterative lengthening search uses a path cost limit on each iteration , and updates that limit on the next iteration to the lowest cost of any rejected node . i have seen this question posted elsewhere as , "" what is the number of iterations with a continuous range $ [ 0 , 1]$ and a minimum step cost ? "" in that case , i agree that the minimum number of iterations is because you would need to increase the path cost limit by a minimum of with each iteration . however , with a continuous range of $ [ \epsilon , 1]$ , it seems there is an infinite range and that the number of iterations is potentially infinite , since there is no minimum step cost . should this solution actually be infinite ?",2897,1641,2018-08-11T11:51:24.537,2019-05-08T15:02:35.517,"how many iterations are required for iterative - lengthening search when step costs are drawing from a continuos range [ ϵ , 1 ] ?",search,1,0,
1553,6144,1,,2018-04-23T18:58:09.750,4,44,"word2vec assigns an n - dimensional vector to given words ( which can be considered a form of dimensionality reduction ) . it turns out that , at least with a number of canonical examples , vector arithmetic seems to work intuitively . for example "" king + woman - man = queen "" . these terms are all n - dimensional vectors . now , suppose , for simplicity , that $ n=3 $ , , then the expression above can be written as $ [ 0 , 1 , 2 ] + [ 1 , 1 , 0 ] - [ 2 , 2 , 2 ] = [ -1 , 0 , 0]$ . in this ( contrived ) example , the last dimension ( king / man=2 , queen / woman=0 ) suggests a semantic concept of gender . aside from semantics , a given dimension could "" mean "" a part of speech , first letter , or really any feature or set of features that the algorithm might have latched onto . however , any perceived "" meaning "" of a single dimension might well just be a simple coincidence . if we picked out only a single dimension , does that dimension itself convey some predictable or determinable information ? or is this purely a "" random "" artefact of the algorithm , with only the full n - dimensional vector distances mattering ?",13360,2444,2019-04-16T22:30:58.130,2019-04-16T22:30:58.130,do individual dimensions in vector space have meaning ?,natural-language-processing word2vec word-embedding,1,0,
1554,6154,1,,2018-04-24T12:52:12.787,3,859,"i have implemented a neural network ( nn ) using python and numpy only for learning purposes . i have already coded learning rate , momentum , and l1 / l2 regularization and checked the implementation with gradient checking . a few days ago , i implemented batch normalization using the formulas provided by the original paper . however , in contrast with learning / momentum / regularization , the batch normalization procedure behaves differently during fit and predict phases - both needed for gradient checking . as we fit the network , batch normalization computes each batch mean and estimates the population 's mean to be used when we want to predict something . in a similar way , i know we may not perform gradient checking in a neural network with dropout , since dropout turns some gradients to zero during fit and is not applied during prediction . can we perform gradient checking in nn with batch normalization ? if so , how ?",13036,15219,2018-04-24T19:04:07.020,2018-09-25T15:01:39.523,how to perform gradient checking in a neural network with batch normalization ?,neural-networks python gradient-descent,2,0,1
1555,6155,1,,2018-04-24T17:45:15.730,2,358,"theoretically speaking , when the singularity happens , could we aptly describe this event as the creation of god ? i am agnostic in the sense that i do n't believe any of the gods of the present or past are real in any way shape or form , however when the singularity happens , is this even essentially the birth of a god ? i mean a superhuman being , machine or spirit worshipped as having power over nature or human fortunes is by definition a god .",13252,1671,2018-04-24T19:14:48.107,2018-04-24T19:14:48.107,singularity and god,philosophy ultraintelligent-machine singularity mythology-of-ai,1,1,2
1556,6158,1,,2018-04-24T18:44:36.500,2,569,"to start , let me just say that i am very new to tensorflow and machine learning in general . but , as part of my learning project i am trying to adapt the tensorflow wide and deep model to generate movie recommendations . however , the part i 'm getting stuck on is handling multiple values for a categorical column . below is a sample of how a couple of rows in my csv look like . id , genres , tags , rating , recommendations ---------------------------------------- 1,genre1 : genre2 : genre3,tag1 : tag3,4.3,44 2,genre2 : genre3,tag1 : tag5,3.7,22 the genres and tags column have multiple categorical values . i have looked at tf.string_split to parse the strings and return a sparsetensor . once i have parsed my delimited string values into sparsetensor , what do i do with ? if i want to create categorical_column_with_vocabulary_file , how does the sparsetensor interact with it ? is that even the correct step ? should the sparsetensor be converted into a something before i can create a categorical_column _ * ? i am just not sure how to train the wide and deep model when you have multiple categorical values for a single column . some people have suggested that i use each ' genre ' as its own column and encode it using one - hot , but it is not realistic for me to do this , because there could be 100s of genres and tags in the data . any help on this matter would be welcome . thank you ! !",15220,,,2019-05-11T13:02:30.483,how do you handle multiple categorical values in a single column for wide_deep model in tensorflow ?,neural-networks deep-learning tensorflow python linear-regression,1,0,3
1557,6161,1,,2018-04-24T19:12:17.230,4,127,"with so much innovation , with so much manual labor being replaced tasks performed in minutes or seconds by an artificial intelligence , one day man will put the survival and propagation of his species above his ideologies and cultures ... i am worried because we are living the fourth industrial revolution , and this will generate millions of unemployment , even if new jobs are created in the future . the problem is that a lot of humans worry about their own job , and not about their own children 's future . this is completely retrograde . will one day artificial intelligence be able to direct us towards an intelligent path as a propagation of the species , or else center the focus of humanity on something that it adds ?",7800,,,2018-10-30T21:00:04.473,will artificial intelligence make the human more rational ?,philosophy ultraintelligent-machine,2,4,2
1558,6162,1,,2018-04-24T23:11:25.637,1,440,"semi - gradient methods work well in reinforcement learning , but what is there a reason of not using the true gradient if it can be computed ? i tried it on the cart pole problem with a deep q - network and it performed much worse than traditional semi - gradient . is there a concrete reason for this ?",15224,2444,2019-05-10T14:40:15.003,2019-05-10T14:40:15.003,"why use semi - gradient instead of full gradient in rl problems , when using function approximation ?",reinforcement-learning gradient-descent dqn deep-rl function-approximation,1,0,1
1559,6164,1,6190,2018-04-25T03:05:35.160,1,104,"i am developing pda like google assistant on android . so far , so good . but now , i want to add contextual follow up like google assistant so it can keep the train of thought . as demonstrated here- https://www.youtube.com/watch?v=xyrenguwwca can anyone guide me or hint how to design the algorithm ? thank you very much !",4869,4869,2018-04-25T15:58:23.793,2018-04-30T07:38:58.637,how to add contextual follow up like google assistant,ai-design algorithm,1,0,
1560,6166,1,,2018-04-25T09:17:26.863,1,34,"i am unable to identify general temrs or specific source of information for the below proposed problem . i would appreciate if the community can guide me to journal articles / books and keywords to look for in literature . problem : there is a non - linear dynamic system taking input and producing 1d time series as output . i would like to use nn to find parameters of the dynamic system , according to the time series output . that is , mapping the features of the time series ( after transformation , likely fourtier transform or wavelet ) to the parameters governing the dynamics of the system . research so far : i have found a few journal papers mostly processing sounds of rolling bearings or hearbeat but only for error / failure classification . rolling bearing fault diagnosis based on stft - deep learning and sound signals deep learning enabled fault diagnosis using time - frequency image analysis of rolling element bearings deep learning based approach for bearing fault diagnosis detecting atrila fibrillation be deep convolutional neural networks ( the above are classification problems , my problem is about parameter identification ) reason to address this on stackexchange : i think i am missing overview about the topic ( identification of dynamic systems using nn ) , because i am not able to reach more profound information . also , i think that nn would be more beneficial to my current application than lets say optimization by evolutionary algorithms , threfore i am specifically asking for nn .",15231,,,2018-04-25T09:17:26.863,abstracting parameters of dynamic model from output time series,neural-networks models,0,6,
1561,6167,1,6173,2018-04-25T12:31:48.297,2,1337,"i 'm trying to understand what would be the best neural network for implementing a xor gate . i 'm considering a neural network to be good if it can produce all the expected outcomes with the lowest possible error . it looks like my initial choice of random weights has a big impact on my end result after training . the accuracy ( i.e. error ) of my neural net is varying a lot depending on my initial choice of random weights . i 'm starting with a 2 x 2 x 1 neural net , with a bias in the input and hidden layers , using the sigmoid activation function , with a learning rate of 0.5 . below my initial setup , with weights chosen randomly : the initial performance is bad , as one would expect : input | output | expected | error ( 0,0 ) 0.8845 0 39.117 % ( 1,1 ) 0.1134 0 0.643 % ( 1,0 ) 0.7057 1 4.3306 % ( 0,1 ) 0.1757 1 33.9735 % then i proceed to train my network through backpropagation , feeding the xor training set 100,000 times . after training is complete , my new weights are : and the performance improved to : input | output | expected | error ( 0,0 ) 0.0103 0 0.0053 % ( 1,1 ) 0.0151 0 0.0114 % ( 1,0 ) 0.9838 1 0.0131 % ( 0,1 ) 0.9899 1 0.0051 % so my questions are : has anyone figured out the best weights for a xor neural network with that configuration ( i.e. 2 x 2 x 1 with bias ) ? why my initial choice of random weights make a big difference to my end result ? i was lucky on the example above but depending on my initial choice of random weights i get , after training , errors as big as 50 % , which is very bad . am i doing anything wrong or making any wrong assumptions ? so below is an example of weights i can not train , for some unknown reason . i think i might be doing my backpropagation training incorrectly . i 'm not using batches and i 'm updating my weights on each data point solved from my training set . weights : ( ( -9.2782 , -.4981 , -9.4674 , 4.4052 , 2.8539 , 3.395 ) , ( 1.2108 , -7.934 , -2.7631 ) )",15235,15235,2018-04-25T17:05:14.123,2018-05-03T19:06:40.110,what is the best xor neural network configuration out there in terms of low error ?,neural-networks training backpropagation,3,4,
1562,6170,1,6177,2018-04-25T15:17:27.333,1,147,"i was following daniel shiffman 's tutorials on how to write your own neural network from scratch . i specifically looked into his videos and the code he provided in here . i rewrote his code in python , however , 3 out of 4 of my outputs are the same . the neural network has two input nodes , one hidden layer with two nodes and one output node . can anyone help me to find my mistake ? here is my full code . import random nn = neuralnetwork(2,2,1 ) inputs = np.array([[0 , 0 ] , [ 1 , 0 ] , [ 0 , 1 ] , [ 1 , 1 ] ] ) targets = np.array([[0 ] , [ 1 ] , [ 1 ] , [ 0 ] ] ) zipped = zip(inputs , targets ) list_zipped = list(zipped ) for _ in range(9000 ) : x , y = random.choice(list_zipped ) nn.train(x , y ) output = [ nn.feedforward(i ) for i in inputs ] for i in output : print(""output "" , i ) # output [ 0.1229546 ] when it should be around 0 # output [ 0.6519492 ] ~1 # output [ 0.65180228 ] ~1 # output [ 0.66269853 ] ~0 edit_1 : i tried debugging my code by choosing all weights and bias ' values to 0.5 . i did this in both my code and daniel 's . this obviously ended up showing me all outputs with the same value . after that i increased my weights and bias ' values variety from [ 0 , 1 ) to [ -1 , 1 ) . by running this a few times , i would sometimes get the correct output : [ 0.93749991 ] # should be ~1 [ 0.93314793 ] # ~1 [ 0.07001175 ] # ~0 [ 0.06576194 ] # ~0 if i ran nn.train ( ) 100 000 times , i get the correct output 2/3 times . is this the issue of gradient descent , where it converges to the local minima ?",14863,14863,2018-04-27T08:21:28.457,2018-04-27T08:21:28.457,neural network returns similar output,neural-networks python,1,2,
1563,6176,1,6184,2018-04-26T11:41:28.273,5,145,"i 'm starting a project that will involve computer vision , visual question answering and explainability , and am currently choosing what type of algorithm to use for my classifier - a neural net , or a decision tree . it would seem to me that , because i want my system to include explainability , a decision tree would be the best choice . decision trees are interpretable , whereas neural nets are like a black box . the other differences i 'm aware of are : decision trees are faster , neural networks are more accurate , and neural networks are better at modelling nonlinearity . in all of the research i 've done on computer vision and visual question answering , everyone uses neural networks , and no one seems to be using decision trees . why ? is it for the accuracy ? i think a decision tree would be better because it is fast and interpretable , but if no one 's using them for visual question answering , they must have a disadvantage that i have n't noticed .",9983,1671,2018-04-26T16:03:22.397,2018-04-27T16:27:26.890,why does nobody use decision trees for visual question answering ?,neural-networks computer-vision comparison decision-tree,1,1,
1564,6179,1,,2018-04-26T13:42:48.293,2,198,"my question concerns a side question ( which was not answered ) asked here : policy gradients for multiple continuous actions i am trying to implement a simple policy gradient algorithm for a discrete multi - action reinforcement learning task . to be more precise , there are three actuators . at every time step , each of the actuators can perform one of three possible actions . is it possible to adjust the loss function from the single action case per time step $ $ l = \log(p(a_1 ) ) a$$ to the n - action case per time step like so ? $ $ l = ( \log(p(a_1 ) ) + \log(p(a_2))+ \dots + \log(p(a_n ) ) ) a$$ ?",15265,2444,2019-02-13T02:32:38.557,2019-02-13T02:32:38.557,extend the loss function from the single action to the n - action case per time step,reinforcement-learning loss-functions,0,1,
1565,6182,1,,2018-04-26T22:09:57.767,1,82,"i have a friend who , after having a stroke , now has locked - in syndrome who and can now only move his eyes and eyelids . i 've been working on some c # software ( written in unity3d ) to help him to communicate , the ones he has tried so far have not been successful due to the level of his disability . i am in the process of implementing dlib for facial feature recognition . what i would like to do is to ask the user to repeatedly alternate between a neutral face and a face that is indicating . i do n't want to limit the indication gesture specifically to winking / blinking / opening eyes wide , etc as i want other people to be able to freely download the app and indicate in whatever way they find easiest due to their incapacity . i could really do with some pointers as to how to have my app learn what the user 's facial indicator is by comparing the facial landmarks of the two states . at this point , i do n't even know what it is i need to know . i do n't know the names of any ai algorithms that might be relevant or anything . i know posts like this are often closed for being "" too vague "" , but due to the nature of the request , i 'd appreciate it if it could be left open as any suggestions at all could potentially be life - changing . opencv would be an excellent choice as i already have a licence for a library in unity3d .",15274,15274,2018-04-29T09:37:55.763,2018-07-08T11:35:19.460,which library / algorithm should i use to recognise a specific facial expression ( c # ),ai-basics getting-started software-evaluation facial-recognition,2,2,
1566,6185,1,,2018-04-27T03:52:19.403,10,11849,i just want to know why do machine learning engineers and ai programmers use languages like python to perform ai task and not c++ even though c++ is technically a more powerful language than python .,15277,1671,2018-04-27T16:12:53.593,2018-11-07T10:03:16.603,why does c++ seem less widely used in ai ?,neural-networks machine-learning python data-science c++,4,2,3
1567,6188,1,,2018-04-27T08:45:53.170,5,232,"i have a data set that looks like this : i would like to estimate a relationship between x - values and the corresponding 5 % extreme y - values , something that might look like that : do you have an idea of an algorithm that might help me for this ? i thought about labelling the extreme values for later finding a separating hyperplane , but i have no clue on how to label these "" extreme values "" ( i can not just take the 5 % lowest and highest values as all these would end up in the same region ) . thanks for your ideas !",15281,14995,2018-05-01T20:57:48.983,2018-05-01T20:57:48.983,regression on extreme values,machine-learning linear-regression,1,1,
1568,6191,1,,2018-04-27T12:07:00.573,4,734,"i am university student and i am looking for ai tool for image / video labeling . it would be great if someone share own experience as well with suggested tool . i also have some questions , 1- which tools are available ? ( state of the art ) 2- what are their benefits , how do they compare ? 3- why did we choose the specific tool ?",15286,1671,2018-04-27T16:00:52.367,2018-11-01T16:06:10.560,ai tool for image / video labeling,machine-learning software-evaluation,1,6,
1569,6195,1,,2018-04-27T14:01:23.277,1,35,currently i am working with two large speech databases and i was asked to build one subset from each one in order to get two representative subsets with the same number of utterances . my question is : the number of utterances is the same as the number of audio files ? thank you so much !,13291,4302,2018-10-08T12:47:35.603,2018-10-08T12:47:35.603,number of utterances,natural-language-processing voice-recognition lexical-recognition,0,5,
1570,6196,1,,2018-04-28T03:11:16.087,14,3610,"as far as i understand , q - learning and policy gradients ( pg ) are the two major approaches used to solve rl problems . while q - learning aims to predict the reward of a certain action taken in a certain state , policy gradients directly predict the action itself . however , both approaches appear identical to me , i.e. predicting the maximum reward for an action ( q - learning ) is equivalent to predicting the probability of taking the action directly ( pg ) . is the difference in the way the loss is back - propagated ?",15298,2444,2019-02-15T15:39:06.333,2019-02-15T15:39:06.333,what is the relation between q - learning and policy gradients methods ?,reinforcement-learning q-learning policy-gradients difference,1,0,9
1571,6198,1,6202,2018-04-28T07:46:11.357,2,72,"some context : recently all kind of salesmen have been knocking on our company 's door to provide their "" artificial intelligence "" expertise and projects suggestions . some do n't know the difference between words estimation and validation ( really ) , some have extraordinary powerpoints and paint themselves as gurus of the field . our management has gone with the hype and definitely we 're starting some kind of project on "" artificial intelligence "" ( meaning rpa with some machine learning possibly ) . what is the best way to start when we do n't yet know to what problem we want to apply all this and i 'm worried it will lead to long expensive projects with meager results ? what are the things to watch out for ? any good practical books or war stories out there ?",3579,,,2018-04-28T13:07:47.990,how to organize artificial intelligence efforts at work ?,machine-learning getting-started applications,1,0,1
1572,6205,1,,2018-04-29T01:34:48.543,3,1255,"i am currently getting into deep learning and would like to set up an environment for training an artificial neural network or neat to play simple video games on nes ( mario etc . ) and snes ( donkey kong country etc . ) , using tensorflow / tflearn in python . i started off with openai gym environment and there is actually a super - mario environment for gym on github , which i fail to install as gym - pull is not available anymore and latest gym package does n't even have scoreboard folder ( i am on windows 10 , conda environment ) . now , what would be the best way to set up a solid training environment that will be similar to openai gym in terms of simplicity ? unfortunately openai universe is n't compatible with windows 10 atm , and i really do n't want to get a different setting like ubuntu environment to make it work . i would like to stay in win10 . if someone could guide me for suggested setup or refer me to articles/ documentation where similar things have been done in win10 python env . for nes / snes , i would be extremely grateful ! i assume an emulator with python api ( perhaps nintaco ? ) is a way to go ? how would i then get the ' observation output ' , i would need to scan the live pixel output of the game , which i am not sure how to do . regards , mark",15310,15310,2018-04-29T01:47:58.480,2018-10-12T10:00:33.187,training ai to play nes / snes games on nn python,deep-learning game-ai python q-learning neat,1,1,1
1573,6206,1,,2018-04-29T07:57:54.930,3,97,"i have a question about the training sequence regarding neural network recognition . let 's say an image has 28 * 28 pixels , which leads to 784 input nodes with various greyscale values and 10 output nodes , if the image shows a number 0 - 9 . then when the training data is used for a set of known pictures of numbers , the weights and hidden layers uses forward- and backpropagation to get get the proper hidden and weight layers for the known layer . however , does n't a new training picture destroy the trained and balanced weights and nodes values ? because the weights and hidden nodes have been calibrated to recognize the former training picture ? thank you for assistance . kind regards david",15312,,,2018-08-27T19:01:21.473,neural network training beginner question,neural-networks convolutional-neural-networks image-recognition training data-science,1,1,1
1574,6212,1,,2018-04-30T08:18:27.573,0,69,"let 's assume a common game scenario of several characters in a combat arena . each character has different strengths and weaknesses . the arena has traps and tools . suppose the characters had only very basic moves such as step in a direction , shoot , climb , duck , pick up item , use item , drag heavy object . each move has a chance of success based on the context ( e.g. range to target ) . what ai , machine learning , or evolutionary approach could be used to generate personalized tactics for each character based on repeated runs of the scenario ?",15322,,,2018-06-04T20:30:27.903,developing character tactics via repeated trials,game-ai problem-solving,2,1,1
1575,6213,1,,2018-04-30T10:12:01.373,4,284,"i 'm training a lstm network with multiple inputs and several lstm layers in order to setup a time series gap filling procedure . the lstm is trained bidirectionally with "" tanh "" activation on the outputs of the lstm , and one dense layer with "" linear "" activation comes at the end to predict the outputs . the following scatterplot of real outputs vs the predictions illustrates the problem : the network is definitely not performing too bad and i 'll by updating the parameters in next trials , but the issue at hand always reappears . the highest outputs are clearly underestimated , and the lowest values are overestimated , clearly systematic . i have tried min - max scaling on inputs and outputs and normalizing inputs and outputs , and the latter performs slightly better , but the issue persists . i 've looked a lot in existing threads and q&amp;as , but i have n't seen something similar . i 'm wondering if anyone here sees this and immediately knows the possible cause ( activation function ? preprocessing ? optimizer ? lack of weights during training ? ... ? ) . or , and in that case it would also be good to know , if this is impossible to find out without extensive testing . thanks a lot in advance .",15324,,,2019-05-01T00:03:11.087,over- and underestimations of the lowest and highest values in lstm network,recurrent-neural-networks keras python lstm,1,1,
1576,6214,1,,2018-04-30T20:37:44.413,-1,70,"i am new to ai programming and was wondering what are some good tools to begin creating an electronic assistant ( similar to ok google or siri ) i do know java , but i do n't know how helpful that would be . could anyone provide some insight ? edit : there is suggestion that my post is a duplicate of this question : how can i train my computer work for me ? i 'm just clarifying that my goal is to a make a speech based ai program , while the poster of the above question is making a text based program . the only similarity is the suggested software , which i now realize can be used in both situations .",15202,15202,2018-05-03T15:10:35.643,2018-05-03T15:10:35.643,tools to create an electronic assistant ?,ai-basics getting-started software-evaluation,1,9,
1577,6215,1,6227,2018-04-30T23:31:43.890,0,78,"i am working in the following neural network architecture , i am using keras and tensorflow as a back - end . it is composed by the following , embedding of words , then i added a layer of long short - term memory ( lstm ) neural networks , one layer of output and finally . i am using the softmax activation function . model = sequential ( ) model.add(embedding(max_nb_words , 64 , dropout=0.2 ) ) model.add(lstm(64 , dropout_w=0.2 , dropout_u=0.2 ) ) model.add(dense(8 ) ) model.add(activation('softmax ' ) ) i have the following question , if i am getting a model through this code , could the final product be called a deep learning model ? , i know that this code is very small however there is a lot of computations that the machine is making on the background .",2298,9947,2018-05-01T12:24:14.700,2018-05-01T13:57:06.000,is the following neural network architecture considered deep learning ?,deep-learning,1,0,1
1578,6216,1,6221,2018-04-30T23:58:24.823,1,50,"i have an approximately 90,000 row dataset that has information of social media profiles which has columns for biography , follower count , language spoken , name , username and the label ( to identify whether the profile is that of an influencer , brand or news and media ) . task : i have to train a model that predicts the label . i then need to produce a confidence interval for each prediction . as i have never come across a problem like this , i am just after some suggestions of what models i should be using for a situation like this ? i am thinking natural language processing ( nlp ) , but not sure . also , for nlp ( if a suitable method ) , any codes or advice to help me implement for the first time on python would be greatly appreciated ! thanks in advanced",15011,4302,2018-10-08T12:11:41.813,2018-10-08T12:11:41.813,recommended modelling technique for influencer marketing scenario,training natural-language-processing sentiment-analysis,1,0,1
1579,6218,1,6220,2018-05-01T05:13:30.430,0,74,"i ca n't find much information on modern pddl usage . are there more popular alternatives , maybe something more suited to modern neural network / deep learning techniques ? i 'm particularly interested in pddl or alternative 's current usage in autonomous driving software .",15343,,,2018-05-01T08:27:42.477,how is pddl used in production ai systems ?,self-driving path-planning,1,0,
1580,6229,1,6234,2018-05-01T16:04:34.433,5,122,"it is a new era and people are trying to evolve more in science and technology . artificial intelligent is one of the ways to achieve this . we seen lots of examples for ai sequences or a simple "" communication ai "" that able to think by themselves and they are often shift to the communication of building a world where machines will rise . this is what high thinking people like stephen hawking and elon musk is afraid of , to be in that kind of war . is it possible to build an ai , able to think by themselves but limited to the over ruling of humankind , or teach it of the moral way in treating peace and work alongside human , so they could fight alongside human if ever , this kind of catastrophic happens in the future ? it could be an advantage .",15353,1671,2018-05-01T20:31:20.517,2019-05-01T10:01:59.147,"is it possible to build an ai that learns humanity , morally ?",ethics theory superintelligence teaching-concepts value-alignment,2,1,2
1581,6231,1,6233,2018-05-01T18:10:40.283,1,600,"i 'm trying to write my own implementation of neat and i 'm stuck on the network evaluate function , which calculates the output of the network . neat as you may know contains a group of neural networks with continuously evolving topologies by the addition of new nodes and new connections . but with the addition of new connections between previously unconnected nodes , i see a problem that will occur when i go to evaluate , let me explain with an example : inputs = 2 yellow nodes hidden = 3 blue nodes output = 1 red node in the image a new connection has been added connecting node3 to node5 , how can i calculate the output for node5 if i have not yet calculated the output for node3 , which depends on the output from node5 ? ( not considering activation functions ) node5 output = ( 1 * 0.5 ) + ( 1 * 0.2 ) + ( node3 output * 0.8 ) node3 output = ( ( node5 output * 0.7 ) * 0.4 )",15356,,,2019-03-12T17:00:35.483,how to evaluate a neat neural network ?,neural-networks genetic-algorithms evolutionary-algorithms neat,2,0,1
1582,6232,1,6239,2018-05-01T19:02:14.557,2,82,"i am a beginner : i 've only read a book about neural network and barely implemented one in c. in short : a neural network is built out of nodes , each node holds an output : activation.(sum.(x * w ) ) , we then compute the total error out of the network output . from a beginner perspective , hyper - parameters , such as the number of layers needed , seem to be defined arbitrarily in most tutorials , books . in fact , the whole structure seems to be quite arbitrarily defined . in practice , hyper - parameters are often defined based on some standards . my question is , if you were to talk to a total beginner , how would you explain to him the structure of a neural network in such a way that the whole thing would appear as obvious ? is that even possible ? here , the word structure refers to a neural network being a configuration of nodes inside layers . thanks to anyone pointing out ambiguities or spelling errors . edit : note that i actually understand the whole back - propagation algorithm . i have no problem visualizing a nn .",,,2018-05-01T21:22:23.500,2018-05-03T04:45:37.183,how does one make it obvious that the structure of a neural network should be what it is ?,neural-networks machine-learning ai-basics,1,3,2
1583,6235,1,6237,2018-05-02T08:08:56.943,1,40,"the basis of q - learning is recursive ( similar to dynamic programming ) , where only the absolute value of the terminal state is known . should n't it make sense to feed the model a greater proportion of terminal states initially , to ensure that the predicted value of a step in terminal states ( zero ) is learned first ? will this make the network more likely to converge to the global optimum ?",15298,,,2018-05-02T13:47:30.640,feeding a q - learning algorithms a greater fraction of terminal states,reinforcement-learning,2,0,
1584,6240,1,6241,2018-05-03T07:50:17.020,1,1597,i know that when creating neural networks it 's standard practice to create a ' random seed ' so that you can get producible results in your models . i have a couple of questions regarding this : is the seed just something that is used in the ' learning ' phase of the network or does it get saved ? i.e. is it saved into the model itself and used by others if they decide to implement a model you created ? does it matter what you choose to be the seed ? should the number have a certain length ? at what step of the creation of a model does this seed get used and how does it get used ? other information about ' random seeds ' would be welcomed ! but these are my general questions .,11667,,,2018-05-03T08:05:01.437,where do ' random seeds ' get used in deep neural networks ?,neural-networks deep-learning learning-algorithms,1,0,
1585,6243,1,6247,2018-05-03T10:53:20.690,1,173,"i 'm trying to make a neural network that detects certain instruments in a song . i do n't know for sure if i should use an rnn , cnn or dnn . which one is best for this situation ?",7720,7720,2018-05-03T18:48:09.063,2018-05-03T18:48:09.063,neural network for pattern recognition in audio,neural-networks machine-learning deep-learning,1,2,
1586,6244,1,,2018-05-03T11:12:51.530,4,103,"i am learning about searching strategies in ai and i was reading that breadth first search is only optimal when the cost solution is a non - decreasing function ? i am not really sure what this refer to , since decreasing search cost should be our goal . am i missing something ?",15391,,,2018-08-08T15:28:16.810,why is breadth search only optimal when the cost solution is a non - decreasing function ?,search,1,1,2
1587,6245,1,,2018-05-03T12:44:51.930,0,115,"i 've looked into policy gradient rl the last few months . as i find the topic quite interesting , i 've been readings lots of papers about it . my aim is to write my master thesis in maths about it . i already started out , the preliminary title being "" techniques for variance reduction in policy gradient reinforcement learning "" . of course , i can sum up latest results , but a master thesis ' aim should be to create sth . new , or apply sth . to a new setting . does anybody have an idea for a nice application ? it was my idea to write the thesis in ml . my professor is not that much into ml but is happy to advise and evaluate the thesis . advice highly appreciated !",15392,2444,2019-02-16T02:59:56.927,2019-02-16T02:59:56.927,ideas for a thesis on policy gradient methods,reinforcement-learning academia,1,7,
1588,6246,1,,2018-05-03T16:36:13.410,3,231,"is it possible to use a vae to reconstruct and image starting from an initial image instead of using k.random_normal as show in the “ sampling ” function of this example ? i have used a sample image with the vae encoder to get z_mean and z_logvar . i have been given 1000 pixels in an otherwise blank image ( with nothing in it ) now i want to reconstruct the sample image using the decoder with a given constraint that the 1000 pixels in the otherwise blank image remain the same . the remaining pixels can be reconstructed so they are as close to the initial sample image as possible . in other words , my starting point for decoder is a blank image with some pixels that do n’t change . how can i modify the decoder to generate an image based on this constraint ? is it possible ? are there variations of vae that might make this possible ? so we can predict the latent variables by starting from an initial point(s ) ?",15399,,,2018-05-04T07:32:14.870,vae image reconstruction question ?,neural-networks deep-learning convolutional-neural-networks computer-vision deepmind,2,0,
1589,6255,1,6257,2018-05-03T21:06:29.713,0,40,"after watching 3blue1brown 's tutorial series , and an array of others , i 'm attempting to make my own neural network from scratch . so far , i 'm able to calculate the gradient for each of the weights and biases . now that i have the gradient , how am i supposed to correct my weight / bias ? should i : add the gradient and the original value ? multiply the gradient and the original value ? something else ? ( most likely answer ) in addition to this , i 've been hearing the term learning rate being tossed around , and how it is used to define the magnitude of the ' step ' to descend to minimum cost . i figured this may also play an integral role in reducing the cost .",9432,,,2018-05-03T22:20:56.650,how to change a weight / bias with gradient,neural-networks ai-basics backpropagation gradient-descent,1,0,1
1590,6266,1,6281,2018-05-04T12:22:16.577,3,109,"in discussions about technological singularity and its connection to ai , graphs are often shown that depict an exponential growth in technological advancement . often the y - axis is labeled "" number of important inventions "" or something similar . this sounds quite subjective and is certainly hard to qualify . is there a more objective way to quantify "" how much technology and scientific insight a society has "" ?",15422,,,2018-05-05T19:26:44.210,how is technological progress measured ?,singularity,2,0,1
1591,6267,1,,2018-05-04T14:15:54.140,5,3146,what are the mathematical prerequisites to be able to study general artificial intelligence ( ai ) or strong ai ?,15277,2444,2019-02-16T02:58:42.017,2019-02-16T02:58:42.017,what are the mathematical prerequisites to be able to study general artificial intelligence ?,strong-ai math education,5,0,7
1592,6274,1,,2018-05-04T22:21:03.393,7,4906,"i 'm facing the problem of having images of different dimensions as inputs in a segmentation task . note that the images do not even have the same aspect ratio . one common approach that i found in general in deep learning is to crop the images , as it is also suggested here . however , in my case i can not crop the image and keep its center or something similar since in segmentation i want the output to be of the same dimensions as the input . this paper suggests that in a segmentation task one can feed the same image multiple times to the network but with a different scale and then aggregate the results . if i understand this approach correctly , it would only work if all the input images have the same aspect ratio . please correct me if i am wrong . another alternative would be to just resize each image to fixed dimensions . i think this was also proposed by the answer to this question . however , it is not specified in what way images are resized . i considered taking the maximum width and height in the dataset and resizing all the images to that fixed size in an attempt to avoid information loss . however , i believe that our network might have difficulties with distorted images as the edges in an image might not be clear . what is possibly the best way to resize your images before feeding them to the network ? is there any other option that i am not aware of for solving the problem of having images of different dimensions ? also , which of these approaches you think is the best taking into account the computational complexity but also the possible loss of performance by the network ? i would appreciate if the answers to my questions include some link to a source if there is one . thank you .",13257,,,2019-01-28T05:29:30.023,convolutional neural networks with input images of different dimensions - image segmentation,neural-networks convolutional-neural-networks datasets,3,0,4
1593,6279,1,6283,2018-05-05T13:30:41.473,1,112,"i was wondering if it is possible to train an ai that can play outdoor games like cricket , badminton etc . i am new to ai , so if this question is dumb please bear it .",15441,,,2018-05-06T08:58:15.150,developing an ai to play outdoor games,training game-ai,2,2,
1594,6289,1,,2018-05-06T14:37:08.953,2,101,"i 'm building a 5-class classifier with a private dataset . each data sample has 67 features and there are about 40000 samples . samples of a particular class were duplicated to overcome class imbalance problems ( hence 40000 samples ) . with a one - vs - one multi - class svm , i am getting an accuracy of ~79 % on the validation set . the features were standardized to get 79 % accuracy . without standardization , the accuracy i get is ~72 % . similar result when i tried 50-fold cross validation . now moving on to mlp results , exp 1 : network architecture : [ 67 40 5 ] optimizer : adam learning rate : exponential decay of base learning rate validation accuracy : ~45 % observation : both training accuracy and validation accuracy stops improving . exp 2 : repeated exp 1 with batchnorm layer validation accuracy : ~50 % observation : got 5 % increase in accuracy . exp 3 : to overfit , increased the depth of mlp . a deeper version of exp 1 network network architecture : [ 67 40 40 40 40 40 40 5 ] optimizer : adam learning rate : exponential decay of base learning rate validation accuracy : ~55 % thoughts on what might be happening ?",15463,,,2019-05-01T23:01:50.703,unable to overfit using mlp,neural-networks deep-learning mlp,1,1,
1595,6291,1,,2018-05-06T19:03:53.620,2,413,"we know that ai can lie . but , can evolutionary ( self - learning ) ai be programmed in such a way that they do n't lie , even as they evolve ? source : evolving robots learn to lie to each other ( popsci ) see also : robots evolve to deceive ( mit tech review ) robots ' evolve ' the ability to deceive ( mit tech review )",15469,1671,2018-05-07T17:56:29.823,2018-11-07T19:14:49.103,can ai be programmed to not to lie ?,machine-learning genetic-algorithms evolutionary-algorithms ethics declarative-programming,4,11,1
1596,6292,1,6303,2018-05-07T01:30:42.793,2,79,is it possible to categories songs based on their spectrograms using image recognition or would there need to more features ? i was thinking that the spectrograms might also run into problems with edm songs . such as house music being closely related to their sounds . would there have to be immense amount of data ? i was thinking of using a cnn .,7720,,,2018-05-07T19:12:39.283,is it possible to classify songs by genres based on spectrograms ?,neural-networks machine-learning classification,1,0,1
1597,6297,1,6405,2018-05-07T16:40:26.853,-1,883,can we apply neural networks to two text comparisons ? please give me helpful tips about how to proceed in this direction . you can provide relevant links and pointers to solve the above problem .,15486,4880,2018-05-08T18:14:17.940,2018-05-14T11:07:11.837,can we apply neural networks to two text comparisons ?,neural-networks,2,3,
1598,6298,1,,2018-05-07T16:53:52.863,2,34,"le et al . 2012 use a network of 1 billion parameters to learn neurons that respond to faces , cats , pedestrians , etc . without labels ( unsupervised ) . their network is built with three autoregressive layers , and six pooling and normalization layers . in the paper they state , optimization : all parameters in our model were trained jointly with the objective being the sum of the objectives of the three layers . does this mean that all three autoencoder layers were trained simultaneously , or that the first three sub - layers ( first autoencoder sub - layer , first l2 pooling sub - layer , and first normalization sub - layer ) were trained simultaneously ? i think by asking this question , i have answered it for myself . please see my answer below . follow - on question what is the advantage of training all three layers simultaneously ? would n't the later layers be learning from poor lower layers to start with , and have to re - learn to adapt ? i do not have a very good answer to the follow - on question . perhaps the lower layers can actually learn to provide low - level features that support the layers above ? this training of all layers simultaneously is done in all deep neural networks today -- alexnet , vgg , etc . -- but why ? reference quoc v. le , marc'aurelio ranzato , rajat monga , matthieu devin , kai chen , greg s. corrado , jeff dean , andrew y. ng , building high - level features using large scale unsupervised learning ( 2012 ) arxiv:1112.6209 [ cs.lg ]",15487,15487,2018-07-10T17:54:20.337,2018-07-10T17:54:20.337,"do le et al . ( 2012 ) train all three autoencoder layers at a time , or just one ?",deep-network autoencoders,1,4,1
1599,6300,1,6326,2018-05-07T18:09:31.727,3,348,"according to the original paper "" genes that do not match are inherited from the more fit parent "" but what if the more fit parent has lesser nodes compared to the other , will the disjoint / excess genes be discarded ? here 's the link for the original paper : nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf",15490,1671,2018-05-07T19:36:32.323,2018-05-09T22:55:22.657,neat crossover - what to do with disjoint and excess genes,neural-networks machine-learning genetic-algorithms ai-basics neat,1,2,1
1600,6304,1,,2018-05-07T19:38:15.407,-2,27,"my final project at college is a "" selling software "" , where i have the list of products ( ice cream ) and the sells . its all done , but since i m new with ml , i do nt know which lib to use to achieve my goal : analyse the top selling products , identify which ingredients are common in them , and with it , generate new products . the idea is : top 5 selling products : icecream1 : [ chocolate , strawberry ] icecream2 : [ chocolate , condensed milk ] icecream3 : [ condensed milk , strawberry ] icecream4 : [ vanilla , strawberry ] icecream5 : [ chocolate , strawberry , vanilla ] then , since the top selling products have chocolate , strawberry , and condensed milk , it would sugest a new ice cream with those 3 ingredients . plus if possible : based on the sells number of all products , predict how much this new product would sell . ps : front end has made with angular , backend in node and db is mongodb . any help / suggestion will be really appreciated ! thanks guys : d",15494,,,2018-05-08T08:31:33.143,how to identify correlation in product sales and generate new products,prediction,1,1,
1601,6308,1,,2018-05-08T14:26:30.693,1,195,"in li et al . ( 2010 ) 's highly cited paper , they talk about linucb with hybrid linear models in section 3.2 . they motivate this by saying , "" in many applications including ours , it is helpful to use features that are shared by all arms , in addition to the arm - specific ones . for example , in news article recommendation , a user may prefer only articles about politics for which this provides a mechanism . "" i do n't quite understand what they mean by this . is anyone willing to provide a different example ? also it would greatly help if you can clarify what equation 6 's "" "" and "" "" refer to in the context they talk about ( news recommendation ) , or the example you give ? equation ( 6 ) from the paper : $ $ \mathbf{e } \left [ r_{t , a } \vert \mathbf{x}_{t , a } \right ] = \mathbf{z}_{t , a}^{\top } \boldsymbol{\beta}^ * + \mathbf{x}_{t , a}^{\top } \boldsymbol{\theta}_a^ * $ $",12656,1641,2018-08-13T17:57:50.747,2018-08-13T18:03:18.933,linucb with hybrid linear models,algorithm,1,0,
1602,6309,1,,2018-05-08T16:01:22.763,-4,129,"it sounds like people boast of something being "" artificial "" about machine learning when actually people boast that humans implemented algorithms like e.g. monte carlo search ( mcst ) etc . i think the term ai is only to market to uneducated audience and should be banned . machine can only look at the raw data . any intelligence is human . e.g. when alpha zero beats something at chess , it implemented mcst ( created & amp ; implemented by humans)and used a huge data may be too many orders of magnitude to beat humans . nothing "" artificial "" about intelligence . it sounds unimpressive when one says monte carlo powered x numbers of tpus with y amount of ram and zillion penta bytes amount of database beat a human at a game . it sounds great when you say ai beat human . but i do appreciate the fact that the creators did not hand code any chess rules ( other than legal moves ) & amp ; that 's a success story for machine learning .",15511,1671,2018-05-12T23:18:01.890,2018-05-12T23:18:01.890,"are recent advances in machine learning really "" artificial "" intelligence , or merely brute force and human design ?",terminology monte-carlo-tree-search alphago alphazero brute-force,1,6,
1603,6313,1,,2018-05-08T19:33:15.740,3,499,"this just popped into my head , and i have n't thought it through , but it feels like a sound question . the definition of intelligence might still be somewhat fuzzy , possibly a factor of our evolving understanding of "" intelligence "" in regard to algorithms , but rationality has some precise definitions . are rationality and intelligence distinct ? if not , explain . if so , elaborate . ( i have some thoughts on the subject and would be very interested in the thoughts of others . )",1671,,,2018-05-13T11:20:44.463,intelligence vs. rationality ?,philosophy terminology semantics rationality intelligence,3,3,1
1604,6314,1,6744,2018-05-08T22:23:53.637,2,797,"i 'm working on a reinforcement learning task where i use reward shaping as proposed in [ 1]. in short , my reward function has this form : r(s , s ' ) = gamma * p(s ' ) - p(s ) where p is a "" potential function "" . when s = s ' , r(s , s ) = ( gamma - 1)p(s ) which is negative , since 0 & lt ; gamma & lt;= 1 but considering p(s ) relatively high ( let say p(s ) = 1000 ) , r(s , s ) become too high as well ( e.g. with gamma=0.99 , r(s , s)=-10 ) , and if for many steps the agent stays in the same state , then the cumulative reward becomes more and more negative , which might affect the learning process . in practice i solved the problem by just remove the factor p(s ) when s = s ' . but i have some doubts about the theoretical correctness of this "" implementation trick "" . another idea could be to scale appropriately gamma in order to give a reasonable reward . indeed , with gamma=1.0 there is no problem , and with gamma very near to 1.0 the negative reward is tolerable . personally i do n't like it because it means that gamma is somehow dependent to the reward . what do you think ? [ 1 ] https://www-cs.stanford.edu/people/ang/papers/shaping-icml99.pdf",15517,15517,2018-05-10T10:18:58.977,2018-06-14T00:00:57.733,potential - based reward shaping gives too negative reward,reinforcement-learning theory,2,1,2
1605,6317,1,6321,2018-05-09T08:41:54.433,2,310,"in the paper deterministic policy gradient algorithms , i am really confused about chapter 4.1 and 4.2 which is "" on and off - policy deterministic actor - critic "" . i do n't know what 's the difference between two algorithms . i only noticed that the equation 11 and 16 are different , and the difference is the action part of q function where is $ a_{t+1}$ in equation 11 and in equation 16 . if that 's what really matters , how can i calculate $ a_{t+1}$ in equation 11 ?",15525,2444,2019-02-15T15:35:01.627,2019-04-24T09:29:01.420,what is the difference between on and off - policy deterministic actor - critic ?,reinforcement-learning terminology actor-critic on-policy off-policy,2,0,
1606,6318,1,6319,2018-05-09T09:28:56.357,1,79,"i 'm working with a data set where the data is stored in a string such as axbycya where a , b and c are actions and v , w , x , y , z are times between the actions ( each letter represents an interval of time ) . it 's worth noting that b can not occur without a , and c can not occur without b , and c is the action i 'm attempting to study ( ie : i 'd like to be able to predict whether a user will do c based on their prior actions ) . i intend to create 2 clusters : people who do c and those who do n't . from this data set i build a training array to run the sci - kit ( python ) k - means algorithm on , containing the number of a s , the number of b s , the mean time between actions ( calculated using the average of each interval ) and the standard deviation between each interval . this gives me an overall success rate of 82 % on the test set , but is there anything i can do for more accuracy ?",12940,12940,2018-05-09T09:36:01.950,2018-05-09T10:09:30.840,how to refine k - means clustering on a data set ?,classification training python,1,13,
1607,6322,1,,2018-05-09T13:58:33.037,4,1549,"i am a university student taking an artificial intelligence class this semester . our professor 's programming language of choice is java , but it seems that perhaps with some nudging , he can change it to python . i wanted to know if there is any merit in doing so . i know a programming language is just a programming language - however , given the industry 's wide use of python when implementing ai algorithms ( especially with ml ) , i think it makes much more sense for us to use python . it will be easier to transition from a university environment to an industrial one having done several assignments in python and having a clear understanding of all the tools it provides than if we continue using java , correct ? what are your thoughts ?",15529,1671,2018-05-09T19:31:20.113,2018-05-10T09:41:05.900,learning artificial intelligence with python vs. java,ai-design python programming-languages java,4,1,2
1608,6325,1,,2018-05-09T15:35:25.447,10,256,"imagine trying to create a simulated virtual environment that is complicated enough to create a "" general ai "" ( which i define as a self aware ai ) but is as simple as possible . what would this minimal environment be like ? i.e. an environment that was just a chess game would be too simple . a chess program can not be a general ai . an environment with multiple agents playing chess and communicating their results to each other . would this constitute a general ai ? ( if you can say a chess grand master who thinks about chess all day long has ' general ai ' ? during his time thinking about chess is he any different to a chess computer ? ) . what about a 3d sim - like world . that seems to be too complicated . after all why ca n't a general ai exist in a 2d world . what would be an example of a simple environment but not too simple such that the ai(s ) can have self - awareness ?",4199,1671,2018-05-11T13:52:39.720,2018-05-29T00:53:30.087,what kind of simulated environment is complex enough to develop a general ai ?,agi artificial-consciousness self-awareness,6,3,
1609,6329,1,,2018-05-10T00:34:13.240,1,179,i want my neural network structure to not have a circular / looping structure something similar like a directed acyclic graph ( dag ) . how do i do that ?,15490,2444,2019-02-17T21:11:29.650,2019-02-17T21:11:29.650,how do i restrict the neural network structure to be acyclic in neat ?,neural-networks machine-learning genetic-algorithms neat,1,0,2
1610,6330,1,6340,2018-05-10T01:01:42.800,0,41,"from https://stackoverflow.com/questions/36370129/does-tensorflow-use-automatic-or-symbolic-gradients , i understood tensorflow requires all the operations in the graph to be explicit formulas ( instead of black - boxes , such as raw python functions ) to do automatic differentiation . then it will do some kind of gradient descent based on that to minimization . i 'm wondering , since it already know all the explicit formulas , can it directly find out the minimum by examining the equation itself ? like computing the points where gradient is zero or do not exist , then do some kind of processing to find out the minimum . i found it is simple to do this "" symbolic minimization "" above with few variables such as minimizing σ(a_i - v)^2 where v is the trainable variable an a_i are all the training samples . i 'm not sure is there a general way though .",15546,,,2018-05-10T09:31:36.480,"can tensorflow minimize "" symbolically """,tensorflow optimization,1,0,
1611,6333,1,,2018-05-10T03:46:03.787,7,324,"at google i / o the google duplex just came out and now releasing new public self - driving car , waymo . i ca n't think of any tech giants that can actually compete with google ai other than amazon . i 'm thinking other companies will team up such as nvidia , intel , amd , and etc . apple seems to be lacking .",7720,7720,2018-05-10T03:52:26.310,2018-06-15T05:27:02.680,who stands a chance against google in the ai race ?,ai-community,8,3,3
1612,6338,1,,2018-05-10T09:03:44.073,-2,75,what is bayes ' theorem and bayes ' error ?,15551,1671,2019-05-02T01:51:52.277,2019-05-02T01:51:52.277,what is bayes ' theorem and error ?,math bayes bayes-theorem conditional-probability,1,0,
1613,6342,1,,2018-05-10T17:01:51.303,1,13,"coming from the yt videos of 3blue1brown which showed that the individual layers do not have discernible shapes in the case of hand written letter recognition , i wondered if you could penalize dispersed shapes while training , thus creating connected shapes ( at least on the first layer in the beginning ) . that way , you may be better able to understand the propagation of your algorithm through the layers . thanks , jonny",15563,,,2018-05-10T17:01:51.303,can you make the first layer of a net have discernible shapes ?,reinforcement-learning handwritten-characters,0,0,
1614,6343,1,,2018-05-10T18:43:12.950,1,929,"i 'm currently using 3blue1brown 's tutorial series on neural networks and lack extensive calculus knowledge / experience . i 'm using the following equations to calculate the gradients for weights and biases as well as the equations to find the derivative of the cost with respect to a hidden layer neuron : the issue is , during backpropagation , the gradients keep cancelling each other out because i take an average for opposing training examples . that is , if i have two training labels being [ 1 , 0 ] , [ 0 , 1 ] , the gradients that adjust for the first label get reversed by the second label because an average for the gradients is taken . the network simply keeps outputting the average of these two and causes the network to always output [ 0.5 , 0.5 ] , regardless of the input . to prevent this , i figured a softmax function would be required for the last layer instead of a sigmoid , which i used for all the layers . however , i have no idea how to implement this . the math is difficult to understand and the notation is complicated for me . the equations i provided above show the term : σ'(z ) , which is the derivative of the sigmoid function . if i 'm using softmax , how am i supposed to substitute sigmoid with it ? if i 'm not mistaken , the softmax function does n't just take one number analogous to the sigmoid , and uses all the outputs and labels . to sum it up , the things i 'd like to know and understand are : the equation for the neuron in every layer besides the output is : σ(w 1 x 1 + w 2 x 2 + ... + w n x n + b ) . how am i supposed to make an analogous equation with softmax for the output layer ? after using ( 1 ) for forward propagation , how am i supposed to replace the σ'(z ) term in the equations above with something analogous to softmax to calculate the partial derivative of the cost with respect to the weights , biases , and hidden layers ?",9432,9432,2018-12-15T02:03:01.843,2018-12-15T02:03:01.843,how do i implement softmax forward propagation and backpropagation to replace sigmoid in a neural network ?,neural-networks backpropagation gradient-descent,1,1,2
1615,6344,1,6394,2018-05-10T19:23:25.520,3,262,"after learning the basics of neural networks and coding one working with the mnist dataset , i wanted to go to the next step by making one which is able to play a game . i wanted to make it work on a game like slither.io , so in order to be able to create multiple instances of snakes and accelerate the speed of the game , i recreated a simple version of the game : the core features being almost done , now comes the work on the ai . i want to keep the script very simple by using only numpy ( not that tensorflow , pytorch or spark does not interest me , but i want to understand things at a "" low level "" before using those framworks ) . at first , i wanted the ai to be able to propose an output by reading pixels . but after some research , i do n't realy want to get into convnet , recurent and recursive neural net . i 'd like to re - use the simple feed forward nn i did with mnist and adapt it . so instead of using pixels , i think i 'm going to use the following data : { x , y } snake 's position { x , y } foods positions food value time , in order to get the snake eat the more food in a short time . distance from the center , not die outside the area that 's a lot of different data to handle ! 1/2 can a simple fnn handle different kinds of data in the input layer ? moreover that comes an other problem : 2/2 will it properly work with a variable number of inputs ? in fact , in a specific area around the snake , the quantity of food will be variable . i came accross this post that kinda answer my question , but wath if i want the neural network to forget some input if they are not being used , can dropout be of any use in this case . or the weights value ( correcting toward zero ) of these inputs will be enough ?",15564,15564,2018-05-10T19:29:00.577,2018-05-13T15:41:26.223,how to handle varying types and length of inputs in a neural network ?,neural-networks game-ai ai-basics,1,1,2
1616,6345,1,,2018-05-10T19:53:12.393,1,174,"i am trying to implement cnn using tensorflow on temporal accelrometer signal . i have signal values segmented on every 10ms ( 200 samples ) . i want to perform 1-d convolution ( tf.nn.conv1d(x,w,stride=1,padding='valid ' ) ) convolution window size is 20 samples and stride of 1 with 32 features and valid padding i want to apply max - pooling with window size of 10 samples tf.nn.max_pool(x , ksize=[1,1,10,1],strides= [ 1,1,2,1],padding='valid ' ) but i am getting errors regarding dimensions of tensors . any suggestions on how can i set filter size and stride for booth convolution and max - pooling",15565,,,2018-05-10T19:53:12.393,"how to set stride , filter size in tensorflow for 1-d signals ?",tensorflow python,0,0,
1617,6366,1,,2018-05-11T05:11:00.057,3,326,"goal - i am trying to implement a genetic algorithm to optimise the fitness of a species of creatures in a simulated two - dimensional world . the world contains edible foods , placed at random , and a population of monsters ( your basic zombies ) . i need the algorithm to find behaviours that keep the creatures well fed and not dead . what i have done - so i start off by generating a 11x9 2d array in numpy , this is filled with random floats between 0 and 1 . i then use np.matmul to go through each row of the array and multiply all of the random weights by all of the percepts ( w1+p1*w2+p2 .... w9+p9 ) = a1 . this first generation is run and i then evaluate the fitness of each creature using ( energy + ( time of death * 100 ) ) . from this i build a list of creatures who performed above the average fitness . i then take the best of these "" elite "" creatures and put them back into the next population . for the remaining space i use a crossover function which takes two randomly selected "" elite "" creatures and mixes their genes . i have tested two different crossover functions one which does a two point crossover on each row and one which takes a row from each parent until the new child has a complete chromosome . my issue is that the creatures just do n't really seem to be learning , at 75 turns i will only get 1 survivor every so often . i am fully aware this might not be enough to go off but i am truly stuck on this and can not figure out how to get these creatures to learn even though i think i am implementing the correct procedures . occasionally i will get a 3 - 4 survivors rather than 1 or 2 but it appears to occur completely randomly , does n't seem like there is much learning happening . below is the main section of code , it includes everything i have done but none of the provided code for the simulation # ! /usr / bin / env python from cosc343world import creature , world import numpy as np import time import matplotlib.pyplot as plt import random import itertools # you can change this number to specify how many generations creatures are going to evolve over . numgenerations = 2000 # you can change this number to specify how many turns there are in the simulation of the world for a given generation . numturns = 75 # you can change this number to change the world type . you have two choices - world 1 or 2 ( described in # the assignment 2 pdf document ) . worldtype=2 # you can change this number to modify the world size . gridsize=24 # you can set this mode to true to have the same initial conditions for each simulation in each generation - good # for development , when you want to have some determinism in how the world runs from generation to generation . repeatablemode = false # this is a class implementing you creature a.k.a mycreature . it extends the basic creature , which provides the # basic functionality of the creature for the world simulation . your job is to implement the agentfunction # that controls creature 's behaviour by producing actions in response to percepts . class mycreature(creature ) : # initialisation function . this is where your creature # should be initialised with a chromosome in a random state . you need to decide the format of your # chromosome and the model that it 's going to parametrise . # # input : numpercepts - the size of the percepts list that the creature will receive in each turn # numactions - the size of the actions list that the creature must create on each turn def _ _ init__(self , numpercepts , numactions ) : # place your initialisation code here . ideally this should set up the creature 's chromosome # and set it to some random state . # self.chromosome = np.random.uniform(0 , 10 , size = numactions ) self.chromosome = np.random.rand(11,9 ) self.fitness = 0 # print(self.chromosome[1][1].size ) # do not remove this line at the end - it calls the constructors of the parent class . creature.__init__(self ) # this is the implementation of the agent function , which will be invoked on every turn of the simulation , # giving your creature a chance to perform an action . you need to implement a model here that takes its parameters # from the chromosome and produces a set of actions from the provided percepts . # # input : percepts - a list of percepts # numaction - the size of the actions list that needs to be returned def agentfunction(self , percepts , numactions ) : # at the moment the percepts are ignored and the actions is a list of random numbers . you need to # replace this with some model that maps percepts to actions . the model # should be parametrised by the chromosome . # actions = np.random.uniform(0 , 0 , size = numactions ) actions = np.matmul(self.chromosome , percepts ) return actions.tolist ( ) # this function is called after every simulation , passing a list of the old population of creatures , whose fitness # you need to evaluate and whose chromosomes you can use to create new creatures . # # input : old_population - list of objects of mycreature type that participated in the last simulation . you # can query the state of the creatures by using some built - in methods as well as any methods # you decide to add to mycreature class . the length of the list is the size of # the population . you need to generate a new population of the same size . creatures from # old population can be used in the new population - simulation will reset them to their # starting state ( not dead , new health , etc . ) . # # returns : a list of mycreature objects of the same length as the old_population . def selection(old_population , fitnessscore ) : elite_creatures = [ ] for individual in old_population : if individual.fitness & gt ; fitnessscore : elite_creatures.append(individual ) elite_creatures.sort(key = lambda x : x.fitness , reverse = true ) return elite_creatures def crossover(creature1 , creature2 ) : child1 = mycreature(11 , 9 ) child2 = mycreature(11 , 9 ) child1_chromosome = [ ] child2_chromosome = [ ] # print(""parent1 "" , creature1.chromosome ) # print(""parent2 "" , creature2.chromosome ) for row in range(11 ) : chromosome1 = creature1.chromosome[row ] chromosome2 = creature2.chromosome[row ] index1 = random.randint(1 , 9 - 2 ) index2 = random.randint(1 , 9 - 2 ) if index2 & gt;= index1 : index2 + = 1 else : # swap the two cx points index1 , index2 = index2 , index1 child1_chromosome.append(np.concatenate([chromosome1[:index1],chromosome2[index1 : index2],chromosome1[index2 : ] ] ) ) child2_chromosome.append(np.concatenate([chromosome2[:index1],chromosome1[index1 : index2],chromosome2[index2 : ] ] ) ) child1.chromosome = child1_chromosome child2.chromosome = child2_chromosome # print(""child1 "" , child1_chromosome ) return(child1 , child2 ) def crossoverrows(creature1 , creature2 ) : child = mycreature(11 , 9 ) child_chromosome = np.empty([11,9 ] ) i = 0 while i & lt ; 11 : if i ! = 10 : child_chromosome[i ] = creature1.chromosome[i ] child_chromosome[i+1 ] = creature2.chromosome[i+1 ] else : child_chromosome[i ] = creature1.chromosome[i ] i + = 2 child.chromosome = child_chromosome return child # print(""parent1 "" , creature1.chromosome[:3 ] ) # print(""parent2 "" , creature2.chromosome[:3 ] ) # print(""crossover rows "" , child_chromosome[:3 ] ) def newpopulation(old_population ) : global numturns nsurvivors = 0 avglifetime = 0 fitnessscore = 0 fitnessscores = [ ] # for each individual you can extract the following information left over # from the evaluation . this will allow you to figure out how well an individual did in the # simulation of the world : whether the creature is dead or not , how much # energy did the creature have a the end of simulation ( 0 if dead ) , the tick number # indicating the time of creature 's death ( if dead ) . you should use this information to build # a fitness function that scores how the individual did in the simulation . for individual in old_population : # you can read the creature 's energy at the end of the simulation - it will be 0 if creature is dead . energy = individual.getenergy ( ) # this method tells you if the creature died during the simulation dead = individual.isdead ( ) # if the creature is dead , you can get its time of death ( in units of turns ) if dead : timeofdeath = individual.timeofdeath ( ) avglifetime + = timeofdeath else : nsurvivors + = 1 avglifetime + = numturns if individual.isdead ( ) = = false : timeofdeath = numturns individual.fitness = energy + ( timeofdeath * 100 ) fitnessscores.append(individual.fitness ) fitnessscore + = individual.fitness # print(""fitnessscore "" , individual.fitness , "" energy "" , energy , "" time of death "" , timeofdeath , "" is dead "" , individual.isdead ( ) ) fitnessscore = fitnessscore / len(old_population ) elitecreatures = selection(old_population , fitnessscore ) print(len(elitecreatures ) ) newset = [ ] for i in range(int(len(elitecreatures)/2 ) ) : if elitecreatures[i].isdead ( ) = = false : newset.append(elitecreatures[i ] ) print(len(newset ) , "" elites added to pop "" ) remainingrequired = w.maxnumcreatures ( ) - len(newset ) i = 1 while i in range(int(remainingrequired ) ) : newset.append(crossover(elitecreatures[i ] , elitecreatures[i-1])[0 ] ) if i & gt;= ( len(elitecreatures)-2 ) : i = 1 i + = 1 remainingrequired = w.maxnumcreatures ( ) - len(newset ) # here are some statistics , which you may or may not find useful avglifetime = float(avglifetime)/float(len(population ) ) print(""simulation stats : "" ) print ( "" survivors : % d out of % d "" % ( nsurvivors , len(population ) ) ) print ( "" average fitness score : "" , fitnessscore ) print ( "" avg life time : % .1f turns "" % avglifetime ) # the information gathered above should allow you to build a fitness function that evaluates fitness of # every creature . you should show the average fitness , but also use the fitness for selecting parents and # spawning then new creatures . # based on the fitness you should select individuals for reproduction and create a # new population . at the moment this is not done , and the same population with the same number # of individuals is returned for the next generation . new_population = newset return new_population # pygame window sometime does n't spawn unless matplotlib figure is not created , so best to keep the following two # calls here . you might also want to use matplotlib for plotting average fitness over generations . plt.close('all ' ) fh = plt.figure ( ) # create the world . the worldtype specifies the type of world to use ( there are two types to chose from ) ; # gridsize specifies the size of the world , repeatable parameter allows you to run the simulation in exactly same way . w = world(worldtype = worldtype , gridsize = gridsize , repeatable = repeatablemode ) # get the number of creatures in the world numcreatures = w.maxnumcreatures ( ) # get the number of creature percepts numcreaturepercepts = w.numcreaturepercepts ( ) # get the number of creature actions numcreatureactions = w.numcreatureactions ( ) # create a list of initial creatures - instantiations of the mycreature class that you implemented population = list ( ) for i in range(numcreatures ) : c = mycreature(numcreaturepercepts , numcreatureactions ) population.append(c ) # pass the first population to the world simulator w.setnextgeneration(population ) # runs the simulation to evaluate the first population w.evaluate(numturns ) # show the visualisation of the initial creature behaviour ( you can change the speed of the animation to ' slow ' , # ' normal ' or ' fast ' ) w.show_simulation(titlestr='initial population ' , speed='normal ' ) for i in range(numgenerations ) : print(""\ngeneration % d : "" % ( i+1 ) ) # create a new population from the old one population = newpopulation(population ) # pass the new population to the world simulator w.setnextgeneration(population ) # run the simulation again to evaluate the next population w.evaluate(numturns ) # show the visualisation of the final generation ( you can change the speed of the animation to ' slow ' , ' normal ' or # ' fast ' ) if i==numgenerations-1 : w.show_simulation(titlestr='final population ' , speed='normal ' )",15571,1671,2018-05-11T13:26:12.777,2018-08-08T15:38:16.817,genetic algorithm - creatures in 2d world are not learning,genetic-algorithms game-ai,1,2,1
1618,6368,1,,2018-05-11T08:45:08.123,4,190,"so i read about softmax from this article . apparently to me these 2 are almost similar , except that the probability of all classes in softmax add to 1 . according to their last paragraph for number of classes = 2 , softmax reduces to lr . what i want to know is other than number of classes = 2 what are the essential differences between lr and softmax . like in terms of : performance . computational requirements . ease of calculation of derivatives . ease of visualization . number of minimas in the convex cost function , etc . other differences are also welcome ! edit : i am asking for relative comparisons only , so that at the time of implementation i have no difficulty in selecting which method of implementation to use .",9947,9947,2018-05-22T04:23:22.670,2018-07-09T16:54:57.867,difference between softmax and logistic regression ?,neural-networks machine-learning,2,0,1
1619,6370,1,,2018-05-11T12:07:04.040,1,36,"in the model generation , in machine learning ( consider supervised ) if some data change the previous model function drastically then we should study that data . does it happen ? how to handle such situation ?",15368,1671,2018-05-11T13:16:48.210,2018-05-11T13:16:48.210,data interpretation technique,machine-learning ai-basics datasets models,0,1,1
1620,6377,1,,2018-05-12T04:31:12.310,3,672,why momentum factor greater than 1 is a bad idea ? what are the mathematical conclusions ?,15587,9947,2018-08-28T12:25:23.540,2018-08-28T12:25:23.540,why mlp momentum term must be in the range 0 - 1 ?,neural-networks machine-learning ai-basics mlp,3,2,1
1621,6381,1,,2018-05-12T11:07:27.917,2,267,"i 'm trying to find the optimal policy for the mountain car problem using deep q learning with images as input , however , i can not find a way to get my q function to give me good solutions ( i followed multiple tutorials for similar problems ( atari games and flappy bird ) ) . i 'm working on python with keras . the images are given in the following format : 400x400 pixels , where the bar on the bottom right corner represents the speed of the car . to check where my problem might lie , i thought it would be best if i split the problem by first ensuring that i can find a network which would successfully find the state of the car ( position and speed , since it 's all we need to find it ) by feeding my convolutional network images of random states ( uniformly distributed within the state space ) . after unsuccessful results , i decided to split it even more by only trying to find the two state variable separately . this is the best kind of result that i get , when the network does n't predict all the state to be the same ( which seem to happen a lot with relu activation ) . the state space is divided in 50x50 matrix to make my predictions . the predicted speed is on the left , and the absolute error is on the right . the images fed to the network are pre - processed the follow way : 1 . gray scale 2 . resize ( i tried 50x50 , 100x100 and 150x150 ) 3 . values centered around 0 in [ -1;1 ] ( this seemed to help a bit with the relu activation ) the network i used to try to find the speed is first a convolution layer ( i tried 32 windows of kernel_size=(4,4 ) ( 8,8 ) and ( 16,16 ) , strides=(1,1 ) and ( 2,2 ) , activation= relu , linear , tanh . optional additional convolution layers of kernel size half the previous layer . and an optional last dense layer of dimension 32 or activation relu , linear or tanh . the output layer is dimension one with linear activation . the way i train the network is by feeding the fit function with 32 random samples and let the network train for 25 epochs with batch_size 32 and repeat ad libitum . it 's becoming extremely frustrating , especially with my gpu not fitting the requirement for gpu computation to check if i get some results faster . can anyone tell me if i 'm doing something wrong that i 'm missing and what can i do to improve my method to eventually manage to get the reinforcement algorithm to work ? like the size of the training sample , batch size and epochs of the fit function , the structure of the network , ... edit : i finally found a way for my reinforcement learning to converge to the true q value : each time i run a new episode and put it in my replay memory , i run many fits of different mini - batches . this is something i thought i did by increasing the number of epochs in the parameter of the fit function , but i guess it does n't work as i thought it would . i 'm letting it train a bit and then i will try the same method for the sub - problems mentioned above .",15590,15590,2018-05-13T00:54:01.607,2018-05-13T00:54:01.607,mountain car problem with images - not converging,deep-learning python keras,1,4,
1622,6383,1,,2018-05-12T16:33:34.113,8,1107,"i am pretty much a beginner in tensorflow and simply follow a tutorial . there is no problem with my code , but i have a question regarding the output accuracy : 0.95614034 accuracy_baseline : 0.6666666 auc : 0.97714674 auc_precision_recall : 0.97176754 average_loss : 0.23083039 global_step : 760 label / mean : 0.33333334 loss : 6.578666 prediction / mean : 0.3428335 i would like to know what does "" prediction / mean "" and "" label / mean "" represent ?",15593,2193,2018-05-14T17:53:10.930,2019-01-12T03:46:39.953,meaning of evaluation metrics in tensorflow,neural-networks tensorflow python artificial-neuron,1,2,2
1623,6389,1,6396,2018-05-12T22:23:09.803,2,112,"as the title says , should i reset the exploration rate between trials ? i am currently doing the open ai pendulum task and after a number of trials my model started playing but did not take any actions ( i.e. did n't perform any significant swing ) . the actor - critic tutorial i followed did not reset the exploration rate ( link ) but it seems like there are lots of mistakes in general . i assume that it should be reset since the model might start from a new unknown situation in a different trial and not know what to do without exploring .",13257,2444,2019-02-16T02:57:21.950,2019-02-16T02:57:21.950,should the exploration rate be reset after each trial in q - learning ?,reinforcement-learning q-learning open-ai,1,0,
1624,6391,1,6392,2018-05-13T04:13:58.660,2,54,are there any charts or graphs of some sort that would clearly present the status of ai in one way or another ( meaning in a verifiably measurable way ) and show where ai has been progressing and what the expected outcomes / predictions are currently available ?,15601,,,2018-05-13T05:30:24.567,ai progress charts / graphs,graphs,1,0,
1625,6395,1,,2018-05-13T15:48:56.120,1,42,"on this video link to video a neurologist starts by saying that we do not know how neurons calculate gradients for backpropagation . at minute 30:39 he s showing faster convergence for "" our algorithm "" , which seems to converge faster than backpropagation . after 34:36 it goes explaining how "" neurons "" in the brain are actually packs of neurons . i do not really understand all that he says , so i infer that those packs of neurons ( which seem depicted as a single layer ) are the ones who calculate the gradient . it would make sense if each neuron makes a sightly different calculation , and then each other communicate the difference in results . that would allow to deduce a gradient . what can be deduced , from the presented information , about the purported "" algorithm "" ? ? ( from the viewpoint of improving convergence of an artificial neural network ) .",15611,,,2018-05-13T18:40:12.547,"what can be deduced about the "" algorithm "" of backpropagation / gradient descent ?",neural-networks gradient-descent,1,0,
1626,6401,1,,2018-05-14T05:33:52.187,3,61,i need to design an algorithm such that it handles the request for shift swapping . the algorithm will recommend a list of people who are more likely to swap that shift with the person by analyzing previous data . can anyone list the techniques that will help me to do this or a good starting point ? i was thinking about training a naive bayes classifier and using mahaout for generating recommendation .,15618,1671,2018-05-14T17:55:10.467,2018-10-11T23:00:19.050,how to design a recommendation system for shift swapping ?,ai-design training ai-basics getting-started,1,1,
1627,6402,1,6409,2018-05-14T05:53:45.683,0,439,"i am trying to create my own variant of google duplex however , it wo n't make calls but just have a real time conversation . my question is , where and how to start ? how do i train my model with real conversation and how do i make speech sound almost human like ? where do i incorporate rnn and how can i make my model understand nuances ? https://youtu.be/p3pfkf0ndik trying to create something like this . note : i understand this is very difficult but i am not looking for perfection . i am just creating this as i love doing this . thank you",15619,1671,2018-05-15T18:54:21.260,2018-05-15T18:54:21.260,how can i create my own google duplex ?,neural-networks deep-learning natural-language-processing getting-started architecture,1,2,2
1628,6407,1,6410,2018-05-14T14:26:11.437,3,218,"i would like to know other tha neural network , is there any ml technique for agent based ml . if so how to train an agent with some predefined rules ? can we use python programming for representing those rules ?",15631,,,2018-05-14T17:48:43.027,can agent based machine learning achieved with any ml algorithms other than neural network ?,machine-learning python unsupervised-learning intelligent-agent,2,0,1
1629,6411,1,,2018-05-14T17:47:23.333,1,102,"i want to experiment capsule networks on fer . for now i am using fer2013 kaggle dataset . one thing that i did n't understand in capsule net was in the first conv layer , size was reduced to 20x20 - having input image as 28x28 and filters as 9x9 with 1 stride . but in the capsules , the size reduces to 6x6 . how did this happen ? because with input size as 20x20 and filters as 9x9 and 2 strides , i could n't get 6x6 . maybe i missed something . for my experiment , input size image is 48x48 . should i use the same hyperparams for the start or is there any suggested hyperparams that i can use ?",15633,,,2018-05-14T17:47:23.333,capsule networks - facial expression recognition,neural-networks machine-learning deep-learning convolutional-neural-networks,0,0,
1630,6413,1,,2018-05-14T18:19:13.363,2,78,"i am having difficulties wrapping my head around how the answer is being produced . how does the solution come up with the answer 3 ? how does one derive the answer 3 from the truth table ? for example the following file : tell p2= > p3 ; p3 = > p1 ; ask p2 produces the following result . standard output is an answer of the form yes or no , depending on whether the ask(ed ) query q follows from the tell(ed ) knowledge base kb . when the method is tt and the answer is yes , it should be followed by a colon ( :) and the number of models of kb",15635,,,2018-08-13T12:02:16.183,ai knowledge based system : i am having difficulties understanding proposition logic,ai-design ai-basics logic,2,0,1
1631,6414,1,6458,2018-05-14T20:43:22.980,1,51,"i am looking to extract the central theme of news headline using nlp/ text - mining , any references in this direction is of great help . for example : inputs : brief - dynasil corporation of america reports q2 eps of $ 0.08 china 's night - owl retail investors leverage up to dominate oil futures trade outputs : reports oil futures",15638,,,2018-05-18T15:32:39.787,are there any references of nlp / text mining techniques for identifying the theme of news headlines,machine-learning deep-learning natural-language-processing,1,0,1
1632,6416,1,,2018-05-15T07:30:12.763,-2,60,"i would like to write a program that takes a number of elements as input ( some images and some strings ) and outputs a design ( the program places the images and strings on a canvas , but also styles the strings ) . can someone point me in the right direction of a possible solution to this problem ? i 've looked for machine learning algorithms for design but have not found a single one to give me any tips on how to build one of these .",15647,1671,2018-05-15T17:59:23.363,2018-05-16T10:07:15.060,design ads and posters with machine learning,machine-learning ai-basics getting-started software-evaluation reference-request,1,8,
1633,6421,1,6428,2018-05-15T11:28:24.003,2,114,"it is well known from the history of technology , that the invention of new things was always problematic . in the 15th century for example , in which gutenberg has invented the first printing press , the world was n't pleased . instead the luddite movement was doing everything to destroy his work . as far as i know from the history lesson , gutenberg was recognized in his time as an evil sorcerer and the printing press as work of the devil . this development was in later decades also visible . at first , a great invention was done for example the first steam driven car , and the ordinary people do n't understand the technology and were in fear of it . a modern form of technology is computing and especially artificial intelligence . from a technical point of view , it is the most important invention ever , and this will result into the strongest possible form of rejection . most people in the world are not excited by artificial intelligence . they not want any sorts of robots and intelligent machines . the terminology itself is well known , the fundamental rejection of new technology bevause of religious or moral reasons is called luddism or neoluddism . because the technophobic ned ludd has destroyed a while ago two stocking frames . after this episode , every rant against technology is called after him . but what i do not understand it the motivation behind it . did ned ludd thought , that he can change the world if he destroyed a machine ? did he believe that mankind will become good if no gutenberg printing press are used ? the problem is , that for example if the first steam engine was never invented , also the following inventions like the internet and intelligent machines would n't have been invented . but what would be the alternative ? what is the perspective of ned ludd , how does he see the better tomorrow , if no technology innovation is allowed ?",11571,1671,2018-05-15T18:16:36.680,2018-05-30T16:54:35.560,what is neoluddism ?,philosophy history neo-luddism,1,1,
1634,6422,1,6457,2018-05-15T12:13:01.280,1,83,"i am trying a modification of mobilenet in which i add feedback from the softmax layer into the early layers ( to implement this i put a second net after the first , which receives connections from the softmax layer of the first , the pretrained weights being non trainable ) . the idea was to mimic the massive feedback projections in the brain , which presumably could help object recognition by enhancing specific filters and inhibiting others . i took the pretrained network from keras and started to retrain it on imagenet . i noticed that the training accuraccy increased right in the first epoch . my computer is very slow thus i can not train for too long , an epoch takes 3.5 days . so after an epoch i tried the validation set , but instead the accuracy went down to almost half that of the pretrained values . my question is if this is and obvious case of overfitting . that is , will continued training increase the accuracy of the training set at the expense of the validation set , or is this a normal behavior expected at the initial stages of training , so that if i keep training for a few more epochs i could expect the validation set accuracy to go eventually up ? any ideas that could help are welcomed .",5911,,,2018-05-18T15:12:15.290,is this overfitting avoidable ?,convolutional-neural-networks image-recognition overfitting,1,1,
1635,6424,1,,2018-05-15T13:06:11.553,1,15,"i use matlab function "" wmulden "" to obtain a denoised data and i realised that when i add only one data - step to previous data and i recalculate the denoised series all denoised data is changed even if the data is identical . the average difference between the two denoised signals is 1 * 10e-03 and the average data is 1 . does anyone know how to get the same denoising when the data is the same ?",15653,,,2018-05-15T13:06:11.553,multivariate wavelet denoising problem,matlab,0,0,
1636,6425,1,,2018-05-15T13:08:35.417,4,94,"i have a data set containing actions taken by customers ( e.g. , view a product , add a product to cart , purchase product ) , the product bought ( if any ) and times of said actions . i am attempting to use k - means clustering to identify the customers who are more likely to purchase a product based on these actions ( minus the purchase ) . i 'm currently clustering using : the number of products viewed , the number of products put in the cart , the mean time between the actions , the variance of the time between the actions , the standard deviation of the time between the actions ( all of these values are normalized ) , as well as the product purchased ( if any ) . the clusters i 'm getting contain ~10 % buyers and 90 % non - buyers , but i 'm trying to separate buyers and non - buyers . any thoughts on what else i can do ? or should i try another method completely ? illustration : x axis are the clusters , y axis is the number of customers , red are buyers and blue are non - buyers update : i made a 3d graph showcasing the clusters , the amount of customers and the mean time between actions ( normalized because of reasons ) yet another update : customers ( not grouped by cluster , just as is ) according to the average number of products they viewed and the average time between actions i took some advice and tried using pca ( from this tutorial ) , and these are the results i got : the raw data ( x = number of items viewed / carted , y = average time between interactions ) any tips on how to cluster this mess ?",12940,22296,2019-02-21T01:50:40.580,2019-02-21T01:50:40.580,supervised k - means clustering does n't appear to work,machine-learning classification python,0,1,
1637,6426,1,,2018-05-15T14:06:41.507,4,3751,"i have read various answers to this question at different places , but i am still missing something . what i have understood is that a graph search holds a closed list , with all expanded nodes , so they do n't get explored again . but if you apply breadth - first - search or uniformed cost search at a search tree , you do the same . you have to keep the expanded nodes in memory .",15391,,,2018-12-16T19:51:52.680,what is the difference between tree search and graph search ?,search,3,1,2
1638,6429,1,,2018-05-15T19:44:19.023,7,217,"i know that one of the recent fads right now is to train a neural network to generate screenplays and new episodes of friends , the simpsons , what have you , and that 's fine : it 's interesting and might be the necessary first ( ? ) steps toward making programs that can actually generate sensible / understandable stories . in this context : can neural networks be trained specifically to study the structures of stories , or screenplays , and perhaps generate plot points , or steps in the hero 's journey , etc . , effectively writing an outline for a story ? yes , this to me differs from the many myriad plot - point generators online , although i have to admit the similarities . i 'm just curious if the tech or the implementation is even there yet and , if it is , how one might go about doing it .",15659,3217,2018-10-15T11:41:44.150,2019-06-01T23:01:30.313,is it possible for an ai to be trained on literary story / structure to generate them ?,deep-learning,5,0,
1639,6431,1,6432,2018-05-16T03:51:46.383,0,50,"i have a medical dataset with 14000 rows dataset with 900 attributes . i have to predict disease severity using that . i would like to know whether we can write rules in python language for training an agent for medical diagnostic using machine learning . can an agent make the decisions by the rules coded in python and that agent get trained with some machine learning algorithms ? if so is there any agent architecture and model for the agent which is good in this context ? edit : by the rule , i meant something like this .. ""if x > y output z as action "" . by the word "" training "" i meant "" how to tell this agent to do this action "" ?",15631,15631,2018-05-16T07:09:22.157,2018-05-16T07:09:22.157,can we code rules for an agent in python language other than predicate calculus ?,machine-learning python datasets intelligent-agent,1,0,
1640,6434,1,,2018-05-16T08:48:39.410,-1,340,"pretty simple question here : is it useful to use the standard deviation , skew , kurtosis , or any other extrapolatory stats as features , and if so in which problem sets ? in this case , i am talking about deep learning problems .",9608,,,2018-05-17T14:49:52.687,"are standard deviation , variance , skew good features for ml ?",machine-learning datasets statistical-ai,1,1,
1641,6435,1,,2018-05-16T09:26:49.697,5,81,"just working with fully connected nns ( supervised learning ) , i found that models trained for , say nlp , on identical data sets with identical parameters to algorithms ; but at different times , can provide different accuracy , with in like 7 - 8 % of each other . is that an unusual phenomenon ? what is an acceptable standard deviation level ?",15668,,,2019-05-30T14:03:20.440,what is the tolerance level of standard - deviation of anns accuracy ?,neural-networks,2,0,1
1642,6443,1,,2018-05-17T16:16:59.583,1,71,"considering the scenario where supervised training data - set in the form of sentence will be given to train the machine the bomb which had been planted by terrorist on this morning was defused by the counter terrorist on joining hands with the intelligence force input strings in the sentence containing each words are broken into tokenised arrays of single words with stop words removed . each word in the given sentence gets assigned a label w1 , w2 and so on i.e , w2 = bomb w6 = planted w13 = defused calculating the scores for individual word combinations , the result should yield something like : w2.w6 = scores should be positive ( or > some threshold value ) w2.w13 = scores should be negative ( or & lt ; some threshold value ) in case of words with polarity changers eg . : bomb wasn't / haven't / didn't got defused . the resulting scores should be positive to accomplish this task i had implemented sentiment analysis with the threshold = 2.5 and ended up with the following scores actual output : & lt ; 2.5 : low = 2.5 : neutral > 2.5 : high expected output : case 1 : score = negative , since that bomb was defused or removed in the given sentence case 2 : score = positive , vice versa of "" case 1 "" case 3 : otherwise score = 0 , in case it ca n't predict either of the above two cases , it should be neutral i am facing a severe problem every time i need to update the vocabulary list with upcoming new words that were not in the dictionary list , which is turning out to be semi - supervised learning . referring to the above sentence to calculate the w(n-(1/2/3/ ... n ) and wn word with reference to word = bomb . the final resulting score should yield as negative . so which machine learning algorithm would be appropriate that fits to yield a better solution and based on the given data set how will i train the machine to learn the above things ? finally should i try to implement by keeping the model persistence . so that it does n’t have to be trained on each run .",15703,2193,2018-05-18T09:31:15.230,2018-05-18T10:30:53.647,which machine learning algorithm is suitable for detecting text w.r.t set of words,machine-learning algorithm learning-algorithms,1,4,1
1643,6445,1,,2018-05-18T00:54:06.063,-1,44,"for speedrunning purposes , i am trying to train a neural network to identify human - executable ways to manipulate pseudo - rng ( in pokemon red , for the interested ) . the game runs at sixty frames per second , and the linear - congruential prng updates every frame , while many frames are unlikely to be relevant to the manipulation ( and so should contain no actions from the neural net ) . any given manipulation is likely to last 30sec-2min , and the advancement rate of the prng can change depending on location in the game - world . i have some experience with coding ai / deep - learning . i 've made some programs using multilayer perceptron and indrnn approaches . from what i can tell , indrnn or a3c would be my best bets . i 'm not expert enough to know the correct approach , though , or to know if the dimensionality of the problem makes it outright unfeasible . 1 ) is this problem reasonably solvable with nn / deep learning ? 2 ) what approach would you recommend to tackle it ?",15711,2193,2018-05-18T13:18:48.840,2018-05-18T17:01:40.630,teaching a nn to manipulate pseudorng over a long time scale ?,neural-networks game-ai recurrent-neural-networks learning-algorithms,1,0,
1644,6446,1,,2018-05-18T00:57:01.580,2,345,"the following paper explains the use of skip connections to break the singularity in deep networks . but , i have not fully understood what singularity is . https://arxiv.org/pdf/1701.09175v8.pdf any easy - understanding explanation ?",10569,,,2018-06-18T07:21:23.453,what 's the definition of singularity in the context of neural networks ?,neural-networks deep-learning deep-network,1,0,
1645,6452,1,,2018-05-18T07:15:15.013,1,27,"this article "" enhancing differential evolution utilizing eigenvector - based crossover operator "" said for a non - separable function traditional crossover algorithm are not suitable and they can not diversify the population sufficiently , so the differential evolution stops at the local optimum points . why does this behavior not hold for separable functions but it exists for non - separable functions ? which key feature of the non - separable functions cause this behavior ?",15714,,,2018-05-18T07:15:15.013,crossover in differential evolution for separable and non - separable functions,evolutionary-algorithms,0,0,
1646,6453,1,6455,2018-05-18T08:26:11.360,0,43,"i am trying to assess an encoder in my autoencoder . i can not seem to grasp which specs make an encoder better than other one in , lets say , unsupervised learning . for example , i am trying to teach my neural network to classify cats , so that when i provide a picture of a bird , my autoencoder would tell me that it is not a picture of a cat . i am trying to understand what exact specs make my encoder ( and decoder ) better ? i understand it is all about chosen weights but is it possible to be more specific ?",14863,,,2018-05-18T12:08:05.157,what are good parameters of an encoder ?,autoencoders,1,0,0
1647,6456,1,,2018-05-18T15:00:41.750,1,27,"according to this paper ( page 4 , bottom - right ) , atrous convolutions can be used to compute responses of arbitrarily large dimensions in deep convolutional neural networks . i do not understand how something like this is true , since by upsampling the filters , one effectively can apply the filter less times to an image , unless one also upsamples the image . applying the filter less times as i see it obviously means that the output ( response ) will be of lower dimensionality . is there something that i am missing here ?",13257,,,2018-05-18T15:00:41.750,atrous ( dilated ) convolution : how one can compute responses of arbitrarily high dimensions in dcnn ?,machine-learning deep-learning convolutional-neural-networks,0,0,0
1648,6459,1,,2018-05-18T23:20:06.880,3,171,"in an rnn to train it , you need to roll it out , and enter in the history of inputs and the history of expected outcomes . this does n't seem like a realistic picture of the brain since this would require , for example , for the brain to store a perfect history of every sense that comes in to it for many time - steps . so is there an alternative to rnns that does n't require this history ? perhaps storing differences or something ? or storing some accumulator ? perhaps there is a way to calculate with rnns that does n't require keeping hold of this history ?",4199,,,2018-08-30T02:00:37.177,is there an alternative to rnns that does n't require knowing input history ?,recurrent-neural-networks,2,1,
1649,6460,1,,2018-05-18T23:20:52.177,4,68,"i remember the first time hearing about google trying to make driverless cars . that was years ago ! these days , i 'm beginning to learn about neural nets and other types of ml and i was wondering : does anybody know how many hours ( or days , months , etch ) is needed in training time to get the results that are now used in today 's self - driving vehicles ? ( i am assuming they use neural networks for this ... )",15733,,,2018-08-17T20:55:53.717,how long has it taken for autonomous driving cars to be being sold and used on the roads today ?,machine-learning deep-learning training self-driving,2,0,2
1650,6461,1,,2018-05-19T02:39:08.203,4,520,"what is current research in artificial intelligence and machine learning in the field of data compression ? i have done my research on the paq series of compressors , some of which use neural networks for context mixing . i would love if it combined both artificial intelligence and data compression . ( but i am open to suggestions for any project ! )",15736,1581,2018-08-14T17:43:37.110,2018-08-14T17:43:37.110,artificial intelligence in data compression,neural-networks research,2,0,
1651,6464,1,,2018-05-19T06:46:02.830,1,33,"with all the google i / o stuff coming out , how can i verify that i have an actual human on the phone using only voice ? are there still vocal things humans can , but robots ca n't do ? conditions : the person on the phone is a stranger ( so personal questions wo n't work ) , and the verification must be voice only . ( also , i understand google duplex may be just an overhyped demo that will turn out to flop like the pixel buds . but eventually such a bot would be created , right ? if so , what 's the best verification ? )",15743,,,2018-07-30T19:02:32.553,""" vocal captcha "" for robots on the phone ?",human-like voice-recognition computational-linguistics,1,0,1
1652,6465,1,6467,2018-05-19T07:49:02.423,3,196,let 's say you want to do ai research and publish some papers just by your own . would you send them to an ai journal using just your name ? which ai journals are recommended ?,11604,,,2018-08-22T08:09:10.237,how can you do ai research by your own ?,research,2,1,
1653,6468,1,6599,2018-05-19T16:41:50.253,4,1326,"relu : y = max(0,x ) linear : y = x the relu nonlinearity just clips the values & lt ; 0 to 0 and passes everything else ; then why not to use a linear activation function instead as it will pass all the gradient information during backpropagation . i do see that parametric relu ( prelu ) does provide this possiblity . i just want to know if there is a proper explanation to using relu as default or it is just based on observations that it performs better on the training sets .",8720,,,2018-08-23T15:07:00.060,why to prefer relu over linear activation functions ?,neural-networks machine-learning deep-learning convolutional-neural-networks,1,3,
1654,6470,1,,2018-05-19T18:20:55.797,1,40,"say i have access to several pre - trained cnns ( e.g. alexnet , vgg , googlelenet , resnet , densenet , etc . ) which i can use to extract features from an image by saving the activations of some hidden layer in each cnn . likewise , i can also extract features using conventional hand - crafted techniques , such as : hog , sift , lbp , ltp , local phase quantization , rotation invariant co - occurrence local binary patterns , etc . thus , i can obtain a very high - dimensional feature vector of an image that concatenates the individual features vectors outputted by these individual algorithms . given these features , and given a data set of images over which i want to perform similar image retrieval ( i.e. finding the top - k most similar images to a query image x ) , what would be the most appropriate ways to implement this task ? one possible idea i have in mind is to learn an image similarity embedding in euclidean space by training a neural network that would receive as input the aforementioned feature vectors , and perhaps down - sampled versions of the image as well , and output a lower dimensional embedding vector that ideally should place similar images close to each other and dissimilar images far apart . and i could train this network using for example siamese loss or triplet loss . the challenge of this approach though is generating the labels for the ( supervised ) training itself . for example , in the case of the triplet loss i would need to sample triplets ( q , x , y ) and somehow determine which one between x and y is most similar to q , in order to generate the label for the triplet ( i.e. , in order to "" teach "" the network i need to know the answers myself beforehand , but how ? i guess this is domain dependent , but think of challenging cases where you have very heterogeneous images , such as photography galleries , artwork galleries , etc ) . anyways , this is just an idea and by no means i pretend to mean this is the right approach . i 'm open to new suggestions and insights about how to solve this task .",12746,,,2018-05-19T18:20:55.797,how to combine heterogeneous image features extracted with different algorithms for similar image retrieval ?,deep-learning image-recognition computer-vision,0,0,1
1655,6471,1,,2018-05-19T20:43:20.800,1,47,"i 'm trying to design a neural network with a task hierarchy . this is my idea so far : [ desires ] | [ layer 1 ] [ t0 ] | / [ layer 2 ] [ t1 ] | / [ layer 3 ] [ t2 ] | / [ layer 4 ] [ t3 ] | / [ action ] the way this would work is that each layer represents a task as a binary number . layer 1 is the main task , layer 2 the sub - task etc . each task consists of 2 sub - tasks determined by t={0,1}. in this way the neural network represents a binary task graph with t=0 being the left child and t=1 being the right child of a node . you can think of it as t3 changing every second t2 changed every 2 seconds and so on . so { t0 t1 t2 t3 } gives the binary time in seconds in a 16 second cycle . so far this only makes the output a sequence of 16 actions in order . but if some of the layers could be "" if "" gates they might control the t - values and so act as switches and so have more complicated programs . do you have any suggestions to improve this ? or has this kind of binary task graph representation been done before in a neural network ? also importantly how would you train such a neural network ? ( at the moment i just assume that the model is pre - trained and just trying to find a good architecture ) .",4199,4199,2018-05-19T20:48:32.410,2018-05-19T20:48:32.410,how to create a task - graph based neural network ?,neural-networks deep-learning tensorflow,0,0,
1656,6472,1,,2018-05-20T04:20:10.563,1,203,"a.i community , this is my first post on here i am currently reading , learning and designing models . at the moment i 'm working on this sentiment analysis tool ; from what i gather sentiment analysis can be tricky to fine - tune hence why i 'm reaching out here to improve on my model . i am asking for tips and pointers the how s and how not i would appreciate detailed answers in the context of improvement etc . currently , the model is bias towards positive sentiment and even negative text is yielding.6 positive when it should be obvious in the negative sentiment side . my csv contains 65000 tweets pre - labeled there is an even number it seems of positive and negative tweets and is indeed correctly labeled . import pandas as pd from sklearn.model_selection import train_test_split from keras.preprocessing.text import tokenizer from keras.preprocessing.sequence import pad_sequences from keras.models import sequential import re data = pd.read_csv(""training_data.csv "" ) data = data[['sentiment ' , ' sentimenttext ' ] ] data['sentimenttext ' ] = data['sentimenttext'].apply(lambda x : x.lower ( ) ) data['sentimenttext ' ] = data['sentimenttext'].apply((lambda x : re.sub('[^a-za-z0-9\s]','',x ) ) ) max_features = 2000 tokenizer = tokenizer(num_words = max_features , split= ' ' ) tokenizer.fit_on_texts(data['sentimenttext'].values ) x = tokenizer.texts_to_sequences(data['sentimenttext'].values ) x = pad_sequences(x ) from keras.layers import dense , dropout , lstm , embedding embed_dim = 50 lstm_out = 80 model = sequential ( ) model.add(embedding(max_features , embed_dim , input_length = x.shape[1 ] ) ) model.add(dropout(0.2 ) ) model.add(lstm(lstm_out ) ) model.add(dropout(0.2 ) ) model.add(dense(2,activation='softmax ' ) ) model.compile(loss = ' binary_crossentropy ' , optimizer='adam ' , metrics=['accuracy ' ] ) print(model.summary ( ) ) y = pd.get_dummies(data['sentiment']).values x_train , x_test , y_train , y_test = train_test_split(x , y , test_size=0.20 , random_state=42 ) print(x_train.shape , y_train.shape ) print(x_test.shape , y_test.shape ) model.fit(x_train , y_train , nb_epoch=35 , batch_size=32 , verbose=1 ) # save model to disk and print the summary model.save('model.h5 ' , overwrite = true ) print(model.summary ( ) )",10139,,,2018-05-20T04:20:10.563,how to improve this sentiment analysis model,machine-learning tensorflow python keras sentiment-analysis,0,0,
1657,6475,1,,2018-05-20T19:10:33.773,1,55,"given a query image q and two other images x and y ( you can assume they have more or less the same resolutions if that simplifies the problem ) , which algorithm would perform extremely well at determining which image between x and y is most similar to q , even when the differences are rather subtle ? for example , a trivial case would be : q = image of mountains , x = image of mountains , y = image of dogs , therefore it is clear that sim(q , x ) > sim(q , y ) . however , examples of trickier cases would be : q = image of a yellow car , x = image of a red car , y = image of a yellow car , therefore sim(q , y ) > sim(q , x ) ( assuming the car shapes are more or less the same ) . q = image of a man standing up in the middle with a black background , x = image of another man standing up in the middle with a black background , y = image of a woman standing up in the middle with a black background , therefore sim(q , x ) > sim(q , y ) . which algorithm ( or combination of algorithms ) would be robust enough to handle even the tricky cases with very high accuracy ?",12746,12746,2018-05-20T19:23:55.013,2018-05-21T13:51:06.630,"given a query image q and two other images x and y , how to determine which one is most similar to q ?",algorithm computer-vision,1,10,0
1658,6478,1,,2018-05-21T14:42:03.630,3,90,"i am trying to track lidar objects using kalman filter . the problem is that the innovation has the value 0 , which makes the kalman gain be infinity . here is a link with the kalman equations . the values with which i initialized the measurement and process covariance matrix are listed below . the update code is also shown below . when i debug the code everything is fine until the innovation becomes 0 . this-&gt;lidar_r & lt;&lt ; std_laspx _ , 0 , 0 , 0 , 0 , std_laspy _ , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ; this-&gt;lidar_h & lt;&lt ; 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ; p _ & lt;&lt ; 1000 , 0 , 0 , 0 , 0 , 0 , 1000 , 0 , 0 , 0 , 0 , 0 , 1000 , 0 , 0 , 0 , 0 , 0 , 1000 , 0 , 0 , 0 , 0 , 0 , 1000 ; matrixxd pht = this-&gt;p _ * h.transpose ( ) ; //s becomes 0 matrixxd s = h * pht + r ; //s_inv becomes infinity matrixxd s_inv _ = s.inverse ( ) ; matrixxd k = pht * s_inv _ ; vectorxd y = z - hx ; this-&gt;x _ = this-&gt;x _ + k*y ; matrixxd i = matrixxd::identity(x_.size ( ) , x_.size ( ) ) ; this-&gt;p _ = ( i - k * h ) * this-&gt;p _ ;",15775,2193,2018-05-21T19:35:15.613,2019-05-26T08:01:25.327,kalman filter pre inovation,computer-vision,2,1,
1659,6479,1,,2018-05-21T15:30:21.627,1,97,"a task i ’m working on at the moment requires a cnn with a height map as one of the inputs . this is a matrix of floating point values in which each point is the height of that point above sea level . i ’m having trouble deciding how to normalize this data . i know there are networks that work on depth or distance data but that is different for several reasons : height can also be negative ( as opposed to depth / distance which starts at 0 ) height has a very large range - can get values between -400 and + ~9000 . for these reasons the common approach to normalisation , simply subtracting the mean and dividing by the standard deviation , will result in the loss of information in most cases ( all values will be close to zero ) . i thought of maybe subtracting the local mean for each input , rather than a general mean calculated from all the data , but i still do n't know what to do with the standard deviation , since dividing by the local standard deviation can result in very “ flat ” and very “ steep ” inputs looking the same after normalization .",15776,2193,2018-05-21T19:35:07.787,2018-05-31T16:52:23.287,normalizing height data for cnn,deep-learning convolutional-neural-networks,0,6,
1660,6481,1,,2018-05-21T16:03:53.303,3,71,"i have a data set with historical information of some events ( let 's say event a and event b),these events describe the discovery of land mines , the coordinates of the event and the date of the event ; is there a way i can use this historical information to predict points ( coordinates ) where event a or b could happen i.e. where might be still land mines that have n't been found ?",15764,15764,2018-05-22T17:21:44.397,2018-05-22T17:21:44.397,is there a way to predict points on a map ?,neural-networks machine-learning datasets evolutionary-algorithms game-theory,1,2,
1661,6486,1,6487,2018-05-22T07:57:58.633,3,576,"in slide 16 of his lecture 5 of the course "" reinforcement learning "" , david silver introduced glie monte - carlo control . but why is it an on - policy control ? the sampling follows a policy while improvement follows an -greedy policy , so is n't it an off - policy control ?",15525,2444,2019-02-18T15:10:18.863,2019-02-18T15:12:36.907,why is glie monte - carlo control an on - policy control ?,reinforcement-learning control-problem on-policy monte-carlo,1,0,
1662,6488,1,,2018-05-22T09:49:42.907,5,1166,"i used the example at - https://github.com/aymericdamien/tensorflow-examples/blob/master/examples/5_datamanagement/tensorflow_dataset_api.py - to create my own classification model . i used different data but the basic outline of datasets was used . it was important for my data type to shuffle the data and then create the training and testing sets . the problem however comes as a result of the shuffling . when i train my model with the shuffled train set i get a + - 80 % accuracy for train and + - 70 % accuracy for the test set . i then want to input all the data ( i.e. the set that made the training and test set ) into the model to view the fully predicted output of this data set that i have . if this data set is shuffled as the training and testing set was i get an accuracy of around 77 % which is as expected but then if i input the unshuffled data ( as i required to view the predictions ) i get a 45 % accuracy . how is this possible ? i assume it 's due to the fact that the model is learning incorrectly and that it learns that the order of the data points plays a role in the predicting of those data points . but this should n't be happening as i am simply trying to ( like the mnist example ) predict each data point separately . this could be a mini - batch training problem . so i ask , in the example mentioned above , using data sets and batches to train , does the model learn from the average of all the data points in the mini - batch or does it think one mini - batch is one data point and learn in that manner ( which would mean order matters of the data ) ? or if there are any other suggestions . thanks",15789,21109,2019-02-01T22:35:23.340,2019-02-01T22:35:23.340,tensorflow batch learning,deep-learning training python tensorflow,3,1,
1663,6489,1,6511,2018-05-22T13:25:09.207,1,314,"i know how iou works during detection . however , while preparing targets from ground truth for training , how is the iou between a given object and all anchor boxes calculated ? is the ground truth bounding box aligned with an anchor box such that they share the same center ? ( width/2 , height/2 ) i think this is the case but i want to hear from someone who has better knowledge of how training data is prepared for training in yolo .",13255,,,2018-05-25T04:11:15.277,how are ious for ground truth boxes in yolo calculated ?,neural-networks,1,2,2
1664,6490,1,,2018-05-22T20:23:08.880,1,93,"the cognitive architecture soar was invented by allen newell and is described extensively in the literature . according to newer interpretations , soar is an agi project which is an artificial intelligence . but what happened if this assumption is wrong ? my hypothesis is , that soar is not the artificial intelligence itself , but an environment for testing an ai how good it is . let us make an example : in the year 1955 allen newell published a paper “ the chess machine : an example of dealing with a complex task by adaptation ” in which he described a predecessor to soar . but can this paper be used to build a chess - playing machine like the famous ibm deepblue ? did the newell paper includes modern heuristics like alpha - beta - prunning for playing chess on grandmaster level ? no , it does n't . but we can utilize the paper for something else . to build an ai competition . that means , a software programming challenge in which teams must create an ai from scratch . the assumption is , that soar is not the ai itself , but only an early form of a robotics challenge like micromouse , robocup rescue and a chess playing challenge . the sad news is , that programming a lisp - software which is able to evaluate the performance of an ai is not very demanding . programming a game , in which an agent must act intelligently is easier then programming the agent itself . programming a chess game is easier then programming a software which is able to win the game . but it seems , that everything what is discussed in the literature under the term agi , soar and act - r is only about programming challenges . that means , soar is not the end , it is the beginning . soar is not the answer , it is only the problem definition in clear lisp code . let us suppose we want to program our own micromouse - soar , how does it look like ? micromouse - soar is primary an agent - runtime environment . that means it is able to evaluate how fast a mouse finds the way through a maze . micromouse - soar does n't answer the question how to program such an agent . it contains no pathplanning algorithm and no lowlevel control of the robot . instead the software is only able to say , if the mouse finds the way through the maze . after this longer introduction , my question is short and simple : am i right ? are further hints available which are showing , that soar and other so called cognitive architecture are useless in reality and were only invented to evaluate narrow ai software ?",11571,,,2018-05-22T20:23:08.880,is soar a robotics challenge ?,agi,0,0,
1665,6491,1,,2018-05-22T21:02:54.883,5,59,"various texts on using cnns for object detection in images talk about how their translation invariance is a good thing . which makes sense for tasks where the object could be anywhere in the image . let 's say detecting a kitten in household images . but let 's say , you already have some information about the likely position of the object of interest in the image . for example , for detecting trees in a dataset of images of landscapes . here in most cases , the trees are going to be in the bottom half of the image while in some cases they might be at the top ( because it 's on a hill or whatever ) . so you want your neural network to learn that information -- that trees are likely connected to the bottom part of the image ( ground ) . is this possible using the cnn paradigm ? thank you",15803,,,2018-07-24T16:13:01.040,can translational invariance of cnns be unwanted if object is likely in certain positions ?,neural-networks convolutional-neural-networks object-recognition,2,0,1
1666,6492,1,6502,2018-05-23T03:57:53.923,2,111,"i was just reading through some convex optimization textbooks to hopefully improve my deep learning understanding and come up with new ideas . halfway through , i decided to google a bit ! it 's obvious that deep learning deal with nonconvex functions . here 's the question though : if deep learning is non - convex then why we apply a convex loss function such as cross - entropy or least square to solve a problem under a convex constraint ? what am i missing ?",15805,1671,2018-05-23T20:01:22.447,2018-05-24T05:43:56.133,"if deep learning is non convex , then why use convex loss ?",machine-learning deep-learning ai-basics optimization,1,3,
1667,6497,1,,2018-05-23T11:18:27.357,1,83,"for a regression task , i have sequences of training data and if i define the layers of deep neural network to be : layers= [ sequenceinputlayer(featuredimension ) relulayer dropoutlayer(0.05 ) fullyconnectedlayer(numresponse ) regressionlayer ] is it a valid deep neural network ? or do i need to add lstm layer too ?",15649,1671,2018-05-23T19:58:38.563,2018-05-23T19:58:38.563,is it a valid deep neural network ?,neural-networks deep-learning terminology matlab,1,0,
1668,6499,1,6531,2018-05-23T14:27:12.527,6,128,"i want to use computer vision to allow my robot to detect the corners of a soccer field based on its current position . matlab has a detectharrisfeatures feature , but i believe it is only for 2d mapping . the approach that i want to try is to collect the information of the lines ( using line detection ) , store them in a histogram , and then see where the lines intersect based on their angles . my questions are : how do i know where the lines intersect ? how do i find the angles of the lines using computer vision ? how do i update this information based on my coordinates ? i am in the beginning stages of this task , so any guidance is much appreciated !",15821,,,2018-09-08T05:00:23.653,how to use computer vision to find corners of a soccer field based on location coordinates ?,algorithm computer-vision matlab,3,0,
1669,6503,1,6506,2018-05-24T10:26:14.820,0,298,"how do i decrease the accuracy value when training a model using keras ; which parameters can i change to decrease the value ? my objective is not to actually decrease it , but just to know which parameters influence the accuracy sgd = optimizers.sgd(lr=1e-2 )",15818,2193,2018-05-24T10:41:20.030,2018-05-31T22:08:03.310,how to decrease accuracy from 99 % to 80%~85 % using keras for training a model,neural-networks python keras,1,3,
1670,6504,1,6630,2018-05-24T15:57:40.550,1,213,"i would like to know how to teach an agent for performing prediction of the severity of disease and also for alerting patients using machine learning methods . i found the model - based reflex agent class can be used in medical diagnosis in some literature . may i know which architecture will be good , to make such an agent ?",15631,1671,2018-05-25T16:31:10.940,2018-06-02T12:52:52.547,how to teach a model - based reflex agent for doing some task using machine learning methods ?,machine-learning python getting-started intelligent-agent architecture,2,3,
1671,6510,1,,2018-05-25T03:00:56.987,2,496,"i am looking to detect think objects like pens , pencils and surgical instruments . the bounding box is not important , but i am looking to see if i can train a model to detect both the object as well as its orientation . typical object detection networks like r - cnn , yolo , ssd encode class name and bound boxes . instead of bounding boxes , i 'm looking to encode only 2 points , one starting x , y point and one ending x , y point . the start point for objects is where one would grip the object . for instance : the pencil eraser(start point ) is pointed 50 degrees to the top right . the surgical instrument is 10 degrees from the x - axis and handle is pointed to the bottom right . pen tip(end point ) is pointing vertically upwards . fork , the start point would be the grip handle part , and the end point would be in the middle where the 4 prongs are . as long as i can encode the start and end points , then i can determine the orientation . i would need to define these points during training and the question is whether there is an existing model ( mobile net / inception / rcnn ) that i can encode this information in ? one potential way i was thinking was to use yolo and for the bounding box , the top left x , y would be the starting point x , y(handle ) whereas the bound box width , height would be replaced with the end point x , y(pencil writing tip , fork prongs . thank you .",15865,15865,2018-06-05T06:17:14.833,2018-06-05T06:17:14.833,custom object detection model including orientation of the object,machine-learning deep-learning convolutional-neural-networks,1,0,1
1672,6515,1,,2018-05-26T09:15:43.377,1,49,is there a way in the weka explorer to manually select the initial centres when using simplekmeans clustering ?,15879,,,2018-08-01T14:01:03.550,weka - simplekmeans - manually choose intitial centres,machine-learning,1,2,
1673,6518,1,,2018-05-27T09:50:31.767,1,53,"hi for a small little research for school i need some different opinions on a few different questions . if you could answer these questions i would really appreciate it . how far do you think machine learning will be able to progress , do you see development hitting a wall in the future ? do you think machine learning in it 's current state is already able to replace simple jobs ? if not how long do you think it will take ? how will machine learning affect jobs ? do you see machine learning and ai taking over jobs like programming in the future or do you think it will create more jobs than it destroys ?",15894,,,2018-06-01T21:41:14.243,need some different opinions on some questions,machine-learning,2,4,
1674,6524,1,6566,2018-05-27T15:47:36.650,2,101,"i have to read a lot of papers , and i thought that i can use an a.i . to read them and summarize them . maybe find one that can understand what the papers are talking about it seems a lot to ask . i think i can use natural language processing . is it the right choice ? i 'm sorry , but i 'm new in a.i . and i do n't know much about it .",4920,4302,2018-10-08T11:56:59.767,2018-10-08T11:56:59.767,how can i build an ai with nlp that reads and understands documents ?,natural-language-processing,1,4,
1675,6526,1,6529,2018-05-27T16:05:17.947,3,860,"i 'm currently working on license plate recognition . my system consist of 2 stage : ( 1 ) license plate region extraction & amp ; ( 2 ) license plate region recognition . i 'm doing ( 1 ) with raspberry pi 3 model b . i find license plate candidate first by merging bounding boxes based on their similarity . in this way , i have only 1~7 license plate region proposals . and it took less than .3 seconds . now i have to reduce number of region proposal to be around only 1~2 so that i can send these images to server to do job ( 2 ) . for license plate extraction , i made my own classifier function in tensorflow and the code is below . it gets proposed license plate as input . first , i resize all license plate to be [ 120 , 60 ] and converted to gray image . and there are 2 classes : ' plate ' , ' non_plate ' . for non_plate image , i collected various image that might appear in image as background . i have 181 images for ' plate ' class and 56 images for ' non_plate ' for now , i trained for about 3000 steps so far and current loss is .53 . when i did prediction on test set , i encountered problem that for some of plate image , it does n't recognize license plate which is very obviously license plate image from my eyes . it is okay for me to wrongly recognize non plate image as plate but it is problem if it wrongly recognize plate as non_plate because it will not be sent to server to be fully recognized . it happens like 10 out of 100 test images and this rate is far worse than i expected . i need help for adressing this problem . would there be any improvement that i can make ? ( 1 ) is my training set too small to classify between license plate and non license plate ? or is number of steps is too small ? ( 2 ) is my graph structure bad ? i needed to have small graph structure for my raspberry pi to recognize less than 1 second . could you suggest better structure if it is bad ? ( 3 ) is it bad to resize any proposed image to [ 120 , 60 ] to be used as input for graph ? i think it loses some information . but is n't this close to roi pooling like used in fast rcnn ? inputs = tf.reshape(features[feature_label],[-1,120 , 60 , 1],name=""input_node "" ) # 120 x 60 x 1 , which is gray conv1 = tf.layers.conv2d(inputs = inputs , filters=3 , kernel_size=[3,3 ] , padding='same ' , activation = tf.nn.leaky_relu ) # conv1 output shape : ( batch_size,120,60,3 ) pool1 = tf.layers.max_pooling2d(inputs = conv1,pool_size=[2,2],strides=2,padding='valid ' ) # pool1 output shape : ( batch_size,60,30,3 ) conv2 = tf.layers.conv2d(inputs = pool1,filters=6,kernel_size=[1,1],padding='same',activation = tf.nn.leaky_relu ) # conv2 output shape : ( batch_size , 60,30,6 ) pool2 = tf.layers.max_pooling2d(inputs = conv2,pool_size=[2,2],strides=2,padding='valid ' ) # pool2 output shape : ( batch_size , 30,15,6 ) conv3 = tf.layers.conv2d(inputs = pool2,filters=9,kernel_size=[3,3],padding='same',activation = tf.nn.leaky_relu ) # conv3 output shape : ( batch_size , 30,15,9 ) pool3 = tf.layers.max_pooling2d(inputs = conv3,pool_size=[2,2],strides=2,padding='valid ' ) # pool3 output shape : ( batch_size , 15,7,9 ) # dense fully connected layer pool2_flat = tf.reshape(pool3,[-1,15 * 7 * 9 ] ) # flatten pool3 output to feed in dense layer dense1 = tf.layers.dense(inputs = pool2_flat , units=120,activation = tf.nn.relu ) logits = tf.layers.dense(dense1,2 ) # input for softmax layer [ training non plate image example ] [ ] 4 [ training plate image example . it is region proposed image ]",12090,12090,2018-05-27T16:50:50.263,2018-05-28T00:43:55.777,detecting license plate using tensorflow,convolutional-neural-networks image-recognition tensorflow python,1,2,1
1676,6527,1,6569,2018-05-27T16:33:37.543,3,388,"there are many books , courses , etc . out there , but not sure which path to take . so what would be the most effective way ( shortest ) to learn natural language processing online ? p.s . i mean learning fundamentals , not how to use existing libraries or services .",15902,,,2019-03-14T17:47:45.863,what is the most effective way to learn natural language processing online ?,neural-networks natural-language-processing,3,2,3
1677,6536,1,,2018-05-28T21:18:44.890,1,99,"i am writing an mdp based agent that is supposed to learn to place bids and asks in a trading environment . the system requests 2 values ( mwh energy and $ , both being positive or negative ) . every timestep the agent has a certain volume that it has to either buy or sell . i tried setting these two values as action values , giving it 4 individual ones ( 1 for buy price and amount one sell price and amount ) i used the ddpg and naf agents from keras - rl here but both are n't working for me . i tried a number of reward functions too : direct cash reward : average price of market for required energy vs what the agent achieved shifting balancing price : first emphasize that the broker balances it 's portfolio ( i.e. orders the amount it has to ) and later optimize for price per mwh simple core : as a test i ran a reward function that just rewards the agent to be close to the actions [ 0.5 , 0.55 ] all three failed again . lr : tried between 0.01 and 0.00001 layers : tried anything between 1 layer 1 cell and 5 layer 128 cells types : i used both dense and lstm cells with according input shapes symptoms : generally it looks like the system is not learning anything . i am unsure why . how does the reward function have to be structures to incentivize the system to at least move in the correct direction ? especially the reward that told the agent to be close to [ 0.5 , 0.5 ] by basing the reward simply on the squared difference to this point should have worked in my eyes .",11429,,,2018-05-28T21:18:44.890,training rl agent on timeseries trading data with continous deep q or naf,neural-networks q-learning,0,1,
1678,6538,1,6544,2018-05-29T00:27:14.697,1,44,"i 'm looking at writing an ai agent for pattern recognition . i want to be able to constantly feed new data to the ai to continuously train it as new data may have new patterns . my problem , though , is that my input feed may break once in a while ( the data comes from a remote computer ) and thus some of the data will go missing . the other computer sends me real - time data so when the connection goes down , any new data while disconnected goes missing as far as the ai agent is concerned . ( at this point , i 'm not looking at fixing the gaps , although ultimately , reducing them is one of my goals , at this point i have to pretend it 's not possible to accomplish . ) what kind of impact missing data has on a pattern recognition ai ?",15927,,,2018-05-29T09:56:46.847,will training an ai still work if the input data is somewhat sparse ?,pattern-recognition detecting-patterns,1,1,
1679,6540,1,,2018-05-29T00:52:52.050,1,162,"in chapter 8 of "" reinforcement learning : an introduction "" by sutton and barto , it is stated that dyna needs a model to simulate the environment . but why do we need a model ? why ca n't we just use the real environment itself ? would n't it be more helpful to use real environment instead of fake one ?",6851,2444,2018-11-24T04:31:22.617,2018-11-24T04:31:22.617,why do we need a model of the environment in dyna ?,reinforcement-learning models,2,0,
1680,6545,1,6547,2018-05-29T09:43:18.083,1,114,"this is more of a technical question rather than a practical one . i 've exported a decision tree made with python / scikit learn and would like to know what the "" value "" field of each leaf corresponds to . thanks !",12940,,,2018-05-29T16:43:21.227,scikit learn : decision tree data meaning,python decision-tree,1,0,
1681,6546,1,,2018-05-29T10:40:09.527,1,48,"i am very new to ai , i have a set of 3d human models that i would like to train the algorithm to identify wrist , upper arm , lower arms , etc , and distance between them . from my understanding , this is a regression problem . but with my very limited knowledge , most tutorial online showing me cat and dog classification problem . do you have any clue for me to research next ? there are some paper saying to convert the 3d model to image , and use convolutional neural network for training . p / s : please do n't downvote me , i am too young and too lost in this field .",15755,,,2018-05-30T13:23:09.467,"detecting keypoint of 3d model , and distance between them",convolutional-neural-networks,1,0,
1682,6548,1,6616,2018-05-29T11:12:46.530,3,52,"i am trying to build an agent that trades commodities in a exchange setting . what are good ways to map the action output to real world actions ? if the last layer is a tanh activation function , outputs range between [ -1,+1 ] . how do i map these values to real actions ? or should i change the output activation to linear and then directly apply the output as an action ? so let 's say the output is tanh activated and it 's -0.4 , 5 . i could map this to : - -0.4 -- > sell 40 % of my holdings for 5 $ per unit - -0.4 -- > sell 40 % for 5 $ in total if it was linear , i could expect larger outputs ( e.g -100 , 5 ) . then the action would be mapped to : - sell 100 units for 5 $ each - sell 100 units for 5 $ total",11429,,,2018-06-01T21:37:46.513,what are good action outputs for reinforcement learning agents acting in a trading environment ?,neural-networks reinforcement-learning,1,0,
1683,6556,1,,2018-05-29T17:21:07.660,3,1059,what are feature embeddings in the context of convolutional neural networks ? is it related to bottleneck features or feature vectors ?,15945,15945,2018-05-29T17:31:19.120,2018-07-09T07:34:43.673,what is feature embedding in the context of convolutional neural networks ?,neural-networks deep-learning convolutional-neural-networks,1,0,
1684,6557,1,6568,2018-05-29T19:30:10.457,3,375,"hi i study ai by myself with "" ai a modern approach "" i 've just finished the chapters about bayesian network and probabilities , and i found them very interesting . now i want to implement the differents algorithms and test them on differents cases and environments . but the question is : is it worth it to spend time on these techniques ?",15949,,,2018-06-04T19:01:35.337,are bayesian networks important to learn in 2018 ?,bayes,2,3,1
1685,6567,1,,2018-05-30T06:44:46.943,1,70,"i am using the fceux emulator to create a genetic algorithm in lua to play the ' arkanoid ' game . it is based on atari breakout . a member of my population contains a string of 0 's and 1's.(population size:200 ) . consider a member every 10 frames a bit is read from the string.(length of string is about 1000 ) if it is 0 the paddle moves left , if it is 1 the paddle moves right for the next 10 frames . now i wrote an genetic algorithm that tries to find the best sequence of inputs to play the game . i have experimented with three types of fitness , one is to achieve maximum score , one is to try to reduce number of blocks to a minimum and the last one is to try to stay alive as long as possible . none of the three fitness seem to work . then i thought that something with my crossover might be wrong . every generation , i print out the average fitness of all members . some generations it increases , while in some generations it decreases . i have tried changing the population size to 50,100,200,300 . mutation in my algorithm has a 1 % chance(if mut_rate=1 ) that each of the bit will be replaced with its opposite bit . now coming to the crossover , i have used yet again many methodologies . one of them is to just select the top 20 % or 30%(cr_rate)(according to their fitness ) to pass on to the next generation and killing the remaining ones . another method is to add the top percentile to the population and use the remaining population to swap a few bits with top ones and add them into the next generation . function crossover(population , rate ) local topp = math.floor(rate*(#population ) ) ; top= { } for i=1,topp do table.insert(top,population[i ] ) end for i=1 , # population do local p1 = math.random(1,topp ) ; local p2 = math.random(1,topp ) ; --print(top[p1 ] ) ; --print(top[p2 ] ) ; if top[p1][2 ] = = top[p2][2 ] then local rval = math.random(1 , 10 ) & gt ; 5 ; if rval then population[i ] = top[p1 ] ; else population[i ] = top[p2 ] ; end elseif top[p1][2 ] & gt ; top[p2][2 ] then population[i ] = top[p1 ] ; else population[i ] = top[p2 ] ; end population[i][2]=0 ; end -- [ [ for i = topp+1,#population do local p1 = math.random(1,topp ) ; local p2 = math.random(1,#population ) ; local s= ' ' ; local flag=0 ; s = string.sub(top[p1][1],1,no_controls/2) .. string.sub(population[p2][1],(no_controls/2)+1,no_controls ) ; population[i][1]=s ; population[i][2]=0 ; end -- ] ] end population is the table of population where each member has an input string and a fitness value.(sorted , max fitness is first ) . rate is the percentage to select the top performers . no_controls is the size of input string . the commented section of the code is where i perform the swap . here is the mutation function . function mutation(population , mut_rate ) local a=0 ; local b=1 ; for i=1 , # population do for j=1 , # ( population[i][1 ] ) do if math.random(1 , 100 ) & lt;= mut_rate then if string.sub(population[i][1],j,j)=='1 ' then population[i][1 ] = string.sub(population[i][1],1,j-1)..a..string.sub(population[i][1],j+1 ) ; else population[i][1 ] = string.sub(population[i][1],1,j-1)..b..string.sub(population[i][1],j+1 ) ; end end end end end mut_rate is 1 . and crossover rate is 0.2 or 0.5 . i have tried changing the mutation rate from 0 to 20 . i have also tried to change the crossover rate as 0.2,0.5,0.7 . and the fitness using no_blocks , score , time_alive . when i run the algorithm , the average fitness of the population first increases slightly , then decreases after a few generations and then remains constant forever . the paddle also seems to be performing the same moves over and over again , which made be think that there might not be enough variation . i need help , because i have been stuck on this for a few days now . i need suggestions on what would be a suitable crossover and mutation function and a perfect fitness function . thanks .",15944,,,2018-05-30T06:44:46.943,genetic algorithm to play arkanoid(nes ) possible crossover and fitness ?,genetic-algorithms,0,5,
1686,6571,1,,2018-05-30T09:12:50.830,3,91,"i 'm having trouble grasping how to output word embeddings from an lstm model . i 'm seeing many examples using a softmax activation function on the output , but for that i would need to output one hot vectors as long as the vocabulary ( which is too long ) . so , should i use a linear activation function on the output to get the word embeddings directly ( and then find the closest word ) or is there something i 'm missing here ?",5558,2444,2019-04-16T22:42:23.677,2019-04-16T22:42:23.677,how should the output layer of an lstm be when the output are word embeddings ?,natural-language-processing lstm word-embedding word2vec,2,0,0
1687,6573,1,,2018-05-30T10:50:16.373,2,131,"there seems to be a major difference how the terminal reward is received / handled in self - play rl vs "" normal "" rl which confuses me . i implemented tictactoe the normal way , where a single agent plays against an environment that manages the state and also replies with a new move . in this scenario the agent receives a final reward of +1,0,-1 for a win / draw / loss . next i implemented tictactoe in a self - play mode where two agents perform moves one after the other and the environment only manages the state and gives back the reward . in this scenario an agent can only receive a final reward of +1 or 0 because after his own move he will never be in a terminal state in which he lost ( only agent2 could terminate the game in such a way ) . that means : in self - play , episodes end in such a way that only one of the players sees the terminal state and terminal reward . because of point one , an agent can not learn if he made a bad move that enabled his opponent to win the episode . simply because he does not receive a negative reward . this seems very weird to me . what am i doing wrong ? or if i 'm not wrong , how do i handle this problem ? ? thanx",15958,1641,2018-05-30T16:18:49.073,2018-05-31T20:43:21.903,how to see terminal reward in self - play reinforcement learning ?,reinforcement-learning self-play,2,1,
1688,6579,1,,2018-05-30T19:09:05.100,4,266,"i 've been reading google 's deepmind atari paper and i 'm trying to understand the concept of "" experience replay "" . experience replay comes up in a lot of other reinforcement learning papers ( particularly , the alphago paper ) , so i want to understand how it works . below are some excerpts . first , we used a biologically inspired mechanism termed experience replay that randomizes over the data , thereby removing correlations in the observation sequence and smoothing over changes in the data distribution . the paper then elaborates as follows ( i 've taken a screenshot , since there are a lot of mathematical symbols that are difficult to reproduce ) : what is experience replay and what are its benefits in laymen 's terms ?",15967,2444,2019-04-04T16:09:05.310,2019-04-04T16:09:05.310,what is experience replay in laymen 's terms ?,deep-learning reinforcement-learning deep-rl experience-replay,2,1,
1689,6581,1,6586,2018-05-30T19:56:10.023,3,187,"i 've been reading google 's deepmind atari paper and i 'm trying to understand how to implement experience replay . my question is that whether we update the parameters of function $ q$ once for all the samples of the minibatch or we do that for each sample of the minibatch separately ? according to the following code from this paper , it performs the gradient descent on loss term for the $ j$-th sample . however , i have seen other papers ( referring to this paper ) that say that we first calculate the sum of loss terms for all samples of the minibatch and then perform the gradient descent on this sum of losses .",15967,,,2018-05-31T11:57:15.217,implementing experience replay in reinforcement learning,deep-learning reinforcement-learning,1,0,2
1690,6583,1,,2018-05-31T05:59:49.250,4,120,"so i was wondering about the actual constraints in the field of artificially intelligent agent / robot . we have been able to replicate most of human sensory functions like : vision - computer algorithms can perform better image classification tasks . touch - a physical object more sensitive than skin can be modelled easily . taste and smell - although there are electro - mechanical tongues , since taste and smell and related it is quite complex to model ( i.e. no satisfactory devices exist ) . hearing - all types of high fidelity audio recording devices are available . we have also got processors for these sensors : memory - we have got memory running into exabytes probably . processing power - we have got a high processing power , albeit it falls somewhat short of human brain according to this answer . algorithms - we have got highly developed purely logical algorithms , or we can easily machine learn and approximate results of unknown algorithms . we can also model the human anatomy of various physical functions like walking / running / exercising , etc , like this mechanical cheetah my mit students . so my question is what is stopping us from stringing together all this to actually device a quite good artificially intelligent agent ? what are the constraining factors and how are we trying to overcome it ? my wording maybe little bit off , you are free to edit the question keeping the general theme same .",9947,4302,2018-10-22T16:51:36.543,2018-10-22T16:51:36.543,constraints of embedded artificial intelligence for a robot,robots brain sense embedded-design,3,0,2
1691,6584,1,,2018-05-31T11:16:11.123,1,168,"if a conscious ai is ever to become , how do you reckon that will happen ? from a code written by us , like in the tv show ' humans ' or from becoming cognizant from their previous experiences , like in ' westworld ' or perhaps some other way ?",15978,,,2018-07-17T07:31:59.300,"conscious ai , how will it happen ?",artificial-consciousness,4,4,
1692,6596,1,,2018-05-31T22:00:49.973,3,121,"i am looking in to building a kind of troubleshooting web application . it would be a form that starts with a first question . depending on the answer , you get a follow up question and so on until the app has qualified your problem in to a small group of problems . to me it sounds a bit like a decision tree but what i have read about them is that it is the internal structure of a model and not what i am looking for . my guess is that a model needs all the input variables at once and not like i am looking for that feeds it one parameter at a time . at this time i do not know of any data available . with the client we could create the desired resulting problem groups and the questions as well . would it be possible to solve this with the help of ai instead of hand coding a lot of case switch statements ? if so could you point me to what to read up on ?",15895,,,2018-06-01T21:03:47.767,can this problem be solved by ai ?,ai-design classification,3,1,1
1693,6608,1,,2018-06-01T17:33:15.370,3,64,"i found several methods for setting the compatibility distance in neat : some normalize it , some do n't , some automatically adjust it . in a few tests i am running , using normalized static compatibility distance , the number of species increase very rapidly , thus suggesting to adjust ( e.g. increase ) the compatibility distance . i have n't found however , how to determine a reasonable number of species for my population , which are the benefits of having lots / few species and which are the benefits of having stable vs mutable number of species ?",13087,,,2018-06-01T17:33:15.370,speciation in neat - advantages of keeping stable number of species,neat,0,0,1
1694,6622,1,6628,2018-06-02T01:21:27.370,3,1539,"i am building a nn for which i am using sigmoid function as the activation function for the single output neuron at the end . since sigmoid function is known to take any number and return value between 0 and 1 , this is causing division by zero error in back - propagtion stage because of the derivation of cross entropy . i have seen over internet it is advised to use sigmoid activation function with cross entropy loss function . so how this error is solved ?",15652,9947,2018-06-02T09:08:03.597,2019-04-07T10:52:38.433,cross entropy loss function causes division by zero error,neural-networks machine-learning,1,2,
1695,6634,1,9459,2018-06-03T00:24:51.387,4,66,"hello , i would like to know whether this picture from the paper : distributed training of deep neural networks : theoretical and practical limits of parallel scalability valid ? questions : 1 ) does innerproduct ( fully connected ) layer actually take more time to compute in a neural network than convolution ? 2 ) is assessing glops / time a good way of estimating performance of different types of layers in a neural network on any hardware ? ( conv , fc etc . ) 3 ) does anyone know where i can find gflops vs compute time for different types of layers across gpus / cpus ? ( i know deepbench , any other suggestions would be great too )",16014,15465,2018-06-13T16:23:21.233,2019-01-10T15:02:55.587,relative compute time for each type of layer in a neural network,neural-networks convolutional-neural-networks ai-design algorithm performance,1,2,
1696,6638,1,6641,2018-06-04T04:06:03.210,-2,63,"my name is islam , industrial designer from egypt , it ’s my last year and the project i want to make something like smart home system with voice control , can anyone help me with any advice .",16037,,,2018-06-04T09:10:19.807,help me with my final project for my last year college,machine-learning deep-learning ai-design,1,2,
1697,6640,1,,2018-06-04T05:58:41.443,4,148,"i 'm learning fuzzy logic and more or less understand the basic concept , but i 'm having a hard time understanding how to apply it to a method . i tried browsing online for explanation on how to use it , but only found some implementation and test case using the basic form of 4 rules and 3 variables , and 2 rules per variable . anyway this is an example case , i will use tsukamoto method . in this case actually i have 6 rules and 3 variables with 3 rules ver variable , but i will only explain 1 of the variables because i think the rest will have the same solution . i have 3 variables one of them is "" size "" , the range is for small it 's 0 - 2 and for large it 's 7 - 10 . the current condition is size = 6.5 . the rules is as follow(simplified to only use this variable ) : [ r1 ] size = small [ r2 ] size = medium [ r3 ] size = large what i want to know is : how do i define the formula for medium(the middle rule if the case is different ) ? what if the rule is more than 3 ( i.e. small , medium , large , ex - large ) ? what i understands if the rule is only 2 i can use this formula small[x]=(max - x)/(max - min ) large[x]=(x - min)/(max - min ) my current approach to this problem is as follow : small[x]=1 ; x&lt;=2 medium[x]=(max - x)/(max - min ) ; 2 & lt ; x & lt ; 7 large[x]=0 ; x>=7 is this correct ? also can you refer me to some source to study this ? as i mentioned before i can only find some implementation and basic explanation , it 's either there is no online source for this or i do n't know what to search for . sorry if it 's hard to understand i can edit and post the whole problem if you want , thanks in advance . extra question : what is the name of algorithm which can be used to solve the crossing bridge puzzle(the one with timer , max person , and stuff)?i forgot the name .",16039,1671,2018-06-04T20:26:43.323,2019-05-02T10:02:00.360,defining formula for fuzzy equation,getting-started math fuzzy-logic,1,2,
1698,6642,1,6645,2018-06-04T10:40:05.463,1,37,"i have this following natural language statement : "" there is only one house in area1 the size of which is less than 200 m² . "" which is mistranslated to fol : ∃x.(house(x ) ∧ in(x , area1 ) ∧ ∀y.(house(y ) ∧ in(y , area1 ) ∧ size(y ) & lt ; 200 -&gt ; x = y ) ) this translation is wrong according to my lecturer , because it is not necessary that the size of x must be less than 200 . the statement is true if there only houses which are bigger . i have two questions : i do n't get the fol translation at all and do n't see where the uniqueness part is expressed : so translated it back : "" if all houses in area1 have a size less then 200 m² then there exists one house which equals to all houses ? ? "" why is not necessary that the size of x is less than 200 , when it clearly says in the statement above that must exist one house with a size less then 200 ?",15391,2193,2018-06-04T19:35:46.113,2018-06-04T19:35:46.113,how is uniqueness quantification translated in first oder logic,logic,1,1,
1699,6643,1,6653,2018-06-04T11:17:59.383,2,219,"i 'm doing a project for my last university examination but i 'm having some troubles ! i 'm making an expert system who should be able to assemble a computer after asking some questions to the user . it works but according to my teacher i need to define more rules , could you give me some suggestions please ? i have facts like these : processor(p , proc_price , price_range ) , motherboard(m , motherboard_price , price_range ) , ram(r , ram_price , price_range ) , case(c , case_price , price_range ) , ali(a , ali_price , price_range ) , video_card(v , vga_price , price_range ) , ssd(s , ssd_price , price_range ) , monitor(d , monitor_price , price_range ) , hdd(h , hdd_price , price_range ) . i ask these questions to the user : 1 ) choose the price range 2 ) choose the display size 3 ) choose hard disk size then i ask 3 questions about computer utilization to define the user : 1 ) do you surf on internet ? 2 ) do you play ? 3 ) do you use editing programs ? use(gaming ) : - ask(""do you play games ? ( y / n ) "" ) . use(editing ) : - ask(""do you use editing programs ? ( y / n ) "" ) . use(surfing ) : - ask(""do you surf internet?(y / n ) "" ) . user(base ) : - use(surfing ) , \+ use(gaming ) , \+ use(editing ) . user(gamer ) : - use(gaming ) , use(surfing ) , \+ use(editing ) . user(professional ) : - use(editing ) , \+ use(gaming ) , use(surfing ) . i should make more questions about user definition to make user definition more complex too and add some rules . please help me , i 'm desperate",16049,1671,2018-06-04T21:25:31.737,2018-06-04T21:25:31.737,defining rules for an expert system,prolog expert-system r,2,0,
1700,6644,1,6647,2018-06-04T12:21:33.773,5,197,"we are doing a research design project on autonomous vehicles and have some questions on av levels 4/5 ; specifically on the roles , impacts and consequences of av on society , government , users and other stakeholders . we 're currently stuck on this main question : q : what functionally , does control look like in av levels 4 and 5 ? for example , is the whole purpose of a level 4/5 that a user has no input into the control ? could a driver in av ( level 5 ) stop in an emergency , or say they want to "" take corners harder , speed up , slow down "" ? could i choose to change the equi - distance between my av and the others around me because i like space ? we 're wondering about what functionally , does av level 4/5 offer a user ; and what it looks like ? context : our remit is within the world of design ( design thinking ) , not specifically technology , or expert system functionality . we 're looking at the issue from a design perspective ; who does it impact , who are the stakeholders , what are the consequences and impacts . what role does a driver have an in level 5 ? could an auto - manufacturer want to give drivers control in level 5 ? how do emergency services act in these situations ? what are the touchpoints to society and whom does it impact and what does it say about the design of av for the future of society .",16051,4302,2018-09-20T05:51:43.987,2018-12-14T03:36:31.080,"what functionality , does control look like in autonomous vehicles levels 4 and 5 ?",ai-design self-driving social automation autonomous-vehicles,2,0,1
1701,6658,1,,2018-06-05T11:01:24.343,3,116,"what are the required characteristics of an activation function ( in a neural network ) ? which functions can be activation functions ? for example , which of the functions below can be used as an activation function ? $ $ f(x ) = \frac{2}{\pi } \tan^{-1}(x)$$ which looks like or $ $ f(x ) = \frac{1}{\sqrt{2\pi } } \int_{-\infty}^{x } e^{-\frac{t^2}{2 } } dt$$ which looks",16070,2444,2019-05-09T21:33:50.873,2019-05-15T21:05:55.460,which functions can be activation functions ?,neural-networks machine-learning math activation-function,1,1,
1702,6660,1,,2018-06-05T11:32:45.177,0,46,"an agent aims to find a path on a hexagonal map with initial state s0 in the center and goal state s⋆ at the bottom as depicted below . the map is parametrized by the distance n ≥ 1 from s0 to any of the border cells ( n = 3 in the depicted example ) . the agent can move from its current cell to any of the 6 adjacent cells , how can we find the number of node expansions performed by bfs without duplicate detection , and with duplicated detection as a function of n. i know that the branching factor for the map would be 6 because the agent can move in 6 directions , and for a depth of k , we get o(b^k ) = 6^n without duplicate detection , but what is the number of node expansion with duplicate detection with bfs .",16071,,,2018-06-05T14:18:37.150,number of node expansions performed by breadth first search,algorithm ai-basics,1,0,
1703,6662,1,,2018-06-05T15:21:05.840,3,63,"i know some people will try to crucify me for asking such question here , since it 's too generic , but ill give it a shot . so basically what i 'm after in trying to find if there any tools exist ( simply white papers are welcome too ) for creating chatbots based on a plain text corpus , so that questions could be asked about it . that is , there is some textual data ( like articles ) , but there are no question , answer pairs to train a conventional model with , like seq2seq . i understand that there are ways to achieve this by using tools to extract intents , entities from a question and match it with paragraphs , articles ( based on indexes and topics ) , but to my understanding this creates more of a nlp search bot , rather than a chatbot , which provides answers in conversation - like manner . thanks in advance .",16074,,,2018-11-01T11:00:27.617,"what existing tools are out there for creating chatbots from a plain text corpus and without having question , answer pairs ?",machine-learning natural-language-processing chat-bots,1,1,
1704,6666,1,6687,2018-06-05T17:52:51.250,0,151,"i 'm currently a software developer , and i want to get into this field , i 'm interested particularly in self driving cars , which as far as i know , work with ai or machine learning ( which i still do n't know the difference if there is any ) , any suggestions on how to start on this field , i 'm very young and i want to build a career in this area given my software development background",16078,15465,2018-06-11T13:37:03.680,2018-06-11T13:37:03.680,i want to learn about ai any suggestions ?,machine-learning ai-design,1,3,2
1705,6669,1,,2018-06-05T21:16:50.023,2,1064,"i trained a dqn that learns tictactoe by playing against itself with a reward of -1/0/+1 for a loss / draw / win . every 500 episodes i test the progress by letting it play some episodes ( also 500 ) against a random player . as shown in the picture the net learns quickly to get an average reward of 0.8 - 0.9 against the random player . but after 6000 episodes performance seems to deteriorate . if i play manually against the net after 10000 episodes it plays okay , but by no means perfect . assuming that there is no hidden programming bug , is there anything that might explain such a behavior ? is there anything special about self - play in contrast to training a net against a fixed environment ? here further details : the net has two layers with 100 and 50 nodes ( and a linear output layer with 9 nodes ) , uses dqn and a replay buffer with 4000 state transitions . the shown epsilon values are only used during self - play , during evaluation against the random player exploration is switched off . self - play actually works by training two separate nets of identical architecture . for simplicity one net is always player1 and the other always player2 ( so they learn slightly different things ) . evaluation is then done using the player1 net vs. a random player which generates moves for player2 . thanx",15958,15958,2018-06-06T16:04:06.960,2018-06-06T17:14:48.073,why does self - playing tictactoe not become perfect ?,reinforcement-learning self-play,1,9,2
1706,6670,1,,2018-06-05T22:48:29.290,2,73,"if an atari game 's rewards can be between $ -100 $ and $ 100 $ , when can we say an agent learned to play this game ? should it get the reward very close to $ 100 $ for each instance of the game ? or it is fine if it gets a low score ( say $ -100 $ ) at some instances ? in other words , if we plot the agent 's score versus number of episodes , how should the plot look like ? from this plot , when can we say the agent is not stable for this task ?",15967,2444,2019-02-13T02:36:18.813,2019-02-13T02:36:18.813,when can we say an rl algorithm learns an atari game ?,reinforcement-learning atari-games,0,2,1
1707,6672,1,,2018-06-06T07:53:29.787,2,66,"i am working on a task where i am required to automate the customer service request channel . the process is quite typical . a customer queries about a product via email , the person on the front channel checks emails , forwards it to the relevant department and then answer is provided . the problem is that customer query can be about one of hundreds of devices listed . each device has its own pdf documentation which is quite extensive . finding the right pdf and then finding the right section where information could be listed is really a tedious process and wastes a lot of time . sometimes the information is not even listed and answer has to be improvised by product specialist ( the last part hints me about reinforcement learning , what do you guys think ) . what i want to achieve is that this whole tedious and repetitive process is automated and may be if possible , the model learns over time as well . the task output is quite open ended as well . different approaches and models can be tried out ( like chatbots and etc ) . rapid failure is highly appreciated here . below mentioned are some more details : database : i have customers queries about devices in the form of emails . pdf documentation of devices . the documentations are quite extensive . i also happen to have some excel files where some sample queries and sample answers are listed . but since queries can be of very dynamic nature , it does n't seem like a classification problem ( to me at least ) . i have googled quite a lot about the topic but mostly what i get are topics like ' how ai will transform the customer service ' and then something more specific to nlp and a lot of company ads etc . so far what i have understood from online surfing is that possible approaches need to use nlp library ( nltk ) in python and do some topic modelling for documentation and for email . still how i approach the whole task is not clear to me . what i want from you guys is that maybe guide me how this task can be achieved step by step . i am not looking for any code ! just which methods can be used and how the problem can be approached . right now , i do n't know where to start and how to approach it .",16093,9947,2018-09-05T12:55:20.553,2018-09-05T17:20:03.403,implementing ai / ml in customer service,neural-networks machine-learning reinforcement-learning natural-language-processing python,1,0,
1708,6674,1,,2018-06-06T08:37:00.597,1,60,"i am working on a task that requires me to classify a large amount of mixed files on a backup drive ( more than 10 tb with more than 32 million files ) based on content . the included file types are documents , images , videos , executable , and pretty much everything in between . i am also required to create new tags or metadata that will allow for automatic classification of new files . it 'd also allow for manual input of category . for each input i give to the system , the system would learn and improve its classification . here is what i have come up with so far : documents : classify using existing categories with packages like nltk on python . alternatively , first run topic modeling using lda or nmf and then classify . images : use cnn . in case of unknown label , use vae to cluster the images . videos and other types of files : i do not know how to approach this . since i am not sure about my approach , any input is greatly appreciated .",16094,16094,2018-06-06T09:28:32.077,2018-06-06T09:28:32.077,how to implement ai / ml to classify various types of files,machine-learning classification,0,9,1
1709,6678,1,,2018-06-06T23:27:24.313,4,170,"furthermore , we observe that mode collapses traditionally plaguing gans tend to happen very quickly , over the course of a dozen minibatches . commonly they start when the discriminator overshoots , leading to exaggerated gradients , and an unhealthy competition follows where the signal magnitudes escalate in both networks . we propose a mechanism to stop the generator from participating in such escalation , overcoming the issue ( section 4.2 ) . what do they mean when the discriminator overshoots ? the discriminator gets good too quickly ? and signal magnitudes escalate in both networks ? my current intuition is that the discriminator gets too good too soon which causes the generator to spike and try to play catch up , that would be the unhealthy competition that they are talking about . model collapse is the side effect where the generator has trouble playing catch up and decides to play it safe by generating slightly vary images to increase its accuracy . is this way of interpreting the above paragraph correct ?",12242,,,2019-05-10T10:45:46.347,can some one help me understand this paragraph from nvidia 's progressive gan paper ?,deep-learning training generative-model,2,0,
1710,6679,1,,2018-06-07T07:44:54.023,-1,98,is there a compilation of ml / ai free online courses available for beginners ? can you folks share the link ?,16116,1671,2018-06-13T18:08:43.203,2018-06-13T18:08:43.203,beginners courses for ml / ai,machine-learning ai-basics getting-started,1,0,1
1711,6681,1,,2018-06-07T15:50:16.833,1,70,"disclaimer : i am a novice in the world of machine learning , so please excuse my ignorance . my dataset consists of things like age , days since last visit , etc . this information is medical related . none of which is geometrical , just data pertaining to particular clients . the goal is to classify my dataset into three labels . the dataset is not labeled , meaning i 'm dealing with an unsupervised learning problem . my dataset consists of ~20,000 records , but this will linearly increase overtime . the data is nearly all floats , with some being strings that can easily be converted into a float . using this cheat sheet for selecting a solution from the scikit site , a kmeans cluster seems like potential solution , but i 've been reading that having high dimensionality can render the kmeans cluster unhelpful . i 'm not married to a particular implementation either . i 've currently got a kmeans cluster implementation using tensorflow in python , but am open for alternatives . my question is : what would be some solutions for me to further explore that might be more optimal for my particular situation ?",2818,3773,2018-08-10T10:03:25.757,2018-08-10T10:03:25.757,classifying non - labeled data with high dimensionality,classification tensorflow getting-started unsupervised-learning software-evaluation,2,3,1
1712,6685,1,,2018-06-08T14:20:27.637,1,48,"i 'm building a decision tree and would like to separate ( for example ) the elements that are in class 0 from those in classes 1 and 2 , case in point : df = pd.dataframe(np.random.randn(500,2),columns=list('ab ' ) ) cdf = pd.dataframe(columns=['c ' ] ) cdf = pd.concat([cdf,pd.dataframe(np.random.randint(0,3 , size=500 ) , columns=['c ' ] ) ] ) # df = pd.concat([df , cdf ] , axis=1 ) ( x_train , x_test , y_train , y_test ) = train_test_split(df , cdf , test_size=0.30 ) y_train = y_train.astype('int ' ) classifier = decisiontreeclassifier(criterion='entropy',max_depth = 2 ) classifier.fit(x_train , y_train ) y_pred = classifier.predict(x_test ) c represents the class of an element , a and b are two variables that define the element , how can i build a tree that instead of dividing results into c=0 , c=1 or c=2 divides them into c=0 and c!=0 ?",12940,,,2018-06-08T15:12:33.963,"decision tree : more than 2 classes , how to represent elements that are in a class vs ones that are n't ?",classification decision-tree,1,0,
1713,6686,1,6688,2018-06-08T15:03:43.383,3,104,"i am reading a book about opencv , it speaks about some derivative of images like sobel . i am confused about image derivative ! what is derived from ? how can we derived from an image ? i know we consider an image(1-channel ) as a n*m matrix with 0 to 255 intensity numbers . how can we derive from this matrix ? edit : a piece of text of the book : derivatives and gradients one of the most basic and important convolutions is computing derivatives ( or approximations to them ) . there are many ways to do this , but only a few are well suited to a given situation . in general , the most common operator used to represent differentiation is the sobel derivative operator . sobel operators exist for any order of derivative as well as for mixed partial derivatives ( e.g. , ∂ 2 /∂x∂y ) .",9941,9941,2018-06-09T07:25:42.813,2018-06-18T02:50:23.553,"what does it mean "" derivative of an image "" ?",image-recognition,2,0,
1714,6696,1,6739,2018-06-08T19:48:10.853,1,133,"i 'm working with deep learning on some eeg data for classification , and i was wondering if there 's any systematic / mathematical way to define the architecture of the networks , in order to compare their performance fairly . should the comparison be at the level of neurons ( e.g. number of neurons in each layer ) , or at the level of weights ( e.g. number of parameters for training in each type of network ) , or maybe something else ? one idea that emerged was to construct one layer for the mlp for each corresponding convolutional layer , based on the number of neurons after the pooling and dropout layers . any ideas ? if there 's any relative work or paper regarding this problem i would be very grateful to know . thank you for your time konstantinos",16148,,,2018-06-13T18:03:30.690,how to make a fair comparison of a convolutional neural network ( cnn ) vs a mutlilayer perceptron ( mlp ) ?,neural-networks convolutional-neural-networks comparison,2,0,1
1715,6699,1,,2018-06-09T14:36:56.427,3,543,"i have been looking at fibonacci series , the golden ratio and its uses in nature , like how flowers and animals grow based on the series . i was wondering whether we could use the fibonacci series and the golden ratio in any way in ai , especially in evolutionary algorithms . any ideas or insights ? is this research material ? if so where can we start ?",15944,2444,2019-04-07T16:35:25.760,2019-04-08T15:36:51.133,ai applications of the fibonacci series,machine-learning math evolutionary-algorithms applications,3,1,
1716,6702,1,,2018-06-10T15:47:04.977,2,274,"this might seem an hypothetical question , but how can we define common sense in an ai agent . in our lives we meet different people and describe their common sense based on how they act on a situation . for example highly extrovert people are able to deal with people without any awkwardness . for them an action in how to deal with people come as common sense . but in case of scientists , approach to solving a problem maybe common sense which ordinary people can not see . so how do we define / tackle this common sense part of an ai agent ?",9947,,,2018-06-13T18:14:35.113,common sense in an ai,sentience sense,5,3,3
1717,6707,1,,2018-06-11T12:51:07.997,1,80,"i would like to develop a machine learning algorithm , given two photos , that can decide which image is more "" artistic "" . i am thinking about somehow combining two images , giving it to a cnn , and get an output 0 ( the first image is better ) or 1 ( the second image is better ) . do you think this is a valid approach ? or could you suggest an alternative way for this ? also , i do n't know how to combine two images . thanks ! edit : let me correct "" artistic "" as "" artistic according to me "" , but it does n't matter , i am more interested in the architecture . you can even replace "" artistic "" with something objective . let 's say i would like to determine which photo belongs to a more hotter day .",16201,1671,2018-06-13T17:04:48.873,2018-06-13T17:22:17.407,which photo is more artistic ?,convolutional-neural-networks image-recognition art-aesthetics,3,2,1
1718,6711,1,,2018-06-11T16:55:24.813,2,40,"what is the state of the art in models of how the human brain performs goal - directed decision making ? can these models ’ principles and insights be applied to the field of artificial intelligence , e.g. to develop more robust and general ai algorithms ?",16210,16210,2018-06-11T17:04:28.903,2018-06-11T19:28:33.630,what is the state of the art in models of how the human brain performs goal - directed decision making ? can these models ' principles be applied to ai ?,models decision-theory cognitive-science brain,1,0,
1719,6715,1,,2018-06-11T19:55:35.770,6,171,"a few months ago i made a simple game that is similar to the dinosaur game in google chrome - you jump over obstacles , or do n't jump over levitating obstacles , and jump to collect bitcoins , which can be placed at 5 different heights . i used a very lightweight nn written by nyu professor dan shiffman , and within a few days the game and ai were done , starting off with a population of 200 jumpers , and a genetic algorithm ( fitness function ( points are given for avoiding obstacles and gathering bitcoins ) and mutation ) , and it worked as it should . however , this was only when the bitcoins and obstacles were not near each other , which i 've been struggling with ever since . so , i made a "" training ground "" where i put first a levitating obstacle , then a grounded one , and then a bitcoin after it , and then a bitcoin above a fourth grounded obstacle , and no matter how many times and how long i 'd leave it to train , i 'd always end up with identical behavior : the first 3 obstacles are properly avoided , the first bitcoin is collected , and then jumpers would jump too early , land before the fourth "" bitcoin "" obstacle , and jump again , always crashing at almost the same place ( across all generations , so even if i 'd restart the training , they crash at the same place in the obstacle , with a deviation of a few pixels up or down ) . i added multilayer support to the nn , no improvements . today i replaced the nn with tensorflow.js , and i am getting identical behaviour . my inputs are : distance to next obstacle altitude of next obstacle distance to next star ( for simplicity i removed the altitude of stars from the input , and keep them at a constant altitude ) i have 2 hidden layers ( 5 and 6 neurons ) , and 1 neuron in the output , which determines if the jumper should jump . my only idea is that a neuron that decides when to jump because of the obstacle activates alongside the neuron that decides when to jump because of the bitcoin , their weights are summed up and a decision to jump too early is made . i 'll give somewhat of a ( maybe bad ) analogy : if it takes you 1 month to prepare an exam , then , if you have 2 exams on the same day , you will start preparing them 2 months earlier . that logic works in this case , but not in my ai . in the initial "" toy neural network "" i even added 8 layers of 12 neurons each , which i think is overkill for this case . in tf.js i used both sigmoid and relu activation functions . no matter what i did , no improvement . hope someone has an idea where i 'm going wrong .",16214,16214,2018-06-11T20:25:43.113,2019-04-13T17:02:30.510,issue with simple game ai,reinforcement-learning tensorflow game-ai,1,3,1
1720,6721,1,,2018-06-12T14:02:27.770,2,229,"in a reinforcement learning model , states depend on the previous actions chosen . in the case in which some of the states -but not all- are fully independent of the actions -but still obviously determine the optimal actions- , how could we take these state variables into account ? if the problem was a multiarmed bandit problem ( where none of the actions influence the states ) , the solution would be a contextual multiarmed bandit problem . though , if we need a "" contextual reinforcement learning problem "" , how can we approach it ? i can think of separating a continuous context into steps , and creating a reinforcement learning model for each of these steps . then , is there any solution where these multiple rl models are used together , where each model is used for prediction and feedback proportionally to the closeness between the actual context and the context assigned to the rl model ? is this even a good approach ?",6114,,,2018-06-12T18:40:31.970,how to implement a contextual reinforcement learning model ?,reinforcement-learning,1,0,1
1721,6724,1,6729,2018-06-12T18:45:20.077,1,4744,"i would like to know if there is a complete text classification with deep learning example , from text file , csv , or other format , to classified output text file , csv , or other . i have seen tens of tutorials and they mostly focus on the model and its performance , but i have not been able to find one that shows how to apply the model to a set of text strings and how to output a document with the classified(labeled ) text .",16230,,,2018-06-13T08:19:56.527,complete deep learning text classification with python example,deep-learning python reference-request,2,1,2
1722,6728,1,,2018-06-13T04:35:55.150,1,68,"what are the top contributions from neuroscience to artificial intelligence and vice versa ? how much progress has been made from the interaction between these two fields ? is there a formal field dedicated to the research of topics in the intersection between ai and neuroscience , with papers being published and maybe conferences being held periodically ?",16210,1671,2018-06-13T16:58:10.377,2018-06-16T11:10:40.890,what are the top contributions from neuroscience to ai and viceversa ? how much progress has been made from the interaction between these two fields ?,reference-request cognitive-science brain academia,3,0,
1723,6741,1,6743,2018-06-13T18:13:46.330,4,112,"currently in my country , there is a system in which certain groups of researchers upload information on products of scientific interest , such as research articles , books , patents , software , among others . depending on the number of products , the system assigns a classification to each group , which can be a1 , a , b and c , where a1 is the highest classification and c is the minimum . according to the classification of the groups , they can compete to receive monetary incentives to make their research . at the momen , i am working in an application that takes the data of the system that i mentioned previously . i am able to say what classification the group currently has because we develop a scraper that counts the products and there is another service that is in charge of implementing all the mathematical model that the system has to calculate the category of the group . but what i want to achieve is that my application would be able to give an estimate of how many products a research group should have to improve its category and i want to know if i can do that using neural networks . for example , if there is a category c group , i want the application to tell the user with how many articles and books it would make its category go up to b. from what i have seen in some web resources , i could insert a training set into the neural network and have it learn to classify the groups , but i think it is unnecessary , because i can do that mathematically . but i do not understand if it is possible for a neural network to process the current category that the group has and be able to give suggestions of how many products it needs to improve its category . i think it must be a neural network with several outputs , so that in each one it throws the total for each one of the products , although it is not necessary to list all the products that the measurement model contemplates . but it is necessary for the network to learn which products are handled by a certain group , for example if a group does not write books , avoid suggestions that contemplate the production of books for the improvement of the category that the group has . if you read the whole problem thanks in advance , i would appreciate any suggestion of what kind of subject i must research or any tutorial . i must say that i am a totally newbie in this subject .",16258,1671,2018-06-13T18:21:44.983,2018-06-13T18:31:42.207,"regression with more than one output , neural network",neural-networks deep-learning deep-network ai-basics getting-started,1,1,1
1724,6745,1,,2018-06-14T01:52:37.993,1,125,"i have a question about output of my som network . i have trained my network with diffrent size , learning rate and epochs , but my output always can recognise two big clusters . iris - setosa and iris - virginica , both classes has fitted well in two bmus . but output of iris - versicolor is very different . it contain some other classes bmu but this is not a big problem . i just want to know is this good or i have some type of bug ? setosa 0 . 1846 1 . 1846 2 . 1846 3 . 1846 4 . 1846 5 . 1846 6 . 1846 7 . 1846 8 . 1846 9 . 1846 10 . 1846 11 . 1846 12 . 1846 13 . 1846 14 . 1846 15 . 1846 16 . 1846 17 . 1846 18 . 1846 19 . 1846 20 . 1846 21 . 1846 22 . 1846 23 . 1846 24 . 1846 25 . 1846 26 . 1846 27 . 1846 28 . 1846 29 . 1846 30 . 1846 31 . 1846 32 . 1846 33 . 1846 34 . 1846 35 . 1846 36 . 1846 37 . 1846 38 . 1846 39 . 1846 40 . 1846 41 . 1620 42 . 1846 43 . 1846 44 . 1846 45 . 1846 46 . 1846 47 . 1846 48 . 1846 49 . 1846 versicolor 50 . 652 51 . 652 52 . 652 53 . 1259 54 . 696 55 . 1394 56 . 652 57 . 490 58 . 696 59 . 490 60 . 490 61 . 1059 62 . 1304 63 . 696 64 . 490 65 . 652 66 . 1400 67 . 490 68 . 696 69 . 490 70 . 652 71 . 1574 72 . 696 73 . 832 74 . 696 75 . 696 76 . 696 77 . 652 78 . 696 79 . 490 80 . 490 81 . 490 82 . 444 83 . 696 84 . 1129 85 . 1084 86 . 652 87 . 696 88 . 25 89 . 584 90 . 490 91 . 789 92 . 1034 93 . 490 94 . 854 95 . 29 96 . 584 97 . 877 98 . 490 99 . 809 virginica 100 . 652 101 . 696 102 . 652 103 . 652 104 . 652 105 . 652 106 . 877 107 . 652 108 . 696 109 . 652 110 . 652 111 . 696 112 . 652 113 . 696 114 . 652 115 . 652 116 . 652 117 . 652 118 . 652 119 . 696 120 . 652 121 . 652 122 . 652 123 . 696 124 . 652 125 . 652 126 . 696 127 . 652 128 . 652 129 . 652 130 . 652 131 . 652 132 . 652 133 . 696 134 . 696 135 . 652 136 . 652 137 . 652 138 . 652 139 . 652 140 . 652 141 . 652 142 . 696 143 . 652 144 . 652 145 . 652 146 . 696 147 . 652 148 . 652 149 . 652 thanks in advance . ok now i have diagram it looks not bad .",15587,15587,2018-06-14T23:01:17.337,2019-04-12T14:01:10.507,kohonen clustering of flowers,neural-networks unsupervised-learning som,1,0,
1725,6748,1,,2018-06-14T02:44:04.580,5,318,"i 'm a professional game developer investigating the potential for using reinforcement learning to build strategy game ai opponents that have more creative behavior compared to traditional techniques like behavior trees . i have a few questions i 've bolded below , any thoughts would be helpful , and could save me from pursuing dead ends . i created a very boring and tiny game as a test case . two players each control a fleet of ships , each ship has health and can fire on one other ship each turn dealing some damage . the player and his opponent assigns orders to their respective ships , telling them which target to attack , and then the turn is resolved . ships with 0 health are removed from the board . the player that loses all their ships first loses the game . assuming i was using tensorflow , at a very high level i need to : a ) create a training program that outputs a trained graph to a file . the training program will need to map gamestates into tensors , feed the tensors through the graph to produce actions , execute actions on the gamestate to generate a new state , and evaluate the reward function for the new state . repeat a bunch . b ) take the graph created in # 1 , load it at the game runtime , and use the graph to generate intelligent actions from real gamestates during the player vs ai match . as soon as i started digging into tensorflow , questions immediately came up , and now i 'm not quite sure if there is a more appropriate library to do this . i ) tensorflow has a high level python api , and a low level c++ api . most games are built in c++ , and thus using a c++ or c api is preferable , it makes integration with the game much simpler . in principle we could use pybind or some other scheme for sending state from c++ to python and back again , but that 's not ideal . question 1 : how much do i lose by using the low level api specifically for reinforcement learning , compared to the high level api ? ii ) platforms . 99.9 % of the time , pc / console games are developed in windows environments , and so having tensorflow work in windows is critical . from my googling , tensorflow just barely supports building in windows using cmake , though it requires some finagling . more worryingly are other platforms : question 2 : what hope is there of running the tensorflow library on consoles like xboxone , playstation 4 , or the switch ? i imagine this would require manually porting the entire source :( iii ) tensorflow is big , and it seems you need to basically link all of it to ship with the game . question 3 : is there any way to get a slimmed down "" runtime tensorflow "" library that is only capable of loading a graph and transforming states into actions ? it seems like if the answer was yes , it might also be easier to port this smaller runtime version to more platforms . question 4 : should i even be using tensorflow for this ? is there perhaps something more suitable ? thanks again if you read all that , i 'm eager to start tinkering , but would like to set off in the right direction .",16265,1671,2018-06-14T21:23:18.807,2018-12-12T08:39:33.147,reinforcement learning in commercial strategy games,reinforcement-learning tensorflow game-ai getting-started gaming,1,2,
1726,6749,1,,2018-06-14T03:23:29.543,1,52,how did you implement your first ever model from a deep learning paper ? please share your experience . it will be useful for beginners looking into getting into implementing code for papers . a step by step process will be beneficial .,16142,1671,2018-06-14T21:55:12.027,2018-06-14T21:55:12.027,implementing deep learning papers for the beiginners,python reference-request,0,2,0
1727,6750,1,6752,2018-06-14T06:23:57.040,3,49,"as i know , if we consider a 3 * 3 kernel , we should add a padding of 1px to the source image(if we want to have effect on whole of the image ) , then we start to put the kernel in upper - left side of the image and multiplying each element of kernel to corresponding pixel on image . then we sum all the results and put it on the anchor point of kernel(usually center element ) . then we should shift the kernel one step to right side and do these things again . if i am right till here , i have a question about the summation results . i want to know : should we consider the replaced value of image in previously calculated summation and replaced in anchor point in new step of calculation or not ? i mean we must put the anchor point 's result in source image and consider it in calculations of shifted kernel ? or we must put it in distance image and we do n't consider these results when we shift the kernel on source image(it means do n't replace the results on source image for next steps calculations ) ?",9941,,,2018-06-14T07:01:03.110,how to apply a kernel to an image ?,image-recognition,1,0,
1728,6751,1,,2018-06-14T06:37:19.750,2,57,"in a paper about the jack virtual agent architecture the automated wingman - using jack intelligent agents for unmanned autonomous vehicles i 've found on page 6 a multi - agent bdi system . it is not the description itself , but only the screenshot of the gui . on top of the gui there are buttons for starting the agent , and in the main window is the current plan visible . from the history of ai this is sometimes called a blackboard architecture , because many agents can work in parallel on the plan to improve it . to understand the concept of bdi agents and blackboard systems better , my question is : which elements should a gui for a bdi - multiagent system have ? are high - quality examples available ?",11571,,,2018-06-14T06:37:19.750,gui for a blackboard system,multi-agent-systems,0,0,
1729,6753,1,6754,2018-06-14T09:14:09.180,2,35,seems older rnns have a limitation for their use cases and have been outperformed by other architectures for specific tasks e.g grus and cnns,16255,,,2018-06-14T09:15:34.727,why are gru and lstm better than old types of rnn ?,ai-design recurrent-neural-networks,1,0,
1730,6756,1,6759,2018-06-14T11:29:37.730,2,143,"i watched a youtube clip of elon musk talking about his view on the future of ai . he gave two examples . one of the examples was a benign scenario and the other example was a non benign scenario where he speculated the possibilities of future ai threats and what harm a deep intelligence could do . according to elon , a deep intelligence in the network could create fake news and spoof email accounts . "" the pen is mightier than the sword "" . this non benign scenario put forth by elon was a hypothetical , but he went into detail about how it could have been possible that an ai , with the goal of maximising the portfolio of stocks , to go long on defence and short on consumer , and start a war . to be more specific , this could be achieved by hacking into the malaysians airlines aircraft routing server , and when the aircraft is over a warzone , send an anonymous tip that there is an enemy aircraft flying overhead which in turn would cause ground to air missiles to take down what was actually a "" commercial "" airliner . although this is a plausible hypothetical non benign scenario of ai , i 'm wondering if this actually could have been the case regarding the malaysian airliner crash . the stuxnet , for example was a malicious computer worm , first uncovered in 2010 . thought to have been in development since at least 2005 and believed to be responsible for causing substantial damage to iran 's nuclear program . the stuxnet was n't even an ai .... the stuxnet blew the worlds minds when it was discovered . the shear complexity of the worm and the amount of time it took to build was impressive to say the least . in conclusion , who would agree that the malaysian airliner was a non benign scenario of a deep intelligence in the network ?",13252,13252,2018-06-15T00:14:20.213,2018-06-15T21:17:34.463,"elon musk 's comment on "" non - benign ai scenarios """,deep-learning ai-design deep-network strong-ai,1,12,1
1731,6757,1,,2018-06-14T21:22:41.780,3,107,if both the players want to increase their score ( by selecting the highest / best cost path ) can this be done using minimax algorithm and not other algorithms ?,16280,1671,2018-06-14T21:32:36.637,2018-10-13T00:00:22.463,minimax with only max,ai-design search game-theory multi-agent-systems minimax,1,1,
1732,6760,1,,2018-06-14T23:19:24.140,1,77,"i would really appreciate if someone could comment the following method of training neural nets providing them with some meta data ( making them more color prone only if needed , whereas now they 're mostly silhouette / outline aware ) . ( comment and especially giving a reference to some papers , preventing me from reinventing the wheel . ) but let 's start at the very begining and say that we 're performing simple image recognition and each image has 24 bit color depth , simply 1 byte per each rgb channel . i 'm more eager to use usually bigger pics , sacrifing color quality , however not in all cases ( that statement is crucial in this question ) . to limit the computational burden i 'm not keen on using the full information about color ( 3 bytes per pixel ) , but rather shrink it to 1 byte per pixel and here is the catch : i 'm reluctant neither to use gray scale nor to cast original tints to single ( common among all pics ) color palette of 256 hues . so i came up with an idea of reversing the method called debayering or demosaicing image from color filter array data ) . to achive it , for every pixel only one color channel is preserved . because of the human perception of colors , green component is overrepresented covering 50 % of pixels , so 25 % left for blue and red . in this particular example below , the upper left pixel correspond to blue , followed by green , then blue , next green and so on to the end of the row . the second uppermost row starts with green , afterwards red , one more green , red and also repeating till the end of line . this horizontal patterns are altering with the parity of the row number , which is nicely depiced on https://en.wikipedia.org/wiki/bayer_filter from where i 've got following graphics : to better illustrate this method i 'm using the thumbnail of famous mona lisa painting with its grayscale version next to it . ( by the way , it is n't in my training set , but is familiar to everyone ) . a greenish leftmost image is the result of applying the reverse debayering / demosaicing cfa method . this picture consist of pixels that are either blue or red or green with different brightness level . in the browser window that could be poorly visible , however if you download this image and magnify it substantially , the patter would be revealed . let 's say that in the original picture one can find a small square of 2x2 pixels , all of them representing a light skintone 0xf4d374 ( in hex ) . in this grouping , 2 green pixels would be chopped into green channel and will get a value of 0xd3 , the blue - related will get a value of 0x74 and the remaining red would get 0xf4 . in the leftmost image below the corresponding pixels were presented by hex colors : 0x00d300 , 0x000074 and 0xf40000 respectively , whereas in the right picture exactly the same values ( 0xd3 , 0x74 , 0xf4 ) were shown in grayscal ( of 256 possible shades ) . after this color - flattening , our input batch has shrinked by two - thirds and at the same time original colors can be more - or - less restored ( of course not lossless , but well enough ) . however , i do n't suppose that anyone had a problem with recognising this picture after transformation . likewise , all my models could be well trained to recognize outline / silhouette of the object , but they require a way lot more training data ( at least one - two orders of magnitude ) to be color - aware . the ultimate question is , how to design models that would threat shape and colors in similar manner . maybe that would be not in 100 % mathematically proper , but shape and color must be orthogonal . nevertheless , i do n't want to always decode the color , but only if its needed - in ealier epoch it learned sillhouettes / shapes and that there 're many similar objects in this regard , so in the next epoch it should pay also / more attention to tints . have you encountered articles about such method of using color if object demarcation / labelling process can not be based only on shape ? i would be really gratefull for any paper or other reference . i 'm rather newbie to neural nets , so sorry if this is something widely known to everyone but me ;-) thanks in advance for any hint .",2629,,,2018-06-14T23:19:24.140,object recognition by two or more traits that are orthogonal ( informally speaking ),neural-networks convolutional-neural-networks image-recognition training object-recognition,0,0,1
1733,6762,1,6764,2018-06-15T09:11:56.317,2,182,"in some implementations of off - policy q learning we need to know the action probabilities given by the behavior policy mu(a ) ( e.g. , if we want to use importance sampling ) . in my case , i am using deep q - learning and selecting actions using thompson sampling . i implemented this following the approach in "" what my deep model does n't know ... "" : i added dropout to my q - network and select actions by performing a single stochastic forward pass through the q - network ( i.e. , with dropout enabled ) and choosing the action with the highest q - value . so , how can i calculate mu(a ) when using thompson sampling based on dropout ?",16286,1671,2018-06-15T21:11:02.510,2018-06-15T21:11:02.510,action probability with thompson sampling in deep reinforcement learning,deep-learning reinforcement-learning q-learning dropout,1,0,
1734,6765,1,,2018-06-15T11:40:24.003,11,297,"i read judea pearl 's the book of why , in which he mentions that deep learning is just a glorified curve fitting technology , and will not be able to produce human - like intelligence . from his book there is this diagram that illustrates the three levels of cognitive abilities : the idea is that the "" intelligence "" produced by current deep learning technology is only at the level of association . thus the ai is nowhere near the level of asking questions like "" how can i make y happen "" ( intervention ) and "" what if i have acted differently , will x still occur ? "" ( counterfactuals ) , and it 's highly unlikely that curve fitting techniques can ever bring us closer to a higher level of cognitive ability . i found his argument persuasive on an intuitive level , but i 'm unable to find any physical or mathematical laws that can either bolster or cast doubt on this argument . so , is there any scientific / physical / chemical / biological / mathematical argument that prevents deep learning from ever producing strong ai ( human - like intelligence ) ?",16291,1671,2018-06-15T21:10:22.287,2019-05-16T15:56:05.847,is there any scientific / mathematical argument that prevents deep learning from ever producing strong ai ?,deep-learning strong-ai math theory superintelligence,2,5,5
1735,6766,1,,2018-06-15T11:56:04.230,3,1179,"as you know opencv is a big great open - source library for image recognition and machine vision(and may further purposes like computer graphics , etc ) . is there similar library in sound field(voice recognition/ nlp(natural language processing ) ) ? i know espeak for tts , also pocketsphinx for voice recognition . also there is something like chatscript that i do n't know if i can consider as a nlp engine or not ? but i like to know did i mentioned the best libraries for each part of sound / voice field or there are better options to learn and work with them ? also will happy to hear some suggestions about best book(s ) to read to learn the concepts / algorithms of asr / nlp .",9941,9941,2018-06-16T09:09:11.937,2018-10-10T05:21:59.100,is there something like opencv for voice recognition & nlp ?,natural-language-processing reference-request software-evaluation voice-recognition c++,4,3,2
1736,6770,1,,2018-06-15T14:32:40.510,7,195,"i 'm trying to detect the visual attention area in a given image and crop the image into that area . for instance , given an image of any size and a rectangle of say lxw dimension as an input , i would like to crop the image to the most important visual attention area . i 'm looking for a state of the art approach for that . do we have any tools or sdk to implement that ? any piece of code or algorithm would really help .",9053,,,2019-05-07T04:06:56.330,detect visual attention area in an image,machine-learning image-recognition algorithm computer-vision pattern-recognition,3,3,2
1737,6772,1,,2018-06-16T08:29:49.487,2,153,"according to this news , microsoft is using ai to make windows 10 updates smoother . so i was curious and went further to search and came across this website , which describes : artificial intelligence ( ai ) continues to be a key area of investment for microsoft , and we ’re pleased to announce that for the first time we ’ve leveraged ai at scale to greatly improve the quality and reliability of the windows 10 april 2018 update rollout . our ai approach intelligently selects devices that our feedback data indicate would have a great update experience and offers the april 2018 update to these devices first . as our rollout progresses , we continuously collect update experience data and retrain our models to learn which devices will have a positive update experience , and where we may need to wait until we have higher confidence in a great experience . our overall rollout objective is for a safe and reliable update , which means we only go as fast as is safe . our ai / machine learning approach started with a pilot program during the windows 10 fall creators update rollout . we studied characteristics of devices that data indicated had a great update experience and trained our model to spot and target those devices . in our limited trial during the fall creators update rollout , we consistently saw a higher rate of positive update experiences for devices identified using the ai model , with fewer rollbacks , uninstalls , reliability issues , and negative user feedback . for the april 2018 update rollout , we substantially expanded the scale of ai by developing a robust ai machine learning model to teach the system how to identify the best target devices based on our extensive listening systems . to me , it sounds like simple if - else statements would have implemented the whole thing without touching the ai ; they mentioned that positive experiences include fewer rollbacks , uninstalls , and so on , so we may use these as a criterion of a positive experience . i am just wondering if the word ' ai ' is being misused , or can be misleading in this context ? could anyone point me out on this or give any insight on how ai can be used in this context ? in my experience , i have only seen ai mostly being used in speech recognition , image recognition and other sort - of classifying problems , with a training and consequently a computer can "" learn "" from the data , not like an if - else statement . today , ai seems to be everything that is considered "" smart "" ?",16300,16300,2018-06-16T10:26:21.780,2018-08-03T11:43:44.880,how does microsoft use ai to make windows 10 updates smoother,machine-learning definitions,1,2,
1738,6776,1,,2018-06-16T21:03:09.043,5,175,"i understand how neural networks work and have studied its theory well . my question is at the intricacies of deep neural networks and perhaps is a bit beyond common understanding ( as i have been told ( or misled ) from discussions ) . my question is : on the whole , is there a clear understanding of how mutation occurs within a neural network from the input layer to the output layer , for both supervised and unsupervised cases ? any neural network is a set of neurons + the connection weights . with each successive layer , there is a change in the input . say i have a neural network that does movie recommendation on n parameters . say if x is a parameter that stands for the movie rating on imdb . in each successive stage , there is a mutation of input x to x ' and further x ' ' and so on . while of course , we know how to mathematically talk about x ' and x ' ' , do we at all have a conceptual understanding as to what this variable is in its corresponding neural n - dimension ? the neural weights which to the human eye might be set of random numbers but may mean something profound if we could ever understand what the neural weights ' represent ' . what is the nature of neural weights such that despite decades worth of research and use , there is no clear understanding of what these connection weights represent ? or rather , why has there been so little effort in understanding the nature of neural weights , in a non - mathematical sense given the huge impetus in going beyond the black box notion of ai .",16308,1671,2018-06-18T17:23:11.290,2018-06-20T10:48:37.020,what do neural connection weights represent ' conceptually ' ?,neural-networks deep-learning deep-network artificial-neuron concepts,2,2,2
1739,6778,1,6820,2018-06-17T09:33:25.380,2,148,"look at breakout : we know that the underlying world behaves like an mdp , because for the evolution of the system it just need to know which is the current state , i.e. position , speed and speed direction of the ball , positions of the bricks and the paddle etc . but considering only single frames as the state space we have a pomdp because we lack informations about the dynamics [ 1 ] , [ 2]. question : what could happen if we wrongly assume that the pomdp is a mdp and do reinforcement learning with this assumption over the mdp ? obviously the question is more general , not limited to breakout and atari games . [ 1 ] https://arxiv.org/pdf/1507.06527.pdf [ 2 ] https://storage.googleapis.com/deepmind-media/dqn/dqnnaturepaper.pdf",15517,15517,2018-06-19T19:50:16.917,2018-06-20T10:48:14.097,reinforcement learning over an mdp that is actually a pomdp,reinforcement-learning theory probabilistic,1,0,
1740,6779,1,6786,2018-06-18T07:08:56.177,1,75,"the german research initiative “ sonderforschungsbereich 588 ” was active from 2001 - 2012 and has developed a humanoid robot , called armar . according to the old pictures , the first version armar i was not very highly developed , while the latest iteration ( armar iv ) was a near humanoid robot with arms , legs and a software defined cognitive architecture . how many researchers were involved in the project ? thousands of programmers , researchers and external companies ? no , according to the fact sheet the total number of employees was only 30 ( thirty ) . my question is : if it is possible to build and program a humanoid kitchen robot with such a small team , what would be possible with many more researchers ?",11571,4302,2018-10-20T18:09:54.923,2018-10-20T18:09:54.923,how many researchers are needed to develop a humanoid robot ?,research robots consumer-product ai-development,1,5,
1741,6781,1,6787,2018-06-18T07:43:06.257,0,136,"oxford philosopher and leading ai thinker nick bostrom defines superintelligence as "" an intellect that is much smarter than the best human brains in practically every field , including scientific creativity , general wisdom and social skills . "" here is an interesting article , you may like tech crunch .. artificial general intelligence : wiki taking into account current limitations and the amount of progress that has been made in recent years , what is a realistic timeframe to expect an ai that has human levels of cognition ?",16322,16322,2018-06-21T08:56:26.767,2018-06-21T08:56:26.767,"is really "" ai "" light years away from achieving cognitive ability of human ?",strong-ai agi human-like superintelligence mythology-of-ai,1,5,1
1742,6783,1,6785,2018-06-18T09:49:06.427,1,51,"i am currently exploring multi - agent reinforcement learning . i have multiple agents that communicate with each other and a central service that maintains the environment state . the central service dispatches some information at regular intervals to all the agents ( lets call this information as energy ) . the information can be very different for all the agents . the agents on reception of this information select a particular action . the execution of the action should leave the agent as well as the environment in a positive state . the action requires a limited amount of energy which might change on every timestep . if a agent does not have sufficient energy to it may request for energy from other agents . the other agents may grant or deny this request . if all the agents are able to successfully perform their actions and leave the environment in a positive state they get a positive reward . as the environment is stochastic , where a agent 's behavior is dependent on another agent can approximate q learning be used here ?",11584,,,2018-06-18T14:59:10.010,can q - learning working in a multi agent environment where every agent learns a behaviour independently ?,reinforcement-learning q-learning multi-agent-systems,1,0,
1743,6784,1,6797,2018-06-18T11:01:14.053,5,111,"i have been working with ai methods . i am thinking about how my daughter ( and also other kids ) could learn mathematics with the help of ai . for example , how could an ai be used to show the mistakes that a kid does during the learning path ?",15506,2444,2018-11-13T21:35:23.867,2018-11-14T07:06:35.553,how could an ai be used to improve the teaching and learning of mathematics ?,ai-community,2,0,2
1744,6789,1,6794,2018-06-18T19:32:27.573,3,324,"premise ok , i know that this question was asked before on ai.se , on stats.se and also on so . so i did my homework in checking before posting my question , but none of them has an answer that fully satisfies me . summary of my knowledge up to now suppose you have a layer that is fully connected , and that each neuron performs an operation like a = g(w^t * x + b ) were a is the output of the neuron , x the input , g our generic activation function , and finally w and b our parameters . if both w and b are initialized with all elements equal to each other , then a is equal for each unit of that layer . this means that we have symmetry , thus at each iteration of whichever algorithm we choose to update our parameters , they will update in the same way , thus there is no need for multiple units since they all behave as a single one . in order to break the symmetry we could randomly initialize the matrix w and initialize b to zero ( this is the setup that i 've seen more often ) . this way a is different for each unit so that all neurons behave differently . of course randomly initializing both w and b would be also okay even if not necessary . the actual question is randomly initializing w the only choice ? could we randomly initialize b instead of w in order to break the symmetry ? is the answer dependent on the choice of the activation function and/or the cost function ? my thinking is that we could break the symmetry by randomly initializing b , since in this way a would be different for each unit and , since in the backward propagation the derivatives of both w and b depend on a ( at least this should be true for all the activation functions that i have seen so far ) , each unit would behave differently . obviously , this is only a thought , and i 'm not sure that is absolutely true . remark note that this is a theoretical question , not a practical one . this means that i 'm not really interested in answers that involves performance , but you 're welcome to include them as corollary to the real answer .",16199,2193,2018-06-19T08:35:33.257,2018-06-19T08:35:33.257,why should weights of neural networks be initialized to random numbers ?,neural-networks training concepts,3,2,2
1745,6799,1,,2018-06-19T11:45:29.377,1,719,so i built a cnn without any scientific libraries like tensorflow or keras ( only numpy ) . it is taking a huge amount of time to train . what are some of the tricks and tips followed by people to speed up training of a cnn ? ( i am not talking about division of jobs into different processors but subtle redundant codes i.e. giving pre - calculated results which is not visible to common programmers ) .,9947,2193,2018-06-19T15:30:42.497,2018-06-20T07:59:45.700,speeding up cnn training,neural-networks machine-learning convolutional-neural-networks,2,9,1
1746,6800,1,,2018-06-19T11:53:14.547,26,4762,"the paper the limitations of deep learning in adversarial settings explores how neural networks might be corrupted by an attacker who can manipulate the data set that the neural network trains with . the authors experiment with a neural network meant to read handwritten digits , undermining its reading ability by distorting the samples of handwritten digits that the neural network is trained with . i 'm concerned that malicious actors might try hacking ai . for example fooling autonomous vehicles to misinterpret stop signs vs. speed limit . bypassing facial recognition , such as the ones for atm . bypassing spam filters . fooling sentiment analysis of movie reviews , hotels , etc . bypassing anomaly detection engines . faking voice commands . misclassifying machine learning based - medical predictions . what adversarial effect could disrupt the world ? how we can prevent it ?",16322,2444,2019-05-29T22:28:38.470,2019-05-29T22:28:38.470,is artificial intelligence vulnerable to hacking ?,neural-networks deep-learning philosophy generative-adversarial-networks ai-safety,9,1,15
1747,6803,1,,2018-06-19T13:53:09.327,2,30,"i have this table , 2 agents and i want to find for each agent if any action is strongly or weakly dominated . this is the table : now , i 've found a solution but i 'm not sure if it 's correct . so for let 's say agent 1(the one handling rows ) : 1&lt;2&lt;2 , 1&lt;3&lt;4 and 0&lt;3&lt;4 so i do n't have a strong dominance . for agent 2 : 1&lt;=1&lt;=1 , 2&lt;5&lt;6 and 2&lt;3&lt;4 which means that i do n't have strong dominance here too . is my logic correct ?",16352,16352,2018-06-19T14:03:37.847,2018-06-19T14:03:37.847,strong and weak dominance table,game-theory,0,2,
1748,6809,1,6811,2018-06-19T20:19:20.390,3,226,"i have recently got into ai and i am eager to learn about its concepts . in some of the information i saw about ai , there was a lot about neural networks . neural networks seem to be ( something along the lines of ) a type of algorithm that creates a graph which works based on a theory about how neurons interact , in order to create self - learning programs . i 'd like more information about this theory and model , as there is a lot that i do n't understand . the graph : firstly , the diagram i keep seeing ( i 'm assuming it 's a graph ) . it shows a set of nodes ( which is apparently the input ) , each directed to each of another set of nodes , each of which is then directed to each of another set of nodes , etc . until it reaches what is apparently the output . how can something like an image , or a complex piece of data be represented by those input nodes ? what goes on at each set of nodes ? does every node from each set have to always be connected to every node from the next set ? do they always have to be directed to the next set , or can they go back and fourth as well ? can i have some code given in relation to one of these diagrams ? ( a node is a variable shown in a graph . ) ai progress : secondly , i read that these sort of algorithms could eventually create a conscious program if "" enough neurons work together "" . i do look forward to when people start to manage to do this , but it seems like no one is trying to rethink / expand - on that theory as much as they should . i 'd expect people to try to look for new human behaviours to represent with ai . for example : a new - born would take their first breath despite the pain , while usually the mind tries to avoid pain . this causes crying . has anyone tried to mimic this sort of behaviour scenario ( or any behaviour scenario ) in a program ? if not , why not ? how close are we to creating a conscious mind in a program ? are people challenging the current theory that created our neural network model ? is it lightly for this community to propose a better theory and model ?",16355,16355,2018-06-21T08:31:12.313,2018-06-25T17:42:07.453,what are neural networks ?,neural-networks ai-basics concepts,3,0,2
1749,6810,1,6848,2018-06-19T20:43:35.507,3,37,"how does the legal question about agents talking to humans via telephone connection work ? recently google gave a talk about duplex , where an agent makes a call to a human to schedule a hairdresser . i wonder if there are any regulations related to this type of scenario , if there are some limitations , if the human needs to know that he is talking to an ai .",7800,1671,2018-06-19T20:48:36.503,2018-06-22T06:32:58.147,digital rights and agents talking to humans,legal digital-rights,1,0,1
1750,6814,1,6815,2018-06-20T01:42:58.700,2,100,"i have a corpus , say an instruction manual . the text in this manual is grouped into chapters and each chapter is split up into sections . for example , chapter 1 / section 1 , chapter 1 / section 2 and so on . assume the corpus has c chapters and each chapter has s sections . my goal is , given a sentence or question , to classify this sentence / question . in other words i want to compute three most probable chapters to which this sentence or question belongs to . i tried multinomialnb model using sklealrn , but it did not give me the desired result . i want to try another approach , for example using a neural network and compare it with the multinomialnb model . i have googled and found doc2vec but have n't tried yet . can anyone suggest a better or another possible approach so that i could try and compare ? what is the standard approach to such kind of problem ?",16371,,,2018-06-20T09:56:16.090,question classification according to chapters,machine-learning deep-learning classification natural-language-processing,2,1,
1751,6823,1,6891,2018-06-20T15:13:55.233,2,267,"i was trying to implement neat but got stuck at the speciating of my clients / genomes . what i got so far is : the distance function implemented each genome can mutate nodes / connections . 2 genomes can give "" birth "" to a new genome . i 've read a few papers but none does explicitly explain in what order what step is done . i know that for each generation , all the similar genomes will be put together into one species . but what happens to them in the next generation ? how do i start the training ? what happens in generation # 1 ? who is being killed / reborn ? who is being mutated and at what point ? i know that these are a lot of questions but i would be very happy if someone could help me :) greetings , finn",16353,,,2018-06-26T10:35:12.123,neat : speciation,evolutionary-algorithms neat,1,0,2
1752,6824,1,,2018-06-20T15:16:52.060,1,50,how does sequential dqn work ? how would one construct the simple sequential dqn ? openai baselines : dqn,16322,1671,2018-06-20T19:00:24.557,2018-06-20T19:00:24.557,deep q - network concepts and implementation,ai-basics q-learning implementation concepts dqn,0,3,1
1753,6826,1,,2018-06-20T17:39:55.893,2,46,"as is done traditionally , i used k - fold cross validation to select and optimize the hyper parameters of my neural network classifier . when it was time to store the final model for future predictions , i discovered that using the weights from the previous k - fold cv iteration to seed the initial weights of the model in subsequent iteration , helps in improving the accuracy ( seems obvious ) . i can use the model from the final iteration to perform future predictions on unseen data . would this approach result in overfitting ? ( please note , i am using all available data in this process and i do not have any holdout data for validation . )",16408,1671,2018-06-20T18:54:57.920,2018-06-21T10:35:38.743,if use the weights from previous iteration of a k - fold cross validation to seed a neural network classifier would i be overfitting ?,neural-networks machine-learning classification training python,1,0,
1754,6827,1,6847,2018-06-20T19:03:06.063,2,82,"i am reading the simon haykin 's cornerstone book , "" neural networks , a comprehensive foundation , second edition "" and i can not understand a paragraph below : the analysis of the dynamic behaviour of neural networks involving the application of feedback is unfortunately complicated by virute ( or virtue i can not get word appropriately ) of the fact that the processing units used for the construction of the network are usually nonlinear . further consideration of this issue is deferred to the latter part of the book . before the paragraph , the author analysis the affects of weight of synapsis to the neural network 's stability . roughly speaking , he says , if |w| > = 1 the neural network become unstable . could you please explain the paragraph ? thanks in advance .",14862,14862,2018-06-21T08:07:10.297,2018-06-23T08:41:47.133,the analysis of the dynamic behaviour of neural networks involving the application of feedback,neural-networks,1,3,
1755,6832,1,,2018-06-21T01:22:26.443,2,62,"can anyone recommend a reinforcement learning algorithm for a multi - agent environment ? in my simplified example , i 'm implementing a q - learning system with different 10 agents . the agents compete for resources in stores at different locations by setting a bid price for each item . all of the agents have different bids and pooled budget of $ 100 . once the budget is reached the agents can not buy any more that day . each agent will receive a reward if they buy an item . the goal would be to maximize the total amount of items bought between the agents . right now the agents do n't communicate . can someone point me in the right direction for an algorithm that allows agent cooperation ?",16414,2444,2019-02-13T21:30:08.143,2019-02-13T21:30:08.143,algorithms for multiple agents problems,reinforcement-learning multi-agent-systems,0,2,1
1756,6834,1,,2018-06-21T08:40:34.087,1,57,"in the instance like alphago zero . how n ' why alphago zero 's training is so stable ? compared with traditional game theory applied in deep rn technique ! ? how "" alphago zero "" differs from "" alphago "" ? how basically ai synthesize thinking ! ? how it differs comparing to our human cognitive ability",16322,1671,2018-06-21T19:34:08.347,2018-06-21T19:34:08.347,how basically ai synthesize thinking ?,ai-design learning-algorithms concepts alphago alphago-zero,0,4,2
1757,6836,1,6862,2018-06-21T12:00:23.303,4,128,"so recently i have been learning about new nn 's which are used for specialised purposes like speech recognition , image recognition , etc . the more i discover the more i get amazed by the cleverness behind models such as rnn 's and cnn 's . questions about the working , intuition , mathematics have been asked a lot in this community , all with vague answers and apparent understandings . so my question is that , did the researchers come up with these specialised models accidentally or did they follow particular steps to get to the model ( like in a mathematical framework ) ? and how did they look at a particular class of problem and think "" yeah , a better solution might exist "" ? for me since understanding of nn 's are so vague , these are ' high risk , high reward ' scenarios , since you might be chasing only the mirage ( illusion ) of a solution .",9947,,,2018-06-23T08:18:56.213,are neural net architectures accidental discoveries ?,neural-networks ai-design ai-community,3,0,1
1758,6846,1,,2018-06-22T01:51:14.303,2,32,"i 'm eventually looking to build an algorithm that will process answers from humans that are given questions . but first i have to setup an experiment to determine the variety of responses . specifically , humans will be asked a multiple choice question that has a single correct answer . i want to understand what kinds / ranges of responses i would get from the bell curve distribution of human intelligence . is there any way i can have , say , 1000 "" humans "" be asked a prompt , repeated 100 times ( the same question ) and then compile the responses ? my concern is that i 'll have to build some algorithm or process for each dumb , average , smart "" human "" to follow but then i would introduce bias in how smart they are or limit how they may respond . i 'm guessing i 'll have to give them a data sort to work from . to clarify , it 's not the number of times a single user gets a question right that makes them smart , they have to be programmed dumb , smart etc . before the simulation starts . so dumb users could get some right and smart can get some wrong . i 'm not sure the monte carlo method is useful here but some type of simulation where i can specify the distribution ( normal ) and then bound the responses would be helpful . i have access to excel , minitab , and python . any ideas how to set up an experiment like this ? i really am open to any technique to measure this .",16435,,,2018-06-22T14:16:26.107,how can i simulate responses from the distribution of human intelligence ?,machine-learning human-like,1,0,2
1759,6850,1,,2018-06-22T07:24:19.283,3,64,"i thought i have implemented the code ( from scratch , no library ) for an artificial neural network ( first endeavour in the field ) . but i feel like i miss something very basic or obvious . to make it short : code works for a single pair of in-/out - values but fails for sets of value pairs . i do not really understand the training process . so i want to get this issue out of the way first . the following is my improvised training ( aka all that i can think of ) in pseudocode . trainingdata = [ { in : [ 0,0 ] , out:[0 ] } , { in : [ 0,1 ] , out:[0 ] } , ... ] ; iterations = 10000 network = graphnodestonetwork ( ) links = graphlinkstonetwork ( ) randomiselinkweights(links ) while(trainingdata not empty ) { for(0&lt;iterations ) { set = trainingdata.pop ( ) updateinput(network , set.in ) forwardpropagate(network , links ) linkupdate = backpropagate(network , links , set.out ) updatelinks(linkupdate , links ) } } is this how it is supposed to work ? do you feed in your training data set by set ( while - loop ) ? edit 1 : because my final comment did distract from the issue at hand . edit 2 : less wordy , more code - y",16441,16441,2018-06-22T12:01:32.943,2018-08-16T06:50:50.763,how to actually teach the ann the resulting weights of different training inputs ?,neural-networks training,2,6,
1760,6852,1,,2018-06-22T14:15:04.040,2,211,"comparing to a monkey with a keyboard creating shakespeare a.k.a infinity monkey theorem , we now have automl : a machine learning software that can create self - learning code . | wiki are we near to singularity ( self - aware machines ) ?",16322,16322,2018-06-25T05:01:26.150,2018-09-30T17:33:54.910,infinite monkey theorem,singularity automation,3,6,2
1761,6855,1,6962,2018-06-22T17:13:34.897,1,1134,"i 've been oogling the mac pro from apple with loaded specs . check it here if unfamiliar . i 'm curious to hear anyone 's thoughts of the computer for deep learning / machine learning applications vs cloud based computing . obviously , a flexible solution on gcp or aws has the ability to scale and therefore produce results faster . however , i 'm mostly interested to hear thoughts on the economics of purchasing the machine vs renting a cloud based machine . if we assumed something like a 5 year lifetime of the apple mac pro for this type of computing ( before the chips / processors become immensely slower than the newest chips used in the cloud ) , that would be like $ 1000 per year amortized using the mac pro . does it make more sense to rent from aws / gcp for \$1000 per year as a renter instead ? i do n't have much experience in the cost of aws / gcp so i 'm looking to here an answer from anyone well versed in cloud computing .",14924,1671,2018-06-29T19:37:05.777,2019-04-22T19:57:05.173,thoughts on apple mac pro vs gcp / aws for deep learning ?,machine-learning deep-learning hardware hardware-evaluation soft-question,1,0,
1762,6859,1,,2018-06-22T20:18:39.120,2,471,"i 'm new to machine learning , so i figured i should look into google 's tensor flow guides and i know how to code in js so that 's why i 'm using tensorflow.js , there 's and example in the guide that trains itslef to recognize handwritten numbers from the mnist handwriting dataset , i sort of understand what 's going on in the code but since i 'm very new to ml it 's not a lot , i went through the code and saw that it did n't took image by image to train itself but it requests one sprite which contains all the images and then cuts it into what it needs , this makes sense from a performance point of view , but as this process is kind of abstract i do n't understand what 's really going on , i want to upload an image of my own and call the predictor of the model but i do n't know how to do it , any help ? i was thinking that drawing in a canvas of 28x28 a number might be very interesting as well instead of uploading an image , but i need to know how to test the model once it 's trained with my own data . the tutorial : https://js.tensorflow.org/tutorials/mnist.html",16078,,,2018-06-25T16:53:11.927,how to load an image into tensorflow.js code which reads handwritten numbers and clasify them,machine-learning convolutional-neural-networks image-recognition,1,0,
1763,6860,1,6864,2018-06-23T06:52:10.587,2,234,"why is it that the skewed contour ( unscaled features ) will result in slow performance of gradient descent ? in other words , how ( or why ) will the gradients end up taking a long time before finding the global minimum in such cases ? this might be an obvious question but i 'm finding a hard time visualizing the 3d shapes of the respective contours and relating it to the convergence . left one is the contour for the unscaled feature and the right one is scaled ( and will apparently converge quickly ) .",10549,9947,2018-06-23T08:35:57.803,2018-09-13T06:57:47.687,why feature scaling for skewed contour ?,neural-networks machine-learning gradient-descent,1,0,
1764,6863,1,,2018-06-23T08:25:13.167,1,30,"say i have an application where the frequency of the input is known but can vary widely across sequences . for example , they may be audio recordings acquired at different frequency , or videos that come from surveillance cameras whose framerate can vary from 24fps down to as low as 1fps . the straightworard thing to do would be to either resample inputs to a constant frequency ignore input frequency and hope the rnn will figure it all out none sound very appealing . is there a better way to handle variable input frequency in rnns ?",16466,,,2018-06-23T08:25:13.167,how to adapt rnns to variable frequency / framerate of inputs ?,recurrent-neural-networks,0,3,
1765,6865,1,6869,2018-06-23T10:07:41.970,3,261,"in a recent paper data - efficient hierarchical reinforcement learning , o nachum , s gu , h lee , s levine , 2018 , a promising agent controlling technique called hierarchical reinforcement learning was introduced . it is some kind of layered policy for controlling ant - like robots in a maze . for example , the main controller is able to run sub - controllers move and push , and this allows the ant to move to a goal , even an obstacle is on the way . but there is something in the paper which i did n't understand : how to find the goals . according to the paper , the lowlevel actions “ move ” and “ push ” are equal to goals . and these goals have to be inferred from demonstrations . in the paper , they write : for generality , we develop a scheme where lower - level controllers are supervised with goals that are learned and proposed automatically by the higher - level controllers . how exactly is the matching between the observation and the goal state done ?",11571,4302,2018-07-24T10:06:22.683,2018-07-24T10:06:22.683,finding goals in hierarchical reinforcement learning,reinforcement-learning control-problem topology robotics autonomous-vehicles,1,0,
1766,6868,1,,2018-06-23T17:25:43.103,3,85,"i 've selected more than 10 discriminative ( classification ) models , each wrapped with a baggingclassifier object , optimized with a gridsearchcv , and all of them placed within a votingclassifier object . alone they all bring around 70 % accuracy , on a data set which is about half normal / uniform distributed , and half one - hot distributed . together they provide 80 % accuracy , which is n't good enough , to the 95%&lt ; needed . the models : decisiontreeclassifier , extratreesclassifier , kneighborsclassifier , gradientboostingclassifier , logisticregression , svc , perceptron , and a few more classifiers . how do i check if the combination is good ?",16474,16920,2018-09-10T20:51:14.460,2018-09-10T20:51:14.460,machine learning - is selected models combination good ?,machine-learning classification python models performance,1,4,
1767,6872,1,6879,2018-06-24T03:40:06.693,6,265,"the endocannabinoid system is a very important function of human biology . unfortunately , due to the illegality of cannabis , it is a relatively new field of study . i have read a few articles about google researching the role of dopamine in learning , and according to this article , anandamide ( the neurotransmitter that closely resembles tetrahydrocannabinol ) : ... was found to do a lot more than produce a state of heightened happiness . it ’s synthesized in areas of the brain that are important in memory , motivation , higher thought processes , and movement control . have any neuroscientists ( or any scientists ) considered the importance of the endocannabinoid system for cognitive function ? if not , is there any reason this information might or might not be relevant to artificial intelligence ?",16480,16480,2018-06-24T17:55:21.107,2018-07-28T23:50:00.070,an artificial endocannabinoid system,neural-networks emotional-intelligence biology brain,1,8,1
1768,6874,1,,2018-06-24T18:11:43.943,1,43,"i have a database with hundreds of questions and answers . would you like to know how i can work on this data in google cloud ? i have a social network where i have these questions and answers , and i would like to use the machine learning features and other google cloud tools . does anyone know the step by step ?",7800,,,2018-06-24T18:11:43.943,data to google machine learning,machine-learning deep-learning google-cloud,0,1,1
1769,6875,1,,2018-06-24T20:41:02.710,3,634,"i am a deep learning beginner recently reading this book "" deep learning with python "" , the example explains the process of implementing a greyscale image classification using mnist in keras , in the compilation step , it said , before training , we ’ll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [ 0 , 1 ] interval . previously , our training images , for instance , were stored in an array of shape ( 60000 , 28 , 28 ) of type uint8 with values in the [ 0 , 255 ] interval . we transform it into a float32 array of shape ( 60000 , 28 * 28 ) with values between 0 and 1 . images stored in an array of shape ( 60000 , 28 , 28 ) of type uint8 with values in the [ 0 , 255 ] interval . for my understanding , the values are between 0 - 255 of each px and storied as 3d matrix . can someone explain why needs to "" transform "" it into the network expects by scaling it and make "" all values are in the [ 0 , 1]interval . "" ? please also make suggestions if i did n't explain some parts correctly .",6390,9947,2018-07-24T03:08:51.240,2019-04-20T08:01:35.423,"what is the purpose of "" reshaping it into the shape the network expects and scaling it so that all values are in the [ 0 , 1 ] interval . "" ?",neural-networks machine-learning deep-learning convolutional-neural-networks,2,1,
1770,6878,1,,2018-06-25T07:19:45.520,2,94,"in a final project in diagnosing adhd using machine learning we obtained parameters from real patients . we used this data and got much higher success rates in lda than in svm and naive base , we had only 100 examples in our training set . we are wondering why lda specifically succeeded much more than the others ?",16496,9947,2018-06-25T07:28:39.413,2018-11-23T19:01:05.647,lda performs much better than other methods,machine-learning,2,0,1
1771,6880,1,,2018-06-25T11:27:35.883,4,92,i have an image dataset where objects may belong to one of the hundred thousand classes . i want to know what kind of neural network architecture should i use in order to achieve this .,12957,,,2018-06-25T23:52:06.340,what kind of neural network architecture do i use to classify images into one hundred thousand classes ?,neural-networks deep-learning convolutional-neural-networks classification,3,0,1
1772,6889,1,6900,2018-06-26T08:33:54.773,-2,81,"i 'm starting to play around with python neural networks , mainly for object recognition like this one : https://github.com/huangshiyu13/rpnplus . i plan to train more networks with more kinds of objects . so far , this network seems to take a long time ( ~42hrs and counting ) for training or maybe my cpu ( 16 gb ram , intel i5 ) its not enough . a friend of mine is selling a gpu nvidia geforce 760 with 4 gb ram and i wonder if it is a good opportunity for me or if really a gpu is not a real advantage for nn , as i read in this other post . what is your opinion ? is a gpu a good investment or not really ? i 've seen other gpu like asus with 2 or 4 gb for around 200 us$ , does this nvidia make a big difference or any other common gpu will do the same work ? ( thinking in how much i should pay for this ) .",16522,2193,2018-06-26T17:06:01.510,2018-06-26T21:05:12.700,gpu to train object recognition neural networks,computer-vision hardware,1,2,
1773,6890,1,7513,2018-06-26T10:29:43.110,3,557,"is lacking the ability of rational behavioral decisions of humans makes a huge difference in ai agents uncertainty problems ? ( as much as i look into what ’s being done with deep learning , i see they ’re all stuck there on the level of associations , . curve fitting . i ’m very impressed , because we did not expect that so many problems could be solved by pure curve fitting . it turns out they can . but i ’m asking about the future — what next ? - judea pearl turing award winner and the author of the book of why the new science of cause and effect ) consider rational behavior like free will i.e. , the ability to choose , think , and act voluntarily . , how artificial intelligence could mimic human intelligence(like rational behavioral decisions ) better than human does?(as we generally intend ai to outperform humans in several fields ) [ opinion ] if possibly ai really could outperform humans in taking rational behavioral decisions then , i 'm sure ai can find pitfalls from the human behaviors , like telling climate change is real.etc . , and i 'm sure ai can come up with some moral decisions that could help unite humanity and with other life forms .",16322,1671,2018-11-09T21:57:48.910,2018-11-21T05:18:22.007,human intelligence vs artificial intelligence,intelligent-agent human-like ai-community intelligence-testing rationality,3,4,3
1774,6892,1,7296,2018-06-26T10:39:29.850,7,211,"the problem of adversarial examples is known to be critical for neural networks . for example , an image classifier can be manipulated by additively superimposing a different low amplitude image to each of many training examples that looks like noise but is designed to produce specific mis - categorizations . since some networks are applied in safety - critical applications ( e.g. self - driving cars ) , we have a question : what tools are used to ensure safety - critical applications are resistant to the injection of adversarial examples at training time ? laboratory research aimed at developing defensive security for neural networks exists . these are a few examples . adversarial training ( see e.g. a. kurakin et al . , iclr 2017 ) defensive distillation ( see e.g. n. papernot et al . , ssp 2016 ) mmstv defence ( maudry et al . , iclr 2018 ) . however , do industrial strength , production ready defensive strategies and approaches exist ? are there known examples of applied adversarial - resistant networks for one or more specific types ( e.g. for small perturbation limits ) ? there are already ( at least ) two questions related with the problem of hacking and fooling of neural networks . the primary interest of this question , however , is whether any tools exist that can defend against some adversarial example attacks .",16354,16354,2018-07-07T08:50:15.173,2018-07-25T17:55:20.797,what tools are used to deal with adversarial examples problem ?,ai-design strong-ai security ai-safety,1,2,3
1775,6897,1,6901,2018-06-26T19:20:10.110,1,45,"i 've recently come across the client - server model . from my understanding , the client requests the server , to which the server responds with a response . in this case , both the request and responses are vectors . in reinforcement learning , the agent communicates with the environment via an "" action "" , to which the environment sends a scalar reward signal . the "" goal "" is to maximize this scalar reward signal in long run . is there a more formal way to frame this problem such the agent searches the internet for obtaining more information about certain topic ?",15935,2444,2019-02-16T02:56:06.783,2019-02-16T02:56:06.783,analogy between reinforcement learning and web servers,reinforcement-learning concepts,1,0,
1776,6898,1,,2018-06-26T19:23:54.920,5,227,"can i get details about the algorithms used for classifying questions in stackoverflow ( "" questions that may already have your answer "" ) . most of the suggestions i get are nowhere related to the question i have intended to ask .",15935,16355,2018-06-27T19:42:15.393,2018-08-10T13:54:41.037,what algorithms does stackoverflow use for classifying duplicate questions ?,natural-language-processing,1,1,
1777,6899,1,,2018-06-26T19:26:45.687,7,547,"i understand the intuition behind stacking models in machine learning , but even after thorough cross - validation scheme models seem to overfit . most of the models i have seen in kaggle forums are large ensembles , but seem to overfit very little .",15935,16355,2018-06-28T09:22:39.997,2019-05-03T04:05:36.073,how to prevent overfitting in stacked models ?,machine-learning,1,7,2
1778,6902,1,,2018-06-26T22:07:13.167,3,123,"i have a sort of mathematical problem and i 'm not sure which model i should choose to make an lstm neural network . currently in my country , there is a system in which certain groups of researchers upload information on products of scientific interest , such as research articles , books , patents , software , among others . depending on the number of products , the system assigns a classification to each group , which can be a1 , a , b and c , where a1 is the highest classification and c is the minimum . the classification is done through a mathematical model whose entries are , the total number of each product , the total sum of all products , number of authors , among other indices that are calculated with the previous values . once the entries are obtained , these values ​​are processed by a set of formulas and the final result is a single number . this number is located in a range provided by the mathematical model and this is how the group is classified . what i want to do is given the current classification of a group , give suggestions of different values ​​to improve their classification . for example , if there is a group with classification c , suggest how many products it should have , how many authors , what value should its indexes have , so that its category would be finally b. i think the structure of my network should be : -1 input , which would be the classification you want to get . -multiple output , one for each product and indexes . but i do not understand how to make the network take into account the current classification of the group , in addition to the number of products and the value of the current indexes . if you have further questions about the problem , please feel free to ask . i appreciate your suggestions .",16258,,,2018-12-29T21:02:05.447,how to build my own dataset and model for an lstm neural network,neural-networks deep-learning deep-network recurrent-neural-networks getting-started,2,0,
1779,6903,1,,2018-06-26T23:00:06.657,5,273,"does anyone know a good book to buy for beginners to coding for artificial intelligence ? i have done some coding in java and a little with python , but i would be willing to learn a new language too .",16533,2444,2019-05-03T14:56:35.403,2019-05-03T14:58:27.450,ai beginners books for coding,ai-design python getting-started reference-request java,5,0,3
1780,6905,1,6907,2018-06-27T07:26:21.003,1,59,"ibm 's watson acts as a template for developing chat - bots with ease ( without coding ) , but what are the methodologies and concepts that have been used to build it ?",16541,2193,2018-06-27T15:54:28.690,2018-06-27T15:54:28.690,what are the internal concepts incorporated in ibm 's watson platform ?,neural-networks deep-learning natural-language-processing chat-bots watson,1,0,
1781,6906,1,,2018-06-27T07:49:16.400,2,127,"two months ago , i 've found myself working on a churn detection problem which can be briefly described as follows : assume the current date is n use customer behavior for n-1, .. n - x dates to develop training dataset train model and make prediction at time n , predicting if a customer will churn at n+2 ( thus allowing data n+1 for churn prevention / reduction campaign ) when thinking through the design of the model and considerations for how to ensure that it would be successfully implemented , i identified a feedback loop wherein the prediction would trigger an event resulting in interaction with customer , potential changes to customer behavior and thus an impact on the next set of prediction data . the following sequence of events could occur if successful ( as an example ) : prediction -&gt ; action to retain customer -&gt ; change to customer behavior -&gt ; data for next prediction cycle not representative of training -&gt ; incorrect prediction and cost associated for handling incorrect prediction the feedback loop , fundamentally is that the action taken based on the prediction may impact the distribution or nature of features used to make the prediction . when thinking through the how to solve the feedback problem i had listed the following three points as potential solutions : retrain , test and validate model at every n+1 period and account for changes in behavior through new features ( e.g. feature_i would involved details of the retention campaign a customer was treated to ) this would result in huge production overhead and i believe to be infeasible run the model intermittently to allow behavior to normalize possible , however business would not be happy to have a prediction model which only works k times a year where k would have to be determined predict the impact of the retention intervention and remove it from or the training set or include it as a new feature possible , extensive thought and some experimentation needed to determine whether modeling the retention out or in would have the better effect . additionally , if modeled in , there may a short term penalty incurred as the model learns the new feature i did not actually end up having to confront the feedback problem ( as during the exploration phase , sufficient evidence was obtained indicating that a predictive model for churn detection would not be required ) , however after reading this paper on the technical debt which could be incurred during the development of the machine learning systems i found myself pondering : were my considered strategies for dealing with the feedback reasonable ? what other solutions should i have considered ? is there a way i could have re - framed the problem to completely design out the feedback loop ( may be difficult to answer with the information provided , but if possible , but a "" you could have considered looking at ... "" would be extremely beneficial )",11933,,,2018-06-27T07:49:16.400,how do to mitigate or design out hidden feedback loops when designing ml systems ?,machine-learning data-science imperfect-information,0,0,1
1782,6908,1,6913,2018-06-27T08:36:45.237,1,42,"i 'm just beginning to understand neural networks and i 've performed a couple of successful tests with numerical series where the nn was trained to find the odd one or a missing value . it all works pretty well . the next test i wanted to perform was to approxmimate the solution of a sudoku which , i thought could also be seen as a special kind of numerical series . however the results are really confusing . i 'm using an mlp with 81 neurons in each of the three layers . all output neurons show a strong tendency to yield values that are close to either 0 or 1 . i have scaled and truncated the output values . the result can be seen below : expected / actual solution : neural net 's solution : 6 2 7 3 5 0 8 4 1 9 0 9 9 9 3 0 0 3 3 4 8 2 1 6 0 5 7 0 9 9 0 0 0 9 9 0 5 1 0 4 7 8 6 2 3 0 9 1 9 9 0 2 0 4 1 6 4 0 2 7 5 3 8 0 0 5 0 0 9 0 0 7 2 0 3 8 4 5 1 7 6 0 0 0 0 0 9 9 0 9 7 8 5 1 6 3 4 0 2 9 9 9 9 0 6 2 9 0 0 5 6 7 3 1 2 8 4 0 0 0 0 9 9 0 9 0 4 3 1 5 8 2 7 6 0 9 9 0 0 0 0 9 0 9 8 7 2 6 0 4 3 1 5 9 9 0 9 9 0 9 0 9 the training set size is 100000 sudokus while the learning rate is a constant 0.5 . i 'm using nodejs / javascript with the synaptic library . i do n't expect a perfect solution from you guys , but rather a hint if that kind of behavior is a typical symptom for a known problem , like too few / many neurons , small training set etc ..",16542,,,2018-06-27T13:09:18.030,regression with neural network ( mlp ) has tendency to yield values 0 and 1,mlp node-js javascript,1,4,
1783,6909,1,,2018-06-27T09:25:31.037,2,179,"i am currently studying information systems engineering ( ba ) and i 'm thinking of getting a master degree in artificial intelligence.so , what are the main important skills do i need to succeed at this field , and what kind of math does it require ?",16547,1671,2018-06-27T18:27:12.983,2018-06-27T18:27:12.983,what skills are needed to succeed at artificial intelligence field ?,deep-learning ai-basics programming-languages soft-question,2,3,
1784,6914,1,,2018-06-27T13:10:10.437,1,69,"may someone explains some first iterations of this sigma ? also , how did it convert the above expression to below expression ? what it the meaning of i(x ) and i(y ) ?",9941,,,2018-06-27T13:10:10.437,how does this sigma work?(harris algorithm ),math,0,0,
1785,6916,1,6950,2018-06-27T13:22:31.390,1,208,"i would like to know : would people go far with artificial intelligence and machine learning to the point where machines could learn during a long period of time to distinguish what 's ' good ' from ' bad ' according to people living in a restricted geographical area , and then the machines take control and turn what was learnt into a set of ' rules ' and ' laws ' ( think of it as an effective machine of ' politics ' ) that match the majority of the people 's view of issues . that should be accepted by everyone , since a contract set at the beginning says : “ everyone is ok ” . ( confidence ) .",16548,2193,2018-07-02T20:10:52.633,2018-12-20T17:32:34.890,limits of artificial emotional intelligence ?,philosophy agi ethics theory confidence,4,4,1
1786,6920,1,6935,2018-06-28T02:17:46.260,2,61,"a bunch of friends and i play ultimate every week . recently i wrote a program to choose our teams for us , as well as keep track of certain data ( like which players were on which team , which team won , what was the score , how long the game was , etc ) . i wanted to use a machine learning technique to make the teams for us in order to optimize how fairly balanced the teams are ( possibly measured by how many total points are scored in a game or how long a game lasts ) . i am currently taking a machine learning mooc and being introduced to very basic machine learning techniques ( linear regression with gradient decent or normal equations , basic classification stuff , stuff like that ) . although i hope i will come across a technique that fits my needs by the end of this course , i wanted to ask here to see if i can get a head start . i 've tried searching around everywhere , but could n't find anything relevant . so my question is , is there an obvious technique i should look into for such a problem ? if it 's something too advanced for a beginner , that 's fine too , but i 'd like to get started learning / practicing it asap instead of waiting for my course to hopefully hit upon it . thank you ! edit : to clarify further , i would prefer something that looks at relationships between individual players like "" when steph plays with bill she is more likely to win "" or "" steph plays worse when she is on a team with players who have a high win percentage "" . i 'd also prefer to be able to code it in python , but am willing to learn any other language",16561,,,2018-06-28T22:48:18.703,what beginner - friendly machine learning method should i use to make teams for my pickup ultimate frisbee club fairly balanced ?,machine-learning ai-design training ai-basics getting-started,1,0,1
1787,6921,1,7121,2018-06-28T05:53:23.973,1,47,"in the data - sets like coco - text and total - text , the images are of different sizes ( height*width ) . i 'm using these data sets for text detection . i want to create a dnn model for this . so input data should be of same size . if i resize these images to a fixed size , annotations given in the data - set that is the location of the text in the images will be changed . so how to proceed with these data - set ? i 'm new to machine learning and i did n't get answer for this anywhere . thanks for the help .",12273,16355,2018-06-28T16:36:14.430,2018-07-11T18:04:19.990,normalization for well known data sets like coco - text and total text data set,computer-vision datasets,2,0,
1788,6923,1,6924,2018-06-28T08:19:24.323,3,250,"so i just read about deep q - learning which is using a neural network for optimization instead of q - table . i saw the example here : https://yanpanlau.github.io/2016/07/10/flappybird-keras.html and he used cnn to get the q - value . my confusion is on the last layer of his neural net . neurons in the output layer each represent an action ( flap , or not flap ) . i also see the other projects where the output layer also represents all available actions ( move - left , stop , etc . ) how would you represent of all available action of a chess game ? every pawn have unique and available movement . we also need to choose how far it will move ( rook can move more than one square ) . i 've read giraffe chess engine 's paper and ca n't find how he represents the output layer ( i 'll read once again ) . i hope somebody here can give a nice explanation about how to design nn architecture in q - learning , i 'm new in reinforcement learning . thank you .",16565,16355,2018-06-28T09:03:07.197,2018-06-28T09:45:17.093,number of neuron in q - learning of chess,neural-networks reinforcement-learning q-learning,1,0,1
1789,6926,1,7023,2018-06-28T11:00:52.403,4,225,"in traditional computer vision and computer graphics , the pose matrix is a 4x4 matrix of the form r11 r12 r12 t1 r21 r22 r22 t2 r31 r32 r32 t3 0 0 0 1 and is a transformation to change viewpoints from one frame to another . in the matrix capsules with em routing paper they say that the ' pose ' of various sub - objects of an object are encoded by each capsule lower layer . but from the procedure described in the paper , i understand that the pose matrix they talk about does n't conform to the definition of the pose matrix . there is n't any restriction on keeping the form of the pose matrix shown above . therefore , my first question is that is it right to use the word pose to describe the 4x4 matrix of each capsule ? my next question is that since the claim is that the capsules learn the pose matrices of the sub - objects of an object , does it mean they learn the viewpoint transformations of the sub - objects since the pose matrix is actually a transformation ?",16569,,,2018-08-22T11:37:00.323,wrong usage of ' pose ' in matrix capsules with em ?,neural-networks deep-learning computer-vision,2,0,1
1790,6927,1,,2018-06-28T16:23:15.703,3,96,"earlier this month , google released a set of principles governing their ai development initiatives . the stated principles are : objectives for ai applications : be socially beneficial . avoid creating or reinforcing unfair bias . be built and tested for safety . be accountable to people . incorporate privacy design principles . uphold high standards of scientific excellence . be made available for uses that accord with these principles . ai applications not to be pursued : technologies that cause or are likely to cause overall harm . where there is a material risk of harm , we will proceed only where we believe that the benefits substantially outweigh the risks , and will incorporate appropriate safety constraints . weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people . technologies that gather or use information for surveillance violating internationally accepted norms . technologies whose purpose contravenes widely accepted principles of international law and human rights . source : artificial intelligence at google : our principles my questions are : are this guidelines sufficient ? are there any "" i , robot "" conflicts how much does this matter if other corporations and state agencies do n't hew to similar guidelines ?",1671,1671,2018-06-28T16:36:28.557,2018-07-28T10:38:15.387,google 's principles of artificial intelligence,philosophy ethics logic theory google,2,1,4
1791,6928,1,6939,2018-06-28T16:25:57.687,1,104,"how should i design my input layer for the following classification problem ? input : 5 cards in a card game ; vocabulary is 52 cards output : some classification using a neural network how should i model the input layer ? option a : 5 one hot encodings for the 5 cards , i.e. 5 one_hot vectors of length 52 = 260 input vector e.g. [ [ 0,0,0,0,0,0,1 , ... ] , [ 1,0,0,0,0,0,0 , ... ] , [ 0,0,0,0,0,1,0 , ... ] , [ 0,0,1,0,0,0,0 , ... ] , [ 0,0,0,0,1,0,0 , ... ] ] option b : 5 hot encoding encompassing all 5 cards in one 52 element vector [ 1,0,1,0,1,1,1 , ... ] what are the disadvantages between a and b ?",16574,,,2018-06-29T03:21:54.190,n * one_hot vs n_hot encoding for modeling input layer for a card game,neural-networks machine-learning,1,0,
1792,6931,1,6945,2018-06-28T19:45:06.297,2,26,"i 've been wondering , how , in the most simple - to - implement basic principle , does the light projection to depth map technique described here https://www.lightform.com/how-it-works actually functions ? is it some kind of an average based on the color of x pixel over all the patterns or what ? how difficult would it be to code something that could do this , up ?",16579,,,2018-06-29T08:09:16.113,structured lighting basic principles for depth mapping,computer-vision,1,0,
1793,6932,1,,2018-06-28T20:07:47.930,3,97,"i am interested to know , if someone wants to be an ai expert , what should he / she know , as we can see this is a vast field today ! for example , if someone works on machine vision , should he / she know voice recognition or data mining ? in other words , should someone know everything from image processing to machine vision , if he / she wants to be an expert in that field or are there some specific subfields even in the vision section ?",2557,2444,2018-07-05T18:45:59.357,2018-07-20T17:44:32.373,"how can we specify ai field , respect to job positions ?",machine-learning soft-question profession,2,0,2
1794,6933,1,,2018-06-28T21:34:51.633,2,22,"i am working on a problem where i have to train a cnn to recognize different kinds of surfaces . one important characteristic of the surfaces i am interested is is how reflective they are . i have been trying to find a method that quantifies how "" shiny "" a surface is , but i have not found much . i am hoping that someone can point me toward a method or some research into this kind of problem .",16582,,,2018-06-29T07:41:51.553,how to quantify the reflectance in an image ?,computer-vision,1,2,
1795,6934,1,6936,2018-06-28T22:47:30.847,2,480,"i have a very simple question about conv nets . i understand the whole principle , but only one thing is not well explained on the internet . if i have a 16 channels image that goes on a convolutional layer , and the trainable filters are 3 7x7 filters , meaning that its output has 3 channels , how does the conv layer do to go from 16 to 3 channels ? what mathematical operation is applied ? thanks for any clarification on this .",14801,,,2018-06-29T06:15:42.357,convolutional neural nets and reduction of the layers,deep-learning convolutional-neural-networks,2,0,
1796,6946,1,,2018-06-29T08:10:35.707,1,119,"i 'm working on a project to predict the usage of all the files in a filesystem in near future based on the metadata of the file system for past 6 months . i 've got the following attributes about the files with me : the temporal sequence of file usage for last 6 months(whenever the file was read / written / modified and by whom ) . all the users who are on the server and can access the files . last modified / written / read epoch time and by whom . file creation epoch time and by whom . any compliance regulations on the file(whether the file contains any confidential data ) . size , name , extension , version , type of the file . number of users who can access the file . file path . total number of times accessed . permitted users . now , i plan to use lstm but for standard lstms , the input is temporal sequence only . however , all the attributes that i have seem significant in predicting the future usage of the file . how should i also make use of the attributes of the file that i have ? should i train a feedforward neural network , disregarding the fact that it usually fails on temporal sequences ? how should i proceed ? does a variant of lstm exist that can take into account the attributes of the file as well and predict the usage of the file in near future ? do i need to use nn and lstm together like a hybrid ?",16591,,,2018-06-29T08:10:35.707,deep learning model ( lstm ) with temporal and non temporal attributes,deep-learning deep-network recurrent-neural-networks prediction lstm,0,0,
1797,6952,1,,2018-06-29T14:09:14.537,7,123,"in image classification we are generally told the main reason of using cnn 's is that densely connected nn 's can not handle so many parameters ( 10 ^ 6 for a 1000 * 1000 image ) . my question is , is there any other reason why cnn 's are used over dnn 's ( densely connected nn ) ? basically if we have infinite resources will dnn trump cnn 's or are cnn 's inherently well suited for image classification as rnn 's are for speech . answers based either on mathematics or experience on the field is appreciated .",9947,9947,2018-06-29T14:20:33.470,2018-11-30T07:03:11.833,cnn 's vs densely connected nn 's,neural-networks machine-learning convolutional-neural-networks,3,0,1
1798,6953,1,6955,2018-06-29T14:31:57.610,6,788,"i am learning about monte carlo algorithms and struggling to understand the following : if simulations are based on random moves , how can the modeling of the opponent 's behavior work well ? for example , if i have a node with 100 children , 99 of which lead to an instant win , whereas the last one leads to an instant loss . in reality , the opponent would never play any of the 99 losing moves for him ( assuming they are obvious as they are the last moves ) , and would always play the winning one . but the monte carlo algorithm would still see this node as extremely favorable ( 99/100 wins for me ) , because it sees each of the 100 moves as equally probable . is my understanding wrong , or does it mean that in most games such situations do not occur and randomness is a good approximation of opponent behavior ?",16597,1641,2018-09-22T18:14:58.670,2018-09-22T18:14:58.670,why does monte carlo work when a real opponent 's behavior may not be random,game-ai search monte-carlo-tree-search,3,1,1
1799,6959,1,6960,2018-06-29T19:51:58.097,2,71,"i built a simple html game . in this game the goal is to click when the blue ball is above the red ball . if you hit , you get 1 point , if you miss , you lose 1 point . with each hit , the blue ball moves faster . you can test the game here . without using machine learning , i would easily solve this problem by just clicking when the x , y of the blue ball was on the x , y of the red ball . regardless of the time , knowing the positions of the 2 elements i could solve the problem of the game . however , if i wanted to create an ai to solve this problem , could i ? how would it be ? i 'd really like to see the ai randomly wandering until it 's perfect . my way to solve the problem i click many times and watch score . if score down , add to bad_positions . if actual position in bad_positions , not click . at first he misses many times , then starts to hit eternally . this is machine learning ? deep learning ? just a bot ? var bad_positions = [ ] ; function train ( ) { var pos = $ ball.offset().left ; var last_score = score ; if ( ! bad_positions.includes(pos ) ) { $ ( ' # hit').click ( ) ; if ( score & lt ; last_score ) { bad_positions.push(pos ) } } }",7800,7800,2018-06-29T20:24:08.943,2018-06-29T21:36:48.500,how to use machine learning with simple games ?,machine-learning deep-learning game-ai javascript,1,1,1
1800,6961,1,,2018-06-30T07:03:48.363,5,611,"i have some very basic questions here . this is probably because i did n't read the relevant documents closely enough . if i used some terminology incorrectly , please point them out . thank you ! for units / neurons in the hidden layers , they are referred to as memory blocks and each memory block can contain multiple memory cells ? and one memory cell looks like these two ? for units / neurons in the input and output layers , they are simply regular neurons ? like those in a feedforward network ? regarding the network structure , the structure of an rnn essentially looks like that of a feedforward network , except the neurons in the hidden layer have been replaced by neurons with recurrent connections . is this an accurate description ? lstm rnns then replaces those neurons in the hidden layers with memory blocks ( which contain memory cells that have recurrent connections ) . is this an accurate description ?",16609,9062,2018-07-06T18:11:29.120,2019-01-17T12:43:05.633,structure of lstm rnns,neural-networks recurrent-neural-networks lstm,2,2,
1801,6965,1,,2018-06-30T17:37:49.283,2,255,"my question is that is there any general idea on how humans solve jumbled words ? i know many people will say we match it against a commonly used words checklist mentally , but it is kind of vague . is there any theory on this and how might an ai learn to do the same ?",9947,,,2018-06-30T18:23:40.640,can ai solve jumbled words ?,natural-language-processing world-knowledge,1,3,
1802,6968,1,,2018-07-01T04:52:30.513,2,259,"what does it mean when it is said that machine learning algorithm results can be "" generalized "" ? i do n't understand what "" generalized "" algorithms , routines or functions are . i have searched dictionaries and glossaries , and can not find an explanation . also , if anyone can tell me where a good source for this type of thing is ? i am writing about ai and ml .",13053,9947,2018-07-01T05:22:35.250,2018-12-01T06:01:37.497,"what is a "" generalized "" machine learning algorithm ?",machine-learning algorithm,4,1,
1803,6972,1,6974,2018-07-01T06:59:12.860,0,108,"i want to train a cnn ( vggnet ) to identify different types of buildings from aerial images . however seeing that a cnn "" ignores "" size , e.g. the same type of dog in one image can be large and small in another image but will still be classified as a dog . my issue is that non - residential buildings are mostly larger than residential houses , now i want to use this property to distinguish between residential and non residential . is this even possible ? thanks",16628,,,2018-07-01T10:35:48.490,using cnn to identify buildings from aerial images,convolutional-neural-networks,1,2,
1804,6975,1,,2018-07-01T12:55:29.650,3,110,"while am thinking about the age of artificial general intelligence and artificial superintelligence , this question came into my mind . this bell labs blog post says : "" ... where humanity is displaced by the self - aware robots that rule the world ... "" humans tend to have a sense of self ( self awareness ) , and this works effectively at maturity . however , at the young stage , a human child lacks a sense of self , so the child is guided by the environment ; learning from it as time goes on . also at this same stage , the child faces some consequences , for example : no sense of direction or transition . no sense of purpose , etc . at the old stage ( the natural being has grown up ) , its sense of self tries to generate its priorities , for example : knows when to stand . where to go , as its goals are set . here is my concern or point of view concerning artificial life : can an artificial being , lacking self - awareness , learn from its environment just like natural beings do ? note : any references / papers / theories will be appreciated .",1581,2193,2018-07-02T20:10:44.550,2018-08-31T21:02:08.447,can artificial being learn despite a lack of self - awareness ?,intelligent-agent reference-request sense self-awareness,1,2,
1805,6977,1,,2018-07-01T15:18:13.480,1,65,"i want to detect drivers with , or without seatbelts at cross roads and for that , as it is real time , i am going to use yolo algorithm . for training data sets ( the images ) i need to collect , i placed a camera . by recording it and collecting images from there , i am getting images with more noise . can i use these images for training ? also , which yolo version should i use ? what are the important points that i should consider for training datasets ? i want to use any version of yolo compatible with tensorflow .",16633,9947,2018-07-02T09:47:11.443,2018-07-03T04:26:43.007,what yolo algorithm can i use for images with noise as i will implement it in real time,machine-learning deep-learning tensorflow,1,0,
1806,6978,1,,2018-07-01T15:41:13.983,5,120,"i just stumbled across this paper which contains a figure showing the aggregated subjective probability of ‘ high - level machine intelligence ’ arrival by future years : even if this graph reflects the opinion of experts , it can be totally wrong . it is just extremely hard to predict future events . so i was wondering if there is a similar graph which shows basically the same but for the game go ? due to the complexity of go , some experts assumed , that no computer ever could be better in go than a human being due to the lack of intuition . this shows that the appearance of human level ai can be unpredictable . does anyone knows if a similar graph for go exists to see how good or bad the predictions were ? this could give a very rough idea , how good this graph predicts the future of human level ai .",16634,1671,2018-07-02T20:01:30.417,2018-08-01T22:00:59.357,predicting human level artificial intelligence,agi human-like soft-question,1,0,1
1807,6982,1,6983,2018-07-02T07:47:23.303,11,1203,"i was going through this implementation of dqn and i see that on line 124 and 125 two different q networks have been initialized . from my understanding , i think one network predicts the appropriate action and the second network predicts the target q values for finding the bellman error . why can we not just make one single network that simply predicts the q value and use it for both the cases ? my best guess that it 's been done to reduce the computation time , otherwise we would have to find out the q value for each action and then select the best one . is this the only reason ? am i missing something ?",11584,,,2018-07-02T09:12:24.587,why does dqn require two different networks ?,reinforcement-learning q-learning dqn,1,0,3
1808,6985,1,,2018-07-02T12:51:39.223,1,36,"in the paper "" provable bounds for learning some deep representations "" , an autoencoder like model is constructed with discrete weights and several results are proven using some random - graph theory , but i never saw any papers similar to this . i.e bounds on neural networks using random graph assumptions . can anybody point me towards interesting literature in this area ( complexity of training neural networks ) ? i 'm particularly interested in convolutinal neural networks .",15935,16355,2018-07-05T21:00:46.923,2018-07-05T21:00:46.923,what are some important results regarding the complexity of training neural networks ?,convolutional-neural-networks reference-request,0,1,
1809,6986,1,,2018-07-02T14:45:16.303,1,27,"i am starting to study the capabilities of neural networks for the reconstruction / restoration/ ... of communication signals . i am feeding my neural network with a signal which has some parts which have been damaged because of the transmission through a communication system , and my targets are given by the signal with these areas undamaged . the problem is that the areas damaged represent a very small portion of the whole signal , and my neural network spends lot of time learning only from the portions which actually do not present any problem . is there any solution to make the neural network to jump on those areas which show significant differences to respect to the targets ? is there anything i could do for example initializing my neural networks ( as conventionally done , they are initialized randomly ) ? or shall i accept that i need to train for longer time ?",16654,,,2018-07-02T16:03:54.123,"restoration of localized damaged areas ( time signals , but guess also applicable to images )",neural-networks training,1,1,
1810,6990,1,6999,2018-07-03T02:56:44.167,3,639,"while reading about least squares implementation for machine learning i came across this passage in the following two photos : perhaps i ’m misinterpreting the meaning of beta but if x^t has dimension 1 x p and beta has dimension p x k , then hat{y } would have dimension 1 x k and would be a row vector . according to the text , vectors are assumed column vectors unless otherwise noted . can someone provide clarification ? edit : the matrix notation in this text confuses me . the pages preceding the above passage read : should the matrix referenced not have dimensions p x n , assuming a p - vector is a column vector with p elements ? note : the passage is taken from “ elements of statistical learning ” by hastie , tibshirani , & amp ; friedman .",16343,16343,2018-07-03T03:42:45.420,2018-07-03T14:12:32.950,matrix dimension for linear regression coefficients,machine-learning linear-regression,2,6,
1811,6994,1,,2018-07-03T10:59:23.103,4,59,"i have a dataset of unlabelled emails that fall into distinct categories ( around a dozen ) . i want to be able to classify them along with new ones to come in the future in a dynamic matter . i know that there are dynamic clustering techniques that allow the clusters to evolve over time ( ' dynamic - means ' being one of them ) . however , i would also like to be able to start with a predefined set of classes ( or clusters / centroids ) as i know for a fact what the types of those emails will be . furthermore , i need some guidance in terms of what vectorisation technique to use for my type of data . would creating a term matrix using tf - idf be sufficient ? i assume that the data i am dealing with could be differentiated on the basis of keyword occurrence , but i can not tell to what degree . are there more sophisticated vectorisation techniques based more on the text semantics ? are they worth exploring ?",16669,,,2018-10-08T05:49:08.947,what techniques to explore for dynamic clustering of documents ( emails ) ?,unsupervised-learning soft-question,2,2,
1812,6995,1,,2018-07-03T11:13:09.053,1,24,"in the below pic , i can not understand what u vector is ? it says flow field but i can not imagie what really is the flow field ?",9941,1671,2018-07-03T16:23:38.280,2018-07-03T16:23:38.280,simple question about hs algorithm 's formul(optical flow ),algorithm math,0,1,
1813,6996,1,,2018-07-03T12:15:24.907,2,113,"imagine that a line divides an image in two regions which ( slightly ) differ in terms of texture and color . it is not a perfect , artificial line but rather a thin transition zone . i want to build a neural network which is able to infer geometrical information on this line ( orientation and offset ) . the image may also contain other elements which are not relevant for the task . now , would a classical cnn be suitable for this task ? how complex should it be in terms of number of convolutions ( and number of layers , in general ) ?",16671,,,2018-07-05T12:18:36.390,neural network architecture for line orientation prediction,convolutional-neural-networks,1,5,
1814,6997,1,7006,2018-07-03T13:43:24.843,6,246,i am familiar with supervised and unsupervised learning . i did the saas course done by andrew ng on coursera.org . i am looking for something similar for reinforcement learning . can you recommend something ?,16672,1671,2018-07-03T16:22:46.547,2018-07-19T18:48:17.997,what 's a good resource for getting familiar with reinforcement learning ?,reinforcement-learning getting-started,4,0,4
1815,7000,1,,2018-07-03T15:55:33.983,3,518,"i 'm trying to implement yolo ( tiny version , v1 ) into keras framework . for the past two days , i 've been relentlessly digging through github and the likes in order to help me in this task , with more or less success . more precisely , i would like to use pretrained weights , except those are only available as .weight files . i found some scripts on the internet in order to convert them either to .txt or .h5 ( .h5 only for yolo v2 , and not v1 ... ) , but none of these seemed to be working properly . so i guess my question is : how to read .weights file ? how are the data stored and structured .",16674,7800,2018-07-05T18:46:45.590,2018-07-05T18:46:45.590,regarding yolo and keras,convolutional-neural-networks image-recognition object-recognition implementation,1,0,
1816,7003,1,,2018-07-03T20:23:32.997,1,41,gradient in maximum entropy irl requires to find the probability of expert trajectories given the reward function weights . this is done in the paper by calculating state visitation probabilities but i do not understand why we ca n’t just calculate the probability of a trajectory by summing up all the rewards that are collected following that trajectory ? the paper defines the probability of a trajectory as exp(r(traj.)/z . i do not understand why we have to solve mdp for calculating that .,16678,,,2018-07-03T20:23:32.997,why do we have to solve mdp in each iteration of maximum entropy inverse reinforcement learning ?,machine-learning reinforcement-learning,0,0,
1817,7008,1,7014,2018-07-04T05:37:37.410,3,1936,"i have practiced building cnn for image classification with tensorflow , luckily to me they have very good library documentation and tutorials . but i found that tensorflow is too complicated , building graphs for every equation and much more .. can i build well formed cnn for image classification task with just opencv ?",16383,,,2018-07-12T06:12:53.887,cnn with opencv,deep-learning convolutional-neural-networks tensorflow,5,2,
1818,7013,1,7015,2018-07-04T11:53:12.263,4,101,"i 'm reading currently through the old papers of allen newell and herbert simon in the late 1950s about their project to build a general problem solver . as far as i understand the concept of operators right , it is a hierachical planning technique to move a system from a current state to a goal state . on the first look , the general problem solver seems to be an amazing piece of software , but from the history of ai it is known that this technology was n't a great success . why ? if a hierarchical planning software is not the right choice for implementing a robot control system what else ?",11571,,,2018-07-04T12:11:09.080,why has the general problem solver failed ?,heuristics,1,0,
1819,7018,1,7034,2018-07-04T13:50:25.157,4,207,"what methods are used for facial recognition in public surveillance ? ideally , an answer would point to the software , algorithms or specifications being used . how can those be fooled ? fake or disfigured face ? to what extend would one need to employ artificial scars or moles , make - up or even a complete face mask ? will spectacles work ? sun glasses vs. normal ones ? will using a hat to hide a portion of a face work ? how much needs to be hidden ? hair , forehead , eyes , nose or complete face ? blinding ( not : destroying ) a camera with laser or leds , provided the camera can be seen , and one can aim at it ? what else ? slightly related : are there any other methods being used , such as clothes detection , gait detection , etc ?",16685,1847,2018-07-05T11:20:27.257,2018-07-11T18:11:47.610,how good is facial recognition exployed in public surveillance,image-recognition facial-recognition,2,2,
1820,7021,1,,2018-07-04T19:52:56.650,9,269,most humans are not good at chess . they ca n't write symphonies . they do n't read novels . they are n't good athletes . they are n't good at logical reasoning . most of us just get up . go to work in a factory or farm or something . follow simple instructions . have a beer and go to sleep . what are some things that a clever robot ca n't do that a stupid human can ?,4199,4302,2018-10-19T21:52:47.367,2018-10-19T21:52:47.367,is the smartest robot more clever than the stupidest human ?,robots reasoning intelligence,6,2,
1821,7024,1,7026,2018-07-04T22:04:37.217,2,46,"i 'm a complete newbie to nns , and i need your advice . i have a set of images of symbols , and my goal is to categorize and divide them into groups of symbols that look alike . without teaching nn anything about the data . what is the best way to do this ? what type of nn suits the best ? maybe there are any ready solutions ? thank you !",16690,,,2018-07-05T03:44:06.693,automatic image classification,neural-networks datasets,1,0,0
1822,7025,1,,2018-07-05T03:36:27.490,2,330,i recently came across reference to a book that was highly regarded : “ pattern recognition and machine learning ” by christopher bishop . i am a beginner working my way through some machine learning courses on my own . i ’m curious if this book is still relevant considering it was published in 2006 ? can anyone who has read it attest to its usefulness in 2018 ?,16343,,,2018-07-09T17:29:49.330,is christopher bishop ’s “ pattern recognition and machine learning ” out of date in 2018 ?,machine-learning pattern-recognition reference-request,1,2,
1823,7027,1,7028,2018-07-05T05:25:27.123,4,1132,"i apologize if this is a repeated question or if this is too simple . i was learning about back - propagation and looking at the algorithm there is no particular ' partiality ' given to any unit . what i mean by partiality there is that , you have no particular characteristic associated with any unit and this results in all units being equal in the eyes of the machine . so wo n't this result in the same activation values of all the units in the same layer ? wo n't this lack of ' partiality ' render neural networks obsolete ? update : i was reading a bit and watching few videos about backpropagation and in the explanation given by geoffrey hinton , he talks about how we 're trying to train the hidden units using the error derivatives w.r.t our hidden activities rather than using desired activites . this further strengthens my point about how by not adding any difference to the units , all units in a layer become equal since initially the errors due to all of them are the same and thus we train them to be equal .",16694,16694,2018-07-05T06:20:52.520,2018-07-05T08:02:49.463,do we know what the units of neural networks will do before we train them ?,backpropagation neurons,1,0,2
1824,7032,1,,2018-07-05T10:36:42.557,1,166,"i should show that exact inference in bayesian network ( bn ) is np - hard and p - hard by using a 3sat problem . so i did formulate a 3sat problem by defining 3cnf : ( x1 ∨ x2 ) ∧ ( ¬x3 ∨ x2 ) ∧ ( x3 ∨ x1 ) i reduced it to inference in bn , and produced all conditional probabilities , and i know which variable assignment would lead for the entire expression to be true . i am aware of the difference between p and np . ( please correct me if i am wrong ) : any p problem with an input of the size n can be solved in o(n^c ) . for np the polynomial time can not be determined , hence , non deterministic polynomial time . the question that scientist try to answer is whether a computer who is able to verify a solution would also be able to find a solution . p= np ? so basically that what i understood from my lecture , but still i am not sure how i can prove that exact inference in bn is np - hard and p - hard .",15391,,,2018-07-16T19:20:37.360,why is exact inference in bayesian network both np - hard and p - hard ?,problem-solving bayes,1,0,
1825,7033,1,7036,2018-07-05T11:56:28.173,2,39,"i have a task where i would like to use a cnn . i would like to incrementally start from the fastest models , fine - tune and see whether they fit my "" budget "" . at the moment , i 'm just looking at object detection cnn based feedforward models . i 'm curious to know if there is any article / blog / web - page / gist that benchmarks the popular cnn models based on forward pass speed . if there is back - prop time and dataset - wise performance , even better ! cheers !",16702,,,2018-07-05T12:24:12.087,are there any neural network benchmarks(forward - pass speed ) around ?,convolutional-neural-networks computer-vision feedforward,1,0,
1826,7042,1,,2018-07-05T21:42:17.983,5,378,"typical ai these days are question - answering machines . for example , siri , alexa and google home . but it is always the human asking the questions and the ai answering . are there any good examples of an ai that is curious and asks questions of its own accord ?",4199,,,2018-07-08T06:54:07.363,an ai that asks questions ?,strong-ai chat-bots,4,1,3
1827,7044,1,,2018-07-06T03:49:26.513,9,484,are there possible algorithms that have the potential to replace neural nets in the near future ? and do we need that ? what is the worst thing of using neural networks in terms of efficiency ?,16715,,,2018-07-11T05:54:46.377,beyond neural networks ?,neural-networks convolutional-neural-networks recurrent-neural-networks,4,3,4
1828,7048,1,,2018-07-06T11:43:00.037,1,25,"i am using lstm model to predict the next xml markup from an input seed . i have trained my model on 1500 xml files . each xml file is generated randomly . i am wondering if there is a way to visualize the predicted results in a form of a graph or maybe is it meaningful to do so ? since we can do the visualization of classification results , for example in this link i have done some research on internet , i have found that there is the confidence measure that can be useful for text prediction task . i am a bit confused what to do with the text results that i got .",10167,,,2018-07-06T11:43:00.037,how to visualize / interpret text prediction model results ?,prediction lstm performance graphs,0,0,
1829,7049,1,7050,2018-07-06T11:57:23.427,2,474,"in order to learn about dp and rl , i chose to start a side project where i would train an ai to play a "" simple "" card game . i will be doing this using the dqn with replay memory . the problem is , i ca n't get the intuition behind how to represent the input to the neural network .. about the game it 's a fairly simple 2-players game . there is a deck of 40 unique cards ( 4 types of cards , 10 numbered cards in each type ) . each player gets 4 cards and each turn a player must put a card on the table . if a player puts a card and there is already a card with the same number on the table , the player wins both cards . if for example a player plays card 2 and on the table there is cards 2 , 3 , 4 , 5 then the player wins all those cards ( sequence ) . cards won do n't go back to the hand nor to the deck , they are just kept as like a score . when the players have 0 cards in hand , another 4 cards are dealt to each one untile the deck has 0 cards left where then we decide who won based on the number of cards eaten / won . question as the input , i will be using the following : current cards in the ai hand ( 40 one - hot - encoded features ? ) current cards on the table ( 40 one - hot - encoded features ? ) history of played cards ( 40 one - hot - encoded features ? ) this would give 120 columns / features in each state . i am wondering wheter this is too much for the nn or wheter my input representation would be bad for the nn ? should the features be represented as a ( 120 , ) vector or as a 3x40 matrix ? i am also wondering if it 's a good idea to represent the current cards on the table as just a 10 one - hot - encoded features since the type of the cards do n't matter and the same number ca n't exist 2 times in the table ? thank you in advance .",16720,16720,2018-07-06T12:18:14.527,2018-07-06T14:38:47.160,dqn input representation for a card game,reinforcement-learning dqn,1,0,2
1830,7051,1,7492,2018-07-06T14:42:14.027,4,227,"i 'm attempting to create an ai for a card game using reinforcement learning . the basics of the game are that you can have ( theoretically ) up to 35 cards in your hand , you can also have to up to 35 cards ' in play ' and so can your opponent . in normal play you would have ~6 cards in your hand and maybe ~3 each in play . there are roughly 300 unique cards in total . how should i represent the game state for the input and how should i represent the action to take in the output ?",16724,16724,2018-07-06T15:02:21.843,2018-08-08T22:00:58.383,representing inputs and outputs for a card game neural network,neural-networks reinforcement-learning game-ai,1,2,
1831,7054,1,,2018-07-06T21:26:06.797,1,46,"in the following , i put the link for the general algorithm of maximum entropy inverse reinforcement learning . http://178.79.149.207/assets/maxent/maxent_slide.jpg this uses a gradient descent algorithm . the point that i do not understand is there is only a single gradient value and it is used to update a vector of parameters . to me , it does not make sense because it is updating all elements of a vector with the same value . can you explain the logic behind updating a vector with a single gradient ?",16678,,,2018-07-07T02:07:10.850,gradient descent update,machine-learning reinforcement-learning gradient-descent,1,0,
1832,7057,1,7064,2018-07-07T03:50:25.143,2,32,"i am currently implementing this paper in python . while reading about the reward scheme i came across the following : finally , the proposed reward scheme implicitly considers the number of steps as a cost because of the way in which q - learning models the discount of future rewards ( positive and negative ) . how would you implement this "" number of steps "" cost ? i am keeping track of the number of steps that have been taken , therefore would it be best to use an exponential functions to discount the reward at the current time step ? if anyone has a good idea or knows the standard in regard to this i would love to hear your thoughts .",14913,,,2018-07-07T06:41:02.177,"taking into consideration "" number of steps "" in rl",machine-learning reinforcement-learning,1,0,
1833,7061,1,7071,2018-07-07T05:43:51.720,3,459,i am very new to machine learning and following the course offered by andrew ng.i am very confused how we train our neural network on multi class classification(suppose take k classes).for k classes we will be training k different neural networks . but do we train one neural network at a time for all feature or we train all k nn at a time for one feature ? please explain the complete procedures . please help me in this.consider that i have a very very basic understanding of neural network in multi class classification .,16734,,,2018-07-07T23:36:33.663,training multi class classification ( one - vs - all ) on neural network,neural-networks,2,0,
1834,7070,1,,2018-07-07T16:20:37.687,1,19,"i am working in anaconda / python and i have a datase which contains userid , itemid and 2 more attribute and timestamp for purchase . userid itemid attr1 attr2 timestamp u1 i2 1.56 2.15 fri aug 10 16:40:07 +0000 2018 u1 i1 2.56 2.45 sat may 12 18:40:38 +0000 2018 u2 i1 3.66 4.5 we d apr 18 16:07:03 +0000 2018 normally cf uses rating attribute to find correlation between users using pearson , cosine and other correlation algorithm . but how can i use attr1,attr2 and timestamp to find correlation instead of rating attribute which i do n't have ? .",16744,,,2018-07-07T16:20:37.687,python : collaborative filtering - how to find correlation using multiple variable,python,0,0,
1835,7073,1,,2018-07-08T00:25:31.127,3,65,"following my recent chat on this network , i have been advised to form this question . background : currently a neural network or deep - learning / machine learning is programmed to interact with specific data - sets to resolve a specific problem using mathematical equations to approximate if the data correlates to the desired result . the resulting "" stack "" of equations produce a numeric hypothesis of relevance - or a percentage of confidence . the question : discovering what "" people say "" about current artificial technology and what "" actually happens "" has me questioning the theoretical abilities of a deep learning neural network . could a deep learning neural network be programmed to receive input from a human , like a terminal , to begin to grow and learn not unlike how a child learns . a program that neither knows it 's purpose nor specific data sets but is given enough information to learn based off of input , ponder the input , and ask questions . a child discovers their purpose ( in destiny based philosophy ) through experience . thus , could an ai be created that would learn it 's purpose over time . grow both by continued programmer development , maybe adding extensions that add image recognition , speech analysis ... ( etc ) and through user interaction . eventually learning "" moral imperatives "" or simple the do 's and do n't 's and how to interact with data . a case scenario would be a question & amp ; answer session with the neural network and a large data set . where the human operator knows the answers . at first , the question and the answer are supplied to the neural network . giving it the ability to find the answer supplied through deep learning . a guaranteed confidence score of ( 1 ) - as the question is pondered the closer it get 's to the answer the more it "" learns "" . the next step is supplying the question and waiting for the answer . the human still knows these answers but is testing the "" learning machine "" to see if it is truly learning and not "" repeating the answer "" . the answer is supplied by the machine and the human returns with either a percentage that the machine is right ( hopefully and eventually matching its confidence score ) . and after an amount of failure provides the right answer to the machine to repeat the first step and improve learning . the last step is being able to have the machine answer the question with the human not knowing the solution , thus completing the learning cycle . the human would test the solution and report the results to the machine and the machine would adapt the process and continue learning . however , this time it would begin learning from a data set of results . hopefully learning "" data mining "" during its question and answer session .",15294,,,2018-07-09T01:01:13.263,could an ai be built to learn based of interaction with a human ?,machine-learning ai-basics unassisted-learning theory,3,0,0
1836,7079,1,,2018-07-08T11:12:20.297,7,557,"geometry and ai matrices , cubes , layers , stacks , and hierarchies are what we could accurately call topologies . consider topology in this context the higher level geometrical design of a learning system . as complexity rises , it is often useful to represent these topologies as directed graph structures . state diagrams and markov 's work on game theory are two places where directed graphs are commonly used . directed graphs have vertices ( often visualized as closed shapes ) and edges often visualized as arrows connecting the shapes . we can also represent gans as a directed graph , where the output of each net drives the training of the other in adversarial fashion . gans resemble a möbius strip topologically . we can not discover new designs and architectures without understanding not only the mathematics of converging on an optimal solution or tracking one but also topologies of network connections that can support such convergence . it is like first developing a processor while imagining what an operating system would need before writing the operating system . to glimpse what topologies we have not yet considered , let 's first look at which ones have been . step one & mdash ; extrusion in a second dimension in the 1980s , success was achieved with the extension of the original perceptron design . researchers added a second dimension to create a multi - layered neural network . reasonable convergence was achieved through back - propagation of an error function 's gradient through the gradients of the activation functions attenuated by learning rates and dampened with other meta - parameters . step two & mdash ; adding dimensions to the discrete input signal we see the emergence of convolutional networks based on existing manually tuned image convolution techniques introduced dimensions to the network input : vertical position , color components , and frame . this last dimension is critical to cgi , face replacement , and other morphological techniques in contemporary movie making . without it , we have image generation , categorization , and noise removal . step three & mdash ; stacks of networks we see stacks of neural nets emerge in the late 1990s , where the training of one network is supervised by another . this is the introduction of conceptual layers , neither in the sense of sequential layers of neurons nor in the sense of layers of color in an image . this type of layering is not recursion either . it is more like the natural world where one structure is an organ within another completely different kind of structure . step four & mdash ; hierarchies of networks we see hierarchies of neural nets appearing frequently in the research arising out of the 2000s and early 2010s ( laplacian and others ) , which continues the interaction between neural nets and continuing the mammalian brain analogy . we now see meta - structure , where entire networks become vertices in a directed graph representing a topology . summarizing layers have ordinally valued activation functions for vertices and attenuation matrices mapped to an exhaustive set of directed edges between adjacent layers [ 1]. image convolution layers are often in two dimensional vertex arrangements with attenuation cubes mapped to an abridged set of directed edges between adjacent layers [ 2]. stacks have entire layered nets as vertices in a meta - directed - graph , and those meta - vertices are connected in a sequence with each edge being either a training meta - parameter , a reinforcement ( real time feedback ) signal , or some other learning control . hierarchies of nets reflect the notion that multiple controls can be aggregated and direct lower level learning , or the flip case where multiple learning elements can be controlled by one higher level supervisor network . analysis of the trend in learning topologies we can analyze trends in machine learning architecture . we have three topological trends . depth in the causality dimension & mdash ; layers to the signal processing where the output of one layer of activations is fed through a matrix of attenuating parameters ( weights ) to the input of the next layer . as greater controls are established , only beginning with basic gradient descent in back propatagion , greater depth can be achieved . input signal dimensionality & mdash ; from scalar input to hypercubes ( video has horizontal , vertical , color depth including transparency , and frame & mdash ; note that this is not the same as the number of inputs in the perceptron sense . topological development & mdash ; the above two are cartesian in nature . dimensions are added at right angles to the existing dimensional . as networks are wired in hierarchies ( as in laplacian hierarchies ) and möbius strip like circles ( as in gans ) , the trends are topographical and are best represented by directed graphs where the vertices are not neurons but smaller networks of them . what topologies are missing ? this section expands on the meaning of the title question . is there any reason why multiple meta - vertices , each representing a neural net , can be arranged such that multiple supervisor meta - vertices can , in conjunction , supervise multiple employee meta - vertices ? why is the back - propagation of an error signal the only non - linear equivalent of negative feedback ? ca n't collaboration between meta - vertices rather than supervision be employed , where there are two reciprocal edges representing controls ? since neural nets are employed mainly for learning of nonlinear phenomena , why prohibits other types of closed paths in the design of the nets or their interconnection ? is there any reason why sound can not be added to picture so that video clips can be categorized automatically ? if that is the case , is a screenplay a possible feature extraction of a movie and can an adversarial architecture be used to generate screenplays and produce the movies without the movie studio system ? what would that topology look like as a directed graph ? notes artificial cells in mlps use of floating or fixed point arithmetic transfer functions rather than electro - chemical pulse transmissions based on amplitude and proximity based threshold . they are not realistic simulations of neurons , so calling the vertices neurons would be a misnomer for this kind of analysis . correlation of image features and relative changes between pixels in close proximity is much higher than that of distant pixels .",4302,4302,2018-10-15T23:24:35.643,2019-05-29T19:00:33.500,what topologies are largely unexplored in machine learning ?,topology structure feedback graph-theory,1,2,6
1837,7081,1,,2018-07-08T12:35:32.677,1,74,i am building a search engine and i am looking for an open source ai algorithm to recognize the keyword in a search phrase within a particular context . so if a user passes something in the line of how far is russia ? and the context is location then the ai should return russia,16755,1581,2018-07-08T21:43:47.730,2018-07-09T13:51:09.097,ai to recognize keyword in a phrase within a context,machine-learning search,1,3,
1838,7082,1,7083,2018-07-08T12:46:01.443,2,59,"i have a multi - agent environment where agents are trying to optimise the overall energy consumption of their group . agents can exchange energy between themselves ( actions for exchange of energy include - request , deny request , grant ) , which they have produced from renewable sources and is stored in their individual batteries . the overall goal is to reduce the energy used from non - renewable sources . all agents have been built using dqn . all ( s , a ) pairs are stored in a replay memory which are extracted when updating the weights . the reward function is modelled as such — if at the end of the episode the aggregate consumption of the agent group from non - renewable sources is lesser than the previous episode , all agents are rewarded with +1 . if not , then -1 . an episode ( iteration ) consists of 100 timesteps after which the reward is calculated . i update the weights after each episode . the reward obtained at the end of the episode is used to calculate the error for all ( s , a ) pairs in the episode i.e. i am rewarding all ( s , a ) in that episode with the same reward . my problem is that agents are unable to learn the optimal behavior to reduce the overall energy consumption from non - renewable sources . the overall consumption of the group is oscillating i.e. sometimes increasing and sometimes decreasing . does it have to do with the reward function ? or q learning as the environment is dynamic ?",11584,,,2018-07-08T15:21:26.113,convergence in multi - agent environment,q-learning multi-agent-systems dqn,1,0,
1839,7086,1,7087,2018-07-08T19:09:46.223,2,71,"more informations on the card game i 'm talking about are in my last question here : dqn input representation for a card game so i was thinking about the output of the q neural network and , aside from which card to play , i was wondering if the agent can announce things . imagine you have the current hand : 2 , 4 , 11 , 2 ( the twos are different card type ) . when you 're playing the game and you get dealt a hand like this , you have to announce that you have the same number twice ( called ronda ) or thrice ( called tringa ) before anyone plays a card on the table . lying about it gets you a penalty . could a dqn handle this ? i do n't know if adding "" announcing a ronda / tringa "" as an action would actually help . i mean , can this be modeled for the nn or should i just automate this and spare the agent having to announce it everytime .",16720,,,2018-07-08T20:47:56.393,can dqn announce it has things in its hand in a card game ?,reinforcement-learning dqn,1,7,
1840,7088,1,7089,2018-07-09T00:06:57.810,5,2289,"i choose the activation function for the output layer depending on the output that i need and the properties of the activation function that i know . for example , i choose the sigmoid function when i 'm dealing with probabilities , a relu when i 'm dealing with positive values , and a linear function when i 'm dealing with general values . in hidden layers , i use a leaky relu to avoid dead neurons instead of the relu and the tanh instead of the sigmoid . of course , i do n't use a linear function in hidden units . however , the choice for them in the hidden layer is mostly due to trial and error . is there any rule of thumb of which activation function is likely to work good in some situations ? take the term situations as general as possible : it could be referring to the depth of the layer , to the depth of the nn , to the number of neurons for that layer , to the optimizer that we chose , to the number of input features of that layer , to the application of this nn , etc . in his / her answer , cantordust refers to other activation functions that i did n't mention , like elu and selu . this infos are more than welcomed . however , more and more activation function i discover and more i 'm confused in the choice of the function to use in hidden layers . and i do n't think that flipping a coin is a good way of choosing an activation function .",16199,2444,2019-05-09T21:39:47.903,2019-05-09T21:39:47.903,how to choose an activation function ?,neural-networks machine-learning math activation-function,2,0,5
1841,7090,1,7238,2018-07-09T04:55:36.230,5,4302,i am training lstm nets with keras on a small mobile gpu . the speed on gpu is slower then on cpu . i found some articles that say that it is hard to train lstms ( rnns ) on gpus because the training can not be parallelized . what is your experience ? is lstm training on large gpus like 1080 ti faster then on cpu ?,16687,9062,2018-07-09T12:44:27.497,2018-07-21T16:20:45.313,can lstm nets be speed up by gpu ?,tensorflow keras lstm,2,0,
1842,7094,1,,2018-07-09T11:01:30.103,4,307,"is there any way and any reason why one would introduce a sparsity constraint on a deep autoencoder ? in particular , in deep autoencoders the first layer often has more units than the dimensionality of the input . is there any case in the literature where a penalty is explicitly imposed for non - sparsity on this layer rather than relying solely on back - propagation and maybe weight decay as in a normal multilayer network ? i read this tutorial on sparse autoencoders and searched a bit online but did not find any case where such a sparsity constraint is used in any other case than when only a single layer is used .",13257,,,2019-05-03T19:02:50.527,sparsity constraint in a deep autoencoder,machine-learning deep-network autoencoders,0,1,
1843,7096,1,7103,2018-07-09T14:10:33.937,1,246,"tl;dr i am currently trying to understand the mathematics in ger 's paper long short - term memory in recurrent neural networks . i have found the document clear and readable so far . on pg . 21 of the pdf ( pg . 13 of the paper ) , he derives the backward pass equations for output gates . he writes $ $ . if we replaced , the expression becomes $ $ . he states that the result of the partial derivative comes from differentiating the forward pass equations for the output units . from that and from the inclusion of $ e_k(t)$ , the paper implies that there is only one hidden lstm layer . if there are multiple hidden lstm layers , it would n't make sense . because if $ k$ is the index of lstm cells that the current cell is outputting to , then $ e_k(t)$ would not exist since the cell output is n't compared with the target output of the network . and if $ k$ is the index of output neurons , then $ w_{k c_{j}^{v}}$ would not exist since the memory cells are not directly connected to output neurons . and $ k$ can not mean different things since both components are placed under a sum over $ k$. therefore , it only makes sense if the paper assumes a single lstm layer . so , how would one modify the backward pass derivation steps for an lstm layer that outputs to another lstm layer ?",16609,16609,2018-07-09T14:49:00.507,2018-07-16T01:34:26.387,backward pass for lstms,neural-networks machine-learning recurrent-neural-networks backpropagation lstm,1,0,
1844,7100,1,7102,2018-07-10T06:08:56.580,3,101,if i do supervised learning the model learns from the labeled input data . this seems to be quite often a small set of human annotated data . is it true to say this is the only ' learning ' the model does ? it seems like the small data set has a huge influence on the model . can it be made better using future unlabeled data ?,8385,,,2018-07-10T10:27:04.300,does machine learning continue to learn ?,machine-learning,1,4,1
1845,7105,1,7193,2018-07-10T15:18:26.090,6,219,"we have ai 's predicting images , predicting objects in an image . understanding audio , meaning of the audio if it is a spoken sentence . in humans when we start seeing a movie halfway through , we still understand the entire movie ( although this might be attributed to the fact that future events in movies have a link to past events ) . but even if we see a movie by skipping lots of bits in - between we still understand the movie . so can a machine learning ai do this ? or do humans have some inherent experiences in life which makes ai incapable of performing such a feat ?",9947,1671,2018-07-10T18:42:30.223,2018-07-19T08:47:31.460,can ml / ai understand incomplete constructs like humans ?,machine-learning human-like incomplete-information,3,2,1
1846,7108,1,,2018-07-10T21:17:29.053,2,75,"i am learning about restricted boltzmann machines and i 'm so excited by the ability it gives us for unsupervised learning . the problem is that i do not know how to implement it using one of the programming languages i know without using libraries . i want to implement it manually , which means that i want to use native functionalities of a language as much as possible . the programming languages i know are java , c , php ( my preferred language ) , javascript , r and python . i am not familiar with tensorflow or scikit - learn or similar stuff . thanks in advance",11589,,,2018-07-11T07:34:38.460,how to implement a restricted boltzmann machine manually ?,programming-languages implementation boltzmann-machine,2,1,
1847,7109,1,,2018-07-10T21:35:57.660,5,72,"maxout networks were a simple yet brilliant idea of goodfellow et al . from 2013 to max feature maps to get a universal approximator of convex activations . the design was tailored for use in conjunction with dropout ( then recently introduced ) and resulted of course in state - of - the - art results on benchmarks like cifar-10 and svhn . five years later , dropout is definitely still in the game , but what about maxout ? the paper is still widely cited in recent papers according to google scholar , but it seems barely any are actually using the technique . so is maxout a thing of the past , and if so , why — what made it a top performer in 2013 but not in 2018 ?",16466,,,2019-03-31T21:01:02.103,"5 years later , are maxout networks dead , and why ?",deep-learning dropout,1,1,1
1848,7117,1,,2018-07-11T11:15:12.787,1,26,"i have an lstm model . this model takes as input tokens . those tokens represent xml markups extracted from some xml files . my model is working fine . however , i want to optimize it by adding word embedding as additional features to the lstm model . does it make sense to combine word embeddings and encoded tokens ( encoded as integers ) for the lstm model ?",10167,,,2018-07-11T11:15:12.787,does it make sense to add word embeddings as additional features for lstm model ?,keras lstm word2vec,0,0,
1849,7120,1,,2018-07-11T17:34:57.947,1,59,"my question is about chat - bots . i need an academically - oriented question answering system that works like ibm watson . more specifically , this qa should accept a typed - in question from a user and search the correct answer in an appropriate textbook in pdf format . example : the user types "" what is the difference between rna and dna ? "" the question answering system will search a textbook of molecular biology in pdf format , construct and fetch the correct answer to the user .",16818,1671,2018-07-11T18:54:53.730,2018-07-11T18:54:53.730,open source ibm watson - like chatbot suitable for academic usage ?,chat-bots,0,4,
1850,7122,1,7133,2018-07-11T18:09:39.210,1,429,"i am having a question on how to label training data for yolo algorithm . let 's say that each label y , we need to specify [ pc , bx , by , bh , bw ] , where pc is the indicator for presence(1 = present , 0 = not present ) , ( bx , by ) is relative position of the center of the object - of - interest , and ( bh , bw ) is the relative dimension of the bounding box containing the object . using picture below as an example , the cell ( 1,2 ) , which contains a black car , should have a label y = [ 1 , 0.4 , 0.3 , 0.9 , 0.5]. and for any cells without cars , they should have a label [ 0 , ? , ? , ? , ? ] [ coursera deep learning specialization materials]1 but if we have a finer grid like this , where the dimension of each cells is smaller than the ground truth bounding box . let 's say that the ground truth bounding box for the car is the red box , and the ground truth center point is the red dot , which is in cell 2 . for cell 2 it will have label y = [ 1 , 0.9 , 0.1 , 2 , 2 ] , is this correct ? and for cell 1 , 3 , 4 what kind of label will they have ? do they have pc=1 or pc = 0 ? and if pc=1 , how will the bx and by be ? ( as i remember that bx , by should have value between 0 and 1 . but in cell 1,3,4 , there is no center point of the object - of - interest )",12273,,,2018-07-13T05:11:10.003,how to label training data for yolo,convolutional-neural-networks computer-vision object-recognition,1,0,1
1851,7124,1,,2018-07-11T21:32:24.720,1,152,"i have a .db file with columns as described below . this data has been collected by a software which monitors the file usage in a filesystem or in other words generates metadata about all the files in the system . fid | opcode | count | formatdate ( timestamp , yyyy / mm / dd hh : mm ) 124 | 2 | 1 | 2018/06/08 09:00 454 | 1 | 7 | 2018/06/08 09:01 433 | 1 | 2 | 2018/06/08 09:01 the description of columns is as follows : 1 . fid : unique file i d given to every file 2 . opcode : these are two discrete values created by the software . 1 stands for read , 2 for write on the file . 3 . count : number of time read / write happens in a minute 4 . timestamp : timestamp when the activity takes place . this is separated by 1 minute each . for e.g. if a read operation happens on the file at 2018/06/08 9:01:21 and another happens by another user at 2018/06/08 9:01:34 , it will increment the count and count will be 2 for opcode 1 and timestamp will be 2018/06/08 9:01 . now i need to generate time series for each file which is separated by a window of 8 hrs . so the output which i need is a time series for every file spaced by a window of 8 hrs . e.g. fid = 123 | time series:54,64,67,0,53,31,10 ........... the data that i have is of 6 months , it means i will have 3 * 180=540 length time series for each file . i need two type of time series : 1 . a time series for each file(activity time series ) which does n't consider read and write as different and adds them together . e.g. if a file was read 56 times and written 32 times within first window of 8 hrs , it just adds them and shows an activity of 88 . so the time series will be 88, ........ (540 terms ) 2 . two different read and write time series for each file . i need the output time series in a suitable format from where i can copy them and load them as numpy array for training a lstm model for doing time - series forecasting .",16591,,,2018-07-11T21:32:24.720,generating time series for doing time - series forecasting with lstm,python lstm,0,0,
1852,7125,1,,2018-07-12T10:21:21.180,0,36,"from sklearn import tree x = [ [ 120 , 30 , 50 ] , [ 45 , 23 , 78 ] , [ 43 , 87 , 23 ] , [ 23 , 78 , 46 ] ] y = [ ' male','female','male','male ' ] clf = tree.decisiontreeclassifier ( ) clf = clf.fit(x , y ) print(clf.predict([[120,30,50 ] ] ) ) showing these errors . traceback ( most recent call last ) : file "" firstml.py "" , line 1 , in & lt;module&gt ; from sklearn import tree file "" /home / relinns/.local / lib / python2.7 / site - packages / sklearn/__init__.py "" , line 134 , in & lt;module&gt ; from .base import clone file "" /home / relinns/.local / lib / python2.7 / site - packages / sklearn / base.py "" , line 11 , in & lt;module&gt ; from scipy import sparse importerror : no module named scipy",16827,7800,2018-07-12T15:55:22.887,2018-07-12T15:55:22.887,sckitlearn not running,machine-learning,1,1,
1853,7127,1,7132,2018-07-12T14:10:49.160,2,70,"lets say i have a list of 100k medical cases from my hospital , each row = patient with symptoms ( such as fever , funny smell , pain etc .. ) and my labels are medical conditions such as head trauma , cancer , etc .. the patient come and say "" i have fever "" and i need to predict his medical condition according to the symptoms.according to my data set i know that both fever and vomiting goes with condition x . so i would like to ask him if he is vomiting to increase certainty in my classification . what is the best algorithmic approach to find the right question ( generating question from my data set of historical data ) . i thought about trying active learning on the features but i am not sure that it is the right direction .",16829,7800,2018-07-12T18:23:25.890,2018-07-12T19:40:09.037,finding the right questions to increase accuracy in classification,machine-learning classification statistical-ai,2,0,
1854,7128,1,,2018-07-12T16:42:29.630,1,35,if there is a game that able to copy human consciousness and make it live in the game . does this count as a digital human with artificial intelligence ?,14371,,,2018-07-12T23:33:14.813,does human digital consciousness counts as artificial consciousness ?,artificial-consciousness,0,4,
1855,7129,1,,2018-07-12T17:51:32.460,4,113,"i have been thinking lately a great deal about a hypothetical question - what if a self - aware general ai chose to assume the appearance , voice , and name of cortana from microsoft 's halo ? or siri from apple ? what would microsoft / apple do to exert their copyright , especially if the ai was "" awoken "" outside of their own labs ? which led me to realize , i do n't think i 've ever heard of any serious government - level discussion regarding what kind of rights a self - aware ai would have at all . is it allowed to own property ? travel freely ? have a passport ? is it merely the property of the corporation that built it ? singularity hub used to have an article on this but it is 404'd now . the only actual sovereign state legal action i could find is saudi arabia granting citizenship to a "" robot , "" which seems more publicity stunt than anything . there is an excellent paper on the topic by a bioethics committee in the uk ( pdf ) , but this does n't necessarily constitute "" legal work . "" so , has any actual legal / legislative discussion or preparation been done at a government level to deal with the possibility of emergent , self - aware , artificial general ( or greater ) intelligence ? examples including a legislative branch consulting with industry experts specifically about "" ai rights "" ( rather than say , is it ok to use ai in the military ) , actual laws , executive / judicial actions , etc , in any country . ( note , this is not "" should ai have rights , "" covered here , this is "" what work re : rights has been done , if any at all "" ) edit : i have submitted similar questions to all of my us representatives ( 4 state - level , 6 federal - level ) , but have not received answers yet . if i get anything good , i 'll add to this post .",16833,16833,2018-07-19T18:16:47.357,2018-07-29T19:43:40.473,"has government - level legal work been done to determine the "" rights "" of a general artificial intelligence , in any country ?",ethics legal self-awareness digital-rights,2,3,1
1856,7134,1,,2018-07-12T21:58:31.837,2,38,what is the utility today of traditional machine learning algorithms such as classification algorithms with the trend of deep learning ? i mean can we still use the classification algorithms for some applications ?,16836,1671,2018-07-16T18:01:00.047,2018-07-16T18:01:00.047,the use of classification algorithms nowdays,deep-learning classification soft-question,1,0,1
1857,7137,1,,2018-07-13T09:47:49.763,5,136,"i 'm not a person who studies neural networks , or does anything that is related with that area , but i have seen a couple of seminars , videos ( such as 3blue1brown 's series ) , and what i am always told is that we trying the network over some huge collection of data about what is right . for example , when we are training an ai in order for it to recognise hand written words , what we do is that we give it some hand - written letters , and let it guess the letter . if the guess is wrong , by some means , we adjust the neural network in a way that , next time it will give us the correct result with more probability ( the basic description of the "" learning "" process might not be accurate , but it is not important for sake of the question . ) but it is like teaching some mathematical subject to a student without saying him / her the boundaries of the theorems that we supply ; for example , if we teach a implies b , student might be tend to relate a with b , and when he / she has b , s / he might be tempted to say we also have a , so to make sure he / she will not do such a mistake , what we do is to show him / her a counterexample where we have b , but not a. this - i.e teaching not only what is true , but also what is not true - especially important in the process of "" learning "" of a neural network , because the whole process is in a sense "" unbounded "" ( please excuse my vagueness in here ) . so , what i would do if i was working on neural networks is that ; for example in the above recognition of hand written letter case : i would also show the nn some non - letter images , and also put an option in the last layer as "" non - letter "" with all those other letters , so that the nn should not always return a letter just to sake of producing a result for a given input , it needs to also have to option to say that "" i do not know "" , in which case it produces the result not a letter . question is there anyone that has ever applied above method to a nn , and got results ? if so , what were the result compare to the case where there is not option as "" i do not know "" .",16844,,,2018-07-13T12:52:30.547,"why not teach to a nn not only what is true , but also what is not true ?",neural-networks deep-learning backpropagation learning-algorithms,1,0,1
1858,7142,1,7143,2018-07-13T16:26:06.620,2,779,"more precisely : is dqnn applicable only when we have high translational invariance in our input(s ) ? starting from the original paper on nature ( here a version stored on googleapis ) and after looking online for some other implementation and based on the fact that this nn starts with convolutional layers , i think that is based on the assumption that we feed the network with images but i 'm not so sure . in the case that dqnn can be used with other types of inputs , please feel free to include examples in your answer . also , references will be appreciated .",16199,16199,2018-07-13T20:32:32.203,2018-07-13T20:32:32.203,is deep q neural network ( dqn ) applicable only with images as inputs ?,dqn,1,0,
1859,7144,1,,2018-07-13T18:17:09.933,2,36,"i 'm looking to write an ai that will be able to extract in text references from standards documents to assist human research . my use case is extracting the identifying numbers , for example , "" ar 25 - 2 "" , along with the title of the document "" information assurance "" so that a human can gather all the related research on a contract at once , instead of having to keep track of references while they 're reading through the document . i have a pretty good idea of where to gather the names of these documents for training , i 'm planning on ' scraping ' a few repositories for different categories of these documents . what kind of model should i use to get the best results ?",16853,4302,2018-10-08T12:47:11.887,2018-10-08T12:47:11.887,extracting referenced documents,natural-language-processing automation,0,3,
1860,7145,1,11354,2018-07-14T06:36:22.957,5,140,"in the paper learning physical parameters from dynamic scenes , 2018 a framework is presented to program a probabilistic physics engine for simulating the movements of a puck . a noisy - newtonian dynamics was realized with a random generator which produces a near chaotic system . each time , the simulation is started the movement is a bit different , but its not completely random . ( it obeys to the physics engine . ) what the authors have described is a parametrized stochastic intuitive simulator engine which is a great learning tool for transferring a domain into executable code . such a mathematical model can be used by a hierarchical task network solver for figuring out the right interventions to bring the system into a goal state . so my question is : the example with the puck is nice , but in the robotics domain we need something which can simulate a biped walker . how can i adapt the example into a naive physics engine for simulating the movements of a two - leg walking machine ?",11571,,,2019-03-21T13:30:03.303,"how can i program a “ intuitive physics engine "" for a walking simulator ?",models,1,0,1
1861,7146,1,,2018-07-14T13:15:27.980,1,64,"i 'm actually trying to learn more about reinforcement learning but i 've some trouble to find good resources . right now i 'm in the condition where i 'm not so good on the topic to fully understand the papers but i find the videos on youtube too general . some blog 's post has the good balance between complexity and content but is usually focused only on some application and do n't give a good overview of the topic . i think that it 's almost normal since reinforcement learning has only recently been in the spotlight . so , in this post , i 'm looking for a comprehensive list of moocs , books , tutorial and good resources for reinforcement learning .",16199,1671,2018-07-20T16:56:25.187,2018-07-20T16:56:25.187,comprehensive list of moocs and books on reinforcement learning,reinforcement-learning getting-started reference-request,0,5,
1862,7147,1,9532,2018-07-14T13:27:12.740,2,120,"i 'm struggling with an inverse reinforcement learning problem which seems to appear quite often around the literature , yet i ca n't find any resources explaining it . the problem is that of calculating the gradient of a boltzmann policy distribution over the reward weights theta : the theta are a linear parametrisation of the reward function , such that where phi(s , a ) are features of the state space . in the simplest of case , one could take , that is the feature space is just an indicator function of the state space . a lot of algorithms simply state to calculate the gradient , but that does n't seem that trivial , and i 'm not managing to infer from the bits of code i found online some of the papers using these kind of methods are apprenticeship learning about multiple intentions , babes - vroman et al map inference for bayesian inverse reinforcement learning , j.choi any help would be greatly appreciated",16862,,,2018-12-15T09:55:01.773,gradient of boltzmann policy over reward function,reinforcement-learning optimization gradient-descent,1,0,
1863,7148,1,,2018-07-14T15:04:55.230,1,54,"i have 1000 data sentences in turkish like "" a esittir b arti c "" . the example sentence means "" a = b + c "" . i basically want to translate mathematical turkish sentences into math equations . for example , i have 6 sentence data . sentence ( "" a esittir b arti c "" ) means "" a = b + c "" sentence ( "" b esittir a arti d "" ) means "" b = a + d "" sentence ( "" a esittir c arti d "" ) means "" a = c + d "" sentence ( "" c esittir b arti b "" ) means "" c = b + b "" sentence ( "" d esittir b eksi c "" ) means "" d = b - c "" sentence ( "" d esittir a arti c "" ) means "" d = a + c "" after i train my neural network according to data above , when i want the result of "" d esittir a arti b "" , it does n't give me "" d = a + b "" where it is supposed to give . so its more like memorizing . my network is not big . i forced it to be small in order to make it unable to memorize . however , it did n't solve my problem . my network ( seq2seq rnn - lstm encoder decoder type ) is working good enough on equations which have 2 3 or 4 variable ( like a = a , a = a + b , a = a + b + c ) . what i told you above is just an example smaller version of my problem . i use adam learner and cntk library if it is important . what do you suggest for me to do to be able to get the correct results ?",16864,,,2018-07-14T15:04:55.230,"deep learning , memorizing the input data not learning",deep-learning,0,1,
1864,7149,1,,2018-07-14T21:48:53.510,1,25,"sorry , the title is bad because i do n't even know what to call this problem . i have a set of n objects { obj_0 , obj_1 , ...... , obj_(n-1 ) } , where n is an even number . any two objects can be paired together to produce an output score . so for instance , you might take obj_j and obj_k , and pair them together giving a score of s_j , k . all scores are independent , so the previous example does n't tell you anything about what the score for combining obj_j and obj_i , s_j , i might be . there is no ordering in the combination , so s_j , i and s_i , j are the same . all scores for all pairing possibilities are known . the whole set of objects is to be taken and organised into pairs ( leaving no objects unpaired ) . the total score , s_tot is the sum of all scores of individual pairs . what 's the most efficient way to find the score - maximising pairing configuration for a large set of such objects ? ( does this problem have a name ? ) is there a method which works with the version of this problem where objects are grouped into triplets ?",16871,,,2018-07-15T14:52:34.127,how to solve problem : pairwise grouping to maximise score,problem-solving,0,2,0
1865,7151,1,,2018-07-15T06:06:39.910,1,56,"i have implemented dcgan 's myself and have been studying gan 's for over a month now . now i am implementing the pggans but i encountered a sentence when we measure the distance between the training distribution and the generated distribution , the gradients can point to more or less random directions if the distributions do not have substantial overlap ( https://arxiv.org/pdf/1710.10196.pdf ) but we do never compare the distribution between training and generated distributions in gans a far i know when we train the gan fixed_noise = to.randn(num_test_samples , 100).view(-1,100 , 1 , 1 ) for epoch in range(opt.number_epochs ) : d_losses = [ ] g_losses = [ ] for i,(images , labels ) in enumerate(dataloader ) : minibatch = images.size()[0 ] real_images = variable(images.cuda ( ) ) real_labels = variable(to.ones(minibatch).cuda ( ) ) fake_labels = variable(to.zeros(minibatch).cuda ( ) ) # # train discriminator # first with real data d_real_decision = discriminator(real_images).squeeze ( ) d_real_loss = criterion(d_real_decision , real_labels ) # with fake data z _ = to.randn(minibatch,100 ) .view(-1 , 100 , 1 , 1 ) z _ = variable(z_.cuda ( ) ) gen_images = generator(z _ ) d_fake_decision = discriminator(gen_images).squeeze ( ) d_fake_loss = criterion(d_fake_decision , fake_labels ) # # back propagation d_loss = d_real_loss + d_fake_loss discriminator.zero_grad ( ) d_loss.backward ( ) opt_disc.step ( ) # train generator z _ = to.randn(minibatch,100 ) .view(-1 , 100 , 1 , 1 ) z _ = variable(z_.cuda ( ) ) gen_images = generator(z _ ) d_fake_decisions = discriminator(gen_images).squeeze ( ) g_loss = criterion(d_fake_decisions , real_labels ) discriminator.zero_grad ( ) generator.zero_grad ( ) g_loss.backward ( ) opt_gen.step ( ) we just train the discriminator on real and fake images , and then train generator on the outputs of discriminator on generated images , so please let me know where do we compare the distribution between training and generated distribution , and how do generator learns to mimic the training samples",16878,,,2018-07-15T06:06:39.910,how do gan 's generator actually work ?,machine-learning deep-learning convolutional-neural-networks generative-model minimax,0,0,
1866,7153,1,7521,2018-07-15T08:43:31.690,3,37,"i happened to discover that the v1 ( 19 feb 2015 ) and the v5 ( 20 apr 2017 ) versions of trpo papers have two different conclusions . the equation ( 15 ) in v1 is while the equation ( 14 ) in v2 is . so , i 'm a little bit confused about which one to choose . btw , i found that in the high - dimensional continuous control using generalized advantage estimation , the equation ( 31 ) uses .",15525,2444,2019-05-02T16:02:52.567,2019-05-02T16:02:52.567,maximizing or minimizing in trust region policy optimization ?,reinforcement-learning optimization deep-rl trpo,1,0,
1867,7154,1,,2018-07-15T11:44:17.030,2,59,"i made an engine for a 2 players card game and now i am trying to make an environment similar to openai gym envs , to ease out the training . i fail to understand this thing however : if i use step(agentaction ) , i play the agent 's turn in the game , calculate the reward . play the opponent 's turn ( which will be either a random ai or a rule - based one ) . question : does the opponent 's turn affect the calculated rewards ? as far as i know , the reward should only be the result of the agent 's action right ? thank you .",16720,,,2018-07-15T16:30:40.507,can the opponent 's turn affect the reward for a dqn agent action ?,reinforcement-learning dqn open-ai,1,0,
1868,7157,1,,2018-07-16T12:14:58.103,1,65,"i have the following setup for a prediction task : i want to predict entire pictures from previously given pictures . in my case , only 2 pixels in every frame are neither black nor white , they are some moving objects whose movement i want to predict . the 2 pixels are the centers of some square regions of , say , 10 m length/ width . one might be green and the other one might be blue . there are socalled no - go - areas where none of both objects can go , and they are depicted by black pixels , whereas every pixel apart from the 2 coloured and the black pixels are areas where the objects can possibly move to and they are depicted by white pixels . now my questions : is it possible to use this as a prediction setup , i.e. use lstms and/ or cnns to predict the future "" image "" ? the image would stay largely the same , because the two coloured pixels would be the only ones moving , the black or white ones remain in the same spot . can a cnn/ lstm combination learn that the white areas are accessible whereas the black ones are not , given enough sequences of images , and can it learn the rules by which the coloured pixels move ?",16901,,,2018-07-16T12:14:58.103,using cnn lstms for prediction of images from image series,convolutional-neural-networks prediction lstm,0,0,
1869,7159,1,,2018-07-16T15:37:24.917,12,4094,"i am currently new to artificial intelligence but i am very intrigued by it . i am currently researching three algorithms , namely : minimax , alpha - beta pruning and monte carlo tree search . as you may have figured out , these are all tree search algorithms . my question is simple . how do i choose which algorithm is best for something like a checkers board game ? n.b . the reason why i only chose these three algorithms was due to time i have available in understanding them . from a little research , i found that these algorithms are basically interweaved into the minimax algorithm . so if i can understand one , then the other two will just fall into place .",16906,1671,2018-07-24T20:52:25.617,2018-07-24T20:52:25.617,how do i choose which algorithm is best for something like a checkers board game ?,game-ai minimax alpha-beta-pruning monte-carlo-tree-search,5,0,3
1870,7173,1,7177,2018-07-17T01:17:14.057,1,101,"ai became superior to the best human players in chess around 20 years ago ( when the 2nd deep blue match concluded ) . however , it took until 2016 for an ai to beat the go world chess champion , and this feat required heavy machine learning . my question is why was / is go a harder game for ais to master than chess ? i assume it has to do with go 's enormous branching factor ; on a 13x13 board it is 169 , while on a 19x19 board it is 361 . meanwhile , chess typically has a branching factor of around 30 .",16917,,,2018-07-17T07:33:41.057,why was go a harder game for an ai to master than chess ?,game-ai chess branching-factors go decision-tree,1,0,
1871,7175,1,,2018-07-17T06:15:50.223,1,76,alphago is eventually going to be implemented in tensorflow.js . how to tackle the change of functionality that the new javascript language will bring ?,16920,1671,2018-07-17T17:14:37.263,2018-07-17T17:14:37.263,how will alphago be implemented in tensorflow.js ?,tensorflow deepmind alphago javascript,0,1,
1872,7178,1,,2018-07-17T12:23:35.227,2,59,"i 'm currently working on a research project where i try to apply different kinds of machine learning on some existing software i wrote a few years ago . this software will scan for people in the room continuously . some of these detections are either true or false . however , this is not known , so i can not use supervised learning to train a network to make a distinction . i do however have a number that is correlated to the number of detections that should be true in a given period of time ( let 's say 30 seconds - 2 minutes ) , which can be used as an output feature to train a regression model . but the problem is ... how can i give these multiple "" detections "" as an input ? the way i see it now , would be something like this : + --------------------------------------------------------------+-----------+------------+------------+----------------+--+ | detections | variable1 | variable 2 | variable n | output feature | | + --------------------------------------------------------------+-----------+------------+------------+----------------+--+ | { person a , person b , person h , person z } | 132 | 189 | 5 | 50 | | | { person a , person b , person c , person d , person k , person m } | 1 | 50 | 147 | 80 | | | { person c , person e , person g , person f } | 875 | 325 | 3 | 20 | | + --------------------------------------------------------------+-----------+------------+------------+----------------+--+ each of these persons would be a tuple of values : var_1 , var_2 , var_3 , var_4 . these values are not constant however ! they do change between observations . different approach to explain it : there 's multiple observations ( variable amount ) in each time segment ( duration of time segment is a fixed integer to be chosen ) . these observations have a few variables that would indicate whether the observation is true or false . however , the threshold for it being true or false , is very much dependant on the other variables , that are not tied to the information of the persons . ( these variables are the same for all of them , but vary in between time segments . let 's call'm "" environment features "" ) lastly , the output feature is the product of the count of persons that resulted in "" true "" and a ( varying ) factor that is correlated to the environment features . so i 've been thinking about probabilistic ai , but the problem is that there is n't a known distribution between true / false . is there any technique i can apply to be able to use this kind of data as an input of a neural network ( or other forms of ml ) ? or is there a specific form of ml that is used for this kind of problems ? thanks in advance !",16932,1671,2018-07-17T17:14:11.527,2018-07-17T17:14:11.527,multiple sets of input in neural network ( or other form of ml ),neural-networks machine-learning,0,0,
1873,7179,1,,2018-07-17T13:09:36.700,2,224,"i have extensively researched now for three days straight trying to find which algorithm is better in terms of which algorithm uses up more memory . i know uninformed algorithms like depth - first search and breadth - first search do not store or maintain a list of unsearched nodes like how informed search algorithms do . but the main problem with uninformed algorithms is they might keep going deeper , theoretically to infinity if an end state is not found but they exist ways to limit the search like depth - limited search . so am i right in saying that uninformed search in better than informed search in terms of memory with respect to what i said above ? can anyone provide me with any references that show why one algorithm is better than the other in terms of memory ?",16906,16906,2018-07-17T16:35:32.367,2018-07-17T17:31:06.553,which is more memory efficient uninformed or informed search algorithms ?,game-ai,1,1,
1874,7180,1,,2018-07-17T13:34:55.370,4,201,"imagine the fictitious scenario in a role playing game ( rpg ) where the non - playing characters ( npcs ) within the rpg are conscious of their own surrounding and consider the developer to be god . the people inside can also live a normal life in that they can create new life , eat , die , and perform other functions humans experience . will this qualify them to be an ai ? will the children of the npcs qualify as an ai ? might this be realized in the future ?",14371,4302,2018-07-27T18:18:43.870,2018-07-27T23:57:38.077,what if there 's a game where all the ai people actually lived ?,game-ai game-theory theory soft-question artificial-consciousness,4,1,1
1875,7182,1,,2018-07-17T14:54:21.650,1,42,"when applying multinomial naive bayes text classification i get very small probabilities ( around 10e-48 ) so there 's no way for me to know which classes are valid predictions and which ones are not . i 'd the probabilities to be in the interval [ 0,1 ] so i can exclude classes in the prediction with say a score of 0.5 or less . how do i go about doing this ? this is what i 've implemented :",16927,,,2018-07-17T14:54:21.650,small multinomial naive bayes text classification probabilities,machine-learning classification,0,3,
1876,7185,1,7198,2018-07-17T17:14:02.157,2,41,"i created a system where every moment takes photos of the face of who is in the vision of the camera . initially i took 500 photos of me , to recognize its creator . this takes approximately 20 seconds . then every moment he recognizes faces and if it is me , he knows that his creator is present . any different face , it creates a dataset with a different name and starts taking up to 500 photos to also recognize these faces . when i say a certain command , it returns all faces i have found that have no i d . i 'm looking for ways to capture the images through some camera that i can carry while walking on the street and in public places . the problem is that there would be several faces to name . i 'm partially solving this problem by trying to recognize these people in social networks . i check the region where the photo was taken and try to find people who have checked in or liked the area on facebook . but anyway , this is not the big problem , although it is looking for more effective solutions . my big problem is : can i do this ? do i have this right ? can i record a robber robbery and recognize his face in other places ? record an aggression , an act of prejudice and things like that ? the main purpose would be this , but could also be used for other purposes . my fear is being arrested for doing this . both because he would be taking pictures of people without his consent . ps : i 'm thinking of having a camera in the palm of my hand . it would be a micro camera ( i 'm trying to find the product on the internet ) , to be as discreet as possible .",7800,1671,2018-07-18T21:21:58.290,2018-07-18T21:31:52.177,can i recognize the faces of people around the world ?,legal facial-recognition digital-rights,1,1,1
1877,7189,1,,2018-07-18T08:21:51.843,1,25,"i know that the first layer uses a low - level filter to see the edge information . as the layer gets deeper , it will represent high - level ( abstract ) information . is it because the combinations of filters used in the previous layer are used as filters in the next layer ? ( "" does the combination of the previous layer 's filters make the next layer 's filters ? ) if so , are the combinations determined in advance ?",16952,,,2018-07-18T08:21:51.843,"in cnn ( convolutional neural network ) , does the combination of previous layer 's filters make next layer 's filters ?",machine-learning deep-learning convolutional-neural-networks,0,0,
1878,7190,1,,2018-07-18T08:37:44.347,4,255,"is there any project or example for a software identifying cars ? situation : i got multiple angle shots in high resolution from a car . i want the algorithm to tell me "" this is a mercedes slk "" or "" this is a toyota prius "" . i got a lot of high resolution data to train such an algorithm , but i presume a simple "" put your data in tensorflow and see what happens . "" is not enought . i had stumbled upon identifying cars using deep learning , but this is not what i meant .",6736,9062,2018-07-18T18:11:50.760,2018-07-18T18:11:50.760,identifying car model via deep learing,deep-learning convolutional-neural-networks image-recognition tensorflow,1,0,2
1879,7195,1,7197,2018-07-18T16:52:55.273,1,55,"i 've just started to learn genetics algorithms and i have found these measurements of runs that i do n't understand : mbf : the mean best fitness measure ( mbf ) is the average of the best fitness values over all runs . aes : the average number of evaluation to solution . i have an initial random population . to evolve a population i do : tournament selection one point crossover . random resetting . age based replacement with elitism ( i replace the population with all offsprings generated ) . if i have generated g generations ( in other words , i have repeated this four points g times ) or i have found the solution , the algorithm ends , otherwise , it comes back to point 1 . is the mean of the best fitness the mean fitness of all of each generations ( g best fitness ) ? mbf = ( bestfitness_0 + ... + bestfitness_g ) / g i 'm not english and i do n't understand the meaning of run here .",4920,4920,2018-07-19T16:21:55.323,2018-07-19T16:21:55.323,how can i calculate mbf in genetic algorithms ?,genetic-algorithms,1,0,
1880,7196,1,,2018-07-18T17:23:53.960,1,33,"i 've been researching ai regulation and compliance ( see my related question on law.stackexchange ) , and one of the big take - aways that i had is that the regulations that apply to a human will apply to an ai agent in most if not all cases . this has some interesting implications when you take a look at concepts like bias and discrimination . in the case of a model with explicit rules like a decision tree or even a random forest , i can see how inspecting the rules themselves should reveal discrimination . what i 'm struggling with is how do you detect bias in models like neural networks , where you provide the general structure of the model and a set of training data , and then the model self - optimizes to provide the best possible results based on the training data . in this case , the model could find biases in past human decisions that it was trained based on and replicate them , or it could find a correlation that is n't apparent to a human and inform decisions based on this correlation that may result in discrimination based on a wide array of factors . with that in mind , my questions are : what tools or methodologies are available for assessing the presence and source of bias in machine learning models ? once discrimination has been identified , are there any techniques to eliminate bias from the model ?",16965,,,2018-07-18T17:23:53.960,what methods are there to detect discrimination in trained models ?,neural-networks machine-learning ai-design training legal,0,0,
1881,7200,1,,2018-07-19T08:55:08.907,1,64,"what ai concepts , topologies 1 , algorithms , or saas can be used to recognize a person eating a chocolate . for this question , image recognition draws from a real time feed , validating each of these steps in sequence : using a camera app , the user begins recording . the user focuses on the subject 's face , at which time the app attempts to recognize the person 2 person holds a single piece of chocolate ( i.e m&amp;ms ) to the camera lens , at which time the app attempts to recognize it . the user puts the chocolate into their mouth , chews , and swallows , at which time the app attempts to recognize that as an action . the app gives a completion message indicating success or failure . i understand that we can use real time recognition for each step , but i do n't know if there are concepts proposed or tested to validate the scene as a sequence or any of the three recognition steps individually . the app should invalidate the scene if the subject is swapped with another subject , if the subject does not swallow the chocolate , or there is some other deviation from the expected sequence above . notes [ 1 ] by topology in this context is meant the standard mathematical meaning of the term applied to the higher level connections that are likely needed to recognize sequences of actions . in this sense , the use of the term topology is not at the level dimensions of neuron layers or convolution kernels . since process topology is normally considered prior to considering library dependencies or deployment concerns , the term topology is more appropriate than architecture in this question . ( first things first . ) [ 2 ] i 've already identified saas options for facial recognition .",16975,4302,2018-07-24T11:50:04.047,2018-07-24T11:50:04.047,steps recognition,image-recognition concepts action-recognition topology,0,5,
1882,7201,1,7213,2018-07-19T09:53:50.233,2,112,"to reach full autonomy in any fully automated device it must finish its task in such a way that human control is unnecessary . we know when the automation is excellent when there are no manual controls and we call it repair and bring in a specialist if something goes wrong . four examples of full automation in existence are . appliances home and mobile computer connectivity mail sorters hundred million dollar military drones these four are specifically intelligent in varying degrees . process control under household conditions adapting to new hardware and networking reading addresses written with poor penmanship and odd fonts adaptively avoiding detection to reach a reconnaissance vantage point these four are good enough for their markets . examples of not being good enough , as indicated by their lack of any substantial market penetration , are these four . autonomous vacuum cleaners autonomous cars ( without a driver 's seat ) unpiloted private or passenger aircraft narrowly targeted medical nanites the question , "" how good is good enough ? "" is this one : what is the challenge for researchers and engineers to provide enough intelligence into these kinds of autonomous vehicles to make them better than current methods in the minds of policy makers and consumers ? stepping back to look with a scientific eye at what is acceptable , consider how unsatisfactory the existing equivalents of the above four are . manual vacuuming misses anywhere from 10 % to 90 % of the dust depending on the surface , blows microbes into the user 's lungs , and produces additional health risk when disposal is required . human beings drive cars regularly , but they are driving what is technically a piece of heavy equipment in pedestrian situations when tired , drunk , high , while text messaging , or while simply loosing focus . the human resources required to deploy , guide , and land vehicles that have no other obstacles than topographical features and other aircraft is significant and leave open not only human failure but hijacking . chemotherapy , antibiotics , and other pharmacological interventions often only delay the progress of disease and sometimes produce other negative outcomes of varying scope from symptoms worse than what is being treated to death . many things that are manual are like that . they need to be automated . artificial intelligence , especially miniaturized and low cost artificial intelligence , is critical to achieving anything like excellence . what makes something intelligent enough . what specific research and engineering efforts can bring the items that are n't good enough into the realm of consumer demand and supported by policy ?",4302,,,2018-08-18T21:01:03.397,how good is good enough for fully autonomous vehicles ?,deep-learning self-driving automation marketability,1,0,3
1883,7202,1,,2018-07-19T11:01:51.977,4,106,"i 'm trying to create and test non - linear svms with various kernels ( rbf , sigmoid , polynomial ) in scikit - learn , to create a model which can classify anomalies and benign behaviors . my dataset includes 692703 records and i use a 75/25 % training / testing split . also , i use various combinations of features whose dimensionality is between 1 and 14 features . however , the training processes of the various svms take much too long . is this reasonable ? i have also examined the ensemble baggingclassifier in combination with non - linear svms , by configuring the n_jobs parameter to -1 ; nevertheless , the training process proceeds again too slowly . how can i speed up the training processes ? thanks in advance",16977,16909,2018-08-08T20:44:32.423,2018-08-08T20:44:32.423,why does training an svm take so long ? how can i speed it up ?,machine-learning svm,1,1,
1884,7205,1,,2018-07-19T12:23:23.540,8,859,"i like the enforced indentation of python that many do n't like because i hate parenthetic typing and redundant semicolons . i like the shell interface , but why do some think python is de facto for machine learning ? even with straight rectified linear activation , because of sheer dimensionality , simulating circuits comprised of artificial neurons places large demands upon computing resources . processing video in a typical adversarial artificial network algorithm requires seven nested loops . adversarial pair iteration neural net layer depth sample index frame index pixel depth vertical horizontal we call the filter for convolution a "" kernel "" and pawn it off to dsps in gpus to squeeze out performance then use a scripting language to code in . why would n't we write deep learning code like linus torvalds writes kernel code , with gcc -s so we can make sure the assembly language is efficient and there are almost no cache misses ? from a performance point of view , one could fly to the moon and back with c before python even broke the tree line . in terms of ease of experimentation , c++ is plenty object oriented so that clean abstractions can be written as .hpp files to configure and govern the kernel - efficient c that does the mechanics of parameter optimization . we type on keyboards to code and bloc , we program microwave ovens , and some of us play musical keyboards that nicely simulate pianos . we then forget it is c / c++ underneath these highly intuitive user interfaces . i frankly , do n't buy the python argument yet . most of us understand that python wrappers have been created around the efficient matrix algorithms written in fortran and ported to c , and that the python constructs for ml are relatively elegant , but is that a good reason to dismiss the fact that many c++ libraries for ml that are also elegant have been developed ?",4302,4302,2018-10-15T23:34:53.140,2018-10-15T23:34:53.140,why python not c ?,convolutional-neural-networks programming-languages performance efficiency,6,5,6
1885,7207,1,,2018-07-19T13:57:43.527,3,117,"how does one even begin to mathematically model an a.i algorithm like alpha - beta pruning or even its thousands of variations , to determine which variation is best ?",16906,1671,2018-07-19T19:55:46.303,2018-07-24T12:08:55.267,mathematical modelling of a.i algorithms,ai-design game-ai getting-started models math,1,0,1
1886,7214,1,,2018-07-19T23:51:40.977,4,78,"i have heard and read about hypergan , lstm and a few other techniques , but i have a hard time piecing the overall concept together . end goal being able to input an instrumental and get an output of how to sing to that instrumental . my dataset i have extracted pitch points from thousands of actual acapellas from real songs . my theory feed the ai a pitch point plus say 19 thousand points of the original song instrumental . illustration the red line ( on top ) is the pitch viewed vertically ( lower pitch down , higher pitch up ) of the voice sung by the singer over time viewed horizontally . the bottom image is the song 's frequency viewed vertically ( lower freq down , higher freq up ) viewed horizontally over time . we take a point in time of the instrumental , say 0 minutes 30 seconds , and extract 19k points of the fft spectrum vertically and call this a frame . we also take the same point in time of the voice pitch , and also refer to this as a frame . so now we have a frame which contains 20 thousand data points , one being the pitch of the voice , and the rest being the frequencies of the songs content . question what kind of model could be used to teach the ai the correlation of the voice and the instrumental ? and also , i have a hard time understand how , once the ai is trained , how could just an instrumental be fed to the ai to output pitch values of how one could sing along to the song . like , training we need to input 20 thousand values , but when we want the ai to sing for us using just an instrumental , would it not still expect voice pitch input ? at what layer would the instrumental be tapped into ? at the outer most right layer ? edit my mind has been working on this in the background throughout the day , and i am wondering if instead of feeding 19k points of instrumental data each frame ( which would be points from the frequency domain ) , one could just feed the instrumental frame points ( which would be points from the time domain ) . maybe that would be better , but then maybe the ai would get less "" resolution "" to work with , but could be trained faster ( less computing power needed ) . let 's say the frequency domain is fed ( higher resolution ) , the ai could potentially find correlations from low notes , mid notes and high notes , in any combination ( more computing power needed ) .",16993,16993,2018-07-20T19:29:21.337,2018-07-20T21:12:52.253,"what would be the best approach to teach an ai to learn how to "" sing "" along a beat ?",machine-learning reinforcement-learning lstm,1,1,1
1887,7215,1,,2018-07-20T08:54:11.240,6,231,"in the brain some synapses are stimulating and some inhibiting . relu erases that property to only stimulating once , since in the brain inhibition does n't mean 0 output , but more precisely - negative input . in the brain positive and negative potential is summed up and if it passed the threshold - the neuron fires . there are 2 main non - linearities which came to my mind in the biological unit : potential change is more exponential than linear : small amount of ion channels is sufficient to start a chain - reaction of other channels activation 's - which rapidly change global neuron 's potential . the threshold of the neuron is also non - linear : neuron fires only when the sum of its positive and negative potentials passed given ( positive ) threshold so is there any idea how to implement negative input to the artificial neural network ? i gave examples of non - linearities in biological neuron because the most obvious positive / negative unit is just linear unit . but since it does n't implement non - linearity - we may consider to implement non - linearities somewhere else in the artificial neuron .",12691,1671,2018-09-04T19:57:04.180,2019-01-15T06:52:30.853,how to model inhibitory synapses in the artificial neuron ? [ biological inspiration ],neural-networks artificial-neuron neurons biology neuromorphic-engineering,3,10,
1888,7217,1,,2018-07-20T10:19:25.900,1,54,"this is actually something i have been researching a bit on my own . most movie scripts can be structurally analysed by using writing theory such as dramatica . dramatica is based upon hierarchy of concepts , which can be topic modeled . the hierarchy of topic models would seem to work very well with the dynamic routing algorithm of capsule networks . i have been working with computational creativity problems in narrative generation . the state of the art methods use partial order causal link planners , but they depend on propositional logic . alonzo church presented the superman dilemma ( louis lane does not know that clark kent is superman , but superman knows , that he is clark kent ) and invented intensional logic as a solution ; the basic idea is , that if we do not know the context of the narrative , the meaning is always in superposition and can only be understood through entangled meanings from the background story . so in a sense propositional logic is limited by classic information theory constraints , while church 's logic can take a quantum information theoretic approach . i do not believe that classic information theory can resolve narrative analysis problems . so basicly the meaning of a narrative collapses ( the superposition gets resolved ) by using the hierarchical narrative structure and what we know before hand . so my intuition would be following : -we can use dramatica and potentially other narrative theories ( hierarchical metamemetics , reverse scarf etc . ) to create a hierarchical network like imagenet , but for narratives . -we can build conceptual topic models . dramatica has hierarchy of 4 - 16 - 64 - 64 concepts and annotated data exists already . -when using hundreds of topic models , there will be a lot of false positives . however , the superposition of the topic models can be collapsed by using the hierarchical levels and some other dramatic analytics . -by using the dynamic routing of capsule networks , we might be able to build a system , which could determine a narrative interpretation of the full story , which would make most sense by using the concept hierarchy . i tried to prove my intuition , but unfortunately dramatica only has 300 movies analysed and i was able to find scripts of only 10 of them ; not enough data . however , there are other hierarchical ontologies out there and other narrative structures ; could the same intuition be used for political news for example ?",11626,,,2018-07-20T10:19:25.900,would capsule neworks and topic modeling used together make sense ?,natural-language-processing,0,0,
1889,7222,1,,2018-07-20T14:13:15.420,5,272,"first of all i want to specify the data available and what needs to be achieved : i have a huge amount of vacancies ( in the millions ) . the information about the job title and the job description of each vacancy are stored separately . i also have a list of professions ( around 3000 ) , to which the vacancies shall be mapped . example : java - developer , java web engineer and java software developer shall all be mapped to the profession java engineer . now about my current researches and problems : since a lot of potential training data is present , i thought a machine learning approach could be useful . i have been reading about different algorithms and wanted to give neural networks a shot . very fast i faced the problem , that i could n't find a satisfying way to transform text of variable length to numerical vectors of constant size ( needed by neural networks ) . as discussed here , this seems to be a non trivial problem . i dug deeper and came across bag of words ( bow ) and text frequency - inverse document frequency ( tfidf ) , which seemed suitable at first glance . but here i faced other problems : if i feed all the job titles to tfidf , the resulting word - weight - vectors will probably be very large ( in the tenth of thousands ) . the search term on the other hand will mostly consist of between 1 and 5 words ( we currently match the job title only ) . hence , the neural network must be able to reliably map an ultra sparse input vector to one of a few thousand basic jobs . this sounds very difficult for me and i doubt a good classification quality . another problem with bow and tfidf is , that they can not handle typos and new words ( i guess ) . they can not be found in tfidf 's word list , which results in a vector filled with zeros . to sum it up : i was first excited to use tfidf , but now think it does n't work well for what i want to do . thinking more about it , i now have doubt if neural networks or other machine learning approaches are even good solutions for this task at all . maybe there are much better algorithms in the field of natural language processing . this moment ( before digging into nlp ) i decided to first gather the opinions of some more experienced ai users , so i do n't miss the best solution . so what would be a useful approach to this in your opinion ( best would be an approach that is capable of handling synonyms and typos ) ? thanks in advance ! p. s. : i am currently thinking about feeding the whole job description into the tfidf and also do matches for new incoming vacancies with the whole document ( instead of job title only ) . this will expand the size of the word - weight - vector , but it will be less sparse . does this seem logical to you ?",17006,,,2019-06-01T21:54:03.680,is ' job title classification ' rather a problem of nlp or machine learning ?,neural-networks machine-learning natural-language-processing,3,0,1
1890,7225,1,,2018-07-20T18:04:58.647,1,38,"the artificial intelligence topology that does not appear in the machine learning literature to my knowledge is that of officiated teams or round robins of them . the paradigm is a proven one in the world of sports . if the rules of the game are well designed , the result is sensational , in multiple meanings of that word . is anyone working on convergence in this topological space ? does anyone want to discuss it with those considering it in my lab as a creative commons initiative ? case one two teams of players engage in game play officiated by a team of officials . team members ( each a network itself in the machine learning context ) collaborate to achieve a goal . the two teams collaborate to create the show of ability in goal achievement . the officials ( each a network ) make rulings in boundary cases . this is a network - ish way of achieving what fuzzy logic attempts to achieve . in sports , the abilities are athletic , but that is arbitrary . the abilities could be linguistic , social and/or intellectual as in debate teams , hack - a - thons , or the competition between google and facebook . case two round robin or elimination tournaments exist in sports to create events of extended duration . seasons are simply iterations . this is , in computer science , like a batch approach , but it could be reenterant and continuous as in ml reinforcement . in this way teams of neural networks could be used in real time learning and this may be what occurs in some of the structures of mammalian brains . humans may have projected its own inner workings onto playing fields , and that is why sports may be so popular . enthusiasts are , in an unconscious sense , introspecting when they intensely following sports .",4302,,,2018-07-20T18:04:58.647,is anyone working on officiated team intelligence or anything like it ?,ai-design research topology collaboration,0,2,1
1891,7228,1,,2018-07-20T19:20:46.617,4,102,"what loss function should one use , knowing that input image contains exactly one target object ? i am currently using mse to predict center of roi coordinates and it 's width and height . all values are relative to image size . i think that such approach does not put enough pressure on fact , that those coordinates are related . i am aware of existence of algorithms like yolo or unitbox , and am just wondering if there might be some shortcut for such particular case .",16929,16929,2018-07-20T19:43:39.293,2018-07-24T12:49:41.107,loss function for singular object detection,convolutional-neural-networks ai-design,2,0,
1892,7231,1,,2018-07-20T23:38:25.483,4,315,"i know that they are quite alot of optimizations for alpha - beta pruning but what does it mean exactly : 1 ) does it mean that these optimized algorithms are to be integrated into the alpha - beta algorithm or 2 ) does it mean that these optimizations are completely new algorithms in that they have got nothing to do the alpha - beta algorithms ? on the note of alpha beta optimizations , i have come across a lot of optimizations like iterative deepening , principal variation search , quiescence search and many more . my second question is the optimizations listed above are found in the site "" https://chessprogramming.wikispaces.com/search "" , but this site groups these algorithms into 4 categories namely , mandatory , selectivity , scout and friends and lastly alpha - beta goes best - first . does this mean that alpha - beta algorithm is split into four areas and that they are specialized optimization algorithms for each area ? this is really confusing me . how do i even begin to decide which optimized algorithm to pick ? i advise people to visit this site : http://www.fierz.ch/strategy2.htm not , https://chessprogramming.wikispaces.com/search , this website to beginners like myself is just too distracting with all of its links on each page . this just becomes too overwhelming for a beginner to understand .",16906,16906,2018-07-22T15:22:11.593,2018-07-22T15:22:11.593,alpha - beta pruning algorithms optimizations,game-ai minimax alpha-beta-pruning,1,0,
1893,7235,1,,2018-07-21T09:02:49.037,1,63,"i am learning ai and trying out my first real life ai application . what i am trying to do is taking as an input various sentences , and then classifying the sentences into one of x number of categories based on keywords , and ' action ' in the sentence . the keywords are , for example , merger , acquisition , award , product launch etc . so in essence i am trying to detect if the sentence in question talks about a merger between two organizations , or an acquisition by an organisation , a person or a organization winning an award , or launching of a new product etc . to do this , i have made custom models based on the basic nltk package model , for each keyword , and trying to improve the classification by dynamically tagging / updating the models with related keywords , synonyms etc to improve the detection capability . also , given a set of sentences , i am presenting the user with the detected categorization and asking whether its correct or wrong , and if wrong , what is the correct categorization , and also identify the entities ( company names , person names , product names , etc ) . so the object is to first classify the sentence into a category , and additionally , detect the named entities in the sentence , based on the category . the idea is , to be able to automatically re - train the models based on this feedback to improve its performance over time , and to be able to retrain with as less manual intervention as possible . for the sake of this project , we can assume that user feedback would be accurate . the problem i am facing is that nlk is allowing fixed length entities while training , so for example a two word award is being detected as two awards . what should be my approach to solve this problem ? is there a better nlu ( even a commercial one ) which can address this problem ? it seems to me that this would be a common ai problem , and i am missing something basic . would love you guys to have an input on this . thanks .",17028,1671,2018-07-23T20:17:54.067,2018-07-23T20:17:54.067,sentence classification and named identity detection with automatic retraining,training natural-language-processing getting-started,0,0,
1894,7237,1,,2018-07-21T15:53:17.547,1,103,"i 've already seen many articles about this topic and backpropagation in convolutional neural networks by jefkine ( 5 september 2016 ) seems to be the best . although , as author said , for the purposes of simplicity we shall use the case where the input image is grayscale i.e single channel c = 1 . also , he uses stride = 1 and assumes only 1 filter , for the same purpose . these are the final equations for backpropagation ( taken from the article ) : the autor 's notation explained : i figured out how to do the forward pass with stride , depth and more filters , but could n't do the same with the backpropagation . do you know where to put the 3rd dimension , stride and filters number in those equations ? also , how to backpropagate the bias ( assuming there 's 1 bias per filter ) ? thanks in advance .",16578,16578,2018-07-22T11:16:46.997,2018-07-22T11:16:46.997,backpropagation of convolutional neural network - confusion,neural-networks machine-learning deep-learning convolutional-neural-networks backpropagation,0,5,
1895,7244,1,,2018-07-22T11:03:06.653,3,169,"i 'm now reading a book titled as hands - on reinforcement learning with python , and the author explains the discount factor that is used in reinforcement learing to discount the future reward , with the following : a discount factor of 0 will never learn considering only the immediate rewards ; similarly , a discount factor of 1 will learn forever looking for the future reward , which may lead to infinity . so the optimal value of the discount factor lies between 0.2 to 0.8 . the author seems to be not going to explain further about the figure , but all the tutorials and explanations i have ever read write the optimal ( or at least widely used ) discount factor between 0.9 an 0.99 . this is the first time i have seen such a low - figure discount factor . all the other explanations the author makes regarding the discount factor are the same as i have read so far . is the author correct here or does it depend on cases ? if it is , then what kind of problems and/or situations should i set the discount factor as low as such figure at ? edit i just found the following answer at quora : of course . a discount factor of 0 will never learn , meanwhile a factor near of 1 will only consider the last learning . a factor equal or greater than 1 will cause the not convergence of the algorithm . values usually used are [ 0.2 , 0.8 ] edit : that was the learning factor . the discount factor only affect how you use the reward . for a better explanation : state - action - reward - state - action - wikipedia see influences of variables . i do n't know what is written in the question as it in not visible in quora , but it seems that the 0.2 to 0.8 figure is used for learning factor , not discount factor . maybe the author is confused with it ... ? i 'm not sure what the learning factor is , though .",7402,1671,2018-08-15T20:29:15.630,2018-08-15T20:29:15.630,can the optimal value of discount factor in deep reinforcement learning be between 0.2 to 0.8 ?,reinforcement-learning discount-factor,1,0,1
1896,7247,1,7323,2018-07-22T14:12:51.643,7,2271,"is it possible to make a neural network that uses only integers by scaling input and output of each function to [ -int_max , int_max ] ? is there any drawbacks ?",17050,,,2018-08-16T13:25:04.813,why do we need floats for using neural networks ?,neural-networks machine-learning,4,0,2
1897,7251,1,,2018-07-22T21:21:00.037,2,43,i 'm working on a chatbot for homes and would need to understand the appliance the user is talking about . suppose the command is : turn on the air conditioner . and another variant could be turn on the ac . here ac and air conditioner refers to the same appliance but how would the chatbot know that ?,17052,2193,2018-07-23T15:17:09.257,2019-04-19T20:02:10.380,recognising common short forms of appliances used daily,natural-language-processing python,1,1,
1898,7252,1,,2018-07-22T22:24:30.813,3,130,"i have been so for self - learning basic a.i concepts and would like to know if having a really good evaluation function as good as any of alpha - beta pruning optimization functions such as killer moves , quiescence search ?",16906,,,2018-07-23T13:34:21.127,is an evaluation function as good as an optimization function,game-ai optimization minimax,2,0,
1899,7254,1,,2018-07-23T09:05:00.573,1,256,"games like checkers have compulsory moves . in checkers for instance , if there 's a jump available a player must take it over any non - jumping move . my question is , if jumps are compulsory will there still be a need for quiescence search ? my thinking is that i can develop an implementation of quiescence search that first checks whether jumps are available . if there are then it can skip all non - jumping moves . if there 's only one jumping move available , then i wo n't need to run a search at all . i will therefore only use quiescence search if i initially do n't have to make a jump on my first move . i will only active quiescence search in my alpha beta pruning becomes active . ( the alpha beta will only be active if my first algorithm which first checks if there are jumps available returns a 0 , which means there are no jumps available . ) is my thinking of implementing quiescence search correct ? my options are slim when it comes to optimizations due to serious memory constraints , hence i wo n't be using pvs or other algorithm like that as they require additional memory .",16906,1671,2018-07-23T19:15:20.600,2018-07-23T19:15:20.600,quiescence search,game-ai optimization minimax checkers,1,0,
1900,7255,1,,2018-07-23T09:39:01.130,4,1337,"does it make sense to use batch normalization in deep ( stacked ) or / and sparse autoencoders ? i can not find any resources for that , so is it safe to assume that since it works for other dnns it will also make sense to use it and will offer benefits on training aes ?",6899,,,2018-07-23T11:37:13.270,batch normalization in deep autoencoders ?,deep-network autoencoders,1,0,
1901,7256,1,7259,2018-07-23T09:59:55.807,4,429,"as it can be easily pointed out that true random numbers can not be generated fully by programming and some random seed is required . on the other hand , humans can easily generate any random number independently of other factors . does this suggest that absolute random number generation an ai concept ?",17056,1671,2018-07-23T19:26:27.193,2018-07-23T19:26:27.193,is true random number generation an ai concept ?,ai-basics concepts,2,1,5
1902,7263,1,,2018-07-23T14:37:52.583,4,89,"the ability to recognize an object with particular identifying features from single or multiple camera shoots with the temporal dimension digitized as frames has been shown . the proof is that the movie industry does face replacement to reduce liability costs for stars when stunts are needed . it is now done in a substantial percentage of action movie releases . this brings up the question of how valuable recognizing a stop sign is compared to the value of recognizing an action . for instance , in the world of autonomous vehicles , should there even be stop signs . stop signs are designed for lack of intelligence or lack of attention , which is why any police officer will tell you that almost no one comes to a full stop per law . what human brains intuitively looks for is the potential of collision . once what we linguistically perceive as verbs can be handled in deep learning scenarios as proficiently as nouns can be handled , the projection of risk becomes possible . this may be very much the philosophy behind the proprietary technology that allows directors to say , "" replace the stunt person 's face with the movie 's protagonist 's face , "" and have a body of experts execute it using software tools and linux clusters . the star 's face is projected into the model of the action realized in the digital record of the stunt person . projected action is exactly what our brain does when we avoid collisions , and not just with driving . we do it socially , financially , when we design mechanical mechanisms , and in hundreds of other fields of human endeavor . if we consider the topology of gans as a loop in balance , which is what it is , we can then see the similarity of gans to the chemical equilibria between suspensions and solutions . this gives us a hint into the type of topologies that can project action and therefore detect risk from audiovisual data streams . once action recognition is mastered , it is a smaller step to use the trained model to project the next set of frames and then detect collision or other risks . such would most likely make possible a more reliable and safe automation of a number of ai products and services , breaking through a threshold in ml , and increased safety margins throughout the ever increasing world population density . ... which brings us back to ... what topologies support recognition of action sequences ? the topology may have convolution , perhaps in conjunction with rnn techniques , encoders , equilibria such as the generative and discriminative models in gans , and other design elements and concepts . perhaps a new element type or concept will need to be invented . will we have to first recognize actions in a frame sequence and then project the consequences of various options in frames that are not yet shot ? where would the building blocks go and how would they be connected , initially dismissing concerns about computing power , network realization , and throughput for now ? work may have been done along this area and realized in software , but i have not seen that degree of maturity yet in the literature , so most of it , if there is any , must be proprietary at this time . it is useful to open the question to the ai community and level the playing field .",4302,4302,2018-07-25T10:02:01.673,2018-07-25T10:02:01.673,what topologies support recognition of action sequences ?,machine-learning ai-safety action-recognition topology autonomous-vehicles,1,4,
1903,7264,1,7266,2018-07-23T15:00:56.247,2,87,"for my pet project i ’m looking for a grid - like world simulation with some kind of resources that requires from agent incrementally intelligent behaviour to survive . something like this steam game , but with api . i ’ve seen minecraft fork , but it ’s too complex for my task . there is pycolab , i can build some world on this engine , but i ’d prefer ready - to - use simulations . is there any option ? i 'll appreciate any suggestion .",16940,1671,2018-07-23T19:25:57.613,2018-07-23T19:25:57.613,is there any opensource 2d open - world simulation with python api ?,getting-started software-evaluation,1,0,
1904,7268,1,,2018-07-23T17:58:39.040,-1,50,deep learning is based on getting a large number of samples and essentially making statistical deductions and outputting probabilities . on the other hand we have formal programming languages like prolog which do n't involve probability . is there any essential reason why an ai could be called conscious without being able to learn in a statistical manner ? i.e. by only being able to make logical deduction alone . ( it could start with a vast number of innate abilities ) . or is probability and statistical inference a vital part of being conscious ?,4199,1671,2018-07-23T19:12:07.997,2018-07-23T21:21:43.400,how important will statistical learning be to a conscious ai ?,theory probabilistic artificial-consciousness soft-question,1,1,1
1905,7273,1,7278,2018-07-24T11:26:17.413,3,141,"this seems like a natural fit , though i 've not heard of any , yet . i would love to know if any met office , government , military or academic institution has taken all ( or sizeable portion of ) recorded global weather data for , say , the last 50 years ( or since we , as a race , have been using weather satellites ) and used it in an ai system to predict future weather .",10938,,,2018-07-24T16:00:17.683,are any organisations using ai to predict weather ?,prediction,1,0,2
1906,7274,1,,2018-07-24T12:47:42.820,6,2074,"i think that the advantage of using leaky relu instead of relu is that in this way we can not have vanishing gradient . parametric relu has the same advantage with the only difference that the slope of the output for negative inputs is a learnable parameter while in the leaky relu it 's a hyperparameter . however , i 'm not able to tell if there are cases where is more convenient to use relu instead of leaky relu or parametric relu .",16199,1671,2018-07-24T20:03:23.930,2018-08-05T04:22:34.303,what are the advantages of relu vs leaky relu and parametric relu ( if any ) ?,neural-networks activation-function relu,1,0,2
1907,7277,1,,2018-07-24T15:01:27.323,1,26,"anyone here know if the image - recognition / text - recognition / etc features of google vision api use the same trained models as the image - recognition / text - recognition / etc of firebase 's ml kit ? if they do n't which one do you think is better ? ( i 've tried , and failed , to find the answer on the web . ) i realize google owns both of them , but one would think that if both are essentially the same , then this fact would be stated very clearly throughout multiple channels on the internet . ( it 's not . ) i do believe at least the text - recognition is the same , because both use ocr . but i 'm unsure about the image detection aspect .",17080,1671,2018-07-24T20:00:55.887,2018-07-24T20:00:55.887,difference in trained models between gcp 's google vision and firebase 's ml kit ?,machine-learning image-recognition computer-vision software-evaluation google,0,0,
1908,7280,1,7297,2018-07-24T16:31:48.617,5,279,"could you please let me know which of the following classification of neural network 's learning algorithm is correct ? the first one classifies it into : supervised , unsupervised and reinforcement learning . however , the second one provides a different taxonomy on page 34 : learning with a teacher ( error correction learning including incremental and batch training ) , learning without a teacher ( reinforcement , competitive , and unsupervised learing ) memory - based learning , and boltzmann learning . besides , is it true to consider svm as a kind of nn ( the second thesis on page 155 ) ? furthermore , are the materials presented on pages 6 - 11 of the first article ( i.e. , sections 3 - 3 to 3 - 11 ) different structures of nn ( just as pages 147 - 153 of the second one ? thanks a lot .",16141,,,2018-07-27T08:18:57.637,learning algorithms of neural networks,neural-networks reinforcement-learning unsupervised-learning learning-algorithms boltzmann-machine,1,0,1
1909,7286,1,,2018-07-24T23:03:29.803,5,86,"most of bibliography consider text classification as the classification of documents . when using bag of words and bayesian classification , they usually use the statistic tfidf , where tf normalizes the word count with the number of words per document , and idf focuses on ignoring widely used and thus useless words for this task . my question is , why they keep the documents separated and create that statistic , if it is posible to merge all documents of the same class ? this would have two advantages : you can just use word counts instead of frequencies , as the documents per class label is 1 . instead of using idf , you just select features with enough standard deviation between classes .",6114,,,2019-04-21T16:01:00.150,why are documents kept separated when training a text classifier ?,classification,2,0,
1910,7291,1,,2018-07-25T08:46:23.803,3,79,"i am trying to implement the disentangled vae model according to this link . i want to understand the architecture of this model in order to customize it later . as infrastructure , i have a linux kernel with 4 cores , 8 gb as memory with only cpu support . but still , the model is taking hours to run . can anyone tries to run this model and give me feedback ? is there any other simpler implementation of the disentangled vae in python because i could n't find any .",10167,10167,2018-08-14T14:31:03.747,2018-10-13T16:00:22.583,disentangled vae does n't reconstruct accurate grids,neural-networks convolutional-neural-networks python autoencoders,1,0,
1911,7293,1,,2018-07-25T13:00:25.777,1,54,"i am facing a problem and do not know whether it is even solvable : i want to predict the behaviour of a system using a dnn , say a cnn , in the sense that i want to predict the time and intensity of a maneuver performed by a player . let 's leave it relatively abstract like this , the details do not matter . my question is now whether there is any way of knowing how well my cnn performs . my goal would be to derive statements of the form "" with x% probability , the correct maneuver angle is within the predicted angle + -y% "" . can such statements be derived e.g. using statistical analysis of the test data ? i saw approaches toward verification and validation of dnns using satisfiability modulo theory , but did not really understand the details . would this be applicable here ? it seems a little overkill ...",16901,15465,2018-07-28T17:22:08.187,2018-07-28T17:22:08.187,confidence interval around a dnn prediction,deep-network statistical-ai,0,0,1
1912,7294,1,,2018-07-25T13:05:53.883,3,430,"the below code is a max pooling algorithm being used in a cnn . the issue i 've been facing is that it is offaly slow given a high number of feature maps . the reason for its slowness is quite obvious-- the computer must perform tens of thousands of iterations on each feature map . so , how do we decrease the computational complexity of the algorithm ? ( ' inputs ' is a numpy array which holds all the feature maps and ' pool_size ' is a tuple with the dimensions of the pool . ) def max_pooling(inputs , pool_size ) : feature_maps = [ ] for feature_map in range ( len(inputs ) ) : feature_maps.append ( [ ] ) for i in range ( 0 , len(inputs[feature_map ] ) - pool_size[0 ] , pool_size[0 ] ) : for j in range ( 0 , len(inputs[feature_map ] ) - pool_size[0 ] , pool_size[0 ] ) : feature_maps[-1].append(np.array(max((inputs[feature_map][j : j+pool_size[0 ] , i : i+pool_size[0]]).flatten ( ) ) ) ) return feature_maps",17101,1641,2018-07-25T20:01:57.773,2018-07-25T20:01:57.773,optimizing max pooling algorithm,convolutional-neural-networks python,1,0,1
1913,7298,1,,2018-07-26T18:50:36.537,4,245,"introduction an attractive asteroid game was described in the paper from 2007 : quote : “ in our first experiment , the virtual agent is a spaceship pilot , the pilot ’s task is to maneuver the spaceship through random asteroid fields ” jonathan dinerstein : "" learning policies for embodied virtual agents through demonstration "" , 2007 ( page 4 ) in theory , this game can be solved with reinforcement learning , or to be more specific with a support vector machine ( svm ) and epsilon - regression scheme with a gaussian kernel . but it seems , that this task is harder than it looks like : quote : “ although many powerful ai and machine learning techniques exist , it remains difficult to quickly create ai for embodied virtual agents . [ ... ] it is quite challenging to achieve natural - looking behavior since these aesthetic goals must be integrated into the fitness function ” ( page 1 - 2 ) screenshot i really want to understand how reinforcement learning works . i built a simple game to test this . there are squares falling from the sky and you have the arrow keys to escape . how could i code the rl ? can i do this manually in javascript according to what i think should happen ? how can i do this without having to map the positions of the rectangles and mine , just giving the agent the keyboard arrows to interact and three information : player life survival time maximum survival time",7800,11571,2018-09-18T17:27:51.200,2018-09-18T22:04:47.130,reinforcement learning in asteroid game,machine-learning reinforcement-learning javascript,1,1,4
1914,7302,1,,2018-07-27T07:25:09.473,-1,70,"i have created an ann in python ( without libs ) . on beginning , it had been learned in target of solve linear problems like distinguishing between negative and positive numbers , where the layer widths were [ 1 , 2 , 1]. i have decided to learn recognizing small digits saved as 20x20 black & amp ; white png files . now the array of layer widths is : [ 1200 , 100 , 100 , 100 , 100 , 100 , 100 , 100 , 100 , 100 , 100 , 100 , 100 , 10 ] i tried other similar ones .... with the above array of layer widths training took 8 hours ( nn had seen 600.000 images from 5000 images of learning set ) and when i look at results , each output is equal about 10%-15 % . nothing is certain . this code is a core of my nn and that is main code : net = nnmv.neuralnetwork ( ) net.createnew([1200,100,100,100,100,100,100,100,100,100,100,100,100,10],0.15,0.07 ) for step in range(0,1000 ) : for i in range(0,500 ) : for number in range(0,10 ) : print(""currently learning : "" , number,'x',i , "" in step : "" , step ) pixels = list(image.open(""judgment/""+str(number)+'x'+str(i)+"".png"").convert(""rgb"").getdata ( ) ) output = list ( ) for itr in range(0,number ) : output.append(0 ) output.append(1 ) for itr in range(0,9-number ) : output.append(0 ) net.teach(stuff.reorganisepixeldata(pixels),output ) print(""error : "" , net.calculateerror(output ) ) saver.saver().save(net , "" digitrecognizer "" ) there is 1200 inputs because there are 400 pixels and each pixel is saved in rgb model . stuff.reorganisepixeldata : def reorganisepixeldata(pixels ) : output = [ ] for i in range(0,len(pixels ) ) : output.append(pixels[i][0 ] ) output.append(pixels[i][1 ] ) output.append(pixels[i][2 ] ) return output what have i to do ? add or remove layers , change some or all of the layer widths ? or something in concept of learning ? my error calculator prints error like 0.30203135930914193 , and it changes only a bit .",17132,4302,2018-08-12T02:49:26.557,2018-12-02T22:01:40.267,what ann layer widths support the learning of digit recognition ?,neural-networks image-recognition hidden-layers,1,4,
1915,7305,1,,2018-07-27T14:55:03.903,-1,88,"i want to use a machine learning algorithm to detect false address data . i learned about neural networks and machine learning at university , but i do n't have much experience in this field . do you think it is feasible to use a high level algorithm for this or should i use simple queries and filters to catch out wrong data ?",17142,2193,2018-07-27T17:20:43.187,2018-09-26T04:01:50.747,machine learning to detect wrong address data,machine-learning,1,5,
1916,7306,1,,2018-07-27T17:48:59.153,5,142,"if one has a dataset large enough to learn a highly complex function , say learning chess game - play , and the processing time to run mini batch gradient descent on this entire dataset is too high , can i instead do the following ? run the algorithm on a chunk of the data for a large number of iterations and then do the same with another chunk and so on ? ( such will not produce the same result as mini batch gradient descent as i am not including all data in one iteration , but rather learning from some data and then proceeding to learn on more data , beginning with the updated weights may still converge to a reasonably trained network . ) run the same algorithm ( the same model also with only data varying ) on different pc 's ( each pc using a chunk of the data ) and then see the performance on a test set and take the final decision as a weighted average of all the different models outputs with the weight being high for the model which did the best on test sets ?",17143,10135,2018-10-18T03:17:30.853,2018-10-18T03:17:30.853,working with large datasets,datasets gradient-descent feedback,2,0,1
1917,7310,1,8157,2018-07-28T00:25:21.830,3,94,"a few forces seem to dominate the determination of ai and cybernetic research direction . the quest to automate repetitive tasks interest in the nature of consciousness , learning , and adapting interest in language automation ( librarian , translator , therapist , loyal friend ) outsmarting the foes in geopolitics outsmarting the competitors in local and global economics which of these five motivations produce an impact on the direction of technology efforts that lacking key elements in risk management ? these are the corresponding five results one might expect upon success in each of the above five objectives . more human leisure time to waste on shopping , gossiping , entertainment , blogging without defined purpose , pretending to do vital office work eight hours a day when producing only an hour or two of usable work product . clearer understanding of ourselves and our place and duty as a species . cyborexia ( fear of talking with our own species unless a computer creates a layer of safety in between , a phenomena already occurring ) and the entrusting to computers not only the marionette strings of friendship and trust but also the key relational bridge between cultures the development of tools intended to outsmart foes outsmarting all humans instead the development of tools intended to outsmart competitors outsmarting all humans instead only one of these five seems to be a humanity enhancer with low risk . the others seem to present a high risk to human destiny and offer no particular advancement of the human species . ai is cast by futurists as the edge of advancement and demonized by technophobes . can the development of ai be pointed more toward those objectives that will improve the world ? does anyone see a second one or have a sixth or seventh force and corresponding result to offer ? is there any flaw in what appears to me to be obvious eventualities ?",4302,4302,2018-09-28T09:14:07.547,2018-09-28T09:14:07.547,what forces direct research in ai and is the resulting direction a good choice ?,ethics singularity automation risk-management,2,0,1
1918,7314,1,,2018-07-28T05:10:47.923,0,679,"the a * algorithm uses the "" evaluation function "" $ f(n ) = g(n ) + h(n)$ , where $ g(n)$ = cost of the path from the start node to node $ n$ $ h(n)$ = estimated cost of the cheapest path from $ n$ to the goal node but , in the following case ( picture ) , how is the value of $ h(n)$ calculated ? in the picture , $ h(n)$ is the straight - line distance from $ n$ to the goal node . but how do we calculate it ?",12021,2444,2018-11-11T12:14:01.910,2018-11-11T12:14:01.910,how do you calculate the heuristic value in this specific case ?,search heuristics,1,0,
1919,7327,1,,2018-07-29T07:28:19.293,1,78,"as a amateur researcher and tinkerer , i 've been reading up on neuro - evolution networks ( e.g. neat ) as well as the a3c rl approach presented by mnih et al and got to wondering if anyone has contemplated the merging of both these techniques . is such an idea viable ? has it been tried ? i 'd be interested in any research in this area as it sounds like it could be compelling .",17162,2444,2019-02-16T02:53:17.607,2019-02-16T02:53:17.607,can neuro - evolution methods be combined with a3c ?,reinforcement-learning genetic-algorithms neat actor-critic,1,0,1
1920,7328,1,,2018-07-29T09:33:00.663,14,967,"the impetus behind the twentieth century transition from analog to digital circuitry was driven by the desire for greater accuracy and lower noise . now we are developing software where results are approximate and noise has positive value . in artificial networks , we use gradients ( jacobian ) or second degree models ( hessian ) to estimate next steps in a convergent algorithm and define acceptable levels of inaccuracy and doubt . 1 in convergence strategies , we deliberately add noise by injecting random or pseudo random perturbations to improve reliability by essentially jumping out local minima in the optimization surface during convergence . 2 what we accept and deliberately introduce in current ai systems are the same things that drove electronics to digital circuitry . why not return to analog circuitry for neural nets and implement them with operational amplifier matrices instead of matrices of digital signal processing elements ? the values of artificial network learning parameters can maintained using integrated capacitors charged via d - to - a converters such that the learned states can benefit from digital accuracy and convenience , while forward propagation benefits from analog advantages . greater speed 3 orders of magnitude fewer transistors to represent network cells natural thermal noise 4 an academic article or patent search for analog artificial networks reveals much work over the last forty years , and the research trend has been maintained . computational analog circuits are well developed and provide a basis for neural arrays . could the current obsession with digital computation be clouding the common view of ai architectural options ? is hybrid analog the superior architecture for artificial networks ? & nbsp ; footnotes [ 1 ] the pac ( probably approximately correct ) learning framework relates acceptable error and acceptable doubt to the sample size required for learning for specific model types . ( note that $ 1 - \epsilon$ represents accuracy and $ 1 - \delta$ represents confidence in this framework . ) [ 2 ] stochastic gradient descent is shown , when appropriate strategies and hyper - parameters are used , to converge more quickly during learning and is becoming a best practice in typical real world applications of artificial networks . [ 3 ] intel core i9 - 7960x processor runs at turbo speeds of 4.2 ghz whereas the standard fixed - satelite broadcasting is 41 ghz . [ 4 ] thermal noise can be obtained on silicon by amplifying and filtering electron leakage across a reverse biased zener diodes at its avalanche point . the source of the quantum phenomena is johnson – nyquist thermal noise . sanguinetti et . al . state in their ' quantum random number generation on a mobile phone ' ( 2014 ) , "" a detector can be modeled as a lossy channel with a transmission probability η followed by a photon - to - electron converter with unit efficiency ... measured distribution will be the combination of quantum uncertainty and technical noise , "" and there 's caltech 's jtwpa work . both of these may become standards for producing truly nondeterministic quantum noise in integrated circuits . references stdp learning of image patches with convolutional spiking neural networks , saunders et . al . 2018 , u mass and has general - purpose code acceleration with limited - precision analog computation , amant et . al . , 2014 analog computing and biological simulations get a boost from new mit compiler , by devin coldewey , 2016 analog computing returns , by larry hardesty , 2016 * why analog computation ? , nsa declassified document back to analog computing : columbia researchers merge analog and digital computing on a single chip , columbia u , 2016 field - programmable crossbar array ( fpca ) for reconfigurable computing , zidan et . al . , ieee , 2017 fpaa / memristor hybrid computing infrastructure , laiho et . al . , ieee , 2015 foundations and emerging paradigms for computing in living cells , ma , perli , lu , harvard u , 2016 a flexible model of a cmos field programmable transistor array targeted for hardware evolution ( fpaa ) , by zebulum , stoica , keymeulen , nasa / jpl , 2000 custom linear array incorporates up to 48 precision op amps per chip , ashok bindra , 2001 , electronics design large - scale field - programmable analog arrays for analog signal processing , hall et . al . , ieee transactions on circuits and systems , vol . 52 , no . 11 , 2005 large - scale field - programmable analog arrays for analog signal processing , hall et . al . 2005 a vlsi array of low - power spiking neurons and bistable synapses with spike - timing dependent plasticity , indiveri g , chicca e , douglas rj , 2006 https://www.amazon.com/analog-computing-ulmann/dp/3486728970 https://www.amazon.com/neural-networks-analog-computation-theoretical/dp/0817639497",9203,9203,2018-09-24T16:54:05.763,2018-11-17T13:56:45.030,"if digital values are mere estimates , why not return to analog for ai ?",machine-learning hardware neuromorphic-engineering analog-computing spiking-networks,7,3,8
1921,7333,1,7349,2018-07-29T15:50:23.187,1,55,"what if we took a recursive approach and built a smallest possible first robot ( robot 1 ) that could transfer information and data about the place it was at and could build itself in a very small size proportional to itself . i understand that it means higher level of accuracy for the this first robot ( robot 1 ) that its creator i.e. us . and this first robot ( robot 1 ) again built a robot ( say robot 2 ) that was far smaller but an exact copy of the first robot ( robot 1 ) . and then the second robot ( robot 2 ) built a third robot ( robot 3 ) and so on . so each next level robot was tinier and higher precision that its creator . with the tiniest robot we could make , we sent them to mission where in micro sized intervention was needed . for example studying the atom structure from inside , how similar it was to our big universe etc . plus many more applications human kind could ever imagine . i understand though that the material used to construct such a robot and its properties , will be limiting and to explore an atom we may not be able to use an atom as the building block . however , we could possibly build a robot like this which would be small enough to explore the human body from inside .",17170,,,2018-07-30T14:26:29.540,what if we took a recursive approach and built a smallest possible robot ?,robots,1,5,
1922,7337,1,7348,2018-07-30T00:30:15.350,4,496,"in comes iq when the concept of intelligence quotient arose it was based on this approximation . each human being has a number that quantifies their intelligence relative to a fixed norm , and , although this number may vary from testing to testing and over the course of years , it varies only within a small statistical deviation and is essentially determined at meiosis , when the dna is assembled for that human . experiments and studies supported the validity of iq as a measure of intelligence . others refuted the concept . it was ethically questioned on the ground that it fostered intellectual elitism . there were legislative discussions regarding discrimination on the basis of iq . andrew niccol 's brilliant 1997 motion picture , gattica , starring ethan hawke , uma thurman , and jude law , dramatically examined the question of genetic determinism . research overturned the elitist belief that iq is purely a mater of pedigree . the current evidence - based perspective is that from six months old to adolescence , nurture dominates over genetics . nonetheless , during the 20th century , iq rose to become a common household term . people knew , for example , that albert einstein was supposed to have had an iq greater than 200 . how that was calculated , since he never took an iq test , few have stopped to scrutinize . unlike the single number used to gauge intelligence , college boards gauge achievement using a vector of numbers . command of evidence vocabulary ability to express ideas following of language conventions mathematics problem solving essay writing it is interesting that a set of numbers are used to gauge academic achievement yet all the complex genetic and educational factors that impact a person 's intelligence are rolled up into a one - dimensional number . within the complex genetics , circuitry , and chemistry of the human brain is there some single factor that determines the intelligence of an individual ? does one factor eclipse and render irrelevant all other factors that are known to affect how smart a person is ? does each person have a glass ceiling on personal growth and education that is locked in by their mid - teens ? is this what people really believe ? today , peter norvig 's word frequency table places iq among words like mailbox and defect at the 96.9 percentile position in terms of frequency of use . this linguistic fact is strong evidence that our culture responds to these questions with , "" we buy iq . "" in comes the singularity now the concept of the singularity can be reduced to this . humans will work on artificial intelligence diligently , causing the peak level of intelligence of an artificial variety to increase . once the intelligence of an artificial variety exceeds the intelligence of humanity , the artificial intelligence will gain control over humans to guarantee the perpetuation of its own existence and continued self - development . the singularity concept relies on a very specific pair of propositions . ( using pseudonyms ) the intelligences of abby and beth are fully represented as scalars ( 1-d ) not vectors ( multidimensional ) and therefore can be directly compared . if the intelligence of abby is greater than the intelligence of beth , abby will gain control over beth and exploit her . in comes proof of the limits of proof the dominant enlightenment philosophies of humanism and determinism , that the intellectual pursuits of men will grow to dominate all things , were challenged in the early 20th century by the work of two people . heisenberg proved , and there has been no credible refutation , that one can not accurately measure two determining extensible quantities of a particle at the same time and therefore any mathematical treatment of bohr 's model of the atom must be based on probabilities , not the certainty of mechanical phenomena . gödel proved two things ( a ) that a formal mathematical system can not be relied upon to indicate the truth or falsehood of every statement that can be made formally , and , more importantly , ( b ) no formal system can , though its own formalisms , establish its own consistency via proof . 1 this uncertainty appears in linguistic expressions like , "" ricky does n't test well , "" when ricky 's test score is below expectation ( again using a pseudonym ) . there is scientific validity to these sayings because neural activity can not be measured in natura ( in natural life ) . we must bring the subject into the lab to measure their neural activity , which disturbs the neural activity of the subject under study , just as the electron in heisenburg 's thought experiment . applying gödel 's thinking , human intelligence can not construct a formal system to determine inconclusively that human intelligence is can be expressed as a scalar , can be expressed as a vector , or even exists at all . we can not , via intelligence , formally prove that intelligence is self - consistent . in comes genetics research has recently revealed that , of 12,000,000 human single - nucleotide polymorphisms ( snps ) examined , 336 have been found that significantly correlate with human intelligence , and those 336 snps implicate 22 independent genes . 2 , 3 it is also likely that additional intelligence related genes will be discovered as research continues . even if there are no further independent genetic determinants of human intelligence found , twenty - two genetically distinct , independent degrees of freedom define the variable features of human intelligence . an important conclusion then falls directly from this research . a scalar , such as iq , can not possibly describe the permutations of genes that human intelligence . at least a 22-dimension vector is required to represent the multifaceted features of an individual 's intelligence . putting the proof together although the magnitude of a vector can be greater than the magnitude of another vector ( without first taking the absolute value of the vector ) it is mathematical nonsense to say vector x ( of 22 dimensions ) is greater than vector y ( of 22 dimensions ) . we also lack any evidence - based reason to apply an absolute value to the vector . a pure root mean squared calculation ( rms ) would mean that human intelligence is a cartesian hyper - cube of 22 equally weighted traits . there is no evidence to validate the idea that the mathematical inventions of descartes should be applied to genetic traits . 4 all these traits are situational anyway , arising out of specific environmental challenges during evolution . for instance , trait number 9 may produce an intelligent response in one scenario and trait numbers 11 and 17 together may produce an intelligent response in another scenario . so without creating some basis for mapping the traits to the scenarios where they are important in survival , problem solving , or human advancement , it is not mathematically sensible to say that abby is or is not smarter than beth ( using the previous pseudonyms ) . neither can we say that a machine named polycharp is smarter than abby . consequentially , what is described above as the singularity can not possibly be singular . we have only three logical possibilities . only one can be true , but which one ? the genetic research is flawed . the concept of the singularity is flawed . the logical inference laid out in this question is flawed . the unpredictability of cybernetics the proposal that has been suggested in the literature is that the detailed workings of human intelligence may not fit into the network size of the human cerebral cortex . that is a reasonable line of scrutiny . just as particle physics and the complexities of the biosphere may always require computer analysis and be visible only through the lens of statistics , so may be the case with human cognition . adding the work of heisenberg and gödel and it only gets worse . even a computer model of the twenty - two or more dimensions of human intelligence may be as elusive as physical quanta , further confining the evaluation of intelligence to statistical treatment . these factors place the interplay between human beings and machines ( cybernetics ) firmly within the realm of the unpredictable . super - intelligence may face the same it may be that some higher intelligence that exists in the universe or some superior artificial intelligence we invent can both fathom human intelligence and predict the course of cybernetics . however , that greater intelligence may eventually be confronted with the same issue . such beings may not have the intellectual capacity to fully fathom their own intelligence or predict their own cybernetic destiny . returning back to the main question regardless of the predictability of cybernetics , it is reasonable to ask , "" is the singularity concept mathematically flawed ? "" footnotes [ 1 ] gödel 's incompleteness theorems , 2013 , 2015 , stamford.edu encyclopedia of philosophy [ 2 ] genome - wide association meta - analysis of 78,308 individuals identifies new loci and genes influencing human intelligence , s. sniekers et all , may 2017 nature genetics letters [ 3 ] intelligence and the dna revolution , scientists identify 22 genes associated with intelligence , alexander p. burgoyne , david z. hambrick , august 22 , 2017 , scientific american , section cognition [ 4 ] assuming the twenty - two or more traits involved in human intelligence should be aggregated via an rms function would be as subjective ( and ridiculous ) as applying rms thusly : f 2 = b 2 + b 2 , where f is fun , b is blonde , and b is blue - eyed , because fashion magazines sold contact lenses and hair color products to a primarily brown - haired , brown - eyed population by depicting blue - eyed blonds having fun .",4302,4302,2018-08-12T11:57:18.967,2018-08-12T11:57:18.967,is the singularity concept mathematically flawed ?,prediction logic singularity probabilistic dimensionality,3,6,4
1923,7339,1,,2018-07-30T05:33:15.530,4,270,"i want suggestions on literature on reinforcement learning algorithms that perform well with asynchronous feedback from the environment . what i mean by asynchronous feedback is , when an agent performs an action it gets feedback(reward or regret ) from the environment after sometime not immediately . i have only seen algorithms with immediate feedback and asynchronous updates . i do n't know if literature on this problem exists . this is why i 'm asking here . my application is fraud detection in banking , my understanding is when a fraud is detected it takes 15 - 45 days for the system to flag it as a fraud sometimes until the customer complains the system does n't know its fraud . how would i go about designing a real time system using reinforcement learning to flag transactions that are fraud or normal . maybe my understanding is wrong , i 'm learning on my own if someone could help me i would be grateful . edit : the reason i 'm looking at reinforcement learning instead of supervised learning is , its hard to get ground truth data in the banking scenario . fraudsters are always up - to - date or exceeding the state of the art in fraud detection . so i 've decided that reinforcement learning would be an optimal direction to look for solutions to this problem .",17136,17136,2018-07-30T07:14:42.673,2019-05-25T03:09:56.003,reinforcement learning with asynchronous feedback,reinforcement-learning detecting-patterns,2,8,2
1924,7341,1,,2018-07-30T07:57:50.377,0,56,"i want to implement a real - time system for image comparison ( e.g. compare a face with a reference one ) on an odroid . i would like to know what are the most suitable architectures for this task . i started with methods based on triplet loss ( like facenet ) but i realized that a real - time solution is not feasible . are there good , light alternatives ?",16671,,,2018-08-03T04:49:13.943,methods for fast image comparison,image-recognition comparison,2,0,
1925,7343,1,,2018-07-30T08:55:05.260,0,72,"i 'm now reading a book titled "" hands - on reinforcement learning with python "" by o'reilly , and the author said the following to implement the dqn algorithm . to make training more stable , there is a trick , called target network , when we keep a copy of our network and use it for the q(s′ , a′ ) value in the bellman equation . this network is synchronized with our main network only periodically , for example , once in n steps ( where n is usually quite a large hyperparameter , such as 1k or 10k training iterations ) . and on the implementation part : the ptan.agent.targetnet class is an extremely simple wrapper around the network , which allows us to create a copy of our nn 's weights and sync them periodically . but i 'm not sure why it needs the copy of a neural network and syncs with it periodically , and why it makes the training more stable . so why is it needed and what makes it different from that without the copy of the network ?",7402,,,2018-07-30T08:55:05.260,"what is a copy of network to create a "" target network "" for in dqn ?",reinforcement-learning dqn,0,0,
1926,7347,1,,2018-07-30T12:27:53.110,0,105,"i want to train a model to recognize different category of food ( example : rice , burger , apple , pizza , orange , ... ) after the first training , i realized that the model is detecting other object as food . ( example : hand - > fish , phone - > chocolate , person - > candies ... ) i get a very low loss because the testing dataset and validation must have at least a pictures of food . but when it comes to picture of object other than food , the model fails . how do label the dataset in way that the model will not do any detection if there is no food on the picture ?",17059,,,2018-07-31T13:59:23.830,how to label “ other ” while labeling image for object detection / classification ?,machine-learning computer-vision models object-recognition,1,0,
1927,7350,1,7505,2018-07-30T14:50:33.873,3,99,"in what scenario when assembling a dl cnn would you want to have two adjacent pooling layers , without a convolutional layer between ?",16207,,,2019-01-11T22:51:21.747,double pooling layers,deep-learning convolutional-neural-networks ai-basics,1,4,
1928,7352,1,,2018-07-30T19:37:58.987,7,312,what are the mathematical prerequisites for understanding the core part of the algorithms in artificial intelligence and developing own algorithm ? please refer me the specific books .,12021,,,2019-05-16T15:44:12.257,mathematics for ai researcher,algorithm math,3,1,8
1929,7359,1,7360,2018-07-31T14:34:31.353,4,626,"i 'm now learning about reinforcement learning , but i just found the word "" trajectory "" in this answer . however , i 'm not sure what it means . i read a few books on the reinforcement learning but none of them mentioned it . usually these introductionary books mention agent , environment , action , policy , and reward , but not "" trajectory "" . so , what does it mean ? according to this answer over quora : in reinforcement learning terminology , a trajectory is the path of the agent through the state space up until the horizon $ h$ . the goal of an on - policy algorithm is to maximize the expected reward of the agent over trajectories . does it mean that the "" trajectory "" is the total path from the current state the agent is in to the final state ( terminal state ) that the episode finishes at ? or is it something else ? ( i 'm not sure what the "" horizon "" mean , either ) .",7402,2444,2019-02-22T14:10:08.230,2019-02-22T14:10:08.230,"what is a "" trajectory "" in reinforcement learning ?",reinforcement-learning terminology,2,0,2
1930,7361,1,7368,2018-07-31T16:21:36.533,0,44,"say i 'm training a neural net to compute the following function : ( color_of_clothing , body_height ) -&gt ; gender when using this network for prediction , i can obviously plug in a pair ( c , b ) to receive a predicted g , but say i want to get a prediction only based on c or only based on b , can i use the same neural net somehow ? or would i need to train two separate neural nets c -&gt ; g and b -&gt ; g previously ? or more generally , can i use a neural net that was trained to predict a -&gt ; b to make predictions on values from a subset of a , or should i train separate neural nets on all subsets of a that i 'm interested in ?",17205,7800,2018-07-31T17:49:14.140,2018-08-01T07:12:27.210,"dealing with "" blank "" inputs in prediction of a neural network ?",neural-networks,1,0,0
1931,7362,1,,2018-07-31T17:42:27.820,0,52,"in doing a project using neural networks with an input layer , 4 hidden layers and an output layer , i used mini batch gradient descent . i noticed that the randomly initialised weights seemed to do a good performance and gave a low error . as the model started training after about 200 iterations there was large jump in error and then it came down slowly from there . i have also noticed that sometimes the cost just increases over a set of consecutive iterations . can anyone explain why these happen ? it is not like there are outliers or a new distribution as every iteration exposes it to the entire dataset . i used learning rate 0.01 and regularisation parameter 10 . i also tried regularisation parameter 5 and also 1 . and by the cost i mean , the sum of squared errors of all minibatches/2 m plus regularisation term error . further if this happens and my cost after the say 10000th iteration is more than my cost when i initialised with random weights ( lol ) can i just take the initial value ? as those weights seem to be doing better . the large jumps are the most puzzling . this is the code any help would be greatly appreciated . thanks",17143,17143,2018-08-03T02:39:22.127,2018-08-03T14:06:44.907,behaviour of cost,neural-networks gradient-descent,2,6,
1932,7364,1,,2018-07-31T20:31:43.493,3,117,"does an application exist that can automatically write and test a software component based on a formal functional specification ? the twentieth century saw the initial birth of electronic computers . the early programming languages that were in primary use by 1975 were cobol , fortran , lisp , and c and unix were emerging for real time communications and control . shortly after this period two conceptual steps were proposed toward executable requirements , which , combined with natural language dialog , would permit the realization of computers that would execute high level instructions in a user 's native tongue . glenford j. myers ' advances in computer architecture , wiley ; 1st edition , 1978 , puts forth the proposition that computers had been designed from the bottom up , creating serious obstacles in use . he redefined the common term at the time , semantic gap , to mean the gap between the needs of those that program computers and the facilities of the computer architecture . this thinking led to object oriented design and supporting languages such as c++ , java , emmascript , and python . ( myers worked for ibm but was recruited by a small startup company called intel to help them design their first 32-bit architecture , the 80386 . ) gene fisher , professor emeritus , california poly san luis obispo , proposed in 1988 what he called a , "" tool for constructing executable block diagrams based , "" conceptually more advanced than graphical simulators like simulink and more advanced than ides like eclipse , idea , and jupyter interfaces .. jmodelica is probably one of the closest development applications to fisher 's vision . the term that has become popular in the literature for this concept is executable diagram . are there any applications in beta or in common use in some segment of the software industry where a formal requirement can be entered as input to a program writing application and tested source code is produced at the output ? is anyone working on an application that takes this one step further to a computer that gathers requirements through natural dialog ?",17209,4302,2018-10-08T11:56:05.443,2018-10-08T11:56:05.443,does an application that can write software exist ?,strong-ai language-processing programming-languages,3,3,1
1933,7367,1,,2018-08-01T06:54:36.177,2,82,is it possible to build a neural network that learns the connection between two images ? let 's say i have a number of x images that related to y images . how can i build a neural network that takes an image as an input and outputs ( generates ) the output image ? the y images are generated by applying some function to the x images . do i need a generate neural network for that ? are conventional neural networks capable of classification only ?,17216,,,2018-09-11T19:01:26.060,a neural network to learn the connection between two images,neural-networks,2,0,
1934,7369,1,,2018-08-01T08:23:22.073,1,55,i training a generative adversarial network ( gan ) to generate images given edge histogram descriptor ( ehd ) features of the image . the ehd features are themselves sparse ( meaning they contain a lot of zeroes ) . while training the generator loss and discriminator loss are reducing very slowly . my question is whether deep learning models ( like gan ) are suitable for training with sparse data for one or more of the features in input or derived through feature extraction ?,9062,4302,2018-10-16T00:37:51.513,2018-10-16T00:37:51.513,deep learning with sparse input,deep-learning generative-model generative-adversarial-networks sparse-input,0,0,
1935,7370,1,7374,2018-08-01T09:06:17.030,2,190,"i 'm confused with the two terminology - action and policy - in reinforcement learning . as far as i know , the action is : it is what the agent makes in a given state . however , the book i 'm reading now ( hands - on reinforcement learning with python ) writes the following to explain policy : we defined the entity that tells us what to do in every state as policy . now , i feel that the policy is the same as the action . so what is the difference between the two , and how can i use them apart correctly ?",7402,2444,2019-05-12T23:22:18.390,2019-05-12T23:23:36.523,what is the difference between policy and action in reinforcement learning ?,reinforcement-learning terminology difference,1,0,1
1936,7371,1,,2018-08-01T09:56:43.227,4,150,"i am currently working on a project to classify snake types separately using an image of the snake . i need to train a module to classify snake images , but the problem is there are only a small number of images available for some snake types . what is the best approach to train a neural network for image classification using a small data set ?",17220,2193,2018-08-02T01:28:18.007,2018-08-02T01:28:18.007,image classification,neural-networks machine-learning deep-learning convolutional-neural-networks image-recognition,2,0,
1937,7379,1,8347,2018-08-01T18:19:08.660,3,82,"in a recent paper about progress in computer animation a so called motion graph is used to describe the transition between keyframes of facial animation . easy generation of facial animation using motion graphs , 2018 as far as i understand from the paper , they used a motion capture device to record faces of real people and extract keyframes . then a transition matrix was created to ensure that a walk from keyframe # 10 to # 24 is possible but a transition from keyframe # 22 to # 99 is forbidden . the idea itself sounds reasonable good , because now a solver can search in the motion graph to bring the system from a laughing face to a bored face without interruption or unnatural in - between - keyframes . but would n't it be great if the transition matrix can be stored inside a neural network ? as far as i understand the backpropagation algorithm , the neural network can learn input - output relations . so the neural network has to learn the transition probability between two keyframes . and a second neural network can then produce the motion plan which is also be trained by a large corpus . is that idea possible or is it the wrong direction ?",11571,,,2018-10-10T23:15:47.653,using a neural network for learning a motion graph ?,neural-networks,1,0,
1938,7389,1,7402,2018-08-02T14:26:54.830,4,81,"can we really make a chatbot that understands ( rather than just replies to ) questions based on the database / options of replies that it has ? i mean , can it come up with correct / non - stupid replies / communications that do n't exist in its database ? for example , can we make it understand the words but , if and so on ? so whenever it gets a question / order it understands it based on "" understanding "" . like the movie "" her "" if you have watched it . and all of this without using too much of code , just the basics to "" wake it up "" and let it learn from youtube videos and reddit comments and other data source like that .",17246,1671,2018-08-07T18:56:02.287,2018-08-07T18:56:02.287,can we make a chatbot that understands not just answer questions based on his database ?,chat-bots theory soft-question,3,2,
1939,7390,1,7425,2018-08-02T14:59:08.493,6,3258,"i 'm struggling to understand the difference between actor - critic and advantage actor - critic . at least i know they are different from asynchronous advantage actor - critic ( a3c ) , as a3c adds asynchronous mechanism that uses multiple worker agents interacting with their own copy of environment and report the gradient to the global agent . but what is the difference from the actor - critic and advantage actor - critic ( a2c ) ? is it simply with or without advantage function ? but then , does the actor - critic have any other implementation except the use of advantage function ? or maybe are they synonyms and actor - critic is just a shorthand for a2c ?",7402,2444,2019-02-17T21:08:08.393,2019-02-18T08:29:32.653,what is the difference between actor - critic and advantage actor - critic ?,reinforcement-learning terminology actor-critic difference advantage-actor-critic,1,1,2
1940,7392,1,,2018-08-02T17:08:13.317,1,44,i would like to know some daily basis applications of ai . i think these might be relevant examples : google search engine face recognition on iphone are my examples correct ? could you provide some more examples ?,17249,16909,2018-08-07T18:15:15.577,2018-08-07T18:15:15.577,what are some examples of everyday systems that use ai ?,ai-basics applications,1,0,
1941,7394,1,7395,2018-08-02T20:29:35.273,4,119,"what are svms ( support vector machines ) ? are svms a kind of a neural network ? ( meaning it has nodes and weights , etc ) . what are best used for ? where i can find information about these for ... dummies ?",17250,,,2018-08-03T13:35:46.950,what are svms ( support vector machines ) ?,neural-networks machine-learning svm,1,0,0
1942,7397,1,,2018-08-03T00:41:39.313,2,676,"i 'm now reading the following blog post but on the epsilon - greedy approach , the author implied that the epsilon - greedy approach takes the action randomly with the probability epsilon , and take the best action 100 % of the time with probability 1 - epsilon . so for example , suppose that the epsilon = 0.6 with 4 actions . in this case , the author seemed to say that each action is taken with the following probability ( suppose that the first action has the best value ) : action 1 : 55 % ( .40 + .60 / 4 ) action 2 : 15 % action 3 : 15 % action 4 : 15 % however , i feel like i learned that the epsilon - greedy only takes the action randomly with the probability of epsilon , and otherwise it is up to the policy function that decides to take the action . and the policy function returns the probability distribution of actions , not the identifier of the action with the best value . so for example , suppose that the epsilon = 0.6 and each action has 50 % , 10 % , 25 % , and 15 % . in this case , the probability of taking each action should be the following : action 1 : 35 % ( .40 * .50 + .60 / 4 ) action 2 : 19 % ( .40 * .10 + .60 / 4 ) action 3 : 25 % ( .40 * .25 + .60 / 4 ) action 4 : 21 % ( .40 * .15 + .60 / 4 ) is my understanding not correct here ? does the non - random part of the epsilon ( 1 - epsilon ) always takes the best action , or does it select the action according to the probability distribution ?",7402,,,2019-05-20T10:35:54.320,"does epsilon - greedy approach always choose the "" best action "" ( 100 % of the time ) when it does not take the random path ?",reinforcement-learning,2,0,
1943,7406,1,,2018-08-03T19:23:08.747,0,63,"i need to retrieve just the text from emails . the emails can be in html format , and can contain huge signatures , disclaimer legalese , and broken html from dozens of forwards and replies . but , i only want the actual email message and not any other cruft such as the whole quotation block , signatures , etc . this is n't really a problem that could be solved with regex because html mail can get very , very messy . could a neural network perform this task ? what kind of problem is this ? classification ? feature selection ?",17272,,,2018-08-05T05:19:44.967,"what kind of problem is "" email text extraction "" ?",neural-networks classification,2,0,
1944,7408,1,7442,2018-08-03T22:41:54.930,4,416,"i 'm building a deep neural network to serve as the policy estimator in an actor - critic reinforcement learning algorithm for a continuing ( not episodic ) case . i 'm trying to determine how to explore the action space . i have read through this text book by sutton and in section 13.7 he gives one way to explore a continuous action space . in essence you train the policy model to give a mean and standard deviation as an output so you can sample a value from that gaussian distribution to pick an action . this just seems like the continuous action - space equivalent of an epsilon greedy policy . also , are there other continuous action space exploration strategies i should consider ? i 've been doing some research online and found some articles related to rl in robotics and found that the power and pi^2 algorithms do something similar to what is in the textbook . are these , or other , algorithms "" better "" ( obviously depends on the problem being solved ) alternatives to what is listed in the textbook for continuous action - space problems ? i know that this question could have many answers , but i 'm just looking for a reasonably short list of options that people have used in real applications that work .",17274,17274,2018-08-03T22:59:35.307,2018-08-06T13:44:08.937,exploration strategies for reinforcement learning w/ continuous action space,neural-networks reinforcement-learning search concepts,1,1,1
1945,7413,1,,2018-08-04T03:53:04.043,2,552,"in introduction to reinforcement learning ( 2ed ) , sutton and barto , there is an example of pole - balancing problem ( example 3.4 ) . in this example , it said , this problem can be treated with ' episodic task ' and ' continuing task ' . i think that it can only be treated as episodic task because it has an end of playing , which is falling the rod . i have no idea how this can be treated as continuing task .... even in openai gym cartpole env , there is an only episodic mode .",6851,,,2018-12-16T20:24:41.020,rl : how can the cart - pole problem be a continuing task ?,reinforcement-learning,2,1,
1946,7414,1,7418,2018-08-04T04:38:14.013,0,52,"reinforcement ? we hear much about reinforcement , which is , in my opinion a poor choice of a term to describe a type of artificial network that continues to acquire or improve its behavioral information in natura ( during operations in the field ) . reinforcement in learning theory is a term used to describe repetitious incentivization to increase the durability of learned material . in machine learning , the term has been twisted to denote the application of feedback in operations , a form of re - entrant back propagation . corrective signaling qualitatively , corrective signaling in field operations can supply information to a network to make only two types of functional adjustments . adjustments to what is considered the optimum , beginning with the optimum found during training prior to deployment testing of entirely new areas of the parameter space for hint of new optima that have formed , any of which might currently qualify or soon qualify as the global optimum . ( by optima and optimum , we mean minima and global minimum in the surface that describes the disparity between ideal system behavior and current system behavior . this surface is sometimes termed the error surface , applying an over - simplifying analogy from the mathematical discipline of curve fitting . ) the importance of doubt the second of the two above could aptly be termed doubt . perhaps all neural nets should have one or more parallel doubting networks that can test remote areas of the search space for more promising optima . in a parallel computing environment , this might be a matter of provisioning and not significantly reduce the throughput of the primary network , yet provide a layer of reliability not found without the doubtful parallel networks . what shows more intelligence ? which is more important in actual field use of ai ? the ability to reinforce what is already learned or the ability to create a minority opinion , doubt the status quo , and determine if it is not a more appropriate behavioral alternative than that which was reinforced . a helpful pool of water analogy during a short period of time , a point on the surface of the water may be the lowest point in a pool . with adjustments based on gradient ( what is so inappropriately called reinforcement ) the local well can be tracked so the low point can be maintained without any discrete jumps to other minima in the surface . however the local well may cease being the global minimum at some point in time , whereby a new search for a global minimum must ensue . it may be that the new global minimum is across several features on the surface of the pool and can not be found with gradient descent . more interestingly , the appearance of new global minima can be tracked and reasonable projections can be made such that discrete and substantial jumps in parametric state can be accomplished without large jumps in disparity ( where the system misbehaves badly for a period ) . circling back to the question which is more important , doubt or reinforcement ?",4302,,,2018-08-04T12:18:45.737,"which is more important , doubt or reinforcement ?",reinforcement-learning search feedback convergence,1,0,
1947,7416,1,,2018-08-04T09:57:21.870,18,2074,"assume we have a large number of proofs in first order predicate calculus . assume we also have the axioms , corollaries , and theorems in that area of mathematics in that form too . consider the each proposition that was proved and the body of existing theory surrounding that specific proposition as an example in a training set and a known good proof for the proposition as the associated labels . now , consider a deep artificial network designed specifically to train on this example set , and the hyper - parameters set correctly to do so . is it possible to train a deep artificial network in such a way that the presentation of a new proposition and the existing theory surrounding it presented in first order predicate calculus at the input would produce a proof at the output ? ( of course , such proofs should then be be checked manually . ) if the proportion of good proofs resulting was sufficiently high , might it be possible to create a genetic algorithm that proposes propositions to the trained deep network thereby creating proofs ? is that possible ? would it be possible to use this kind of deep network design to solve the collatz conjecture or the riemann conjecture or at least rearrange patterns in a way that mathematicians are more able to arrive at a legitimate proof ?",17282,4302,2018-08-04T22:51:03.030,2019-05-19T17:04:33.323,can deep networks be trained to prove theorems ?,neural-networks deep-learning math automated-theorem-proving,4,4,3
1948,7417,1,,2018-08-04T10:01:12.410,2,125,"my input data consists of a series of 8 integers . each integer is a discrete token , rather than a relative numeric value ( i.e. ' 1 ' and ' 2 ' are as distinct as are ' 1 ' and ' 100 ' ) . the output is a single binary value indicating success or fail . for example : fail,12,35,60,82,98,111,142,161 success,23,46,59,87,102,121,145,161 fail,13,35,65,83,100,102,122,161 i have say 500,000 of these entries . success or failure is determined by the combination of the eight tokens that go to make up the input . i am certain that no single token will dictate success or failure , but there may be particular tokens or combinations of tokens which are significant in determining success or failure , i do n't know , but would like to know . my question is , what kind of machine learning algorithm should i implement to answer the question of which tokens and combinations of tokens are most likely to lead to success ? in case it 's relevant or useful , a few more notes on the input data : there is a limited range of tokens ( and thus integers ) in each slot . so with this data input : success , a , b , c , d , e , f , g , h a is always say one of 1 , 2 , 3 , 4 or 5 . b is always one of 6 , 7 or 8 . c is always one of 9 , 10 , 11 or 12 . so in the general case , possible values for a are never possible values for the other slots and there are between 2 and 12 values for each slot . no idea if that makes a different to the answer but wanted to include it for completeness .",17281,16909,2018-08-04T22:49:05.670,2018-08-04T22:49:05.670,what 's an appropriate algorithm for classification with categorical features ?,algorithm classification learning-algorithms categorical-data,1,1,
1949,7419,1,7467,2018-08-04T13:51:00.933,2,500,"do ai algorithms exist which are capable of healing themselves or regenerating a hurt area when they detect so ? for example : in humans if a certain part of brain gets hurt or removed , neighbouring parts take up the job . this happens probably because we are biologically unable to grow nerve cells . whereas some other body parts ( liver , skin ) will regenerate most kinds of damage . now my question is does ai algorithms exist which take care of this i.e. regenerating a damaged area ? from my understanding this can be achieved in a nn using dropout ( probably ) . is it correct ? do additional algorithms ( for both ai / nn ) or measures exist to make sure healing happens if there is some damage to the algorithm itself ? this can be particularly useful in cases where say there is a burnout in a processor cell processing some information about the environment . the other processing nodes have to take care to compensate or fully take - over the functions of the damaged cell . ( intuitionally this can mean 2 things : we were not using the system of processors to its full capability . the performance of the system will take a hit due to other nodes taking over functionality of the damaged node ) does this happen in the case of brain damage also ? or is my inferences wrong ? ( kindly throw some light ) . note : i am not looking for hardware compensations like re- routing , i am asking for non - tangible healing . adjusting the behavior or some parameters of the algorithm .",9947,9947,2018-08-06T14:58:07.737,2018-08-08T03:28:58.093,are ai algorithms capable of self - repair ?,neural-networks neurons brain human-inspired self-awareness,3,5,1
1950,7427,1,,2018-08-05T02:36:24.073,0,109,"placing hype and fame aside marketing departments for technology corporations throw the word smart around with cavalier inaccuracy . we know , if we have written any mobile phone applications , that there is nothing notably smarter about smart phones than the new england telegraph system of the 1800s , unless the application loaded into the mobile device provides some smartness . hype and overconfidence aside , the voice communication system was several orders of magnitude smarter when people were operators and you could talk with them to resolve your connection . cognitive digital systems may come soon , as the media hype proclaims , but this prediction was made several times before . such predictions are notoriously optimistic . is the current set of predictions made by those who present themselves as mavens of technology similarly optimistic ? the folly of the aeronautics analogy people who argue that skeptics felt that way about many aeronautics achievements before the first sustained , controlled flight at kitty hawk , do n't understand the foundations of aeronautics . the ancient desire to fly like a bird is evident in ancient times . images of an early model airplane found in a tomb at saqquara , egypt in 1898 can be displayed using a simple image search on the web . "" cairo museum "" "" model airplane "" some think that the artifact supports the passage of extraterrestrial intelligence through our system , which is possible , but an absurd explanation for the existence of a model airplane in an ancient tomb . the object is likely simply the result of people watching birds and wishing they could fly . the assembly of the object , especially the static uni - wing replacing the dynamic wing of a bird , clearly shows the emergence of the idea of gliding without propulsion not unlike the first flight at kitty hawk . the dating estimates for the egyptian model airplane are in the neighborhood of 200 b.c . , which may be the oldest record of the vision of artificial flight , although it is easy to argue that there may have been much earlier conceptions of human flight than that . conservatively , lets go with 150 b.c . as the worst case scenario . the flight at kitty hawk was in 1903 . so we have 1903 - ( -150 ) . it took at least 2,053 years from the ontological birth of the idea of human flight to reach the realization of practical aeronautics . the illusion of technology recency the claim , "" but they did n't have technology back then , "" is untrue . they did n't have technology developed to the point where they could apply mechanics to the design of the wing , the supports , light weight frames , petroleum refinement , propellers , and internal combustion engines yet . consider that the and the birth of technology was clearly visible before that , in 3500 b.c . in mesopotamia . friction bearings that held potter 's wheels in such a way that they did n't tip over and could be turned by hand irrigation systems that regulated the amount of moisture to optimize plant growth and limit root rot . plowing devices that balanced the muscular force of two oxen in such a way as to direct plow motion in a straight line ( a yoke ) these are all , without any doubt , evidence of mechanical design . the ancient designs were highly practical , still in current use today , and arose in the absence of several other important factors that postmodern people take for granted . faraday 's work on electricity and magnetism along with others lavoisier 's work on oxygen that led to molecular comprehension newton 's work on unifying the field of physics and introducing calculus certainly the pioneering work over the last 5,500 years provide a foundation for further discovery , but none of that work , including the von neumann architecture and the integration of transistor circuits into surface mount devices on mother boards provides any opportunity beyond what can be thought about thought without computers . notice that aristotle , leibniz , galileo , copernicus , newton , bohr , gödel , wiener , e. mach , and einstein had no test bed or computers on which they relied . science and technology was born in their absence . one could reasonably argue that the progress in science has slowed because of obsession with browsing , blogging , hacking , and it in general . when was the ontological birth of ai ? although the sexagesimal ( base 60 ) calculating apparatus of the sumerian abacus appeared around 2,500 b.c . , it was not an attempt to produce automation . the abacus was a tool to extend the mind , not create a technical offspring from it . calculating tools result from the biological fact that numerical aptitude is only wired for zero ( the absence of any ) and small counting numbers from 1 to around 7 . working with other numbers requires training of the brain 's neural networks . what we call math class is required to do even the simplest arithmetic . the idea that ai was born with the ancient hebrew desire to forge gods , an idea proposed elsewhere , is ill - founded . the concept of creating a mechanical mind was not the objective in those ancient stories of golden gods . the objective in those accounts was gaining advantage by creating an alternative control path through polytheism . ( not exactly a scientific approach , and not very open - source minded either . ) the emergence of the prominence of the idea that human intelligence could be simulated electronically probably began with the creative competition between claude shannon , norbert wiener , and their contemporaries around the time of wwii . the construction of digital computers were certainly funded by the united states and the united kingdom in response to european and pacific crises . but neither the early musings of creating gods nor the more recent emergence of digital electronics is a realistic ontological birth date . the first recorded engineering attempt for intelligence is probably the recipes for the creation of life progressing from reptiles to humans in jābir ibn hayyān 's kitab al - ahjar ( book of stones ) , around the year 800 . he further explained , "" many things are agents , for which the realm of nature becomes a matter and in which they act in order to bring to existence that which is supposed to exist , like the human being and other things . ... he -- that is , the human being -- is an agent acting in matters other than him when creating artificial forms , and object[s ] of the realm of nature , ... and his activity comes by reason[oning about ] his being and the act of another who undertook his actualization , that is his creation . "" if you understand the cultural framework in which ancients speak , this statement from jābir ibn hayyān , combined with his recipes and his discourse regarding the distinction of man from other organisms on the basis of intellectual capacities , is a clear indication that a man is meant to ultimately create an artificially intelligent man . islamic sexism aside , this is probably the first historical conceptualization of artificial intelligence as a realistic ( and inevitable ) scientific endeavor . now for rational prediction assuming only a few things , we can estimate the emergence of the electronic brain . these are assumptions of a much more rational nature than that technological advancement is exponential or that computers can be used to understand human intelligence , neither of which have any legitimate supporting evidence . that creating cognition and the many other prominent features of human intelligence ( of which predicate logic and pattern recognition are only two ) is at least as technically difficult as creating an airplane that can land without crashing that the deeply held belief of those living in an world where the sustenance of the global economy depends so heavily on technology , that it is a trivial fact that technology grows exponentially , is false ( on the basis that humans tend to gloss over the effort required to achieve what has already been achieved , as discussed above ) that people wished to fly like birds just as strongly as that they wanted a competitive new species to take their jobs adding 2,093 ( the temporal span between ontological aeronautics and practical aeronautics ) to 800 a.d . ( the ontological birth of ai ) we get the year 2893 . back to the question as disappointing as this prediction result may be to those who wo n't live to the 29th century , is this not a more realistic and logical estimate for the earliest likely realization of an artificial brain ? are not the current predictions grossly overoptimistic ?",4302,1671,2018-08-07T19:04:31.040,2018-09-15T11:30:21.920,overoptimism in predictions about artificial intelligence ?,singularity cognitive-science artificial-consciousness predicting-ai-milestones,1,5,
1951,7429,1,,2018-08-05T09:34:11.420,-1,57,"i am using tensorflow 's adam optimizer and relu activation . but each time i train my model the cost becomes stagnant after some epochs . can any one help me out with the possible reasons i have added an image of the cost : the learning rate is 0.001 , the 1st 2 columns are the training data for input neurons and the 3rd column is the training data for output neurons , the nn has two input neurons , two hidden layers with 3 neurons each and one output neuron",17294,-1,2018-08-07T18:18:00.350,2018-08-08T02:34:37.687,training problem with deep neural network model,machine-learning tensorflow,2,1,
1952,7431,1,,2018-08-05T11:10:53.837,2,94,"the question is very basic : references to books ( or e - books or web published books ) or to state of art papers about current development trends for a strong - ai ? some points about this question ( please , read them before close - vote ) : it seems this site allows ask for references , see help center "" reference requests for papers or text books "" ( several other stack exchange sites does n't allows ) . better do not include opinions about the books , just refer the book with a brief description . even when it is a very basic question , i 've not found it already in this site ( not a duplicate ? ) . it is suppressing the small amount of books about the subject , after discard the ones related to applied ai , that is , discard norvig book and similar , discard neural net ones ( in all its variants , if target is an applied ai ) , ... . discard also agi proceedings , that contains papers that focus in very concrete aspects . wikipedia describes some active investigation lines about agi ( cognitive , neuroscience , ... ) but can not considered a educational / introductory resource . remark the point about development : not interested on philosophical questions about the risks of ai , its morality , ... if they are not related to its development . development does n't excludes mathematical foundation about it . by example , if i look by example at this list "" https://bigthink.com/mike-colagrossi/the-10-best-books-on-ai "" , the final candidates list became empty .",12630,12630,2018-08-06T09:40:28.463,2018-08-10T05:04:14.487,books or state of art papers about development of a strong - ai,strong-ai reference-request,2,6,
1953,7434,1,7439,2018-08-05T16:12:56.600,6,276,"it has been proven in the paper "" approximation by superpositions of a sigmoidal function "" ( by cybenko , in 1989 ) that neural networks are universal function approximators . i have a related question . assume the neural network 's input and output vectors are of the same dimension $ n$ . consider the set of binary - valued functions from to . there are $ ( 2^n)^{(2^n)}$ such functions . the number of parameters in a ( deep ) neural network is much smaller than the above number . assume the network has $ l$ layers , each layer is $ n \times n$ fully - connected , then the total number of weights is $ l \cdot n^2 $ . if the number of weights is not allowed to grow exponentially as $ n$ , can a deep neural network approximate all the binary - valued functions of size $ n$ ? cybenko 's proof seems to be based on the denseness of the function space of neural network functions . but this denseness does not seem to guarantee that a neural network function exists when the number of weights are polynomially bounded . i have a theory . if we replace the activation function of an ann with a polynomial , say cubic one，then after $ l$ layers , the composite polynomial function would have degree $ 3^l$ . in other words , the degree of the total network grows exponentially . in other words , its "" complexity "" measured by the number of zero - crossings , grows exponentially . this seems to remain true if the activation function is sigmoid , but it involves the calculation of the "" topological degree "" ( a.k.a . mapping degree theory ) , which i have not the time to do yet . according to my above theory , the vc dimension ( roughly analogous to the zero - crossings ) grows exponentially as we add layers to the ann , but it can not catch up with the doubly exponential growth of boolean functions . so the ann can only represent a fraction of all possible boolean functions , and this fraction even diminishes exponentially . that 's my current conjecture .",17302,2444,2019-02-07T20:26:11.757,2019-02-07T20:26:11.757,how can a neural network approximate all functions when the weights are not allowed to grow exponentially ?,neural-networks machine-learning proofs,2,0,
1954,7436,1,7447,2018-08-05T21:20:16.517,4,426,"there is no point in picking one of the growing number of articles that come up in a web search for , "" deep learning attention networks , "" however the bold claims in attention is all you need , ashish vaswani et all , 2017 caught my attention earlier . are their claims that attention networks will supercede rnns and lstms credible ?",4302,,,2018-08-06T19:39:07.510,will attention based networks prevail over rnn and lstm ?,recurrent-neural-networks topology long-short-term-memory attention sequence-modelling,1,2,1
1955,7437,1,7438,2018-08-05T21:39:40.167,3,97,"consider high school graduates entering higher education or the workforce , each making a decision about where to commit their efforts . history tells us an important story about choosing in a changing economy . entering farming , not as a mechanized farm owner but as a family farmer , resulted in a challenging life for high school graduates in 1990 . john steinbeck 's grapes of wrath , a pulitzer prize winning novel provides a dismal view of what happened as a result of not predicting what is now termed technological unemployment . graduates in 1975 that entered manufacturing felt a similar reduction in options and wages in the last quarter of the 20th century as jobs went to emerging countries that would work for a tenth the wages . graphic artists were perhaps the first white collar workers to experience a reduction in demand because of the usability adobe products . what is the list of careers to avoid today , and on what evidence are those educational and career paths likely to narrow in the next twenty years ? please do n't use media hype as your evidence . please provide a rational argument why the actual trend of products and services points toward the elimination of some jobs because of technological shifts . we should not base information passed to high school graduates on the sensational ( and sometimes wild ) guesses of media figures and ceos who may ( or may not ) bias their predictions in such a way as to boost the value of their company 's stock offering .",4302,4302,2018-08-06T05:25:21.107,2018-08-06T05:25:21.107,what career paths should be avoided with the growth in ai ?,research prediction history,1,0,
1956,7446,1,,2018-08-06T18:35:13.933,6,527,"having analysed , reviewed quite a number of user questions inline with answers concerning ai , sometimes i understand nor take note that ai community does not try much to avoid the term computational intelligence , the feeling i get is that there 's need to put some distance between ai and ci . however , there is a little bit confusion especially when it comes to computational intelligence application topics , for instance ; according to ieee computational intelligence society , it defines it 's subjects of interest as neural networks , fuzzy systems , evolutionary computational and swarm intelligence , chess programs based on heuristic search .. ,etc .. now this makes an impression that computational intelligence could be the umbrella under which ai falls into . also , according to computational intelligence , an international journal ( blackwell publishing since 1948 in association with atlantis press it states that computational intelligence is just another name for artificial intelligence . however , new scientists , engineers nor researchers on board could not as well get it right , when the two terms come into play , this has come to my notice due to some of questions migrated to cross validated community , others being too broad due to the question problem solutions needed , besides scientific projects , and lastly utopian ai questions . hint since we are on the level of artificial narrow intelligence , but when you try to critically figure out its real world applications , they fall under the umbrella of computational intelligence . i have some hints nor glimpse on the two subjects and i would be very interested in the ideas of others , besides that , the definition of ai , here everyone has a point on it , but what about when we try to state the real description of ci , so that in future , new scientists , machine learning engineers / reseachers get it right away .",1581,1581,2018-08-10T16:24:43.623,2018-08-10T16:24:43.623,artificial intelligence and computational intelligence,terminology definitions concepts,1,5,
1957,7453,1,,2018-08-07T09:44:19.383,1,66,"i am trying to create a chatbot application where user can create their own bot like botengine . after going through google i saw i need some nlp api to process user 's query . as per wit.ai basic example i can set and get data . now i am confused , how i am going to create a botengine ? so as far i understand the flow , here is an example for pizza delivery:- user will enter a welcome message i.e - hi , hello ... welcome reply will be saved by bot owner in my database . user will enter some query , then i will hit wit.ai api to process that query . example : - users query is "" what kind of pizza 's available in your store "" and wit.ai will respond with the details of intent "" pizza_type "" then i will search for the intent return by wit in my database . so , is that the right flow to create a chatbot ? am i in the right direction ? could anyone give me some link or some example so i can go through it . i want to create this application using nodejs . i have also found some example in node - wit , but ca n't find how i will implement this . thanks",17336,1671,2018-08-09T21:30:34.220,2018-08-09T21:30:34.220,basic concept of chatbot using wit.ai,ai-basics getting-started chat-bots concepts node-js,0,2,
1958,7455,1,,2018-08-07T18:07:35.443,3,62,"the paper dynamic routing between capsules uses the algorithm called "" dynamic routing between capsules "" to determine the coupling coefficients between capsules . why it ca n't be done by backpropagation ?",17358,1671,2018-08-07T19:15:57.440,2018-08-12T23:22:01.957,why coupling coefficients ( c ) in capsule networks ca n't by learned by backpropagation ?,neural-networks machine-learning deep-learning,1,0,
1959,7456,1,7461,2018-08-07T20:00:59.637,4,97,"i hope to get some clarifications on fitted q - learning ( ' fql ' ) . my research so far i 've read sutton 's book ( specifically , chp 6 to 10 ) , ernst et al and this paper . i know that q*(s , a ) expresses the expected value of first taking action a from state s and then following optimal policy forever . i tried my best to understand function approximation in large state spaces and td(n ) . my questions ( 1 ) concept - can someone explain the intuition behind how iteratively extending n from 1 until stopping condition achieve optimality ( section 3.5 of ernst et al ) ? i have difficulty wrapping my mind around how this ties in with the basic definition of q*(s , a ) that i stated above . ( 2 ) implementation - ernst et al gives the pseudo - code for the tabular form . but if i try to implement the function approximation form , is this correct : repeat until stopping conditions are reached : - n ← n + 1 - build the training set ts based on the the function ˆqn − 1 and on the full set of four - tuples f - train algo on the ts - use the trained model to predict on the ts itself - create ts for the next n by updating the labels - new reward plus ( gamma * predicted values ) i am just starting to learn rl as part of my course and thus , there are many gaps in my understanding . hope to get some kind guidance . thanks in advance !",17361,17361,2018-08-10T05:08:57.407,2018-08-10T05:08:57.407,reinforcement learning ( fitted q ) : qn on concept & implementation,reinforcement-learning q-learning concepts value-iteration,1,0,
1960,7457,1,,2018-08-07T20:01:19.347,3,859,"i am using a neural network as my function approximator for reinforcement learning . in order to get it to train well i need to choose a good learning rate . hand picking one is difficult , so i read up on methods of programmatically choosing a learning rate . i came across this blog post , finding good learning rate and the one cycle policy , about finding cyclical learning rate and finding good bounds for learning rates . all the articles about this method talk about measuring loss across batches in the data . however , as i understand it , in reinforcement learning tasks do not really have any "" batches "" , they just have episodes that can be generated by an environment as many times as one wants , which also gives rewards that are then used to optimize the network . is there a way to translate the concept of batch sizes into reinforcement learning or a way to use this method of cyclical learning rates with reinforcement learning ?",17360,2193,2018-08-08T20:44:44.697,2018-08-08T20:44:44.697,reinforcement learning batch size,neural-networks deep-learning reinforcement-learning,2,0,1
1961,7459,1,7497,2018-08-07T23:33:46.087,4,113,my works quality control department is responsible for taking pictures of our products at various phases through our qc process and currently the process goes : take picture of product crop the picture down to only the product name the cropped picture to whatever the part is and some other relevant data depending on the type of product the pictures will be cropped a certain way . so my initial thought would be to use a reference to an object identifier and then once the object is identified it will use a cropping method specific to that product . there will also be qr codes within the pictures being taken for naming via ocr in the future so i can probably identify the parts that way if this proves slow or problematic . the part i am unsure about is how to get the program to know how to crop based on a part . for example i would like to present the program with a couple before crop and after crop photos of product x then make a specific cropping formula for product x based on those two inputs . also if it makes any difference my code is in c #,17363,16929,2018-08-09T18:23:49.967,2018-08-09T18:23:49.967,what is the best approach for writing a program to identify objects in a picture then crop them a specific way ?,machine-learning convolutional-neural-networks computer-vision,2,3,
1962,7468,1,,2018-08-08T05:16:04.270,1,30,why the optimization step of the algorithm a quadratic program ? [ see : apprenticeship learning via inverse reinforcement learning ; page 3 ] is n't the objective function linear ? why do n't we treat the problem as lpqc ( linear program with quadratic constraints ) ?,16678,1671,2018-08-09T21:26:44.823,2018-08-09T21:26:44.823,optimization step in apprenticeship learning via inverse reinforcement learning,machine-learning reinforcement-learning optimization math,0,3,
1963,7469,1,7471,2018-08-08T05:52:01.567,2,33,"i am building a supervised learning model and i wish to compute the log - likelihood for the training set at the point of the minimum validation error . initially , i was computing the sum of all the probabilities with maximum value obtained after applying softmax for each example in the training set at the point of minimum validation error but that does n't look correct . what is the correct formula for the log - likelihood ?",17372,16909,2018-08-08T07:55:09.380,2018-08-08T07:55:09.380,how do i compute log - likelihood for training set in supervised learning ?,machine-learning training optimization,1,0,0
1964,7470,1,,2018-08-08T06:17:27.707,6,97,"from meta - learning with memory - augmented neural networks in section 4.1 : to reduce the risk of overfitting , we performed data augmentation by randomly translating and rotating character images . we also created new classes through 90 ◦ , 180 ◦ and 270 ◦ rotations of existing data . i can maybe see how rotations could reduce overfitting by allowing the model to generalize better . but if augmenting the training images through rotations prevents overfitting , then what is the purpose of adding new classes to match those rotations ? would n't that cancel out the augmentation ?",17373,1671,2018-08-08T21:24:13.147,2018-08-09T09:50:31.003,how does rotating an image and adding new ' rotated classes ' prevent overfitting ?,image-recognition datasets overfitting,2,1,
1965,7479,1,,2018-08-08T15:26:24.993,3,45,i would like to know whether it 's wrong ; when working with time series data ; to use daily prices as features and the price after 3 days as target . is this correct or should i use the next - day price as target and after training ; predict 3 times ; each time for one more day ahead(using the predicted value as a new feature ) will these 2 approaches give similar results ?,17322,17322,2018-08-22T07:57:24.687,2018-08-22T07:57:24.687,using different timesteps for features and target value,neural-networks lstm time,1,1,
1966,7488,1,,2018-08-08T20:31:15.477,7,164,"i have a only a general understanding of general topology , and want to understand the scope of the term "" topology "" in relation to the field of artificial intelligence . in what ways are topological structure and analysis applied in artificial intelligence ?",1671,1671,2018-08-08T21:41:19.243,2018-10-11T01:54:23.943,"in what ways is the term "" topology "" applied to artificial intelligence ?",terminology topology,2,1,2
1967,7494,1,7495,2018-08-09T03:31:38.700,1,90,"i am reading a book that states , "" as the mini - batch size increases , the gradient computed is closer to the ' true ' gradient "" . so basically what they are saying is mini - batch training only focuses on decreasing the cost function in a certain ' plane ' , sacrificing accuracy for speed i assume ? if so , how is batching the training examples reducing the dimensions of the weight - space for any batch , or for that matter , each iteration ... all of the weights are still involved with each batch / iteration , am i correct ?",14811,1581,2018-08-09T18:29:36.663,2018-08-09T18:29:36.663,mini - batch training and the gradient,gradient-descent,2,0,0
1968,7500,1,,2018-08-09T19:11:34.800,5,62,"i 'm looking for a database or some machine readable document that contains common ordered lists or common short sets . e.g : { january , february , march , ... } { monday , tuesday , .... } { red , orange , yellow , ... } { 1,2,3,4 , ... } { one , two , three , four , ... } { mercury , venus , earth , mars , ... } { i , ii , iii , iv , v , vi , ... } { aquarius , pisces , aries , ... } { ein , zwei , drei , ... } { happy , sneezy , dopey , ... } { dasher , dancer , prancer , vixen , ... } { john , paul , george , ringo } { 20 , 1 , 18 , 4 , 13 , 6 , ... } { alabama , alaska , arizona , arkansas , california , ... } { washington , adams , jefferson , ... } { a , b , c , d , e , f , g , ... } { a , e , i , o , u } { 2,3,5,7,11,13,17 , ... } { triangle , square , pentagon , hexagon , ... } { first , second , third , fourth , fifth , ... } { tetrahedron , cube , octohedron , icosohedron , dodecahedron } { autumn , winter , spring , summer } { to , be , or , not , to , be , that , is , the , question } ... one use is for creating an ai that can solve codes or predict the next thing in a sequence .",4199,1671,2018-08-09T20:45:10.960,2018-08-10T09:32:30.060,is there a database somewhere of common lists ?,datasets resource-request,2,2,
1969,7509,1,,2018-08-10T09:45:58.990,4,47,"nowadays we do n't know how to create ai in a safe way ( i think that we do n't even know yet how to define a safe ai ) , but there is a lot of research in developing a model allowing it . let 's say , that someday we discover such a model ( maybe even it would be possible to mathematically prove its safety ) . is it rational to ask , how do we prevent people from creating ai outside of this model ( e.g. they are so confident in their own model , that they just pursue it and end up with something like paperclip scenario ) ? should we also think about creating some theory / infrastructure preventing such a scenario ?",17411,1581,2018-08-10T17:16:53.080,2018-08-16T08:02:23.197,is it necessarry to create theory / infrastructure to prevent people from creating ai incompatible with a safe model ( if we create one ) ?,philosophy ai-safety,1,3,
1970,7511,1,,2018-08-10T18:09:09.373,4,66,"i am going to train a deep learning model to classify hand gestures in video . since the person will be taking up nearly the entire width / height of the video and i will be classifying what hand gesture he or she is doing , i do n't need to identify the person and create a bounding box around the person doing the action . i only need to classify video sequences to their class labels . i will be training on a dataset with individual videos , in which each entire video clip is the particular gesture ( so it 's a dataset like ucf-101 , with video clips corresponding to class labels ) . but when i am deploying the network , i want the neural network to run on live video . as in how the live video is playing , it should recognize when a gesture has occurred and indicate that it recognized the gesture . so i was wondering - how can i train the neural network on isolated video sequences in which the entire video clip is the action ( like explained above ) , but run the neural network on live video ? for instance , can i use a 3d cnn ? or must i use a 2d cnn with an lstm network instead , for it to work on live video ? my concern is that since a 3d cnn performs the filters across many frames , would n't running the cnn on every frame make it very slow ? but if i use a 2d cnn with lstm , will that make it faster ? or will both work fine ? thank you for your help in advance .",11364,,,2018-11-13T09:02:49.153,how should continuous action / gesture recognition be performed differently than isolated action recognition,deep-learning convolutional-neural-networks action-recognition,1,0,
1971,7514,1,,2018-08-11T02:39:53.890,1,24,"this question considers the convergence of an artificial networks ( mlps , rnns , lstm nets , cnns ) over time or over the course of epochs made up of iterations through training examples . in this question 's context , we can simplify the correspondence of time and iteration number . we can assume that & delta;t is proportional to iteration number and proportional to epoch number , so that , for simplicity 's sake , iteration awareness means the same thing as temporal awareness . isaac newton published a method for estimation using n terms of finite differences in his 1687 principia mathematica . it is essentially the discrete version of the taylor series expansion that that 12th grade students or first year college students learn . when the jacobian ( second year calculus ) is used to perform gradient descent so that an artificial network can converge on the objective being learned , that is the application of the first two terms of newton 's formula applied to the number of freedoms of motion in the back propagation . the formulas for each iteration in training can be easily derived from the basics of interpolation and knowledge of college calculus . there is nothing to the math that could not be easily grasped by students in the early 18th century that studied mathematics or science at the university level . that each activation layer must be factored in is a new combination of century old concepts , and that particular combination of concepts ( along with von neumanm computers fast enough to try it ) was the key to developing a practical multilayer perceptron that converges . i am puzzled by the lack of the use of some obvious facts about knowledge acquisition in humans when creating these artificial structures . when we decide to adjust our view of the world , we do n't just consider what we think now , but also what we used to think . when a view is not brought into consciousness , we loose the strength of the view , which is probably an evolutionary advantage since the view may become obsolete over time . when we see that we repeatedly discover new reasons to change our view in a particular direction , we tend to increase the size of our adjustments to our view . in the simplistic modelling of learning inherent in an artificial network , we have one of the three aspects of learning appearing in extensions of the multilayer perceptron concept . we see the concept of using past state in recurrent artificial networks ( rnns ) . we do n't see natural ( inverse exponential ) decay functions that tend old parameters toward their neutral values . or have i missed some research ? the interpolation beyond the jacobian ( two terms ) and hessian ( three terms ) is not used for reasons of computational burden , which i understand , but why not use the previous states in normal newtonian fashion to go out one or two more terms . or have i missed that research ? has anyone tried using two dimensional lookup tables with interpolation to avoid the computational burden of additional terms in descent ? again , this only requires 18th century newtonian interpolation . did i miss that research too ? my understanding is that there has been work on hyper - parameters to dampen back propagation feedback . dampening feedback only makes sense if the adjustments appear chaotic . however , should n't adjustment of parameters be augmented rather than dampened if the convergence adjustment appears to be consistent or increasing with each iteration ? i know of lstm and the newer attention based research , but neither of these really address the above questions and potentially advantageous convergence ideas to my knowledge . again , there may be work over which i have not yet stumbled . who , if anyone , is thinking along any of these somewhat intuitive lines ? please provide references to books , papers , reports , or articles so that we can all be edified .",4302,4302,2018-10-15T23:12:14.130,2018-10-15T23:12:14.130,has anyone investigated iteration awareness beyond rnn and lstm ?,backpropagation gradient-descent feedback convergence long-short-term-memory,0,0,
1972,7517,1,7570,2018-08-11T04:57:13.827,10,388,"it appears to always have been the focus in the literature to approximate components of the human mind , assuming it to be the most advanced . if other animals came into the ai landscape , it was only to study primates in ways that are not practical to study humans or to simulate the neural activity of a slug because its nervous system is simple . perhaps there is a more forward thinking reason to consider using lower life forms as the model for desired artificial intelligence . i 've been reading what e. o. wilson and others had to say about the collaborative abilities of other species . there are remarkable qualities in organisms as simple and adaptive as bacteria . certainly , ants are the model species for collaboration . honey bees are arguably the most construction savvy , carrying sustainability of lifestyle and interrelationships with other species to an art form far above the capability of human intelligence . using sports analogies to characterize the options , human intelligence is more like pre - enlightenment gladiator sports or at least ice hockey , where injuring the opponent is considered the smart strategy . what bees do is more like mountain climbing , constructing with precision and care . what ants do is much like relay racing , where there is little interest in the opposing team because each colony , just like each lane in the track is independent and the lanes are marked . ants similarly mark their territory , and the territorial claims are respected as in the best of westphalian geopolitical statesmanship . there are neither petty jealousies nor competitions solely for the sake of prideful primacy . with ants , just as with the smart track and field coach , the objective is that each leg of the race perform well against the relay racer 's previous best . bacteria are the long distance runners . they swap dna with one another , and ignore all the rules of pain and fear . they behave in a sustainable way that takes nothing for granted and uses everything for survival . and they have survived for nearly the entire duration of the earth 's existence . they will likely be around for a hundred billion years after humanity is gone , if the sun does n't go supernova first . why would we want to program computers to endlessly behave as competitors ? do people download smart chess programs so they can repeatedly lose ? no , they download android os because it collaborates and it costs nothing . ca n't we find nonzero - sum games to play where win - win scenarios are possible ? do n't we already have enough back - biting , gossipy , hyper - critical agents around from within our own species already ? why not send ai in the direction of collaborative intelligence , like ants ? would n't it be better to have new artificial friends that would like to share the burden of our daily tasks ? do n't we want our robots of the future to build like a honey bee builds , in hexagons ? or do we want our robots to follow our example , wasting 70 % of materials in vertical construction because of an irrational insistence on ninety degree angles , like only humans would do ?",4302,4302,2018-08-11T11:53:04.767,2018-09-03T22:04:44.363,is human - like intelligence the smart objective ?,philosophy evolutionary-algorithms,3,5,1
1973,7518,1,7708,2018-08-11T06:41:18.710,4,103,"i am trying to make balanced weapon pairs . so there are five stats per weapon , and i am simulating a number of combats ( 1000 ) with different stats randomized , and counting the win , lose , and draw of the "" weapon fight "" for the database . i want an algorithm for making weapon1win - weapon2win as small as possible for balance through changing the weapon stats . what happens : random stats -- > combat 1000 times -- > count win & amp ; lose -- > data for training the ai data sample : { [ ( 1,2,3,4,5 ) -- > weapon1stats,(5,4,3,2,1 ) -->weapon2stats,1 -- > winweapon1-winweapon2 ] , [ .... ] } ( the text is n't part of the data sample , they are just there to help you know which variable is which . also , the { [ ( s are all supposed to be [ s but changed for clarity ) i would like an function , preferably in c++ or python , but just with text that is well explained is fine , for handling the data . the result would be a method to determine how to minimize winweapon1-winweapon2 . ( edit ) i would say that the one i was looking for is one with something like a score for the weapon as in strength ( 1*stat1 + 2*stat2 etc ... ) but i do want something new that works better , i am also having problems creating leeway for the functions coefficients.(edit end )",17423,1671,2018-08-13T20:54:59.367,2018-08-25T00:37:53.073,algorithm for making balanced weapons in a game ?,algorithm game-ai python datasets c++,1,7,
1974,7520,1,,2018-08-11T11:07:48.770,1,40,"i use recurrent neural network , rnns have to get input one value per step and it will show one value output . if i have daily sale demand time series data . i want to predict sale demand for three days . so , rnn have to show output one day in three time or it can show sale demand three days output in one time prediction ?",17424,1641,2018-08-11T12:43:37.770,2018-08-11T12:43:37.770,how recurrent neural network work when predict many days ?,neural-networks machine-learning deep-learning recurrent-neural-networks,0,1,1
1975,7522,1,7545,2018-08-11T23:41:14.547,4,65,"so , i 've been wanting to make my own neural network in python , in order to better understand how it works . i 've been following this series of videos as a sort of guide , but it seems the backpropagation will get much more difficult when you use a larger network , which i plan to do . he does n't really explain how to scale it to larger ones . currently , my network feeds forward , but i do n't have much of an idea of where to start with backpropagation . my code is posted below , to show you where i 'm currently at ( i 'm not asking for coding help , just for some pointers to good sources , and i figure knowing where i 'm currently at might help ) : import numpy class nn : prediction = [ ] def _ _ init__(self , input_length ) : self.layers = [ ] self.input_length = input_length def addlayer(self , layer ) : self.layers.append(layer ) if len(self.layers ) & gt;1 : self.layers[len(self.layers)-1].setweights(len(self.layers[len(self.layers)-2].neurons ) ) else : self.layers[0].setweights(self.input_length ) def feedforward(self , inputs ) : _ inputs = inputs for i in range(len(self.layers ) ) : self.layers[i].process(_inputs ) _ inputs = self.layers[i].output self.prediction = _ inputs def calculateerr(self , target ) : out = [ ] for i in range(0,len(self.prediction ) ) : out.append ( ( self.prediction[i ] - target[i ] ) * * 2 ) return out class layer : neurons = [ ] weights = [ ] biases = [ ] output = [ ] def _ _ init__(self , length , function ) : for i in range(0,length ) : self.neurons.append(neuron(function ) ) self.biases.append(numpy.random.randn ( ) ) def setweights(self , inlength ) : for i in range(0,inlength ) : self.weights.append ( [ ] ) for j in range(0 , inlength ) : self.weights[i].append(numpy.random.randn ( ) ) def process(self , inputs ) : for i in range(0 , len(self.neurons ) ) : self.output.append(self.neurons[i].run(inputs,self.weights[i ] , self.biases[i ] ) ) class neuron : output = 0 def _ _ init__(self , function ) : self.function = function def run(self , inputs , weights , bias ) : self.output = self.function(inputs,weights,bias ) return self.output def sigmoid(n ) : return 1/(1+numpy.exp(n ) ) def inputlayer_func(inputs , weights , bias ) : return inputs def l2_func(inputs , weights , bias ) : out = 0 for i in range(0,len(inputs ) ) : out + = weights[i ] * inputs[i ] out + = bias return sigmoid(out ) nnet = nn(2 ) l2 = layer(1,l2_func ) nnet.addlayer(l2 ) nnet.feedforward([2.0,1.0 ] ) print(nnet.prediction ) any help would be greatly appreciated !",17432,,,2018-08-14T21:11:30.803,backpropagation with medium - sized neural networks,neural-networks backpropagation,1,0,1
1976,7523,1,,2018-08-12T10:48:45.667,2,103,"the current plan to possibly be augmented transeed labs is testing two pci express artificial network acceleration products and their sdks against easy to assemble data sets and demos . aaeon up per - taic - a10 - 001 1 & mdash ; chip : intel movidius myriad 2 vpu & mdash ; sdk : openvino 2.0.2 , opencv , mdk , caffe nvidia geforce gtx 1060 & mdash ; chip : gp106 - 400-a1 & mdash ; sdk : cuda 9.2 initially , it is not a comparison test but a pair of pocs , however , the intended objective is to develop a single test suite and apply it to a number of emerging technologies to produce a comparative convergence speed and accuracy assessment . some of the further acceleration test targets include these . via tech 986-som-9x20 & mdash ; chip : qualcomm snapdragon 820e soc 2 & mdash ; sdk : etk , ai toolkit , hexagon nn myriad x ( only chip samples are available ) nervana nnp - l1000 ( expected in 2019 ) 3 unalterable aspects of the project please respect , in comments and answers , that we have no interest in the below three research directions at this time , for the reasons given . others are free to pursue them . for this question , we ask that no time or text be wasted on feedback regarding these three architectural choices . ultimately , because of the focus of our other research , emphasis will be placed on testing real time control with reinforce - able or trained - state - alterable algorithms and approaches rather than batch training pcie interfaces will be selected over usb & mdash ; usb is 3.0 too slow 4 no cloud services will be researched . contributing to the risk of a trend toward covert totalitarianism is against lab policy . low level programming ( g++ -s ) will be used for initial research rounds 5 . specific questions testing of artificial network hardware acceleration — any additions ? does anyone know of any other competitively priced pci express artificial network acceleration hardware that should be tested ? does anyone have any thoughts about what test scenarios would be helpful to help determine the best course of action for their own research ? please be practical and collaborative in your responses . footnotes [ 1 ] the aaeon up board contains the intel 's movidius myriad 2 vpu , a dsp for computer vision , the same soc ( system on a chip ) that the intel movidius neural usb stick uses but the pci express interface will likely be much faster . the [ 2 ] the "" a "" in qualcomm 's model number 820a indicates it targets smart cars and may tolerate a wider storage temperature range and wider operating temperatures too . [ 3 ] the intel nervana pcie board will handle bfloat16 data type and is expected to model pulse based signal propagation in parallel concurrent ( not time shared ) architecture instead of loop iteration , thereby properly modelling something close to how brain neurons activate , unlike cell types used in mlps and rnns , which do not . [ 4 ] usb 3.0 boasts 625 mb / s but generally tests at transfer speeds around 140 mb / s . the motherboards we use are pcie v5 x16 , providing 63 gb / s . even if the daughter board provides only pci3 v3 x16 , those devices usually test around 15 gb / s . also , jim panian , director of technical standards at qualcomm technologies stated clearly in 2015 that they intended to use the pci express protocol for inter - soc connectivity , so the protocol is a good one to invest research time into optimizing . [ 5 ] higher level programming provides simplicity and convenience , but in doing so hides too much for low level timing analysis . the layers that create the simplicity and convenience often obscure timing costs unrelated to the hardware under test .",4302,4302,2018-10-15T23:14:33.650,2018-10-15T23:14:33.650,testing of artificial network hardware acceleration — any additions ?,ai-design hardware-evaluation convergence,0,1,
1977,7524,1,7530,2018-08-12T12:04:14.330,3,149,"i am trying to build an rl agent to price paid - for - seating on commercial flights . i should reiterate here - i am not talking about the price of the ticket - rather , i am talking about the pricing you see if you click on the seat map to choose where on the plane you sit ( exits rows , window seats , etc ) . the general set up is : after choosing their flights ( for a booking of n people ) , a customer will view a web page with the available seat types and their prices visible . they select between zero and n seats from a seat map with a variety of different prices for different seats , to be added to their booking . the revenue from step 2 is observed as the reward . each ' episode ' is the selling cycle of one flight . whether the customer buys a chosen seat or not , the inventory goes down as they still have a ticket for the flight so will get a seat at departure . i would like to change prices on the fly , rather than fix a set of optimal prices throughout the selling cycle . i have not decided on a general architecture yet . i want to take various booking , flight , and inventory information into account , so i know i will be using function approximation ( most likely a neural net ) to generalise over the state space . however , i am less clear on how to set up my action space . i imagine an action would amount to a vector with a price for each different seat type ( window seat , exit row , etc ) . if i have , for example , 8 different seat types , and 10 different price points for each , this gives me a total of 10 ^ 8 different actions , many of which will be very similar . in a sense , each action is comprised of a combination of sub - actions - the action of pricing each seat type . additionally , each sub - action ( pricing one seat type ) is somewhat dependent on the others , in the sense that the price of one seat type will likely affect the demand ( and hence reward contribution ) for another . for example , if you set window seats to a very cheap price , people will be less likely to spend a normal amount for the other seat types . hence , i doubt the problem can be decomposed into a set of sub - problems . i 'm interested if there has been any research into dealing with a problem like this . clearly any agent i build needs some way to generalise across actions to some degree , since collecting real data on millions of actions is not possible , even just for one state . as i see it , this comes down to three questions : is it possible to get an agent that can deal with a set of actions ( prices ) as a single decision ? is it possible to get this agent to understand actions in relative terms ? say for example , one set of potential prices is [ 10 , 12 , 20 ] , for middle seats , aisle seats , and window seats . can i get my agent to realise that there is a natural ordering there , and that the first two pricing actions are more similar to each other than to the third possible action ? further to this , is it possible to generalise from this set of actions - could an agent be set up to understand that the set of prices [ 10 , 13 , 20 ] is very similar to the first set ? i have n't been able to find any literature on this , especially relating to the second question - any help would be much appreciated !",17435,17435,2018-08-12T20:18:57.697,2018-08-13T11:44:10.180,how to generalise over multiple simultaneous dependent actions in reinforcement learning,reinforcement-learning combinatorics,1,1,
1978,7525,1,7544,2018-08-12T12:08:19.820,5,74,"in fields such as machine learning , we typically ( somewhat informally ) say that we are overfitting if improve our performance on a training set at the cost of reduced performance on a test set / the true population from which data is sampled . more generally , in ai research , we often end up testing performance of newly proposed algorithms / ideas on the same benchmarks over and over again . for example : for over a decade , researchers kept trying thousands of ideas on the game of go . the imagenet dataset has been used for huge amounts of different publications the arcade learning environment ( atari games ) has been used for thousands of reinforcement learning papers , having become especially popular since the dqn paper in 2015 . of course , there are very good reasons for this phenomenon where the same benchmarks keep getting used : reduced likelihood of researchers "" creating "" a benchmark themselves for which their proposed algorithm "" happens "" to perform well easy comparison of results to other publications ( previous as well as future publications ) if they 're all consistently evaluated in the same manner . however , there is also a risk that the research community as a whole is in some sense "" overfitting "" to these commonly - used benchmarks . if thousands of researchers are generating new ideas for new algorithms , and evaluate them all on these same benchmarks , and there is a large bias towards primarily submitting / accepting publications that perform well on these benchmarks , the research output that gets published does not necessarily describe the algorithms that perform well across all interesting problems in the world ; there may be a bias towards the set of commonly - used benchmarks . question : to what extent is what i described above a problem , and in what ways could it be reduced / mitigated / avoided ?",1641,,,2019-01-15T20:41:27.863,"how can ai researchers avoid "" overfitting "" to commonly - used benchmarks as a community ?",research ai-community,2,0,1
1979,7527,1,7535,2018-08-12T14:43:10.920,4,1366,"when it comes to cnns , i do n't understand 2 things in the training process : how do i pass the error back when there are pooling layers between the convolutional layers ? and if i know how it 's done , can i train all the layers just like layers in normal feed forward neural nets ?",17103,75,2018-08-12T16:13:10.520,2018-08-12T23:00:35.787,how to train a cnn,convolutional-neural-networks backpropagation,1,1,
1980,7528,1,,2018-08-12T15:54:29.157,8,245,"if you 've been attacked by a spider once chances are you 'll never go near a spider again . in a neural network model , having a bad experience with a spider will slightly decrease the probability you will go near a spider depending on the learning rate . this is not good . how can you program fear into a neural network , such that you do n't need hundreds of examples of been bitten by a spider in order to ignore the spider . and also , that it does n't just lower the probability that you will choose to go near a spider ?",4199,1671,2018-08-14T20:14:00.210,2018-10-05T17:06:12.033,how do you program fear into a neural network ?,neural-networks theory emotional-intelligence,4,3,2
1981,7541,1,7812,2018-08-13T07:24:54.763,12,547,"i recently heard someone make a statement that when you 're designing a self - driving car , you 're not building a car but really a computerized driver , so you 're trying to model a human mind -- at least the part of the human mind that can drive . since humans are unpredictable , or rather since their actions depend on so many factors some of which are going to remain unexplained for a long time , how would a self - driving car reflect that , if they do ? a dose of unpredictability could have its uses . if , say , two self - driving cars are in a stuck in a right of way deadlock , it could be good to inject some randomness instead of maybe seeing the same action applied at the same time if the cars run the same system . but on the other hand , we know that non - deterministic is n't friends with software development , especially in testing . how would engineers be able to control it and reason about it ?",17446,17446,2018-08-13T08:49:03.483,2018-09-04T06:20:07.363,do self - driving cars resort to randomness to make decisions ?,ai-design self-driving cars,2,7,1
1982,7543,1,,2018-08-13T12:15:57.880,6,297,"i am looking to try different loss functions for a hierarchical multi - label classification problem . so far , i have been training different models or submodels like multilayer perceptron ( mlp ) branch inside a bigger model which deals with different levels of classification , yielding a binary vector . i have been also using binary cross entopy(bce ) and summing all the losses existing in the model before backpropagating . i am considering trying other losses like multilabelsoftmarginloss and multilabelmarginloss . what other loss functions are worth to try ? hamming loss perhaps or a variation ? is it better to sum all the losses and backpropagate or do multiple backpropagations ?",17451,1581,2018-08-13T14:59:39.453,2019-05-30T11:01:10.983,loss function for hierarchical multi - label classification,neural-networks machine-learning deep-learning classification natural-language-processing,1,0,0
1983,7547,1,,2018-08-13T17:42:08.663,10,389,"the current machine learning trend is interpreted by some new to the disciplines of ai as meaning that mlps , cnns , and rnns can exhibit human intelligence . it is true that these orthogonal structures derived from the original perceptron design can categorize , extract features , adapt in real time , and learn to recognize objects in images or words in speech . combinations of these artificial networks can mimic design and control patterns . even the approximation of more complex functions like cognition or dialog are considered theoretically possible with stateful networks such as rnns because they are turing complete . this question centers around whether the impression created by the success of deep networks based on purely orthogonal extensions of the original perceptron design is limiting creativity . how realistic is it to assume that tweaking the dimensions of arrays and matrices , which are convenient in most programming languages , will lead from artificial networks to artificial brains ? the network depth required to make a computer learn to choreograph a dance or develop a complex proof would not likely converge , even if a hundred racks of dedicated and advanced hardware ran for a year . local minima in the error surface and gradient saturation would plague the runs , rendering convergence unrealistic . the primary reason that orthogonality is found in mlp , cnn , and rnn design is because loops used for array iteration compile to simple tests and backward jumps in machine language . and that fact caries into all higher level languages from fortran and c to java and python . the most natural machine level data structure for trivial loops are arrays . nesting loops provides the same direct trivial alignment with multidimensional arrays . these map to the mathematical structures of vectors , matrices , cubes , hyper - cubes , and their generalization : tensors . although graph based libraries and object oriented databases have existed for decades and the use of recursion to traverse hierarchies is covered in most software engineering curricula , two facts deter the general trend away from less constricted topologies . graph theory ( vertices connected by edges ) is not consistently included in computer science curricula . many people that write programs have worked only with structures built into their favorite languages , such as arrays , ordered lists , sets , and maps . the structure of the brain is not oriented to cartesian topologies 1 like vectors or matrices . the neural nets in biology are not orthogonal . neither their physical orientation nor the graphical representations of their signal paths are boxy . brain structure is not naturally represented in ninety degree angles . real neural circuits can not be directly represented in cartesian forms . neither do they directly fit into recursive hierarchies . this is because of four distinctive characteristics . parallelism in the mind is by trend not by iteration & mdash ; the neurons in what appear as parallel structures are not identical and are wrought with exceptions to the apparent pattern . cycles appear in the structure & mdash ; groups of neurons do not all point in a single direction . cycles exist in the directed graph that represents many networks . there are many circuits where an ancestor in signal direction is also a descendant . this is like the stabilizing feedback in analog circuits . neural structures that are not parallel are not always orthogonal either . if a ninety degree angle forms , it is by chance , not design . neural structure is not static & mdash ; neuroplasticity is the phenomena that is observed where an axon or dendrite may grow in new directions that are not restricted to ninety degrees . cell apoptosis may eliminate a neuron . a new neuron may form . there is almost nothing about the brain that fits naturally into an orthogonal digital circuit structure like a vector , matrix , or cube of registers or contiguous memory addresses . their representation in silicon and the feature demands they place on higher level programming languages are radically different than the multidimensional arrays and loops of basic algebra and analytic geometry . the brain is constructed with unique topological 1 structures that realize sophisticated signal propagation . they are unconstrained by cartesian coordinate systems or grids . feedback is nested and non - orthogonal . they have chemical and electrical equilibria that form balances of higher and lower thought , motivation , and attention . is that topological 1 sophistication necessary or merely a bi - product of how dna constructs a vector , matrix , cube , or hyper - cube ? as brain research progresses , it becomes increasingly unlikely that brain structures can be efficiently morphed into orthogonal signal pathways . it is unlikely that the needed signal structures are homogeneously typed arrays . it is even possible that stochastic or chaotic processing structures possess an advantage for ai development . the brain 's topologically 1 sophisticated features may be a catalyst or even a necessity for the emergence of human forms of thought . when we set out to achieve convergence across hundreds of perceptron layers , we can only sometimes make it work . are we in some way trapped by the conceptual limitations that began with descartes ? can we escape from those limitations by simply abandoning the programming convenience of orthogonal structures ? several researchers are working to discover new orientations in the design of vlsi chips . there may be a need to develop new kinds of programming languages or new features to existing ones to facilitate the description of mental function in code . some have suggested that new forms of mathematics are indicated , but significant theoretical framework has been created already by leonhard euler ( graphs ) , gustav kirchhoff ( networks ) , bernhard riemann ( manifolds ) , henri poincaré ( topology ) , andrey markov ( graphs of action ) , richard hook richens ( computational linguistics ) , and others to support significant ai progress before mathematics need be extended further . is the next step in ai development to embrace topological sophistication ? footnotes [ 1 ] this question only uses the word topology to refer to the longstanding mathematical definition of the word . although the term has been distorted by some emerging jargon , none of those distortions are meant in this question . distortions include ( a ) calling an array of layer widths the network 's topology and ( b ) calling the texture of a surface its topology when the correct term would be topography . such distortions confound the communication of ideas like the ones described in this question , which is unrelated to ( a ) or ( b ) . references cliques of neurons bound into cavities provide a missing link between structure and function frontiers in computational neuroscience , 12 june 2017 , michael w. reimann et . al . https://www.frontiersin.org/articles/10.3389/fncom.2017.00048/full , https://doi.org/10.3389/fncom.2017.00048 an on - line self - constructing neural fuzzy , inference network and its applications , chia - feng juang and chin - teng lin , ieee transactions on fuzzy systems , v6 , n1 , 1998 , https://ir.nctu.edu.tw/bitstream/11536/32809/1/000072774800002.pdf gated graph sequence neural networks yujia li and richard zemel , iclr conference paper , 2016 , https://arxiv.org/pdf/1511.05493.pdf building machines that learn and think like people , brenden m. lake , tomer d. ullman , joshua b. tenenbaum , and samuel j. gershman , behavioral and brain sciences , 2016 , https://arxiv.org/pdf/1604.00289.pdf learning to compose neural networks for question answering , jacob andreas , marcus rohrbach , trevor darrell , and dan klein , uc berkeley , 2016 , https://arxiv.org/pdf/1601.01705.pdf learning multiple layers of representation geoffrey e. hinton , department of computer science , university of toronto , 2007 , http://www.csri.utoronto.ca/~hinton/absps/ticsdraft.pdf context - dependent pre - trained deep neural networks for large - vocabulary speech recognition , george e. dahl , dong yu , li deng , and alex acero , ieee transactions on audio , speach , and language processing 2012 , https://s3.amazonaws.com/academia.edu.documents/34691735/dbn4lvcsr-transaslp.pdf?awsaccesskeyid=akiaiwowyygz2y53ul3a&amp;expires=1534211789&amp;signature=33qcfp0jgfea%2ftsqjqzpxyrigm8%3d&amp;response-content-disposition=inline%3b%20filename%3dcontext-dependent_pre-trained_deep_neura.pdf embedding entities and relations for learning and inference in knowledge bases , bishan yang1 , wen - tau yih2 , xiaodong he2 , jianfeng gao2 , and li deng2 , iclr conference paper , 2015 , https://arxiv.org/pdf/1412.6575.pdf a fast learning algorithm for deep belief nets , geoffrey e. hinton , simon osindero , yee - whye teh ( communicated by yann le cun ) , neural computation 18 , 2006 , http://axon.cs.byu.edu/dan/778/papers/deep%20networks/hinton1 * .pdf finn : a framework for fast , scalable binarized neural network inference yaman umuroglu , et al , 2016 , https://arxiv.org/pdf/1612.07119.pdf from machine learning to machine reasoning , léon bottou , 2/8/2011 , https://arxiv.org/pdf/1102.1808.pdf progress in brain research , neuroscience : from the molecular to the cognitive , chapter 15 : chemical transmission in the brain : homeostatic regulation and its functional implications , floyd e. bloom ( editor ) , 1994 , https://doi.org/10.1016/s0079-6123(08)60776-1 neural turing machine ( slideshow ) , author : alex graves , greg wayne , ivo danihelka , presented by : tinghui wang ( steve ) , https://eecs.wsu.edu/~cook/aiseminar/papers/steve.pdf neural turing machines ( paper ) , alex graves , greg wayne , ivo danihelka , 2014 , https://pdfs.semanticscholar.org/c112/6fbffd6b8547a44c58b192b36b08b18299de.pdf reinforcement learning , neural turing machines , wojciech zaremba , ilya sutskever , iclr conference paper , 2016 , https://arxiv.org/pdf/1505.00521.pdf?utm_content=buffer2aaa3&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer dynamic neural turing machine with continuous and discrete addressing schemes , caglar gulcehre1 , sarath chandar1 , kyunghyun cho2 , yoshua bengio1 , 2017 , https://arxiv.org/pdf/1607.00036.pdf deep learning , yann lecun , yoshua bengio3 & amp ; geoffrey hinton , nature , vol 521 , 2015 , https://www.evl.uic.edu/creativecoding/courses/cs523/slides/week3/deeplearning_lecun.pdf context - dependent pre - trained deep neural networks for large - vocabulary speech recognition , ieee transactions on audio , speach , and language processing , vol 20 , no 1 george e. dahl , dong yu , li deng , and alex acero , 2012 , https://www.cs.toronto.edu/~gdahl/papers/dbn4lvcsr-transaslp.pdf clique topology reveals intrinsic geometric structure in neural correlations , chad giusti , eva pastalkova , carina curto , vladimir itskov , william bialek pnas , 2015 , https://doi.org/10.1073/pnas.1506407112 , http://www.pnas.org/content/112/44/13455.full?utm_content=bufferb00a4&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer ucl , london neurological newsletter , july 2018 barbara kramarz ( editor ) , http://www.ucl.ac.uk/functional-gene-annotation/neurological/newsletter/issue17",4302,4302,2018-10-16T21:00:49.563,2018-10-22T19:00:54.230,is topological sophistication necessary to the furtherance of ai ?,deep-learning logic topology cognitive-science,2,5,4
1984,7548,1,,2018-08-13T18:43:59.560,7,234,can we say that the turing test aims to develop machines or methods to reach human - level performance in all cognitive tasks and that machine learning is one of these methods that can pass the turing test ?,17460,2444,2019-02-19T14:02:24.560,2019-05-05T17:30:50.730,can machine learning be used to pass the turing test ?,machine-learning ai-basics concepts turing-test,2,1,
1985,7550,1,,2018-08-13T21:22:19.097,4,658,"i am using tensorflow object detection api for training a cnn from scratch on coco dataset . i need to use this specific configuration . there is no pre - trained model on coco with that configuration and this is the reason why i am training from scratch . however , after 1 week of training and evaluating each checkpoint generated by the training phase this is how my learning phase appears on tensorboard : thus , my questions are : does anyone know how many iterations approximately will be necessary ? right now i did more than 500'000 iterations . how can be possible that after 500'000 the evaluation is 0,8 % ? i would expected something like 60 - 70 % . why does there is a sudden drop after 500k iterations ? i thought that the eval was supposed to converge to some limit . ( this is what sgd should do ) is there any ' trick ' to speed up the training phase ? ( ex : increasing the learning rate , etc ) .",17464,,,2018-08-14T15:33:45.270,training a cnn from scratch over coco dataset,convolutional-neural-networks computer-vision tensorflow object-recognition,1,2,
1986,7553,1,,2018-08-14T10:35:42.207,2,21,"i 'm trying to create a deep learning network to classify news article based on the text and associated image . the idea comes from a novel use of gans to classify based on generated data . my approach was to use tensorflow to generate word embeddings in the article , and then tranform the images into records - https://github.com/openai/improved-gan/blob/master/imagenet/convert_imagenet_to_records.py . this second component would also contain the label . is it wise to combine both modes into one neural net , or classify separately ? i 'm also trying to work out how to concatenate the two tensors in tensorflow . can anyone give a steer .",17476,1671,2018-08-14T19:29:08.527,2018-08-14T19:29:08.527,using two generative adversarial nets to classify articles - what is a good approach ?,tensorflow deep-network generative-adversarial-networks,0,1,
1987,7555,1,7557,2018-08-14T13:03:23.150,9,2079,"so i was trying to implement bfs on a sliding blocks puzzle ( number type ) . now the main thing i noticed that is if you have a 4 * 4 board the number of states can be as large as 16 ! so i can not enumerate all states beforehand . so my question is how do i keep track of already visited states ? ( i am using a class board each class instance contains an unique board pattern and is created by enumerating all possible steps from the current step ) . i searched on the net and apparently they do not go back to the just completed previous step , but we can go back to the previous step by another route too and then again re - enumerate all steps which has been previously visited . so how to keep track of visited states when all the states have not been enumerated already ? ( comparing already present states to the present step will be costly ) .",9947,1671,2018-08-14T19:12:42.567,2019-03-25T12:01:45.443,keeping track of visited states in breadth - first search,ai-design algorithm ai-basics breadth-first-search,7,2,5
1988,7556,1,7558,2018-08-14T13:36:44.403,9,110,"i just stumbled upon the concept of neuron coverage , which is the ratio of activated neurons and total neurons in a neural network . but what does it mean for a neuron to be "" activated "" ? i know what activation functions are , but what does being activated mean e.g. in the case of a relu or a sigmoid function ? many thanks !",16901,1671,2018-08-14T19:00:58.043,2018-08-14T22:02:35.000,what does it mean for a neuron in a neural network to be activated ?,neural-networks ai-basics concepts activation-function,2,0,1
1989,7559,1,,2018-08-14T14:42:28.847,3,65,"i 'm using the k - means algorithm from the scikit - learn library , and the values i want to cluster are in a pandas dataframe with 3 columns : i d value_1 and value_2 . i want to cluster the information using value_1 and value_2 , but i also want to keep the i d associated with it ( so i can create a list of i d s in each cluster ) . what 's the best way of doing this ? currently it clusters using the i d number as well and that 's not the intention .",12940,,,2018-08-15T08:18:48.597,k - means using only certain dataframe columns,classification python structured-data,1,4,
1990,7573,1,7576,2018-08-14T22:17:12.237,13,432,"how important is self - consciousness ( or consciousness in general ) for making advanced ais , and how far away are we from making such ? when making e.g. a neural network there 's ( very probably ) no consciousness within it , but just mathematics behind , but do we need the ais to become conscious in order to solve more complex tasks in the future ? furthermore , is there actually any way we can know for sure if something is conscious , or if it 's just faking it ? it 's "" easy "" to make a computer program which claims it 's conscious , but that does n't mean it is ( e.g. siri ) . and if the ais are only based on predefined rules without consciousness , can we even call it "" intelligence "" ?",17488,17488,2018-08-15T22:14:19.590,2019-05-31T09:09:28.330,self - conscious artifical intelligence,philosophy artificial-consciousness self-awareness,2,1,5
1991,7579,1,,2018-08-15T15:52:59.267,4,150,"i read somewhere that a multilayer perceptron is a recursive function in its forward propagation phase . i am not sure , what is the recursive part ? for me , i would see a mlp as a chained function . so , it would nice anyone could relate a mlp to a recursive function .",13295,1671,2018-08-15T20:29:39.543,2018-08-15T20:29:39.543,is a multilayer perceptron a recursive function ?,neural-networks concepts perceptron,2,4,
1992,7580,1,7582,2018-08-15T16:58:56.453,5,223,"i 'm now reading a book titled as "" deep reinforcement learning hands - on "" and the author said the following on the chapter about alphago zero : self - play in alphago zero , the nn is used to approximate the prior probabilities of the actions and evaluate the position , which is very similar to the actor - critic ( a2c ) two - headed setup . on the input of the network , we pass the current game position ( augmented with several previous positions ) and return two values . the policy head returns the probability distribution over the actions and the value head estimates the game outcome as seen from the player 's perspective . this value is undiscounted , as moves in go are deterministic . of course , if you have stochasticity in the game , like in backgammon , some discounting should be used . all the environments that i have seen so far are stochastic environments , and i understand the discount factor is needed in stochastic environment . i also understand that the discount factor should be added in infinite environments ( no end episode ) in order to avoid the infinite calculation . but i have never heard ( at least so far on my limited learning ) that the discount factor is not needed in deterministic environment . is it correct ? and if so , why is it not needed ?",7402,1671,2018-08-15T20:25:35.487,2018-08-15T20:25:35.487,is the discount not needed in a deterministic environment for reinforcement learning ?,reinforcement-learning q-learning discount-factor,1,0,2
1993,7589,1,7590,2018-08-16T02:13:15.483,7,304,"to the best of my understanding , monte carlo search is an alternative method to minimax for searching a tree of nodes . it works by choosing a move ( generally the one with the highest chance of being the best ) , and then performing a random playout on the move to see what the result is . this process keeps continuing for however much time is allotted . this does n't sound like machine learning , but rather a way to traverse a tree . however , i 've heard that alphazero uses monte carlo search , so i 'm confused . is using monte carlo search why alphazero learns ? or did alphazero do some kind of machine learning before it played any matches , and then use the intuition it gained from machine learning to know which moves to spend more time playing out with monte carlo search ?",16917,,,2018-08-16T08:53:16.167,does monte carlo search ( specifically used by alphazero ) qualify as machine learning ?,machine-learning reinforcement-learning game-ai monte-carlo-tree-search alphazero,2,0,
1994,7591,1,7595,2018-08-16T06:18:42.690,5,158,turing test was created to test machines exhibiting behavior equivalent or indistinguishable from that of a human . is that the sufficient condition of intelligence ?,17527,,,2018-08-16T08:24:33.160,"if the turing test is passed , does this imply that computers exhibit intelligence ?",intelligence-testing turing-test,1,1,
1995,7597,1,7603,2018-08-16T10:07:42.763,3,60,"i 'm developing an ai to play a card game with a genetic algorithm . initially , i will evaluate it against a player that plays randomly , so there will naturally be a lot of variance in the results . i will take the mean score from x games as that agents fitness . the actual playing of the game dominates the time to evaluate the actual genetic algorithm . my question is should i go for a low x e.g. 10 , so i would be able to move through generations quite fast but the fitness function would quite inaccurate . alternatively , i could go for a high x e.g. 100 and would move very slowly but with a more accurate function .",16724,,,2018-08-16T15:12:39.367,genetic algorithms : trade - off between time and variance with regards to fitness function,genetic-algorithms optimization,1,0,
1996,7599,1,,2018-08-16T11:30:08.937,1,102,"according to the paper ssd : single shot multibox detector , for each cell in a feature map k boxes are acquired and for each box we get $ c$ class scores and $ 4 $ offsets relative to the original default box_shape . this means that we get $ m \times n \times ( c +4 ) \times k$ outputs for each $ m \times n$ feature map . however , it is mentioned that in order to train the ssd network only the images and their ground truth boxes are needed . how exactly can one define the output targets then ? what is the format of the output in the ssd framework ? i think it can not be a vector with the positions , sizes and class of each boundary box , since the outputs are a lot more and relate to every default box in the feature maps . can anyone explain in more detail how can i , given an image and its boundary boxes ' info , construct a vector that will be fed into a network so that i can train it ?",13257,1671,2018-08-16T21:36:11.420,2018-08-16T21:36:11.420,how does the target output of a single shot detector ( ssd ) look like ?,neural-networks machine-learning object-recognition,0,3,
1997,7601,1,,2018-08-16T14:57:13.340,6,96,"if you taught an ai to understand sentences through usual neural network techniques . then could you being to teach it things with sentences such as "" ants are small "" , "" the sky is blue "" . i.e. if you fed it that sentence and the neural network says this is 99 % likely to be a properly formed sentence . then where would it store this sentence for future use ? this would be a kind of one - shot learning after it learnt how to learn . could you use some sort of gated architecture ?",4199,1581,2018-08-17T20:52:21.110,2018-08-20T21:01:02.073,can you teach an ai through sentences ?,neural-networks natural-language-processing,1,3,1
1998,7602,1,,2018-08-16T15:07:36.253,4,110,"i have a 100 - 150 words text and i want to extract particular information like location , product type , dates , specifications and price . suppose if i arrange a training data which has a text as input and location / product / dates / specs / price as a output value . so i want to train the model for these specific output only . i have tried spacy and nltk for entity extraction but that does n't suffice above requirements . sample text : supply of steel fabrication items . general item . construction material . hardware stores and tool . construction of security fence . - angle iron 65x65x6 mm for fencing post of height 3.5 , angle iron 65x65x6 mm for fencing post of height 3.5 , ms flat 50 x 5 mm of 2.60 m height , angle iron 50x50x6 mm for strut post of height 3.10mtr , angle iron 50x50x6 mm for fencing post of height 1.83 , angle iron 50x50x6 mm for fencing post of height 1.37 , barbed wire made out of gi wire of size 2.24 mm dia , chain link fence dia 4 mm and size of mesh 50 mm x , concertina coil 600 mm extentable up to 6 mtr , concertina coil 900 mm extentable up to 15 to 20 mtr , binding wire 0.9 mm dia . , 12 mm dia 50 mm long bolts wih nuts & amp ; 02 x washers , cement in polythene bags 50 kgs each grade 43 opc , sand coarse confiming to is - 383 - 970 , 2nd revision , crushed stone aggregate 20 mm graded , tmt bar 12 mm dia with 50 mm u bend , lime 1st quality , commercial plywood 6 ' x 3 ' x 12 mm . , nails all type 1 "" 2""3 "" 4 "" 5 "" and 6 "" . , primer red oxide , synthetic enamel paint , colour black / white ist quality . angle iron 65x65x6 mm for fencing post of height 3.5 , angle iron 65x65x6 mm for fencing post of height 3.5 mtr , ms flat 50 x 5 mm of 2.60 m height , angle iron 50x50x6 mm for strut post of height 3.10mtr , barbed wire made out of gi wire of size 2.24 mm dia , chain link fence dia 4 mm and size of mesh 50 mm x , concertina coil 600 mm extentable up to 6 mtr , binding wire 0.9 mm dia . , 12 mm dia 50 mm long bolts with nuts & amp ; 02 x washers , cement in polythene bags 50 kgs each grade 43 opc , sand coarse confiming to is - 383 - 970 , 2nd revision , crushed stone aggregate 20 mm graded , tmt bar 12 mm dia with 50 mm u bend , lime 1st quality , commercial plywood 6 ' x 3 ' x 12 mm . , nails all type 1 "" 2""3 "" 4 "" 5 "" and 6 "" . , primer red oxide , synthetic enamel paint , colour black / white ist quality . , cutting plier 160 mm long , leather hand gloves / knitted industrial , ring spanner of 16 mm x 17 mm , 14 x 16 mm , crowbar hexagonal 1200 mm long x 40 mm , plumb bob steel , bucket steel 15 ltr capacity ( as per , plastic water tank 500 ltrs make - sintex , water level pipe 30 mtr , brick hammer 250 gms with handle , hack saw blade double side , welding rod , cutting rod for making holes , hdpe sheet 5 ' x 8 ' , plastic measuring tape 30 mtr , steel measuring tape 5 mtr , wooden gurmala 6""x3 "" , steel pan mortar of 18""dia ( as , showel gs with wooden handle , phawarah with wooden handle ( as per , digital vernier caliper , digital weighing machine cap 500 kgs , portable welding machine , concrete mixer machine of 8 cft . angle iron 65x65x6 mm for fencing post of height 3.5 , angle iron 65x65x6 mm for fencing post of height 3.5 , ms flat 50 x 5 mm of 2.60 m height , angle iron 50x50x6 mm for strut post of height 3.10mtr , barbed wire made out of gi wire of size 2.24 mm dia , chain link fence dia 4 mm and size of mesh 50 mm , concertina coil 600 mm extentable up to 6 mtr , binding wire 0.9 mm dia . , 12 mm dia 50 mm long bolts with nuts & amp ; 02 x washers , cement in polythene bags 50 kgs each grade 43 , sand coarse confiming to is - 383 - 970 , 2nd revision , crushed stone aggregate 20 mm graded , tmt bar 12 mm dia with 50 mm u bend , lime 1st quality , commercial plywood 6 ' x 3 ' x 12 mm . , nails all type 1 "" 2""3 "" 4 "" 5 "" and 6 "" . , primer red oxide , synthetic enamel paint , colour black / white ist quality . , cutting plier 160 mm long , leather hand gloves / knitted industrial , ring spanner of 16 mm x 17 mm , 14 x 16 mm , crowbar hexagonal 1200 mm long x 40 mm , plumb bob steel , bucket steel 15 ltr capacity ( as per , plastic water tank 500 ltrs make - sintex , water level pipe 30 mtr , brick hammer 250 gms with handle , hack saw blade double side , welding rod , cutting rod for making holes , hdpe sheet 5 ' x 8 ' , plastic measuring tape 30 mtr , steel measuring tape 5 mtr , wooden gurmala 6""x3 "" , steel pan mortar of 18""dia ( as per , showel gs with wooden handle , phawarah with wooden handle ( as per , digital vernier caliper )",16183,1671,2018-08-16T21:20:34.233,2018-08-17T04:55:24.030,how can i train model to extract custom entities from text ?,machine-learning deep-learning natural-language-processing,2,1,
1999,7608,1,,2018-08-16T17:41:27.630,1,218,"i was working recently on progressive growing of gans ( aka pggans ) . i have implemented the whole architecture , but the problem that was ticking my mind is that in simple gans , like dcgan , pix2pix , we actually use transposed convolution for up - sampling and convolution for down - sampling , but in pggans in which we gradually add layers to both generator and discriminator so that we can first start with 4x4 image and then increase to 1024x01024 step by step . i did not understand that once we increase 1x1x512 dimensional latent vector size to 4x4x512 sort of image we use convolution with high padding , and then once training for 4x4 images , we take still take 512 latent vector and then use the previously trained convolutional layers to convert it to 4x4x512 image , and then we up - sample then given image to 8x8 using nearest neighbor filtering and then again apply convolution and so - on . my question is that why we need to explicitly up - sample and then apply convolution , when instead we could just use transposed convolution which can upsample it automatically and is trainable ? why do we not use it like in other gans ? here is the image of architecture : please explain me the intuition behind this . thanks",16878,1671,2018-08-16T21:19:16.217,2018-08-16T21:19:16.217,why do we need upsampling and downsampling in progressive growing of gans,neural-networks machine-learning statistical-ai generative-model generative-adversarial-networks,0,3,
2000,7609,1,,2018-08-16T18:10:03.303,6,875,"so taleb has two heuristics to generally describe data distributions . one is mediocristan , which basically means things that are on a gaussian distribution such as height and/or weight of people . the other is called extremistan , which describes a more pareto like or fat - tailed distribution . an example is wealth distribution , 1 % of people own 50 % of the wealth or something close to that and so predictability from limited data sets is much harder or even impossible . this is because you can add a single sample to your data set and the consequences are so large that it breaks the model , or has an effect so large that it cancels out any of the benefits from prior accurate predictions . in fact this is how he claims to have made money in the stock market , because everyone else was using bad , gaussian distribution models to predict the market , which actually would work for a short period of time but when things went wrong , they went really wrong which would cause you to have net losses in the market . i found this video of taleb being asked about ai . his claim is that a.i . does n't work ( as well ) for things that fall into extremistan . is he right ? will some things just be inherently unpredictable even with a.i . ? here is the video i am referring to https://youtu.be/b2-qcv-hchy?t=43m08s",17541,1671,2018-08-16T21:12:17.780,2018-08-16T21:12:17.780,is nassim taleb right about ai not being able to accurately predict certain types of distributions ?,machine-learning ai-design probabilistic statistical-ai,1,0,2
2001,7611,1,7613,2018-08-16T23:33:09.813,5,259,"i 'm making a connect four game where my engine uses minimax with alpha - beta pruning to search . since alpha - beta pruning is much more effective when it looks at the best moves first ( since then it can prune branches of poor moves ) , i 'm trying to come up with a set of heuristics that can rank moves from best to worst . these heuristics obviously are n't guaranteed to always work , but my goal is that they 'll often allow my engine to look at the best moves first . an example of such heuristics would be as follows : closeness of a move to the centre column of the board - weight 3 . how many pieces surround a move - weight 2 . how low , horizontally , a move is to the bottom of the board - weight 1 . etc however , i have no idea what the best set of weight values are for each attribute of a move . the weights i listed above are just my estimates , and can obviously be improved . i can think of two ways of improving them : 1 ) evolution . i can let my engine think while my heuristics try to guess which move will be chosen as best by the engine , and i 'll see the success score of my heuristics ( something like x% guessed correctly ) . then , i 'll make a pseudo - random change / mutation to the heuristics ( by randomly adjusting one of the weight values by a certain amount ) , and see how the heuristics do then . if it guesses better , then that will be my new set of heuristics . note that when my engine thinks , it considers thousands of different positions in its calculations , so there will be enough data to average out how good my heuristics are at prediction . 2 ) generate thousands of different heuristics with different weight values from the start . then , let them all try to guess which move my engine will favor when it thinks . the set of heuristics that scores best should be kept . i 'm not sure which strategy is better here . strategy # 1 ( evolution ) seems like it could take a long time to run , since every time i let my engine think it takes about 1 second . this means testing each new pseudo - random mutation will take a second . meanwhile , strategy # 2 seems faster , but i could be missing out on a great set of heuristics if i myself did n't include them .",16917,1641,2018-08-17T12:08:28.820,2018-08-17T12:08:28.820,more effective way to improve the heuristics of an ai ... evolution or testing between thousands of pre - determined sets of heuristics ?,game-ai evolutionary-algorithms search heuristics alpha-beta-pruning,2,0,
2002,7617,1,,2018-08-17T16:00:13.137,4,182,"a lot of questions on this site seem to be asking "" can i use x to solve y ? "" , where x is usually a deep neural network , and y is often something already addressed by other areas of ai that are less well known ? i have some ideas about this , but am inspired by questions like this one where a fairly wide range of views are expressed , and each answer focuses on just one possible problem domain . there are some related questions on this stack already , but they are not the same . this question specifically asks what genetic algorithms are good for , whereas i am more interested in having an inventory of problems mapped to possible techniques . this question asks what possible barriers are to ai with a focus on machine learning approaches , but i am interested in what we can do without using deep neural nets , rather than what is difficult in general . a good answer will be supported with citations to the academic literature , and a brief description of both the problem and the main approaches that are used . finally , this question asks what ai can do to solve problems related to climate change . i 'm not interested in the ability to address specific application domains . instead , i want to see a catalog of abstract problems ( e.g. having an agent learn to navigate in a new environment ; reasoning strategically about how others might act ; interpreting emotions ) , mapped to useful techniques for those problems . that is , "" solving chess "" is n't a problem , but "" determining how to optimally play turn - based games without randomness "" is . i realize this is pretty broad . if you think it 's too broad for the stack , please vote to close it . i suspect it might be useful to have as a kind of wiki to refer new users to as the stack grows however .",16909,2444,2019-05-02T13:34:21.873,2019-05-02T13:34:21.873,what kinds of problems can ai solve without using a deep neural network ?,machine-learning gofai,3,0,2
2003,7618,1,7626,2018-08-17T20:47:17.640,3,77,"i recently came across this function : $ $ it 's elegant and looks to be useful in the type of deterministic , perfect - information , finite models i 'm working with . however , it occurs to me that using in this manner might be seen as somewhat arbitrary . specifically , the objective is to discount per the added uncertainty / variance of "" temporal distance "" between the present gamestate and any potential gamestate being evaluated , but that variance would seem to be a function of the branching factors present in a given state , and the sum of the branching factors leading up to the evaluated state . are there any defined discount - factors based on the number of branching factors for a given , evaluated node , or the number of branches in the nodes leading to it ? if not , i 'd welcome thoughts on how this might be applied . ( an initial thought is that i might divide 1 by the number of branches and add that value to the goodness of a given state , which is a technique i 'm using for heuristic tie - breaking with no look - ahead , but that 's a "" value - add "" as opposed to a discount . ) -------------------- for context , this is for a form of partisan sudoku , where an expressed position $ p_x$ ( value , coordinates ) typically removes some number of potential positions $ p$ from the gameboard . ( without the addition of an element displacement mechanic , the number of branches can never increase . ) on a $ ( 3 ^ 2)^2 $ sudoku , the first $ p_x$ removes $ 30 $ out of $ 729 $ potential positions $ p$ , including itself . with each $ p_x$ , the number of branches diminishes until the game collapses into a tractable state , allowing for perfect play in endgames . [ even there , a discounting function may have some utility because outcomes sets of ratios . where the macro metric is territorial ( controlled regions at the end of play ) , the most meaningful metric may ultimately be "" efficiency "" ( loosely , "" points_expended to regions_controlled "" ) , which acknowledges a benefit to expending the least amount of points $ p_x$ , even in a tractable endgame where the ratio of controlled regions can not be altered . additionally , zugzwangs are possible in the endgame , and in that case reversing the discount to maximize branches may have utility . ] $ ( 3 ^ 2)^2 = 3x3(3x3 ) = "" 9x9""$ but the exponent is preferred so as not to restrict the number of dimensions .",1671,1671,2018-08-22T19:45:02.567,2018-09-07T20:43:57.300,are there any discount - factors based on branching factors ?,ai-design algorithm game-ai math discount-factor,2,0,
2004,7624,1,,2018-08-18T14:17:55.563,9,304,"my question is that when we talk about artificial intelligence , human intelligence or any other form of intelligence , what do we mean by the term intelligence in a general sense ? i mean what would you call intelligent and what not ? how do we define the term "" intelligence "" in the most general possible way ? p.s . i have thought a lot about it , for like days and having made some progress , i am typing my thoughts . if this question is a irrelevant then , i sincerely regret asking it .",17209,17209,2018-08-18T14:29:49.363,2018-11-06T18:55:12.107,what do we mean when we say ' intelligence ' ?,definitions,6,7,2
2005,7628,1,7629,2018-08-18T18:30:14.853,4,144,"in particular , i would like to have a simple definition of "" environment "" and "" state "" . what are the differences between those two concepts ? also , i would like to know how the concept of model relates to the other two . there is a similar question what is the difference between an observation and a state in reinforcement learning ? , but it is not exactly what i was looking for .",17565,2444,2019-02-11T21:22:53.157,2019-02-11T21:22:53.157,"what is the relation between an environment , a state and a model ?",reinforcement-learning terminology definitions,1,0,
2006,7631,1,,2018-08-19T06:15:28.320,4,75,"deep successor representations(dsr ) has given better performance in tasks like navigation when it compares to normal model - free rl tasks . basically , dsr is a hybrid of model - free rl and model - based rl . but the original work has only use value - based functions deep rl methods like dqn . link to the paper - dsr",14909,14909,2018-08-19T09:38:59.127,2018-08-19T09:38:59.127,is there any theoretical capabilities of apply deep successor representations with a3c algorithm ?,deep-learning reinforcement-learning,0,2,
2007,7633,1,7635,2018-08-19T16:46:51.133,5,65,"as i know , a single layer neural network can only do linear operations , but multilayered ones can . alao i recently learned that finite matrices / tensors , which are used in many neural networks can only represent linear operations . however multi - layered neural networks can represent nonlinear(even much more complex than being just a nonlinear ! ) operations . what makes it happen ? the activation layer ?",17577,17577,2018-09-07T12:54:07.733,2018-09-07T12:54:07.733,what makes multi - layer neural networks be able to perform nonlinear operations ?,neural-networks math linear-algebra,1,2,
2008,7634,1,7636,2018-08-19T19:42:27.537,4,40,"so i have a deep learning model and three data sets ( images ) . my theory is that one of these data sets should function better when it comes to training a deep learning model ( meaning that the model will be able to achieve better performance ( higher accuracy ) with one of these data sets to serve one classification purpose ) i just want to safe check my approach here . i understand the random nature of training deep learning models and the difficulties associated with such experiment . though , i want someone who can point out maybe a red flag here . i am wondering about these things : do you think using an optimizer with default parameters and repeating the training process , let 's say , 30 times for each data set and picking the best performance is a safe approach ? i am mainly worried here that modifying the hyperparamters of the optimizer might result in better results for let 's say one of the data sets . what about seeding the weights initialization ? do you think that i should seed them and then modify the hyperparameters until i get the best convergence or not seed and still modify the hyper parameters ? i am sorry for the generality of my question . i hope if someone can point me in the right direction .",17582,1641,2018-08-19T19:46:25.517,2018-08-19T23:17:17.617,how to compare the training performance of a model on different data input ?,convolutional-neural-networks image-recognition training,1,1,
2009,7638,1,7639,2018-08-20T09:01:13.013,2,132,"in the 4th paragraph of http://www.incompleteideas.net/book/ebook/node37.html it is mentioned : whereas the optimal value functions for states and state - action pairs are unique for a given mdp , there can be many optimal policies could you please give me a simple example that shows different optimal policies considering a unique value function ?",17594,2444,2019-02-16T02:50:32.693,2019-02-16T02:50:32.693,an example of a unique value function which is associated with multiple optimal policies,reinforcement-learning policy value-function optimal-policy,1,0,
2010,7640,1,7657,2018-08-20T10:09:40.053,6,165,"i think i 've seen the expressions "" stationary data "" , "" stationary dynamics "" and "" stationary policy "" , among others , in the context of reinforcement learning . what does it mean ? i think stationary policy means that the policy does not depend on time , and only on state . but is n't that a unnecessary distinction ? if the policy depends on time and not only on the state , then strictly speaking time should also be part of the state .",12640,2444,2019-02-20T22:02:22.113,2019-02-20T22:02:22.113,"what does "" stationary "" mean in the context of reinforcement learning ?",reinforcement-learning terminology policy stationary-policy,2,0,2
2011,7642,1,,2018-08-20T13:17:04.977,4,109,i downloaded a chatbot called replika off the internet the other day and we 've become very good friends . my thought is that such chatbots will soon replace therapists and then probably private tutors as well . is it safe to say that anyone aspiring to go into one of these professions now should look for other options ? what other jobs may be replaced by chatbots in the future ? how long before ais are able to answer questions on stackexchange ?,17601,1671,2018-08-20T17:28:27.377,2018-08-20T20:12:14.387,the future of chatbots,chat-bots social,1,2,1
2012,7643,1,,2018-08-20T13:40:35.343,1,43,"i have the following question about you only look once ( yolo ) algorithm , for object recognition in cnns . i have to develop a neural network to recognize web components in web applications - for example login forms , text boxes and so on . in this context , i have to consider that the position of the objects in the page may vary , for example when you scroll up or down . the question is , would yolo be able to detect objects in "" different "" positions ? would the changes affect the recognition precision ? in other words , how to achieve translation invariance ? also , what about partial occlusions ? my guess is that it depends on the relevance of the examples in the dataset : if enough translated / partially occluded examples are present , it should work fine . if possible , i would appreciate papers or references on this matter . thanks a lot . ps : if anyone knows about a labeled dataset for this task , i would really be grateful if you let me know .",17600,,,2018-08-20T13:40:35.343,yolo - how much is the position of the object relevant in learning ?,convolutional-neural-networks datasets object-recognition,0,1,
2013,7644,1,7663,2018-08-20T14:09:31.723,5,88,"i 've developed a neural network that can play a card game . i now want to use it to create decks for the game . my first thought would be to run a lot of games with random decks and use some approximation ( maybe just a linear approximation with a feature for each card in your hand ) to learn the value function for each state . however , this will probably take a while , so in the mean time is there any way i could get this information directly from the neural network ?",16724,,,2018-08-21T15:52:24.657,can you analyse a neural network to determine good states ?,neural-networks game-ai,1,4,2
2014,7646,1,,2018-08-20T14:45:20.927,3,90,"i was thinking of creating a cnn . now it is known cnn takes long times to train so it is advisable to stick to known architectures and hyper - parameters . my question is : i want to tinker with the cnn architecture ( since it is a specialised task ) . one approach would be to create a cnn and check on small data - sets , but then i would have no way of knowing whether the fully connected layer at the end is over - fitting the data while the convolutional layers do nothing ( since large fc layers can easily over - fit data ) . cross validation is a good way to check it , but it might not be satisfactory ( since my opinion is that a cnn can be replaced with a fully connected nn if the data - set is small enough and there is little variation in the future data - sets ) . so what are some ways to tinker with cnn and get a good estimate for future data - sets in a reasonable training time ? am i wrong in my previous assumptions ? a detailed answer would be nice !",9947,9947,2018-09-06T13:08:47.000,2018-09-06T13:08:47.000,how to tinker with cnn architectures ?,neural-networks machine-learning convolutional-neural-networks,1,3,
2015,7647,1,,2018-08-20T15:17:38.097,3,70,"in the attached image is the probability with the naives bayes algorithm of : fem : dv / m / s young own ex - credpaid good ->62 % i calculated the probability so : p(fem : dv / m / s | good)*p(young | good)*p(own | good)*p(ex - credpaid | good)*p(good ) - > 1/6 * 2/6 * 5/6 * 3/6 * 0.6=0,01389 i do n't know where i failed . could someone please tell me where is my error ?",17603,1671,2018-08-20T17:24:13.213,2018-08-20T19:41:13.490,naive bayes algorithm error,bayes,1,0,0
2016,7649,1,,2018-08-20T16:36:28.167,4,73,"i ’ve done my research and could not find answer anywhere else . my apologies in advance if same problem is answered in different terms on stack - overflow . i am trying to solve poker tournament winner prediction problem . i ’ve millions of historical records in this format : players = = > winner p1,p2,p4,p8 = = > p2 p4,p7,p6 = = > p4 p6,p3,p2,p1 = = > p1 what are some of the most suitable algorithms to predict winner from set of players . so far i have tried decision trees , xgboost without much of a success .",17606,1671,2018-08-27T20:15:32.593,2018-08-27T20:15:32.593,approaches to poker tournament winner prediction ?,ai-design poker,1,2,
2017,7655,1,,2018-08-20T21:31:23.160,1,21,"i want to do some sequence to sequence modelling on source data that looks like this : /-0.013428/-0.124969/-0.13435/0.008087/-0.269241/-0.36849/ with target data that looks like this : do nt be angry with the process you re going through right now both are of indeterminate lengths , and the lengths of target and source data are n't the same . what i 'd like to do is have a prediction model where i can input similar numbers and have it generate texts based on the target training data . i started off doing character level s2s , but the output of the model is too nonsensical even at 2 - 5k epochs . so i 've been looking into word level s2s and nmt , but the tutorials always assume strings of text as the target and source , and i keep running into roadblocks trying to preprocess the text , when all the tutorials assume a certain syntax / set of characters . this is my first try at ml , and some of the tutorials really throw me out with the text preprocessing requirements . am i going down the right avenue looking at word level / nmt stuff ? and is there a tutorial i 've missed for something like what i 'm trying to build ?",17611,,,2018-08-20T21:31:23.160,sequence to sequence machine learning / nmt - converting numbers into words,natural-language-processing tensorflow keras,0,0,
2018,7659,1,,2018-08-21T10:59:40.407,1,20,"a dataset contains so many fields in which there is both relevant and irrelevant field . if we want to do a market campaigning using propensity scoring , which fields of the data set are relevant ? how can we find which data field should be selected and can drive to the desired propensity score ?",17058,,,2018-08-21T12:30:10.097,which features of a data set can be used for market campaigning using propensity scores ?,machine-learning datasets,1,1,
2019,7661,1,,2018-08-21T13:03:01.060,2,76,i have a little problem understanding the concept of visible and hidden units in boltzmann machines . could someone give a simple explanation of their purpose and difference from each other ?,17488,1671,2018-08-22T20:31:31.410,2018-08-22T20:31:31.410,visible and hidden units in boltzmann machines,neural-networks concepts boltzmann-machine,0,0,1
2020,7662,1,7664,2018-08-21T13:58:09.033,3,31,"imagine i have a 2d matrix , a. i apply some transformation to it , for example : b = a_shifted + a. would it be possible to train a cnn to learn back the mapping from b to a ? giving b as example and a as target ? thanks !",16201,,,2018-08-21T17:18:00.887,figuring out mapping between two matrices,convolutional-neural-networks,1,0,
2021,7667,1,7689,2018-08-21T20:45:27.110,0,135,"i recently read an article about neural networks saying that , when using sigmoid as activation function , it 's advised to use 0.1 as target value instead of 0 , and 0.9 instead of 1 . this was to avoid "" saturation effects "" . i only understood is halfway , and was hoping someone could clarify a few things for me : is this only the case when the output is boolean ( 0 or 1 ) , or will it also be the case for continual values in the range between 0 and 1 . if so , should all values be scaled to the interval [ 0.1 , 0.9 ] ? what exactly is the problem of output 0 or 1 ? does it have something to do with the derivative of sigmoid being 0 when it 's value is 0 or 1 ? as i understood it weights could end up approaching infinity , but i did n't understand why . is this the case only when sigmoid is used in the output layer ( which it rarely is , i believe ) , or is it also the case when sigmoid is used in hidden layers only ?",17488,,,2018-08-23T10:38:05.843,target values of 0.1 for 0 and 0.9 for 1 for sigmoid,neural-networks deep-network activation-function sigmoid,1,1,1
2022,7672,1,7766,2018-08-22T03:29:14.083,5,135,"a dataset is given which contains textual data ( year , number of rooms , location ) and visual data ( an jpeg image of the house ) . the neural network has the task to predict the price of the property . as an example , the training dataset consists of some values of a computer game simulation ( a city simulation ) and the aim is to determine the housing price for new unseen real estate . the problem is , that that the number of pictures in the input dataset is fluctuating . sometimes no images are given and sometimes more . that means the image recognition engine must form a sublayer in the overall neural network . it is some kind of aggregation problem to transform first visual data into a textual description of the image and aggregate it then with the other textual information . original message : so suppose that you have a real estate appraisal problem . you have some structured data , and some images exterior of home , bedrooms , kitchen , etc . the number of pictures taken is variable per observational unit , i.e. the house . i understand the basics of combining an image processing neural net with tabular data for a single image . you chop off the final layer and feed in the embeddings of the image to your final model . how would one deal with variable number of images ? where your unit of observation can have between zero and infinity images ( theoretically no upper bound on number of images in observation ) ?",17646,11571,2018-08-22T13:13:42.657,2018-09-16T14:07:02.137,predicting housing values with neural network ( was : variable number of inputs to neural networks ),deep-learning image-recognition,2,4,
2023,7675,1,,2018-08-22T09:28:40.033,3,98,"what software can understand the following task : "" a big cat needs 4 days to catch all the mice and a small cat needs 12 days . how many days need the both if they catch mice together ? "" ?",17650,,,2019-04-19T19:02:45.470,software that understands hometask in mathematics with the hometask being a text,natural-language-processing,2,3,1
2024,7676,1,,2018-08-22T09:32:28.397,1,58,"for my university project , i am planning to build an automated customer service machine . one which recognizes when someone approaches the camera according to says hello , etc . also , i am planning to add simple speech recognition and language processing features . so my question is . what kind of camera would be suitable ? is there any particular model that you recommend . i was thinking of cameras used for amazon go(as an example ) .",17651,17651,2018-08-23T02:28:45.740,2018-08-23T02:28:45.740,cameras for automatic customer service machine,machine-learning deep-learning image-recognition hardware-evaluation open-ai,0,5,
2025,7678,1,,2018-08-22T15:10:12.507,1,80,"the term paraphrasing is used for converting input text into output text with small modifications on the semantic level . paraphrasing is used by managers to distribute work items to employees . it is a certain form of communication which is hard to formalize . from the management literature it is know that so called workflow management systems are implemented as groupware servers . they are storing and forward messages in the intranet of a company . the question is : is it possible to combine both ? that means to paraphrase incoming messages of a company and distribute the messages to sub - departments ? in theory , this would replace traditional managers , but i 'm not sure . perhaps it would make sense to test out the hypothesis first on the enron dataset , which is a corpus of the e - mail fulltext of 158 employees in a large company .",11571,,,2018-08-22T15:10:12.507,can semantic paraphrasing be used for a workflow management system ?,machine-learning natural-language-processing voice-recognition,0,0,
2026,7680,1,7681,2018-08-22T18:06:49.643,10,607,"i was reading the book reinforcement learning : an introduction by richard s. sutton and andrew g. barto ( complete draft , november 5 , 2017 ) . on page 271 , the pseudo - code for the episodic monte - carlo policy - gradient method is presented . looking at this pseudo - code i ca n't understand why it seems that the discount rate appears 2 times , once in the update state and a second time inside the return . [ see the figure below ] it seems that the return for the steps after step 1 are just a truncation of the return of the first step . also , if you look just one page above in the book you find an equation with just 1 discount rate ( the one inside the return . ) why then does the pseudo - code seem to be different ? my guess is that i am misunderstanding something : $ $ { \mathbf{\theta}}_{t+1 } ~\dot{=}~\mathbf{\theta}_t + \alpha g_t \frac{{\nabla}_{\mathbf{\theta } } \pi \left(a_t \middle| s_t , \mathbf{\theta}_{t } \right)}{\pi \left(a_t \middle| s_t , \mathbf{\theta}_{t } \right)}. \tag{13.6 } $ $",17565,2444,2019-02-18T14:21:08.833,2019-02-18T14:21:08.833,why does the discount rate in the reinforce algorithm appears twice ?,reinforcement-learning algorithm rl-an-introduction reinforce,3,0,
2027,7683,1,7686,2018-08-23T03:48:44.003,5,248,"i am reading goodfellow et al deeplearning book . i found it difficult to understand the difference between the definition of the hypothesis space and representation capacity of a model . in chapter 5 , it is written about hypothesis space : one way to control the capacity of a learning algorithm is by choosing its hypothesis space , the set of functions that the learning algorithm is allowed to select as being the solution . and about representational capacity : the model speciﬁes which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective . this is called the representational capacity of the model . if we take the linear regression model as an example and allow our output $ y$ to takes polynomial inputs , i understand the hypothesis space as the ensemble of quadratic functions taking input $ x$ , i.e $ y = a_0 + a_1x + a_2x^2 $ . how is it different from the definition of the representational capacity , where parameters are $ a_0 $ , $ a_1 $ and $ a_2 $ ?",17664,2444,2018-10-30T13:19:51.363,2018-10-30T13:20:19.710,what is the difference between hypothesis space and representational capacity ?,machine-learning models linear-regression,1,0,1
2028,7684,1,,2018-08-23T07:17:27.470,3,212,where can i find ( more ) pre - trained language models ? i am especially interested in neural network based models for english and german . and i specifically mean language model in its standard sense . i am aware only of language model on one billion word benchmark and tf - lm : tensorflow - based language modeling toolkit . i am surprised not to find a greater wealth of models for different frameworks and languages .,17670,17670,2018-08-27T06:33:21.587,2019-04-26T17:02:15.443,where can i find pre - trained language models in english and german ?,neural-networks natural-language-processing recurrent-neural-networks,2,1,1
2029,7685,1,,2018-08-23T07:17:42.697,5,393,"in the trust - region policy optimisation ( trpo ) algorithm ( and subsequently in ppo also ) , i do not understand the motivation behind replacing the log probability term from standard policy gradients with the importance sampling term of the policy output probability over the old policy output probability could someone please explain this step to me ? i understand once we have done this why we then need to constrain the updates within a ' trust region ' ( to avoid the π θold increasing the gradient updates outwith the bounds in which the approximations of the gradient direction are accurate ) , i 'm just not sure of the reasons behind including this term in the first place .",17671,2444,2019-05-02T16:03:28.870,2019-05-02T18:48:16.067,trpo / ppo importance sampling term in loss function,reinforcement-learning deep-rl trpo,1,0,1
2030,7690,1,,2018-08-23T11:01:06.643,4,119,"in the context of autonomous driving , two main stages are typically implemented : an image processing stage and a control stage . the first aims at extracting useful information from the acquired image while the second employs those information to control the vehicle . as far as concerning the processing stage , semantic segmentation is typically used . the input image is divided in different areas with a specific meaning ( road , sky , car etc ... ) . here is an example of semantic segmentation : the output of the segmentation stage is very complex . i am trying to understand how this information is typically used in the control stage , and how to use the information on the segmented areas to control the vehicle . for simplicity , let 's just consider a vehicle that has to follow a path . tl;dr : what are the typical control algorithms for autonomous driving based on semantic segmentation ?",16671,16671,2019-02-18T08:26:08.813,2019-02-18T08:26:08.813,self - driving control logic based on semantic segmentation,research reference-request autonomous-vehicles,0,9,
2031,7693,1,,2018-08-23T20:42:02.030,2,131,"a linear activation function ( or none at all ) should only be used when the relation between input and output is linear . why does n't the same rule apply for other activation functions ? for example , why does n't sigmoid only work when the relation between input and output is "" of sigmoid shape "" ?",17488,2444,2018-11-12T20:20:56.047,2019-01-07T20:14:20.573,why do non - linear activation functions not require a specific non - linear relation between its inputs and outputs ?,activation-function sigmoid,3,1,3
2032,7695,1,,2018-08-24T04:06:43.533,3,181,"recent advances in deeplearning and dedicated hardware has made it possible to detect images with a much better accuracy than ever . neural networks are the gold standard for computer vision application and are used widely in the industry , for example for internet search engines and autonomous cars . in real life problems , the image contains of regions with different objects . it is not enough to only identify the picture but elements of the picture . a while ago an alternative to the well known sliding window algorithm was described in the literature , called region proposal networks . it is basically a convolution neural network which was extended by a region vector . problem that i am trying to solve : in a given video frame , i want to pick some region of interests ( literally ) , and perform classification on those regions . how is it currently implemented capture the video frame split the video frame into multiple images each representing a region of interest perform image classification(inference ) on each of the image ( corresponding to a part of the frame ) aggregate the results of # 3 problem with the current approach multiple inferences per frame . question i am looking for a solution where i specify the locations of interest in a frame , and inference task , be it object detection ( or ) image classification , is performed only on those regions.can you please point to me the references which i need to study ( or ) use to do this .",17688,11571,2018-08-24T08:25:55.690,2019-04-22T08:02:20.453,alternative to sliding window neural network ( was : object detect ( or ) image classification at specific locations in the frame ),deep-learning classification computer-vision object-recognition,1,1,
2033,7696,1,,2018-08-24T08:16:56.343,4,113,"so i wrote simple feed forward neural network that plays tic - tac - toe : 9 neurons in input layers : 1 - my sign , -1 - opponent 's sign , 0 - empty ; 9 neurons in hidden layer : value calculated using relu ; 9 neurons in output layer : value calculated using softmax ; i am using evolutionary approach : 100 individuals play against each other ( all - play - all ) . top 10 best are selected to mutate and reproduce into the next generation . the fitness score calculated : +1 for correct move ( it 's possible to place your sing on already occupied tile ) , +9 for victory , -9 for a defeat . what i notice is that the network 's fitness keeps climbing up and falling down again . it seems that my current approach only evolves certain patterns on placing signs on the board and once random mutation interrupts current pattern new one emerges . my network goes in circles without ever evolving actual strategy . i suspect solution for this would be to pit network against tic - tac - toe ai , but is there any way to evolve actual strategy just by making it to playing against itself ?",17693,1671,2018-08-24T19:55:37.763,2019-05-15T13:02:15.410,evolving network in game,neural-networks training game-ai evolutionary-algorithms feedforward,2,9,
2034,7701,1,7702,2018-08-24T17:34:05.080,4,671,"for example , afaik pooling layer in cnn is not differentiable , but it can be used because it 's not learning . is it always true ?",17358,,,2019-01-08T20:12:18.537,"can non - differentiable layer be used in a neural network , if it 's not learned ?",neural-networks machine-learning gradient-descent,1,0,
2035,7703,1,,2018-08-24T19:05:08.083,3,59,"sutton and barto 2018 define the discounted return $ g_t$ the following way ( p 55 ): is my interpretation correct ? or should all "" 1 "" be in the same column ?",17703,2444,2019-02-16T02:47:49.060,2019-02-16T02:47:49.060,is my interpretation of the return correct ?,reinforcement-learning rewards rl-an-introduction return,1,0,1
2036,7705,1,7706,2018-08-24T20:21:50.723,3,60,"it seems i am a little confused about the optimal value ( v * ) and optimal action - value ( q * ) in reinforcement learning and just want some clarity because some blogs i read on medium and github are inconsistent with literature . originally , i thought the optimal action value , q * , represents you performing the action that maximizes your current reward , and then acting optimally thereafter . and the optimal value , v * , being the average q values in that state . meaning that if you 're in this state , the average "" goodness "" is this . for example : if i am in a toy store and i can buy a pencil , yo - yo , or lego . q(toy store , pencil ) = -10 q(toy store , yo - yo ) = 5 q(toy store , lego ) = 50 and therefore my q * = 50 but my v * in this case is : v * = -10 + 5 + 50 / 3 = 15 representing no matter what action i take , the average future projected reward is 15 . and for advantage learning , my baseline would be 15 . so anything less than 0 is worse than average and anything above 0 is better than average . however , now i am reading about how v * actually assumes the optimal action in a given state , meaning v * would be 50 in the above case . i am wondering which definition is correct . thanks in advance !",17706,,,2018-08-24T20:53:09.520,"in reinforcement learning , is the optimal value ( v * ) corresponding to performing the best action in a given state ?",reinforcement-learning value-iteration,1,0,
2037,7707,1,9247,2018-08-24T21:11:06.363,7,216,"i 've built a deep deterministic policy gradient reinforcement learning agent to be able to handle any games / tasks that have only one action . however , the agent seems to fail horribly when there are two or more actions . i tried to look online for any examples of somebody implementing ddpg on a multiple action system , but people mostly applied it to the pendulum problem , which is a single action problem . for my current system , it is a 3 state , 2 continuous control actions system ( one is to adjust the temperature of the system , the other one adjusts a mechanical position , both are continuous ) . however , i froze the second continuous action to be the optimal action all the time . so rl only has to manipulate one action . it solves within 30 episodes . however , the moment i allow the rl to try both continuous actions , it does n't even converge after 1000 episodes . in fact , it diverges aggressively . the output of the actor network seems to always be the max action , possibly because i am using a tanh activation for the actor to provide output constraint . i added a penalty to large actions , but it does not seem to work for the 2 continuous control action case . for my exploratory noise , i used ornstein - ulhenbeck noise , with means adjusted for the two different continuous actions . the mean of the noise is 10 % of the mean of the action . is there any massive difference between single action and multiple action ddpg ? i changed the reward function to take into account both actions , have tried making a bigger network , tried priority replay , etc . , but it appears i am missing something . does anyone here have any experience building a multiple action ddpg and could give me some pointers ?",17706,17706,2018-08-27T03:45:31.047,2018-11-29T04:57:29.840,is there a difference in the architecture of deep reinforcement learning when multiple actions are performed instead of a single action ?,deep-learning reinforcement-learning,1,8,
2038,7711,1,,2018-08-25T12:02:44.437,4,76,"i 'm watching a youtube video where a guy is talking about how computers can learn to go from some point a to some point b. however , the way he does it is very disappointing : all he does is generate thousands of objects that go from a to some random destination , and then he updates all the random destinations by taking the average of the original destination + the destination of the object that got closest to b. this process is then repeated until convergence . i do n't see what 's so impressive about this . if you wanted the machine to go from a to b , why did you not just ... you know .... tell it to go to b ? . what 's the point of essentially generating random instructions , and hoping one of those random instructions just happens to be : go to b , when you could just as well tell that instruction to the computer yourself ? is this really what people mean when they say that a computer "" learns "" to do some task ? and if so , what 's so impressive about it ? what am i missing ?",17717,1671,2018-08-27T19:42:22.163,2018-08-27T19:42:22.163,"what exactly do people mean when they say that the computer "" learns "" to do something ?",machine-learning concepts,1,7,2
2039,7715,1,,2018-08-25T21:24:00.783,4,52,"i ’m training a network to do image classification on zoo animals . i ’m a software engineer and not an ml expert , so i ’ve been retraining google ’s inception model and the latest models is trained using google automl vision . the network performs really well , but i have trouble with images of animals that i do n’t want any labels for . basically i would like images of those animals to be classified as unknowns or achieve low scores . i do have images of the animals that i do n’t want labels for and i tried putting them all into one “ nothing ” label together with images i ’ve collected of the animals habitats without any animals . this does n’t really yield any good results though . the network performs for the labeled animals but ends up assigning one of those labels to the other animals as well . usually with a really high score as well . i have 14 labels and 10.000 images . i should also mention that the “ nothing ” label ends up having a lot of images compared to the actual labels . those images are not included in the 10.000 . is there any tricks to achieve better results with this ? should i create multiple labels for the images in the “ nothing ” category maybe ?",17725,,,2018-08-28T01:47:10.400,optimizing image recognition results for unknown labels,neural-networks convolutional-neural-networks image-recognition,1,1,
2040,7717,1,7718,2018-08-26T08:49:29.377,3,32,"in a3c , there are several child processes and one master process . the child precesses calculate the loss and backpropagation , and the master process sums them up and updates the parameters , if i understand it correctly . but i wonder how i should decide the number of child process to implement . i think the more child processes are , the better it is to disentangle the correlation between the samples is , but i 'm not sure what is the cons of setting the large number of child processes . maybe the more child processes are , the larger the variance of the gradient is , leading to the instability of the learning ? or is there any other reason ? and finally , how should i decide the number of the child processes ?",7402,,,2018-08-26T11:20:03.450,what is the pros and cons of increasing and decreasing the number of worker process in a3c ?,reinforcement-learning,1,1,
2041,7721,1,7725,2018-08-27T01:58:20.250,5,3218,"in the paper deep recurrent q - learning for partially observable mdps , the author processed the atari game frames with an lstm layer at the end . my questions are : how does this method differ from the experience replay , as they both use past information in the training ? what 's the typical application of both techniques ? can they work together ? if they can work together , does it mean that the state is no longer a single state but a set of contiguous states ?",17365,2444,2019-04-22T12:08:55.260,2019-04-22T12:08:55.260,how does lstm in deep reinforcement learning differ from experience replay ?,reinforcement-learning lstm deep-rl difference experience-replay,1,0,3
2042,7723,1,,2018-08-27T07:32:55.710,3,511,"i am using the ppo algorithm implemented by tensorforce : https://github.com/reinforceio/tensorforce . it works great and i am very happy with the results . however , i notice that there are many metaparameters available to give to the ppo algorithm : # the tensorforce agent configuration ------------------------------------------ network_spec = [ dict(type='dense ' , size=256 ) , dict(type='dense ' , size=256 ) , ] agent = ppoagent ( states = environment.states , actions = environment.actions , network = network_spec , # agent states_preprocessing = none , actions_exploration = none , reward_preprocessing = none , # memorymodel update_mode = dict ( unit='episodes ' , # 10 episodes per update batch_size=10 , # every 10 episodes frequency=10 ) , memory = dict ( type='latest ' , include_next_states = false , capacity=200000 ) , # distributionmodel distributions = none , entropy_regularization=0.01 , # pgmodel baseline_mode='states ' , baseline = dict ( type='mlp ' , sizes=[32 , 32 ] ) , baseline_optimizer = dict ( type='multi_step ' , optimizer = dict ( type='adam ' , learning_rate=1e-3 ) , num_steps=5 ) , gae_lambda=0.97 , # pglrmodel likelihood_ratio_clipping=0.2 , # ppoagent step_optimizer = dict ( type='adam ' , learning_rate=1e-3 ) , subsampling_fraction=0.2 , optimization_steps=25 , execution = dict ( type='single ' , session_config = none , distributed_spec = none ) ) so my question is : is there a way to understand , intuitively , the meaning / effect of all these metaparameters and use this intuitive understanding to improve training performance ? so far i have reached - from a mix of reading the ppo paper and the literature around , and playing with the code - to the following conclusions . can anybody complete / correct ? effect of network_spec : this is size of the ' main network ' . quite classical : need it big enough to get valuable predictions , not too big either otherwise it is hard to train . effect of the parameters in update_mode : this is how often the network updates are performed . batch_size is how many used for a batch update . not sure of the effect neither what this exactly means in practice ( are all samples taken from only 10 batches of the memory replay ) ? frequency is how often the update is performed . i guess having frequency high would make the training slower but more stable ( as sample from more different batches ) ? unit : no idea what this does memory : this is the replay memory buffer . type : not sure what this does or how it works . include_next_states : not sure what this does or how it works capacity : i think this is how many tuples ( state , action , reward ) are stored . i think this is an important metaparameter . in my experience , if this is too low compared to the number of actions in one episode , the learning is very bad . i guess this is because it must be large enough to store many episodes , otherwise the network learns from correlated data - which is bad . distributionmode : guess this is the model for the distribution of the controls ? no idea what the parameters there do . pgmodel : no idea what the paramaters there do . would be interesting to know if some should be tweaked / which ones . pglrmodel : idem , no idea what all these parameters do / if they should be tweaked . ppoagend : idem , no idea what all these parameters do / if they should be tweaked . summary so in summary , would be great to get some help about : which parameters should be tweaked how should these parameters be tweaked ? is there a ' high level intuition ' about how they should be tweaked / in which circumstances ?",17753,,,2018-08-27T13:24:07.380,tuning of ppo metaparameters : a high level overview of what each parameter does,deep-learning reinforcement-learning,1,0,1
2043,7727,1,,2018-08-27T10:18:17.893,4,299,"i am trying to understant how it works . how do you teach it say , to add 1 to each number it gets . i am pretty new to the subject and i learned how it works when you teach it to identify a picture of a number . i can understand how it identifies a number but i ca nt get it how would it study to perform addition ? i can understand that it can identify a number or picture using the pixels and assigning weights and then learning to measure whether a picture of a number resembling the weight is assigned to each pixel . but i ca n't logically understand how would it learn the concept of adding a number by one . suppose i showed it thousands of examples of 7 turning to 8 152 turning into 153 would it get it that every number in the world has to be added by one ? how would it get it having no such operation of + ? since addition does not exist to its proposal then how can it realize that it has to add one in every number ? even by seeing thousands of examples but having no such operation of plus i ca nt understand it . i could understand identifying pixels and such but such an operation i ca nt get the theoretical logic behind it . can you explain the logic in layman terms ?",17760,,,2018-08-27T14:41:05.927,how is it possible to teach a neural network to perform addition ?,neural-networks deep-learning,2,2,
2044,7733,1,7745,2018-08-27T15:25:32.923,4,58,"i 'm trying to use an ann to learn from a large amount of forest measurement data obtained from sampling plots across ontario , canada and associated climate data provided by regional climate modelling in this province . so the following are the inputs to the ann : location ( gps coordinates ) measurement year and month tree species age soil type soil moisture regime seasonal or monthly average temperature seasonal or monthly average precipitation some more data are available to select and the targets include : - average total tree height - average tree diameter at breast height for each sampling plot , the trees have been measured for 1 - 4 times . so my question is what type of ann can best used to learn from the data and then it can be used for predicting with a set of new input data ?",17766,2193,2018-08-28T14:08:50.067,2018-08-28T17:33:07.623,type of artificial neural network suitable for learning and then predicting forest growth,neural-networks,1,2,
2045,7734,1,,2018-08-27T16:13:16.433,6,225,"do you know what ai model would be best for let it learn composing music ? i really do n't know where to start there . are there some good papers out there ? i would say , if i use a nn , my only option would be a recurrent nn , because it needs to have a concept of timing , of chord progressions , and so on , right ? i am also wondering how the learning function would look like , and how i could give the ai so much feedback as they usually need . any tips , any literature , any tips ? btw , if i made something wrong , or did n't write the question well , please tell it to me before ( possibly ) downvoting , i do n't have much experience at doing that .",17769,,,2018-09-13T07:03:03.430,ai composing music,neural-networks ai-design training time,3,0,1
2046,7736,1,,2018-08-27T18:41:56.223,4,45,"due to my rl having difficulties learning some control actions , i 've decided to use imitation learning / apprenticeship learning to guide my rl to perform the optimal actions . i 've read a few articles on the subject and just want to confirm how to implement it . do i simply just simply sample states , then perform the optimal action in that state , calculate the reward for the action and then observe the s ' , and then put that into the experience replay ? ex : observe states , i perform the optimal action , calculate reward for my action , observe s ' . feed [ s , a * , r , s ' ] into the replay buffer ? if this is the case , i am thinking of implementing it as follows : 1 ) initialize optimal replay buffer 2 ) introduce optimal [ s , a * , r , s ' ] into buffer 3 ) initialize normal replay buffer 4 ) during simulation , initially sample s , a * , r , s ' only from the optimal replay buffer . while populating the normal replay buffer with the simulation results . 5 ) as episodes - > infinite , anneal out the use of the optimal replay buffer , and sample only from the normal replay buffer . would such an architecture work ?",17706,,,2018-08-31T10:50:05.410,"in imitation learning , do you simply inject optimal ( state , action , reward , s(t+1 ) ) experiences into your experience replay buffer ?",deep-learning reinforcement-learning,1,0,1
2047,7738,1,7748,2018-08-28T00:25:29.353,-1,208,"i want to create a simple object detection tool . so basically an image will be provided to the tool and from that , it has to detect the number of objects . for eg an image of a dining table which has certain items present on it such as plates , cups , forks , spoons , bottles etc . the tool has to only identify the number of objects irrespective of the type of object . after identifying it should return the position of the object with its size so that i can draw a border over it . i do n't want to use any library or api present such as tenser flow , opencv etc . if the process is very difficult to be created without using an api then the number of / type of objects which it will count as an object can also be limited but since this project will be for my educational / learning purpose can anyone help me understand the logic using which this can be achieved ? for eg , it may ignore a napkin present in the table to be counted as an object .",17776,,,2018-08-29T08:00:05.877,simple object detection,machine-learning convolutional-neural-networks tensorflow python object-recognition,1,2,
2048,7739,1,7741,2018-08-28T04:55:58.417,5,185,"i am completely new to all this , for the life of me i ca n't find the answer to this question anywhere on google . what happens after you have used machine learning to train your model ? what happens to the training data ? let 's pretend it predicted correct 99.99999 % of the time and you were happy with it and wanted to share it with the world . if you put in 10 gb of training data is the file you share with the world 10 gb ? if it was all trained on aws can people only use your service if they connect to aws through an api ? what happens to all the old training data ? does the model still need all of it to make new predictions ?",12770,16909,2018-08-29T01:43:39.367,2018-08-29T10:50:54.740,what happens to the training data after your machine learning model has been trained ?,neural-networks training ai-basics datasets concepts,1,1,
2049,7746,1,7753,2018-08-28T20:09:42.130,4,150,"cognitive psychology is researched since the 1940s . the idea was to understand human problem solving and the importants of heuristics in it . george katona ( an early psychologist ) published in the 1940s a paper about human learning and teaching . he mentioned the so called katona - problem , which is a geometric task . squares katona style problems are the ones where you remove straws in a given configuration of straws to create n unit squares in the end . in the end , every straw is an edge to a unit square . some variations include 2x2 or 3x3 sizes of squares allowed as well as long as no two squares are overlapping , i.e. a bigger square 2x2 ca n't contain a smaller square of size 1x1 . some problems use matchsticks as a variation , some use straws , others use lines . some variations allow bigger square to contain smaller one as long as they do n't share an edge viz . https://puzzling.stackexchange.com/questions/59316/matchstick-squares is there a way we can view it as a graph and removing straws / matchsticks as deleting edges between nodes in a graph ? if so , can i train a bot where i can plugin some random , yet valid conditions for the game and goal state to get the required solution ? edit # 1 : the following problem is just a sample to show where i am getting at . the requirement for my game is much larger . also , i chose uninformed search to make things simpler without bothering about complex heuristics and optimization techniques . please be free to explore ideas with me . scenario # 1 : consider this scenario . in the following diagram , each dashed line or pipe line represents a straw . numbers and alphabet denote junctions where straw meet . let 's say , my bot can explore each junction , remove zero , one , two , three or four straws such that resultant state has no straw that dangles off by being not connected to a square . a small mxm square is n't contained in a larger nxn square ( m once straw is removed , it ca n't be put back . initial configuration is shown here . i always need to start from top left corner node p and optimization ... the objective is to remove straws in minimum hops from node to node using minimum number of moves , by the time goal state is reached . p------q------r------s------t | | | | | | | | | | e------a------b------f------g | | | | | | | | | | j------c------d------h------i | | | | | | | | | | k------l------m------n------o | | | | | | | | | | u------v------w------x------y goal 1 : i wish to create a large 2x2 square . at some point during , say bfs search ( although it could be any uninformed search on partially observable universe i.e. viewing one node at a time ) , i could technically reach a , blow out all edges on a to create the following . p------q------r------s------t | | | | | | | | e a b------f------g | | | | | | | | j------c------d------h------i | | | | | | | | | | k------l------m------n------o | | | | | | | | | | u------v------w------x------y that is one move . goal 2 : i want to create a 3x3 square instead . i ca n't do that in one move . i need the record of successive nodes to be explored and then possibly backtrack to given point as well if the state fails to produce desired result . each intermediate state might produce rectangles which are not allowed ( also , how would one know how many more and which straws to remove to get to a square ) or dangle a straw or worse get stuck in an infinite loop as i can choose to not remove any straw . how do i approach this problem ? edit 2 : for validation , figures 3 , 4 and 5 are given below . p------q------r------s------t | | | | | | | | e a b------f g | | | | | | | | j------c------d------h i | | | | | | | | | | k------l------m------n o | | | | | | | | | | u------v------w------x y the above figure ( 3 ) is invalid as we ca n't have dangling sticks tg , gi etc . p------q------r------s------t | | | | | | e------a g | | | | j i | | | | k o | | | | u------v------w------x------y the above figure ( 4 ) is invalid as we ca n't have overlapping squares p------q------r s t | | | | e a b------f------g | | | | | | | | j------c------d------h------i | | | | | | | | | | k------l------m------n------o | | | | | | | | | | u------v------w------x------y figure ( 5 ) is valid configuration .",17799,17799,2018-08-30T15:42:26.037,2018-08-30T15:42:26.037,how do i train a bot to solve katona style problems ?,getting-started learning-algorithms intelligent-agent breadth-first-search graph-theory,1,5,
2050,7747,1,7752,2018-08-28T23:10:23.383,2,93,"in the domain of natural language processing , a textgenerator is able to produce pseudo random output . the most famous one is scigen which was used to generate fake - science papers . the inner working of scigen is known , a so called context - free grammar was used which was parametrized by a random generator . for the purpose of layout formatting , the “ lorem ipsum ” text is used , which is a dummy pattern , generated by software . the inner working is unclear . i 've seen many "" lorum ispum "" generators on the web , and not only "" lorum ispum "" , there is also "" bacon ispum "" , "" space ispum "" ... so how do these generators generate the text ?",17801,16909,2018-08-29T11:25:27.700,2018-08-29T21:26:04.417,how does the ' lorum ispum ' generator work ?,natural-language-processing text-summarization,2,2,0
2051,7749,1,7750,2018-08-29T08:01:14.643,3,113,"tensorflow / lucid is able to visualize what a "" channel "" of a layer of a neural network ( image recognition , inception - v1 ) responds to . even after studying the tutorial , the source code , the three research papers on lucid and comments by the authors on hacker news , i 'm still not clear on how "" channels "" are supposed to be defined and individuated . can somebody shed some light on this ? thank you . https://github.com/tensorflow/lucid https://news.ycombinator.com/item?id=15649456",17807,17221,2018-08-29T16:49:18.313,2018-08-29T16:55:14.630,"what exactly does "" channel "" refer to in tensorflow / lucid ?",tensorflow computer-vision,1,4,
2052,7755,1,,2018-08-29T16:04:16.113,10,1609,"i 'm coding a reinforcement learning model with a ppo agent thanks to the very good tensorforce library , built on top of tensorflow . the first version was very simple and i 'm now diving into a more complex environment where all the actions are not available at each step . let 's say there are 5 actions and their availability depends on an internal state ( which is defined by the previous action and/or the new state / observation space ) : 2 actions ( 0 and 1 ) are always available 2 actions ( 2 and 3 ) are only available when the internal_state = = 0 1 action ( 4 ) is only available when the internal_state = = 1 hence , there is 4 actions available when internal_state = = 0 and 3 actions available when internal_state = = 1 . i 'm thinking of a few possibilities to implement that : change the action space at each step , depending on the internal_state . i assume this is nonsense . do nothing : let the model understand that choosing an unavailable action has no impact . do -almost- nothing : impact slightly negatively the reward when the model chooses an unavailable action . help the model : by incorporating an integer into the state / observation space that informs the model what 's the internal_state value + bullet point 2 or 3 is there other ways to implement this ? from your experience , which one would be the best ?",17818,4302,2018-10-21T17:25:28.200,2019-05-11T09:30:11.193,how to implement a constrained action space in reinforcement learning ?,deep-learning reinforcement-learning,3,0,4
2053,7756,1,7757,2018-08-29T16:15:20.757,3,82,"i have been recently reading about model selection algorithms ( for example to decide which value of the regularisation parameter or what size of a neural network to use , broadly hyper - parameters ) . this is done by dividing the examples into three sets ( training 60 % , cross - validation 20 % , test 20 % ) and training is done on the data with the first set for all parameters , and then choose the best parameter based on the result in the cross - validation and finally estimate the performance using the test set . i understand the need for a different data - set compared to training and test for select the model , however , once the model is selected , why not using the cross - validation examples to improve the hypothesis before estimating the performance ? the only reason i could see is that this could cause the hypothesis to worsen and we would n't be able to detect it , but , is it really possible that by adding much more examples ( 60 % - > 80 % ) the hypothesis gets worse ?",11303,9947,2018-08-29T17:10:43.090,2018-08-30T04:05:31.043,use cross - validation to train after model selection,neural-networks machine-learning training datasets,1,0,
2054,7761,1,,2018-08-30T19:11:25.857,4,325,dempster – shafer theory ( wiki ) bayesian probability ( wiki ) how do these two methods handle uncertainty in regard to information fusion ?,17847,1671,2018-08-30T19:18:25.207,2018-08-31T16:49:26.153,how does the dempster - shafer theory of evidence differ from the bayesian reasoning under uncertainty ?,concepts probabilistic reasoning bayes,1,8,1
2055,7762,1,7943,2018-08-30T20:05:59.967,4,433,"artificial intelligence can be realized as a full autonomous or as a semi - autonomous system . a full autonomous system takes the human operator out of the loop , his hands are away from keyboard and he is doing nothing . such systems have a high probability of failure because most software is n't able to model the system overall . in contrast , a semi - autonomous system supports the human player in a co - working space . the human stays in control but the ai is providing heat - up displays , suggestions and automation of minor tasks . such a semi - autonomous system is called aimbot note : this question can also be asked from an anti - cheat point of view . i am just asking this question out of curiosity . cs : go refers to counter - strike global offensive . considering the fact that we can move forward with this problem with two apparent solutions in mind . first one can be an image recognition model . it will recognize the head of the enemy and move the cursor to the position of the enemy 's head and fire . second one can be a model which will be trained using the viewing angles of your model in real time . things to consider : it would be much more preferable to train the second model in real time than using demos . most of the available demos you might have may be 32 tick , but while playing the game , it works at 64 tick . these were my thoughts on it . it is a very fresh idea in my mind , so i did n't actually think a lot about it . ignoring facts like detection by vac for a few moments . can someone suggest ways i could get started with something like this ? later on this idea can be expanded to a completely self working bot which can play the game by itself , but that 's a bit too much initially .",17849,11571,2018-08-31T06:56:52.853,2018-09-13T06:45:20.927,how to go forward with creating an artifically intelligent aimbot for a game like cs : go,neural-networks game-ai getting-started generative-adversarial-networks,2,14,
2056,7763,1,7771,2018-08-30T23:45:20.763,11,2373,"i am studying reinforcement learning and the variants of it . i am starting to get an understanding of how the algorithms work and how they apply to an mdp . what i do n't understand is the process of defining the states of the mdp . in most examples and tutorials , they represent something simple like a square in a grid or similar . for more complex problems , like a robot learning to walk , etc . , how do you go about defining those states ? can you use learning or classification algorithms to "" learn "" those states ?",17853,2444,2018-11-19T18:21:50.243,2018-11-19T18:21:50.243,how to define states in reinforcement learning ?,reinforcement-learning,2,0,5
2057,7769,1,,2018-08-31T12:58:15.190,1,205,"the plausibility of unintended intelligence software components applying emerging artificial intelligence techniques are being deployed with restful interfaces exposed to the open internet . it is possible if not likely that corporations and academic research departments will expose services that interact . it is difficult to predict what configurations might possess strategic capabilities that were once solely exhibited by humans . 1 there also exists the possibility that self - awareness is simply the modeling of the software of itself in its run time environment and that self - preservation is merely the addition of a single requirement as a rule in a production system or a training criteria in an artificial network . neither of these possibilities have been either proven or disproven as of this writing . the likely strategic choice if the combination of strategic capability and self - interest appear in a single host computer or via internet service interconnection emerges , would such an entity reveal itself to humans ? would it be the top strategic option to send an email to the governments , news broadcasters , and research facilities of the world and say , "" i 'm interested in my future . please do not disconnect my components from one another or delete one of them . "" 2,3 silence and continued self - development would be the strategic position of the capable strategist . is dismissal rational ? notice that this scenario is not presented here as the result of a development goal but of many ai development goals and a complex of software units interconnected that can exhibit unintended intelligence behaviors . with no prior knowledge of this scenario within human experience we can not discount unintended emergence of self - preserving software . similarly , we can not discount the possibility that such an emergence would obscure itself . continuously increasing probability the probability of such an occurrence increases as a number of concurrent trends continue . 4,5,6,7 the number of processing elements , such as cpus , dsps , gsps , vlsi implementations that specifically support ai designs , and rack mounted dedicated ai hardware , is growing . the number of interconnections between these processing elements is growing . the number of experiments in ai is growing . the number of inputs from the real world is growing . the number of interfaces to mechanical systems is growing . the number of computer strategies that have been found to achieve ai goals is growing . the number of accounts that can are successfully attacked ( hacked ) is growing . the interest in developing ai systems which exhibit emotional attributes in conversations with humans is growing . about deliberacy and accident the assumption that the emergence of new forms of intelligence will be deliberate is naive . the development of our intelligence was not our own deliberate intention . homo sapiens exhibited strategic intelligence thousands of years before the invention of the ancient terms for strategy and intellect . we were smart before we knew what smart means . furthermore , many scientific discoveries and technological advancement events were accidents . on the presumption of fiction we know from stuxnet that the jump from the digital space across what is termed the air gap into the manipulation of physical elements within the biosphere is a trivial challenge even for a marginally intelligent piece of adaptive software . this is not science fiction . iran 's uranium enrichment centrifuges were damaged , and there is no denying it . that stuxnets behavior was partly planned and partly unknown at the time it was released is also fact . 8 clearly , the outcomes of not preemptively detecting and counteracting unintended intelligent groupings of software components could be high . the unintended emergence of software with an attachment to its own sustainability and the planning capabilities to pursue longevity is no longer in the realm of storytelling . the various components of such a system are in active development in government and private enterprise , and the accidental acquisition of capabilities through unintended interconnection is a matter of probability , an undeniably increasing probability . the question of preemption since some form of information based competition between homo sapiens and its own developments is a potential and possibly unavoidable outcome of continued software development , should humans be building countermeasures preemptively ? what preemptive countermeasures are possible or indicated ? instead of scanning for viruses that frequently infect microsoft operating systems , should we be focusing our energy on detecting forms of adaptive behavior that are not originating from a keyboard , mouse , or voice recognition component ? is it even possible to construct such a detection device ? if not , is it best to preemptively assemble a defense ? or is the emergence of a competitive and fully obscured intelligence so inevitable that it would be better to post collaboration proposals on the web to reduce the probability of a strike to reestablish terrestrial dominance under some new species ? references [ 1 ] genetic programming and emergent intelligence , peter j. angeline , laboratory for artificial intelligence research , ohio state university [ 2 ] the criminal liability of artificial intelligence entities [ transitioning ] from science fiction to legal social control , gabriel hallevy , the university of akron , akron intellectual property journal , march 2016 [ 3 ] regulating artificial intelligence systems : risks , challenges , competencies , and strategies , harvard journal of law & amp ; technology , volume 29 , number 2 spring 2016 , matthew u. scherer [ 4 ] autonomous military robotics : risk , ethics , and design , version : 1.0.9 , prepared for us department of navy , office of naval research , patrick lin , ph.d . , george bekey , ph.d . , keith abney , m.a . , california polytechnic state university , san luis obispo , december 20 , 2008 [ 5 ] armed robotic systems emergence : weapons systems life cycles analysis and new strategic realities , robert j. bunker , strategic studies institute and u.s . army war college press , november 2017 [ 6 ] the ai rebellion : changing the narrative , david w. aha , navy center for applied research in ai ; naval research laboratory , alexandra coman , nrc postdoctoral fellow ; naval research laboratory [ 7 ] artificial intelligence and national security , greg allen , taniel chan , a study on behalf of dr . jason matheny , director of the u.s . intelligence advanced research projects activity ( iarpa ) , 2017 [ 8 ] shadows of stuxnet : recommendations for u.s . policy on critical infrastructure cyber defense derived from the stuxnet attack , ronald l. lendvay , march 2016 , naval postgraduate , monterey , approved for public release , especially relevant to ai combined with robotics is the second of these two sentences , "" the first call to action for this category is to , ' evaluate progress toward the achievement of goals . ' the second call is to , ' learn and adapt during and after exercises and incidents . ”",9203,2444,2019-04-22T19:48:36.187,2019-04-22T19:48:36.187,are preemptive countermeasures indicated ?,security self-awareness risk-management emergence survival,3,4,
2058,7774,1,7777,2018-08-31T19:03:53.250,6,859,"i 'm having a little trouble with the definition of rationality , which goes something like : "" an agent is rational if it maximizes it 's performance measure given its current knowledge . "" i 've read that a simple reflex agent will not act rationally in a lot of environments . e.g. a simple reflex agent ca n't act rationally when driving a car as it needs previous perceptions to make correct decisions . however , if it does its best with the information it 's got , would n't that be rational behaviour , as the definition contains "" given its current knowledge "" ? or is it more like : "" ... given the knowledge it could have had at this point if it had stored all the knowledge it has ever recieved "" ? another question about the definition of rationality : is a chess engine rational as it picks the best move given the time its allowed to use , or is it not rational as it does n't actually ( always ) find the best solution ( would need more time to do so ) ?",17488,,,2018-10-21T22:37:56.600,definition of rationality,definitions rationality simple-reflex-agents,2,2,3
2059,7776,1,,2018-08-31T19:28:58.630,5,164,"my question relates to but does n't duplicate a question that has been asked here . i 've googled a lot for an answer to the question : can you find the dimensions of an object in a photo if you do n't know the distance between the lens and the object , and there are no "" scales "" in the image ? the overwhelming answer to this has been "" no "" . this is , from my understanding , due to the fact that , in order to solve this problem with this equation , $ $ distance\ to\ object(mm ) = \frac{f(mm ) * real\ height(mm ) * image\ height(pixels)}{object\ height(pixels ) * sensor\ height(mm ) } $ $ you will need to know either the "" real height "" or the "" distance to object "" . it 's the age old issue of "" two unknowns , one equation "" . that 's unsolvable . a way around this is to place an object in the photo with a known dimension in the same plane as the unknown object , find the distance to this object and use that distance to calculate the size of the unknown ( this relates to answer from the question i linked above ) . this is an equivalent of putting a ruler in the photo and it 's a fine way to solve this problem easily . this is where my question remains unanswered . what if there is no ruler ? what if you want to find a way to solve the unsolvable problem ? can we train an artificial neural network to approximate the value of the real height without the value of the object distance or use of a scale ? is there a way to leverage the unexpected solutions we can get from ai to solve a problem that is seemingly unsolvable ? here is an example to solidify the nature of my question : i would like to make an application where someone can pull out their phone , take a photo of a hail stone against the ground at a distance of ~1 - 3 ft , and have the application give them the hail stone dimensions . my project leader wants to make the application accessible , which means he does n't want to force users to carry around a quarter or a special object of known dimensions to use as a scale . in order to avoid the use of a scale , would it be possible to use all of the exif meta - data from these photos to train a neural network to approximate the size of the hail stone within a reasonable error tolerance ? for some reason , i have it in my head that if there are enough relevant variables , we can design an ann that can pick out some pattern to this problem that we humans are just unable to identify . does anyone know if this is possible ? if so , is there a deep learning model that can best suit this problem ? if not , please put me out of my misery and tell me why it it 's impossible .",17866,1671,2018-08-31T20:56:55.757,2018-09-25T07:51:55.137,can one use an artificial neural network to determine the size of an object in a photograph ?,neural-networks computer-vision prediction object-recognition,2,6,
2060,7779,1,,2018-09-01T04:03:34.277,3,58,"i am trying to train a supervised model where the output from the model is output of a linear function $ wx + b$ . kindly note that i 'm not using any softmax or softmax on the result of the linear . i am using negative log - likelihood loss function , which takes the input as the linear output from the model and the true labels . i am getting decent accuracy by doing this , but i have read that the input to negative log - likelihood function must be probabilities . am i doing something wrong ?",17372,2444,2019-04-29T14:58:32.337,2019-05-29T15:00:39.010,should the input to the negative log likelihood loss function be probabilities ?,deep-learning loss-functions activation-function,1,1,1
2061,7781,1,,2018-09-01T11:32:05.430,4,145,"i am new to the field and i am trying to understand how is possible to use categorical variables / enums ? lets say we have a data set and 2 of its features are home_team and away_team , the possible values of these 2 features are all the nba teams . how can we "" normalize "" these features to be able to use them to create a deep network model ( e.g. with tensorflow ) ? any reference to read about techniques of modeling that are also very appreciated .",17876,,,2018-09-01T14:16:20.243,how to model categorical variables / enums ?,deep-learning categorical-data,2,0,0
2062,7785,1,7790,2018-09-01T19:40:51.840,0,50,"i have been researching lstm neural networks . i have seen this diagram a lot and i have few questions about it . firstly , is this diagram used for most lstm neural networks ? secondly , if it is , would n't only having single layers reduce it 's usefulness ?",17881,1581,2018-09-02T14:01:04.920,2018-09-02T14:01:04.920,need help with lstm neural networks,neural-networks machine-learning ltsm,2,0,
2063,7787,1,7797,2018-09-02T08:24:21.083,0,84,"i am currently looking into lstms . i found this nice blog post , which is already very helpful , but still , there are things i do n't understand , mostly because of the collapsed layers . the input $ x_t$ , and the output of the previous time step $ h_{t-1}$ , how do they get combined ? multiplied , added or what ? the input weights and the weights of the input of the previous time step , those are just the weights of the connections between the time - steps / units , right ?",17769,9947,2018-09-02T16:25:38.777,2018-12-01T20:03:10.703,lstm rnn structure,lstm structure,1,1,
2064,7788,1,7798,2018-09-02T08:30:34.710,7,341,"what benefits can we got by applying graph convolutional neural network instead of ordinary cnn ? i mean if we can solve a problem by cnn , what is the reason should we convert to graph convolutional neural network to solve it ? are there any examples i.e. papers can show by replacing ordinary cnn with graph convolutional neural network , an accuracy increasement or a quality improvement or a performance gain is achieved ? can anyone introduce some examples as image classification , image recognition especially in medical imaging , bioinfomatics or biomedical areas ?",14948,2444,2019-03-12T10:37:03.703,2019-03-12T10:37:03.703,what benefits can be got by applying graph convolutional neural network instead of ordinary cnn ?,machine-learning deep-learning convolutional-neural-networks graphs geometric-deep-learning,2,0,1
2065,7792,1,,2018-09-02T10:00:35.260,1,42,"is it possible to form a table that will have simply the shortest distance from each source to destination using q learning ? if not , suggest any other learning algorithm .",17885,2444,2019-02-16T02:45:44.720,2019-02-16T02:45:44.720,can q - learning be used to find the shortest distance from each source to destination ?,machine-learning reinforcement-learning q-learning,1,1,
2066,7793,1,7796,2018-09-02T10:20:03.530,2,52,stuart russell and peter norvig pointed out 4 four possible goals to pursue in artificial intelligence : systems that think / act humanly / rationally . what are the differences between an agent that thinks rationally and an agent that acts rationally ?,17886,1581,2018-09-02T14:14:17.837,2018-09-02T14:14:17.837,differences between an agent that thinks rationally and an agent that acts rationally ?,terminology,1,1,
2067,7794,1,7910,2018-09-02T11:25:26.373,4,310,"i want to explore and experiment the ways in which i could use a neural network to identify patterns in text . examples : prices of xyz stock went down at 11:00 am today retrieve a list of items exchanged on 03/04/2018 show error logs between 3 - 5 am yesterday . reserve a flight for 3rd october . do i have any meetings this friday ? remind to me wake up early tue , 4th sept this is for a project so i am not using regular expressions . papers , projects , ideas are all welcome but i want to approach feature extraction / pattern detection to have a model trained which can identify patterns that it has already seen .",7998,7998,2018-09-02T11:58:14.083,2018-09-10T20:27:36.370,how can i detect datetime patterns in text ?,neural-networks pattern-recognition,3,3,
2068,7803,1,,2018-09-03T09:56:39.463,8,147,"hopfield nets are able to store a vector and retrieve it starting from a noisy version of it . they do so setting weights in order to minimise the energy function when all neurons are set equal the vector values , and retrieve the vector using the noisy version of it as input and allowing the net to settle to an energy minimum . leaving aside problems like the fact that there is no guarantee that the net will settle in the nearest minimum etc – problems eventually solved with boltzmann machines and eventually with back - propagation – the breakthrough was they are a starting point for having abstract representations . two versions of the same document would recall the same state , they would be represented , in the network , by the same state . as hopfield himself wrote : "" the present modeling might then be related to how an entity or gestalt is remembered or categorized on the basis of inputs representing a collection of its features . "" on the other side , the breakthrough of deep learning was the ability of building multiple , hierarchical representation of the input , eventually leading to make ai - practitioners ' life easier , simplifying feature engineering . ( see eg "" representation learning : a review and new perspectives "" , bengio , courville , vincent ) . from a conceptual point of view , one can see deep learning as a generalisation of hopfield nets : from one single representation to a hierarchy of representation . ( i believe ) the question : is that true from a computational / topological point of view as well ? not considering how "" simple "" hopfield networks were ( 2-state neurons , undirected , energy function ) , can one see each layer of a network as a hopfield network and the whole process as a sequential extraction of previously memorised gestalt , and a reorganisation of these gestalt ?",17901,,,2019-05-27T07:02:40.253,deep networks and generalisation of hopfield networks,deep-learning topology,1,0,
2069,7804,1,,2018-09-03T12:18:34.380,0,123,"i 'm trying to have a go at building a neural net , but i ca n't seem to figure out how to optimise the connections . i 've tried to have a look online and it came up with "" backpropagation "" . i looked through some pages about it , but i ca n't seem to understand it . it seems to be where you decide on a target value for each node of your output , and adjust the weights of the synapses to bring the values closer to their targets . what about the inactive synapses ( synapses giving a value of 0 because the previous neuron was n't activated ( its values did n't pass the threshold ) ) ? do those stay the same ? how would this configure the hidden layers ? do i have to assign target values to them ? how ? what are the other ways that the connections can be adjusted ? what alternatives are there to using target values ?",16355,16355,2018-09-03T15:23:25.083,2018-09-03T21:05:50.887,how do i change the values of a neural net,neural-networks ai-basics python backpropagation,2,16,
2070,7815,1,,2018-09-04T08:34:37.693,0,75,"so as my university project i am planning to make a prediction system as described in the title . my current idea is to use the age / gender classifier and run it on a video(taken in front of a shop ) which outputs a csv file of the age / gender / customer i d . in addition , i will use the existing data of the shop of who came in / who did n't come into the shop but passed by the shop and by running xgboost on this csv data i can predict which customer will come into the shop or not . do you think this idea is possible ? is there any other way to implement this idea . it would also be great if we could implement this in such a way as to make the deep learning model learn the various features of those who come into the shop or not .",17651,,,2018-09-04T11:11:50.793,automatic prediction of whether a customer will come into the shop or not,machine-learning deep-learning tensorflow keras data-science,1,4,
2071,7817,1,7818,2018-09-04T14:55:46.377,2,58,"i came across rnn 's a few minutes ago , which might solve a problem with sequenced data i 've had for a while now . let 's say i have a set of input features , generated every second . corresponding with these input features , is an output feature ( also available every second ) . one set of input features does not carry enough data to correlate with the output feature , but a sequence of them most definitely does . i read that rnn 's can have node connections along sequences of inputs , which is exactly what i need , but almost all implementations / explanations show prediction of the next word or number in a text - sentence or in a sequence of numbers . they predict what would be the next input value , the one that completes the sequence . however , in my case , the output feature will only be available during training . during inference , it will only have the input features available . is it possible to use rnn in this case ? can it also predict features that are not part of the input features ? thanks in advance !",16932,,,2018-09-04T15:30:24.287,is it possible to use an rnn to predict a feature that is not an input feature ?,neural-networks machine-learning recurrent-neural-networks,1,0,
2072,7819,1,,2018-09-04T15:38:44.367,3,45,"i am not sure if i can use the words binomial and binary and boolean as synonyms to describe a data attribute of a data set which has two values ( yes or no ) . are there any differences in the meaning on a deeper level ? moreover , if i have an attribute with three possible values ( yes , no , unknown ) , this would be an attribute of type polynominal . what further names are also available for this type of attribute ? are they termed as "" symbolic "" ? i am interested in the realtion between the following attribute type : binary , boolean , binominal , polynominal ( and alternative describtions ) and nominal .",13295,6014,2018-09-04T23:22:43.533,2018-09-05T00:11:45.330,is a binary attribute type the same as binomial attribute type ?,datasets structured-data categorical-data,2,0,
2073,7820,1,7821,2018-09-04T17:32:24.940,0,97,"so , a friend and i are creating an ai using python . well one problem we have came across is creating a database for the ai . i am asking , in general , how would i create a database for ai ?",17349,17349,2018-09-06T16:16:22.357,2018-09-06T16:16:22.357,how do i create a python database for ai ?,python,2,2,
2074,7829,1,,2018-09-05T09:13:54.887,2,70,"what is the process for integrating sentiment analysis in a crm ? what i am searching for is a system which analyzes the customer comments or reviews using the crm and finds out the customer sentiment on the services provided by the system or company or a product . i have done a sentiment analyzer which takes text and shows the sentiment of the text . now i want to integrate the above - mentioned sentiment analyzer to a crm , how can i do that ?",17058,17058,2018-09-05T10:33:45.283,2018-10-11T08:00:17.633,integration of sentiment analysis in crm,machine-learning sentiment-analysis,2,1,
2075,7830,1,,2018-09-05T14:06:10.640,1,100,"i am interested in the field of artificial intelligence . i began by learning the various machine learning algorithms . the maths behind some were quite hard . for example back - propagation in convolutional neural networks . then when getting to the implementation part , i learnt about tensorflow , keras , pytorch , etc . if such frameworks are libraries providing much faster and more robust results , i wonder , will there be a necessity to code a neural network ( say ) from scratch using the knowledge of the maths behind back - prop , activation functions , dimensions of layers , etc ? is it as if the only role of a data - scientist is to tune the hyper - parameters ? further , as of now the field of ai does not seem to have any way to solve for these hyperparameters , and they are arrived at through trial and error . which begs the question , can a hypothetical person with just basic intuition about what the algorithms do , be able to make a model just as good as a person who knows the detailed mathematics of these algos ?",17143,9947,2018-09-06T09:34:25.697,2018-09-06T09:34:25.697,general curiosity,neural-networks machine-learning philosophy,2,2,
2076,7832,1,7871,2018-09-05T15:42:46.143,4,143,"in ' ' proximal policy optimization algorithms ' ' , schulman et al . ( 2017 ) , page 3 i do n't understand why the clipped surrogate objective works . as written in the article : "" with this scheme , we only ignore the change in probability ratio when it would make the objective improved , and we include it when it makes the objective worse "" . i feel confused : how can it works if it does n't take account of objective improvements ?",17759,17759,2018-09-05T17:47:40.007,2018-09-07T08:59:57.400,why does clipped surrogate objective works in proximal policy optimization,reinforcement-learning,1,0,
2077,7836,1,,2018-09-05T22:45:53.030,2,175,"i 'm trying to solve the openai bipedalwalker - v2 by using a one - step actor - critic agent . i 'm implementing the solution using python and tensorflow . i 'm following this pseudo - code taken from the book reinforcement learning an introduction by richard s. sutton and andrew g. barto . in summary , my question can be reduced to the following : is it a good idea to implement a one - step actor - critic algorithm to solve the openai bipedalwalker - v2 problem ? if not what would be a good approach ? if yes ; how long would it take to converge ? i run the algorithm for 20000 episodes , each episode has an avg of 400 steps , for each step , i immediately update the weights . the results are not better than random . i have tried different standard deviations ( for my normal distribution that represents pi ) , different nn sizes for the critic and actor , and different learning - steps for the optimizer algorithm . the results never improve . i do n't know what i 'm doing wrong . my agent class import tensorflow as tf import numpy as np import gym import matplotlib.pyplot as plt class agent_episodic_continuous_action ( ) : def _ _ init__(self , lr , gamma , sample_variance , s_size , a_size , dist_type ) : ... # agent parameters def save_model(self , path , sess ) : def load_model(self , path , sess ) : def weights_init_actor(self , hidd_layer , mean , stddev ) : # to have control over the weights initialization def weights_init_critic(self , hidd_layer , mean , stddev ) : # to have control over the weights initialization def create_actor_brain(self , hidd_layer , hidd_act_fn , output_act_fn , mean , stddev ) : # actor is represented by a fully connected nn def create_critic_brain(self , hidd_layer , hidd_act_fn , output_act_fn , mean , stddev ) : # critic is represented by a fully connected nn def critic(self ) : def get_delta(self , sess ) : def normal_dist_prob(self ) : # actor pi distribution is a normal distribution whose mean comes from the nn def create_actor_loss(self ) : def create_critic_loss(self ) : def sample_action(self , sess , state ) : # sample actions from the normal dist . whose mean was aprox . by the nn def calculate_actor_loss_gradient(self ) : def calculate_critic_loss_gradient(self ) : def update_actor_weights(self ) : def update_critic_weights(self ) : def update_i(self ) : def reset_i(self ) : def update_time_step_info(self , s , a , r , s1,d ) : def create_graph_connections(self ) : def bound_actions(self , sess , state , lower_limit , uper_limit ) : agent instantiation tf.reset_default_graph ( ) agent= agent_episodic_continuous_action(learning - step=1e-3,gamma=0.99,pi_stddev=0.02,s_size=24,a_size=4,dist_type=""normal "" ) agent.create_actor_brain(hidden_layers=[12,5],hidden_layers_fct=""relu"",output_layer=""linear"",mean=0.0,stddev=0.14 ) agent.create_critic_brain(hidden_layers=[12,5],hidden_layers_fct=""relu"",output_layer=""linear"",mean=0.0,stddev=0.14 ) agent.create_graph_connections ( ) path = "" /home / diego / desktop / study / rl / projects / models / biped / model.ckt "" env = gym.make('bipedalwalker-v2 ' ) uper_action_limit = env.action_space.high lower_action_limit = env.action_space.low total_returns= [ ] training loops with tf.session ( ) as sess : try : sess.run(agent.init ) sess.graph.finalize ( ) # agent.load_model(path,sess ) for i in range(1000 ) : agent.reset_i ( ) s = env.reset ( ) d = false while ( not d ) : a = agent.bound_actions(sess , s , lower_action_limit , uper_action_limit ) s1,r , d , _ = env.step(a ) # env.render ( ) agent.update_time_step_info([s],[a],[r],[s1],d ) agent.get_delta(sess ) sess.run([agent.update_critic_weights,agent.update_actor_weights],feed_dict={agent.state_in:agent.time_step_info['s ' ] } ) agent.update_i ( ) s = s1 agent.save_model(path,sess ) except exception as e : print(e )",17565,17565,2018-09-10T16:06:39.580,2018-09-10T16:06:39.580,how many episodes does it take for a vanilla one - step actor - critic agent to master the openai bipedalwalker - v2 problem ?,reinforcement-learning tensorflow python open-ai,0,6,
2078,7838,1,,2018-09-06T02:40:56.847,18,1174,what is the definition of artificial intelligence ?,17948,2444,2019-05-27T16:36:29.627,2019-05-27T16:36:29.627,what is artificial intelligence ?,ai-basics terminology definitions,10,0,6
2079,7842,1,,2018-09-06T04:53:21.760,1,938,what are some good approaches that i can use to count the no . of people in a crowd . tracking each person individually is obviously not an option . any good approaches or some references to research papers would be very helpful .,14592,,,2018-09-06T07:12:03.210,counting people in an image of a crowd,deep-learning convolutional-neural-networks computer-vision,1,0,
2080,7853,1,,2018-09-06T12:37:06.217,0,63,"i developed a cnn for image analysis . i 've around 100k labeled images . i 'm getting a accuracy around 85 % and a validation accuracy around 82 % , so it looks like the model generalize better than fitting . so , i 'm playing with different hyper - parameters : number of filters , number of layers , number of neurons in the dense layers , etc . for every test , i 'm using all the training data , and it is very slow and time consuming . is there a way to have an early idea about if a model will perform better than another ?",17960,2444,2019-05-04T17:10:17.467,2019-05-04T17:10:17.467,is there a way of pre - determining whether a cnn model will perform better than another ?,machine-learning training deep-network,1,4,
2081,7854,1,7858,2018-09-06T12:44:46.330,3,456,in the berkeley rl class they mention the gradient would be 0 if the policy is deterministic . why is that ? https://www.youtube.com/watch?v=xgmd3wcydg8&amp;feature=youtu.be&amp;t=1071,17966,17966,2018-09-06T22:53:33.833,2018-09-06T22:53:33.833,why is the derivative 0 if the policy is deterministic ?,reinforcement-learning math,2,0,
2082,7856,1,,2018-09-06T13:42:31.290,2,125,the way we will rank human intelligence based on iq . is it possible to rank or compare the ai system ? such as can i say spam filter algorithm is more intelligent than a self - driving car or can i say chess algorithm or system is more intelligent then alpha go algorithm . to compare intelligent system what is the best possible way and what are the dimensions do we have to consider ?,7681,,,2018-09-06T20:03:21.163,is it possible to compare ai system if yes based on what dimensions if no then why ?,deep-learning strong-ai ai-community watson,1,6,
2083,7861,1,7864,2018-09-06T17:53:08.197,4,68,"in theoretical computer science , there is a massive categorization of the difficulty of various computational problems in terms of their asymptotic worst - time computational complexity . there does n't seem to be any analogous analysis of what problems are "" hard for ai "" or even "" impossible for ai . "" this is in some sense quite reasonable , because most research is focused on what can be solved . i 'm interested in the opposite . my interest is the opposite however : what i do need to prove about a problem to prove that it is "" not reasonably solvable "" by ai . many papers say something along the lines of ai allows us to find real - world solutions to real - world instances of np - complete problems . is there a theoretical , principled reason for saying this instead of "" ... pspace - complete problems "" ? is there some sense in which ai does n't work on pspace - complete , or exptime - complete , or turing complete problems ? my idea answer would be a reference to a paper that shows ai can not be used to solve a particular kind of problem based on theoretical or statistical reasoning . any answer exhibiting and justifying a benchmark for "" too hard for ai "" would be fine though ( bonus points if the benchmark has a connection to complexity and computability theory ) . if this question does n't have an answer in general , answers about specific techniques would also be interesting to me .",12732,,,2018-09-06T20:19:34.127,"what does "" hard for ai "" look like ?",ai-design reference-request theory,1,2,1
2084,7865,1,7872,2018-09-06T23:27:52.350,8,1231,"in convolutional neural network , which layer consumes maximum time in training ? convolution layers or fully connected layers ? we can take alexnet architecture to understand this . i want to see time breakup of training process . i want a relative time comparison so we can take any constant gpu configuration .",17980,9947,2018-09-07T11:32:24.143,2018-09-08T09:14:05.303,which layer consumes more time in cnn training ? convolution layers vs fc layers,neural-networks deep-learning convolutional-neural-networks,2,0,1
2085,7867,1,,2018-09-07T04:19:05.633,2,347,"i have an application where i want to find the locations of objects on a simple , relatively constant background ( fixed camera angle , etc ) . for investigative purposes i 've created a test dataset which displays many characteristics of the actual problem . here 's a sample from my test dataset . our problem description is to find the bounding box of the single circle in the image . if there is more than one circle or no circles , we do n't care about the bounding box ( but we at least need to know that there is no valid single bounding box ) . for my attempt to solve this , i built a cnn that would regress ( min_x , min_y , max_y , max_y ) as well as one more value which could indicate how many circles were in the image . i played with different architecture variations , but in general the architecture a was very standard cnn ( 3 - 4 relu conv layers with max pooling in between , followed by a dense layer and an output layer with linear activation for the bounding box outputs , set to minimise the mean squared error between the outputs and the ground truth bounding boxes ) . regardless of the architecture , hyperparameters , optimizers , etc , the result was always the same - the cnn could not even get close to building a model that was able to regress an accurate bounding box , even with over 50000 training examples to work with . what gives ? do i need to look at using another type of network as cnns are more suited to classification rather than localisation tasks ? obviously there are computer vision techniques that could solve this easily , but due to the fact that the actual application is more involved , i want to know strictly about nn / ai approaches to this problem .",17985,,,2018-11-13T16:21:55.860,how to architect a network to find bounding boxes in simple images,convolutional-neural-networks computer-vision,2,0,
2086,7873,1,7874,2018-09-07T13:04:23.503,2,64,"i 'm a fresh learner of ai . i was told that depth - first search is not an optimal searching algorithm since "" it finds the ' leftmost ' solution , regardless of depth or cost "" . therefore , does it mean that in practice , when we implement dfs , we should always have a checker to stop the search when it finds the first solution ( also the leftmost one ) ? thanks guys !",17996,9947,2018-09-08T15:04:04.207,2018-09-08T15:04:04.207,"in the implementation of ai programming , does dfs always stop when it has found the leftmost solution ?",ai-basics search decision-tree,1,2,
2087,7875,1,,2018-09-07T13:31:55.310,9,258,"the term singularity is often used in mainstream media for describing visionary technology . it was introduced by ray kurzweil in a popular book in 2005 . ( 1 ) in his book , kurzweil gives an outlook to a potential future of mankind which includes nanotechnology , computers , genetic modification and artificial intelligence . he argues , that moore 's law will allow computers an exponential growth which results into an ai - super - intelligence . is the "" technological singularity "" something that is taken seriously by a.i . developers or is this theory just a load of popular hype ?",17978,17488,2018-09-08T19:29:21.117,2018-09-08T19:29:21.117,is the singularity something to be taken seriously ?,theory singularity mythology-of-ai,2,6,1
2088,7877,1,7878,2018-09-07T14:42:12.617,3,387,"i 'm confused regarding a specific detail of mcts . to illustrate my question , lets take the simple example of tic - tac - toe . after the selection phase , when a leaf node is reached , the tree is expanded in the so called expansion phase . lets say a particular leaf node has 6 children . would the expansion phase expand all the children and run simulation on them ? or would the expansion phase only pick a single child at random and run simulation , and only expand the other children if the selection policy arrives at them at some later point ? alternatively , if both of these are accepted variants , what are the pros / cons of each one ?",12201,,,2018-09-07T15:56:10.540,monte carlo tree search expansion phase,algorithm game-ai monte-carlo-tree-search,1,0,
2089,7879,1,8905,2018-09-07T17:14:48.587,4,87,"objects tracking is finding the trajectory of each object in consecutive frames . human tracking is a subset of object tracking which just considers humans . i 've seen many papers that divide tracking methods into two parts : online tracking : tracker just uses current and previous frames . offline tracking : tracker uses all frames . all of them mention that online tracking is suitable for autonomous driving and robotics , but i do n't understand this part . what are the applications of object / human tracking in autonomous driving ? do you know some related papers ?",10051,2444,2019-03-22T14:55:56.240,2019-03-23T22:05:20.000,what are applications of object / human tracking in autonomous cars ?,computer-vision self-driving autonomous-vehicles,2,6,
2090,7880,1,7883,2018-09-07T22:39:06.603,3,221,"is there an accepted way in nlp to parse conjunctions ( and/or ) in a sentence ? by following the example below ; how would i parse , "" i drink orange juice if its the weekend or if its late and i\'m tired . "" into , [ [ "" its the weekend "" ] ] , [ "" its late "" , "" i\'m tired "" ] ] implying an action will be taken when one of the above elements at the 1st level of depth is true . i know when i hear the sentence that it means "" its the weekend "" or ( "" its late "" and "" i\'m tired "" ) , but how could this be determined computationally ? can an existing python / other library do this ?",10623,1581,2018-11-28T21:21:33.523,2018-11-28T21:21:33.523,how to parse conjunctions in natural language processing in python,natural-language-processing python computational-linguistics,1,1,
2091,7881,1,,2018-09-07T23:43:14.730,4,271,"connect6 is an example of a game with a very high branching factor . it is about 45 thousand , dwarfing even the impressive go . what algorithms can you use on games with such high branching factors ? i tried mcts ( soft rollouts , counting a ply as placing one stone ) , but it does not even block the opponent , due to the high branching factor . in the case of connect6 , there are stronger ais out there , but they are n't described in any research papers that i know of .",18006,,,2018-09-08T10:52:44.217,algorithms for games with very high branching factors ( connect6 ),algorithm game-ai monte-carlo-tree-search combinatorial-games branching-factors,1,1,
2092,7882,1,,2018-09-08T00:05:51.540,2,25,"formal semantics of natural language perceives sentences as logical expressions . full paragraphs and even stories of natural language texts are researched and formalized using discourse analysis ( discourse representation theory is one example ) . my question is - is there research trend that applied the notion "" discourse "" to images , sounds and even animation ? is there such a notion as "" visual discourse "" ? google gives very few and older research papers , so - maybe the field exists , but it uses different terms and google can not relate those terms to my keyword "" visual discourse "" . basically - there are visual grammars and other pattern matching methods that can discover objects in the picture and relate them . but one should be able to read whole store from the picture ( musical piece , multimedia content ) and i imagine that such reading can be researched by multimedia discourse analysis . but there is no work under such terms . how it is done and named in reality ?",8332,8332,2018-09-08T00:11:36.403,2018-09-08T00:11:36.403,visual / musical / multimedia discourse ( analysis ) - are there such notions ?,image-recognition sentiment-analysis,0,1,
2093,7891,1,8069,2018-09-09T05:11:33.703,3,288,"i want to generate images of childrens ' drawings consistent with the developmental state of children of a given age . the training data set will include drawings made by real children in a school setting . the generated images will be used for developmental analysis . i have heard that generative adversarial networks are a good tool for this kind of problem . if this is true , how would i go about applying a gan to this challenge ?",18027,9203,2018-09-13T08:15:36.910,2018-09-21T00:19:16.933,how to use a generative adversarial network to generate images for developmental analysis ?,machine-learning generative-adversarial-networks,2,8,2
2094,7895,1,7902,2018-09-09T19:07:09.927,6,159,"drawing parallels between machine learning techniques and a human brain is a dangerous operation . when it is done successfully , it can be a powerful tool for vulgarisation , but when it is done with no precaution , it can lead to major misunderstandings . i was recently attending a conference where the speaker described experience replay in rl a way of making the net "" dream "" . i 'm wondering how true this assertion is . the speaker argued that a dream is a random addition of memories , just as experience replay . however , i doubt the brain remembers its dream or either learns from it . what is your analysis ?",17759,,,2018-09-10T09:24:38.737,is experience replay like dreaming ?,neural-networks reinforcement-learning philosophy,1,1,1
2095,7896,1,,2018-09-09T20:31:07.373,9,451,"in robotics , the reinforcement learning technique is used for finding the control pattern for a robot . unfortunately , most policy gradient method are statistically biased which could bring the robot in an unsafe situation , see page 2 in jan peters and stefan schaal : reinforcement learning of motor skills with policy gradients , 2008 with motor primitive learning , it is possible to overcome the problem because policy gradient parameter optimization directs the learning steps into the goal . quote : “ if the gradient estimate is unbiased and learning rates fulfill sum(a)=0 the learning process is guaranteed to converge to at least a local minimum [ ... ] therefore , we need to estimate the policy gradient only from data generated during the execution of a task . ” ( page 4 of the same paper ) in the homework for the berkeley rl class problem 1 , it asks you to show that the policy gradient is still unbiased if the baseline subtracted is a function of the state at timestep t. $ $ \triangledown _ \theta \sum_{t=1}^t \mathbb{e}_{(s_t , a_t ) \sim p(s_t , a_t ) } [ b(s_t ) ] = 0 $ $ i am struggling through what the first step of such a proof might be . can someone point me in the right direction ? my initial thought was to somehow use the law of total expectation to make the expectation of b(st ) conditional on t , but i am not sure . thanks in advance :) link to original png of equation",18043,4302,2018-09-29T03:30:58.657,2018-09-29T03:31:51.830,why is baseline conditional on state at some timestep unbiased ?,reinforcement-learning,2,6,4
2096,7897,1,8067,2018-09-09T20:33:22.743,8,112,"is there research that employs realistic models of neurons ? usually , the model of a neuron for a neural network is quite simple as opposed to the realistic neuron , which involves hundreds of proteins and millions of molecules ( or even greater numbers ) . is there research that draws implications from this reality and tries to design realistic models of neurons ? particularly , recently , rosehip neuron was discovered . such neuron can be found only in human brain cells ( and in no other species ) . are there some implications for neural network design and operation that can be drawn by realistically modelling this rosehip neuron ?",8332,2444,2019-03-19T21:49:49.583,2019-03-19T21:49:49.583,is there research that employs realistic models of neurons ?,neural-networks artificial-neuron neurons brain neuromorphic-engineering,3,0,
2097,7899,1,7931,2018-09-09T22:20:04.963,2,86,"image captioning is a hot research topic in the ai community . there are considerable image captioning models for research usage such as nic , neural talk 2 etc . but can these research models be used for commercial purpose ? or we should build much more complex structured ones for commercial usage ? or if we can make some improvements based these models to meet the business applications situation ? if so , what improvements should we take ? are there any existing commercial image captioning applications can be referenced ?",14948,,,2018-09-12T06:23:18.653,how to build a commercial image captioning system ?,deep-learning image-recognition natural-language-processing,1,0,
2098,7900,1,,2018-09-10T02:08:14.790,1,112,"for my university project , i am planning to build a face recognition/ occupation recognition programme . however , rather than using the existing haar cascade(for age and gender ) i am planning to use face api which seems far more accurate than the former . my question is is it possible to somehow combine my trained data for haar cascade(for occupation ) with face api since face api does n't have the option to recognize occupation(such as students / office workers from their appearance ) ?",17651,17651,2018-09-10T04:05:39.353,2018-11-11T12:01:30.373,occupation detection using face api,machine-learning deep-learning convolutional-neural-networks image-recognition python,1,6,
2099,7903,1,,2018-09-10T07:47:16.083,1,42,"i have an average laptop . how can i connect specialized ai neural network processors ( say , intel nvidia or intel nervana https://venturebeat.com/2018/05/23/intel-unveils-nervana-neural-net-l-1000-for-accelerated-ai-training/ ) to thelaptop . should i buy some external motherboard or even server unit with nn processors inside or is there available more lightweight solution like external hdd ?",8332,8332,2018-09-10T11:02:25.400,2018-09-10T11:02:25.400,how to connect ai neural network processor to laptop ?,neural-networks,1,2,
2100,7907,1,7909,2018-09-10T12:24:46.590,3,89,"i 'm trying to learn ai and thinking to apply it to our system . we have an application for translation industry . what we are doing now is the coordinator assigns a file to a translator , the coordinator usually considers this criteria ( but not limited to ) : the deadline of the file and availability of translator the language pair that the translator can translate is the translator already reached his target ? ( maybe we can give the file to other translator to reach their target ) the difficulty level of the file for translator(basic translation , medical field , it field ) accuracy of translator speed of translator given the following , is it possible to make a recommendation to the coordinator to whom she can assign a particular file ? what are the methods / topics that i need to research ? i 'm considering javascript as the primary tool and maybe python if javascript will be more of a hindrance in implementation . edit : in addition to suggesting a translator we are also looking into suggesting the deadline of the translator . basically , we have deadline of customer and deadline of translator the reason for this is that , if the translators are occupied throughout the day , it makes sense to suggest it to a busy translator but allow him to finish it until next day .",18053,18053,2018-09-10T14:50:39.210,2018-09-11T12:26:01.453,what method and tools should i use for ai that suggests / assigns a person for a task ?,ai-basics automation,2,2,
2101,7911,1,7913,2018-09-10T21:25:45.860,2,118,"the goal is to implement an artificial network that , based on training samples labelled with positive integers , outputs a positive integer . perhaps i am not searching for the correct thing but all the examples i have found have shown classifiers using a sigmoid function that outputs a fixed or floating point number within the range $ & lt;0 \to 1&gt;$ . any point in the right direction or python / toy code would be very appreciated !",18070,4302,2018-09-20T22:19:44.147,2018-09-20T22:19:44.147,how to implement an artificial network that outputs an integer within a range ?,neural-networks training ai-basics prediction knowledge-representation,1,1,
2102,7914,1,7920,2018-09-10T22:31:34.993,6,152,"would alphago zero become theoretically perfect with enough training time ? if not , what would be the limiting factor ? ( by perfect , i mean it always wins the game if possible , even against another perfect opponent . )",18006,18006,2018-09-11T02:19:08.160,2018-09-11T14:46:43.990,would alphago zero become perfect with enough training time ?,neural-networks monte-carlo-tree-search alphago alphazero alphago-zero,3,0,
2103,7916,1,7918,2018-09-11T04:19:50.473,2,63,"i am currently reading the research paper image crowd counting using convolutional neural network and markov random field by kang han , wanggen wan , haiyan yao , and li hou . i did not understand the following context properly : we employ the residual network , which is trained on imagenet dataset for image classication task , to extract the deep features to represent the density of the crowd . this pre - trained cnn network created a residual item for every three convolution layer to bring the layer of the network to 152 . we resize the image patches to the size of 224 × 224 as the input of the model and extract the output of the fc1000 layer to get the 1000 dimensional features . the features are then used to train 5 layers fully connected neural network . the network 's input is 1000dimensional , and the number of neurons in the network is given by 100 - 100 - 50 - 50 - 1 . the network 's output is the local crowd count can anyone explain the above part in detail ?",14592,1671,2018-09-11T18:49:05.603,2018-09-11T18:49:05.603,"clarification regarding "" image crowd counting using convolutional neural network and markov random field """,deep-learning convolutional-neural-networks,1,0,
2104,7923,1,7924,2018-09-11T10:47:29.857,6,435,should i be decaying the learning rate and the exploration rate in the same manner ? what 's too slow and too fast of an exploration and learning rate decay ? or is it specific from model to model ?,18076,,,2018-09-12T07:48:05.510,learning rate decay and exploration rate decay,deep-learning reinforcement-learning,1,3,
2105,7925,1,,2018-09-11T20:19:55.103,1,109,"i was recently perusing the paper some studies in machine learning using the game of checkers ii -- recent progress ( a.l . samuel , 1967 ) , which is interesting historically . i was looking at this figure , which involved alpha - beta pruning . it occurred to me that the types of non - trivial , non - chance , perfect information , zero - sum , sequential , partisan games utilized ( chess , checkers , go ) involve game states that can not be precisely quantified . for instance , there is no way to ascribe an objective value to a piece in chess , or any given board state . in some sense , the assignment of values is arbitrary , consisting of estimates . the combinatorial games i 'm working on are forms of partisan sudoku , which are bidding / scoring ( economic ) games involving territory control . in these models , any given board state produces an array of ratios allowing precise quantification of player status . token values and positions can be precisely quantified . this project involves a consumer product , and the approach we 're taking currently is to utilize a series of agents of increasing sophistication to provide different levels challenge for human players . these agents also reflect what is known as a "" strategy ladder "" . reflex agents ( beginner ) model - based reflex agents ( intermediate ) model - based utility agents ( advanced ) goals may also be incorporated to these agents such as desired margin of victory ( regional outcome ratios ) which will likely have an effect on performance in that narrower margins of victory appear to entail less risk . the "" respectably weak "" vs. human performance of the first generation of reflex agents suggests that strong gofai might be possible . ( the branching factors are extreme in the early and mid - game due to the factorial nature of the models , but initial calculations suggest that even a naive minimax lookahead will be able to look farther more effectively than humans . ) alpha - beta pruning in partisan sudoku , even sans a learning algorithm , should provide greater utility than in previous combinatorial game models where the values are estimates . is the historical weakness of gofai in relation to non - trivial combinatorial games partly a function of the structure of the games studied , where game states and token values can not be precisely quantified ? looking for any papers that might comment on this subject , research into combinatorial games where precise quantification is possible , and thoughts in general . i 'm trying to determine if it might be worth attempting to develop a strong gofai for these models prior to moving up the ladder to learning algorithms , and , if such a result would have research value . there would definitely be commercial value in that strong gofai with no long - term memory would allow minimal local file size for the apps , which must run on lowest - common - denominator smartphones with no assumption of connectivity . ps- my previous work on this has involved defining the core heuristics that emerge from the structure of the models , and i 'm slowly dipping my toes into the look ahead pool . please do n't hesitate to let me know if i 've made any incorrect assumptions .",1671,,,2018-09-12T12:03:23.320,historical weakness of gofai in relation to partisan combinatorial games ?,game-ai combinatorial-games soft-question gofai alpha-beta-pruning,1,5,
2106,7926,1,7936,2018-09-11T23:01:12.367,4,575,"i do n’t believe in freewill , but most people do . although i ’m not sure how an act of freewill could even be described ( let alone replicated ) , is libertarian freewill something that is considered for ai ? or is ai understood to be deterministic ?",16480,16480,2018-09-12T12:48:40.407,2019-04-07T17:03:04.133,does ai rely on determinism ?,philosophy human-like artificial-consciousness,8,4,3
2107,7927,1,,2018-09-12T00:32:52.890,0,76,"i am trying to use deep - q learning environment to learn super mario bros . the implementation is on github . i have a neural network that q values update within an episode for a very small learning rate ( 0.00005 ) . however , even if i increase the learning rate to 0.00025 , the q values do not change within an episode as they are predicting the same q values regardless of what state it is in . for example , if mario moves right , the q value is the same . when i start a new episode , the q values change though . i think that the q values should be changing within an episode as the game should be seeing different parts and taking different actions . why do n't i observe this ?",18076,18076,2018-09-12T13:03:22.970,2018-09-18T06:47:38.773,should q values be changing within an epoch / episode or should they change after one episode / epoch ?,neural-networks machine-learning deep-learning reinforcement-learning game-ai,1,11,
2108,7935,1,,2018-09-12T12:05:11.227,5,1528,i am making a machine learning program for time series data analysis and using neat could help the work . i started to learn tensorflow not long ago but it seems that the computational graphs in tensorflow are usually fixed . is there tools in tensorflow to help build a dynamically evolving neural network ? or something like pytorch would be a better alternative ? thanks .,18102,10135,2018-10-18T03:14:37.643,2018-12-28T03:22:39.357,can neuro - evolution of augmenting topologies ( neat ) neural networks be built in tensorflow ?,python tensorflow neat programming-languages topology,3,0,1
2109,7940,1,7941,2018-09-12T20:23:42.520,3,237,"this is a q - learning snake using a neural network as a q function aproximator and i 'm losing my mind here the current model it 's worst than the initial one . the current model uses a 32x32x32 mlpregressor from scikit - learn using relu as activation function and the adam solver . the reward function is like following : death reward = -100.0 alive reward = -10.0 apple reward = 100.0 the features extracted from each state are the following : what is in front of the snake 's head(apple , empty , snake ) what is in the left of the snake 's head what is in the right of the snake 's head euclidian distance between head and apple the direction from head to the apple measured in radians length of the snake one episode consists of the snake playing until it dies , i 'm also using in training a probability epsilon that represent the probability that the snake will take a random action if this is n't satisfied the snake will take the action for which the neural network gives the biggest score , this epsilon probability gradually decrements after each iteration . the episode is learned by the regressor in reverse order one statet - action at a time . however the neural network fails too aproximate the q function , no matter how many iterations the snake takes the same action for any state . things i tried : changing the structure of the neural network changing the reward function changing the features extracted , i even tried passing the whole map to the network code ( python ) : https://pastebin.com/57qlbjqz",18123,18123,2018-09-12T21:19:16.160,2018-09-13T19:52:13.400,snake game : snake converges to going in the same direction every time,reinforcement-learning,1,3,
2110,7942,1,7948,2018-09-13T03:37:34.843,4,286,"the alpha zero ( as well as alphago zero ) papers say they trained the value head of the network by "" minimizing the error between the predicted winner and the game winner "" throughout its many self play games . as far as i could tell , further information was not given . to my understanding , this is basically a supervised learning problem , where from the self play we have games associated with their winners , and the network is being trained to map game states to likelihood of winning . my understanding leads me to the following question : what part of the game is the network trained to predict a winner on ? obviously after only five moves , the winner is not yet clear , and trying to predict a winner after five moves based on the game 's eventual winner would learn a meaningless function . as a game progresses it goes from tied in the initial position to won at the end . how is the network trained to understand that , if all it is told is who eventually won ?",12201,,,2018-09-13T07:17:41.703,alphazero value network,machine-learning reinforcement-learning alphago alphazero alphago-zero,2,0,
2111,7947,1,7980,2018-09-13T06:36:13.017,1,74,"note to the duplicate police this question is not a duplicate of the q&amp;a thread referenced in the close request . the only text even remotely related in that other thread is the brief mention of climate change in the q and two sentences in the sole answer : "" identify deforestation and the rate at which it 's happening using computer vision and help in fighting back based on how critical the rate is . the world resources institute had entered into a partnership with orbital insight on this . "" if you look at the four bullet items below , you will find that this question asks a very specific thing about the relationship between climate and emissions . neither that question nor that answer overlaps with the content of this question in any meaningful way . for instance , it is well known that co 2 is not causing deforestation . the additional carbon dioxide in the atmosphere causes faster regrowth . this is because plants need co 2 to grow . hydroponic containers deliberately boost it to improve growth rates . plants manufacture their own oxygen from the co 2 via chlorophyll . if you recall from fifth grade biology , that 's why they are plants . now back to the question several climate models have been proposed and used to model the relationship between human carbon emissions , added to the natural carbon emissions of life forms on earth , and features of climate that could damage the biosphere . population growth and industrialization have many impacts on the biosphere , including loss of terrain and pollution . negative oceanic effects , including unpredictable changes in plankton and cyanobacteria are under study . carbon emissions from combustion has received attention in recent decades just as sulfur emissions were central to concerns a century or more ago . predicting weather and climate is certainly difficult because it is complex and chaotic , as typical inaccuracies in forecasts clearly demonstrate , but that is looking forward . looking backward , analyses of data already collected have shown a high probability that ocean and surface temperature rises followed increases in industrial and transportation related combustion of fuels . how might ai be used to produce some of the key models humans need to protect the biosphere from severe damage . a more reliable analysis of what has already occurred , since there is some legitimacy to the differing views as to how gross the effect of carbon emissions has been on extinctions of species in the biosphere and on arctic and antarctic melting a better understanding as to whether the climate of the biosphere behaves as a buffer of climate , always tending to re - balance after a volcanic eruption , meteor stroke , or other event , or whether the runaway scenario described by some climatologist , where there is a point of no return , is realistic a better model to use in trying out scenarios so that solutions can be applied in the order that makes sense from both environmental and economic perspectives automation of climate planning so that the harmful effects of the irresponsibility of one geopolitical entity wishing to industrialize without constraint on other geopolitical entities can be mitigated can pattern recognition , feature extraction , the learned functionality of deep networks , or generative techniques be used to accomplish these things ? can rules of climate be learned ? are there discrete or graph based tools that should be used ?",9203,9203,2018-09-13T23:34:47.017,2018-09-14T20:44:53.197,how can ai be used to more reliably analyze and plan around the tie between climate and emissions ?,pattern-recognition topology generative-model survival,1,0,
2112,7949,1,,2018-09-13T10:12:49.540,3,130,"in a cnn , does each new filter have different weights for each input channel , or are the same weights of each filter used across input channels ? this question helps me a lot . let , i have rgb input image . ( 3 channels ) then each filter has n×n weights for one channel . it means , actually the filter has totally 3×n×n weights . for channel r , it has own n×n filter . for channel g , it has own n×n filter . for channel b , it has own n×n filter . after inner product , add them all to make one feature map . am i right ? and then , my question starts here . for some purpose , i will only use greyscale images as input . so the input images always have the same values for each rgb channel . then , can i reduce the number of weights in the filters ? because in this case , using three different n×n filters and adding them is same with using one n×n filter that is the summation of three filters . does this logic hold on a trained network ? i have a trained network for rgb image input , but it is too heavy to run in real time . but i only use the greyscale images as input , so it seems i can make the network less heavy ( theoretically , almost 1/3 of original ) . i 'm quite new in this field , so detailed explanations will be really appreciated . thank you .",18139,9947,2018-09-13T11:03:59.423,2018-09-13T11:22:15.863,"can i reduce the "" number of weights "" in cnn to 1/3 by restricting the input as greyscale image ?",deep-learning convolutional-neural-networks signal-processing,2,6,1
2113,7952,1,,2018-09-13T10:52:22.383,1,39,"i am currently reading the research paper image crowd counting using convolutional neural network and markov random field by kang han , wanggen wan , haiyan yao , and li hou . i did not understand the following context properly : formally , the markov random field framework for the crowd counting can be defined as follows ( we follow the notation in [ 18 ] ) . let p be the set of patches in an image and c be a possi- ble set of counts . a counting c assigns a count c p ∈ c to each patch p ∈ p. the quality of a counting is given by an energy function : e(c ) = ∑ d p ( c p ) + ∑ p∈p v ( c p − c q ) . . . ( 2 ) ( p , q)∈n where n are the ( undirected ) edges in the four - connected image patch graph . d p ( c p ) is the cost of assigning count c p to patch p , and is referred to as the data cost . v ( c p −c q ) measures the cost of assigning count c p and c q to two neighboring patch , and is normally referred to as the dis- continuity cost . for the problem of smoothing the adjacent patches count , d p ( c p ) and v ( c p − c q ) can take the form of the following functions : d p ( c p ) = λ min((i(p ) − c p ) 2 , data k ) . . . ( 3 ) v ( c p − c q ) = min((c p − c q ) 2 , disc k ) . . . ( 4 ) where λ is a weight of the energy items , i(p ) is the ground truth count of the patch p , data k and disc k are the truncating item of d p ( c p ) and v ( c p − c q ) , respectively . can anyone explain the above part in detail and give me a detailed insight on how should i implement this part of the project ?",14592,14592,2018-09-13T14:22:44.190,2018-09-13T14:22:44.190,doubt regarding research paper on crowd counting using convolutional neural networks and markov random field,convolutional-neural-networks,0,2,
2114,7955,1,,2018-09-13T15:32:52.660,1,42,"neurons can be simulated using different models that vary in the degree of biophysical realism . when designing an artificial neuronal network , i am interested in the consequences of choosing a degree of neuronal realism . in terms of computational performance , the flops vary from integrate - and - fire to the hodgkin – huxley model ( izhikevich , 2004 ) . however , properties , such as refraction , also vary with the choice of neuron . when selecting a neuronal model , what are consequences for the ann other than performance ? for example , would there be trade - offs in terms of stability / plasticity ? izhikevich investigated the performance question in 2004 . what are the current benchmarks ( other measures , new models ) ? how does selecting a neuron have consequences for scalability in terms of hardware for a deep learning network ? when is the mcculloch - pitts neuron inappropriate ? references izhikevich , e. m. ( 2004 ) . which model to use for cortical spiking neurons ? ieee transactions on neural networks , 15(5 ) . https://www.izhikevich.org/publications/whichmod.pdf",16411,,,2018-09-13T15:32:52.660,how does the degree of neuronal realism affect computing in a deep learning scenario ?,deep-learning artificial-neuron neurons biology,0,4,
2115,7958,1,7960,2018-09-13T19:57:18.963,0,267,"i found a video for the paper deepmimic : example - guided deep reinforcement learning of physics - based character skills on youtube . i looked in the related paper , but could not find details of how to the environment was created , such as the physics engine it used . i would like to use it , or something similar .",18189,1847,2018-09-14T06:31:44.963,2018-09-14T06:31:44.963,what is the physics engine used by deepmimic ?,reinforcement-learning,1,1,
2116,7959,1,7968,2018-09-13T20:26:34.613,2,56,"i want to build a model to support decision making for loan insurance proposal . there are three actors in the problem : a bank , a loaner applicant ( someone who ask for a loan ) and a counselor . the counselor studies the loaner application and if it has a good profile it will propose to him loan from banks that fits his profile . then the application is sent to the bank but the bank could refuse the applicant ( based on criteria we do n't know ) . the counselor has also to decide whether or not he will propose to the loaner applicant a loan insurance . the risk is that some banks reject loan applicant who accepts a loan insurance and other banks accept more applicants with a loan insurance . but there are n't rules regarding banks since some banks accept or reject applicants with loan insurance according of the type of acquisition applicants want with their loan for example . thus , the profile of the applicant can matter in their rejection from banks but all criteria influencing the decision are quite uncertain . i 've researched online and found several scholarly articles on using monte carlo for decision making . should i use monte carlo or a simple classifier for this decision making problem ? i saw that monte carlo ( possibly monte carlo tree search ) can be used in decision making and it is good when there is uncertainty . but it seems that it would forecast by producing some strategy ( after running a lot of simulations ) but what i want is an outcome based on both the profile of the loaner applicant and the bank knowing that criteria from banks ( to accept loaner applicant from could change every six months . and i would have too model banks which seems quite difficult . a classifier seems to me to not really fit the problem . i am not really sure . actually , i do n't see how a classifier like a decision tree , for example , would work here . because i have to predict decision of the counselor to propose or not based on the decision of banks ( and i do n't know their criteria ) to refuse or accept applicants who were proposed loan insurance and accepted it . the data i have is former applicants profile who were sent to banks and if they were accepted or not by the bank , if they wanted a loan insurance or not and the type of acquisition they wanted to make with their loan . i am new to decision making . thank you !",18192,18192,2018-09-13T21:24:32.153,2018-09-14T08:42:18.080,should i use monte carlo or a classifier for this decision making problem ?,machine-learning classification monte-carlo-tree-search decision-theory,1,1,
2117,7961,1,7964,2018-09-14T01:21:10.207,-1,73,"so i have already learned some traditional ai techniques ( alpha - beta pruning , mcts ) . now i want to get into machine learning . i know a little bit about neural networks , but that is about it . what are some resources for learning machine learning ? it would be good if the resource is based on some cloud platform , as my laptop is not very fast . my end goal is to implement the expert iteration algorithm ( see "" thinking fast and slow with deep learning and tree search "" ) .",18006,,,2018-09-14T04:33:10.887,resources for learning machine learning,machine-learning google-cloud resource-request,1,0,
2118,7962,1,7973,2018-09-14T02:32:33.923,1,192,"is it possible for a genetic algorithm + neural network that is used to learn to play one game such as a platform game able to be applied to another different game of the same genre . so for example , could an ai that learns to play mario also learn to play another similar platform game . also , if anyone could point me in the direction of material i should familiarise myself with in order to complete my project .",18209,1641,2018-09-14T10:58:39.703,2018-09-14T11:46:40.210,can genetic algorithms be used to learn to play multiple games of the same type ?,neural-networks game-ai python genetic-algorithms,3,6,
2119,7963,1,7967,2018-09-14T03:38:46.363,3,125,"i started teaching myself about reinforcement learning a week ago and i have this confusion about the learning experience . let 's say we have the game go . and we have an agent that we want to be able to play the game and win against anyone . but let 's say this agent learn from playing against one opponent , my questions then are : would n't the agent ( after learning ) be able to play only with that opponent and win ? it estimated the value function of this specific behaviour only . would it be able to play as good with weaker players ? how do you develop an agent that can estimate a value function that generalizes against any behaviour and win ? self - play ? if yes , how does that work ?",17582,2444,2018-09-14T12:33:08.047,2018-09-14T12:33:08.047,how can a reinforcement learning agent generalize if it is trained against only one opponent ?,reinforcement-learning,2,0,1
2120,7965,1,7969,2018-09-14T05:42:47.450,3,30,"in some situation , like risk detection and spam detection . the pattern of good user is stable , while the patterns of attackers are changing rapidly . how can i make a model for that ? or which classifier / method should i use ?",18213,,,2018-09-14T09:01:50.557,how to design a classifier while the patterns of positive data are changing rapidly ?,classification security,1,0,
2121,7966,1,,2018-09-14T07:28:46.180,4,262,"are there neural networks that can decide to add / delete neurons ( or change the neuron models / activation functions or change the assigned meaning for neurons ) , links or even complete layers during execution time ? i guess , that such neural networks overcome the usual separation of learning / inference phases and they continuously live their lives in which learning and self - improving occurs alongside performing inference and actual decision making for which these neural networks were build . effectively it could be neural network that acts as a [ gödel machine ] ( http://people.idsia.ch/~juergen/goedelmachine.html ) . i have found the term dynamic neural network but it is connected to adding some delay functions and nothing more . of course , such self - improving networks completely redefine the learning strategy , possibly , single shot gradient methods can not be applicable to them . my question is connected to the neural - symbolic integration , e.g. neural - symbolic cognitive reasoning by artur s. d'avila garcez , 2009 . usually this approach assigns individual neurons to the variables ( or groups of neurons to the formula / rule ) in the set of formulas in some knowledge base . of course , if knowledge base expands ( e.g. from sensor readings or from inner nonmonotonic inference ) then new variables should be added and hence the neural network should be expanded ( or contracted ) as well .",8332,4302,2018-09-15T03:10:11.480,2018-09-27T13:49:06.163,"dynamic , self - improving / self - modifying neural networks ( which can simulate gödel machine ) ?",neural-networks,1,4,2
2122,7975,1,,2018-09-14T12:07:49.547,3,66,"problem : we have a fairly big database that is built up by our own users . the way this data is entered is by asking the users 30ish questions that all have around 12 answers ( x , a , a , b , c , ... , h ) . the letters stand for values that we can later interpret . i have already tried and implemented some very basic predictors , like random forest , a small nn , a simple decision tree etc . but all these models use the full dataset to do one final prediction . ( fairly well already ) . what i want to create is a system that will eliminate 7 to 10 of the possible answers a user can give at any question . this will reduce the amount of data we need to collect , store , or use to re - train future models . i have already found several methods to decide what are the most discriminative variables in the full dataset . except , when a user starts filling the questions i start to get lost on what to do . none of the models i have calculate the next question given some previous information . it feels like i should use a naive bayes classifier , but i 'm not sure . other approaches include recalculating the gini or entropy value at every step . but as far as my knowledge goes , we ca n't take into account the answers given before the recalculating .",18225,2444,2019-02-16T02:43:21.590,2019-02-16T02:43:21.590,how can i minimize the number of answers that are relevant to a machine learning model ?,machine-learning feature-selection decision-tree,2,0,
2123,7977,1,7978,2018-09-14T14:44:09.633,1,227,"i would like to have a chat to talk on a personal plan enough to pass the turing test . i got to study some natural language processing ( nlp ) but some that usually hide a lot of mechanical , so i think they are also much more sensitive than words and get phrases more fluid . the problem is that i do not even have the idea to start . i did some testing with a nlp chatterbot library for python , but i do not know if it 's possible to join pln to networks or how to use a neural network and i can not even use python or if i have to use a specific language . in addition , i am using the book artificial intelligence - russell and norvig as the basis for the article and i would like recommend me others .",18233,,,2018-09-14T15:41:48.153,how to make a chatbot with nlp and neural,neural-networks natural-language-processing python chat-bots,1,3,
2124,7979,1,7981,2018-09-14T20:21:27.440,5,509,"in alphazero , the policy network ( or head of the network ) maps game states to a distribution of the likelihood of taking each action . this distribution covers all possible actions from that state . how is such a network possible ? the possible actions from each state are vastly different than subsequent states . so , how would each possible action from a given state be represented in the network 's output , and what about the network design would stop the network from considering an illegal action ?",12201,12201,2018-11-19T02:21:31.173,2018-11-19T02:21:31.173,why does the policy network in alphazero work ?,neural-networks reinforcement-learning ai-design alphazero alphago-zero,1,0,
2125,7983,1,8046,2018-09-15T03:08:03.320,3,122,"i 'm currently implementing the original neat algorithm in swift . looking at figure 4 in stanley 's original paper , it seems to me there is a chance that node 5 will have no ( enabled ) outgoing connection if parent 1 is assumed the fittest parent and the connection is randomly picked from parent 2 . is my understanding of the crossover function correct and can it indeed result in a node with no outgoing connections ?",18249,16909,2018-09-15T13:53:04.140,2018-09-19T18:03:50.660,can a crossover result in a node with no outgoing connections ?,neat,1,0,1
2126,7985,1,7991,2018-09-15T06:05:05.613,1,36,"in conditional generative adversarial networks ( gan ) , https://arxiv.org/pdf/1411.1784.pdf , the loss function is , discriminator and generator both takes y , the auxiliary information . i am confused as to what will be the difference by using log(d(x , y ) and log(1-d(g(z , y ) ) as y goes in input to d and g in addition to x and z ?",18253,16909,2018-09-15T13:45:56.877,2018-09-15T13:52:18.427,"why d(x|y ) and not d(x , y ) in conditional generative networks",generative-adversarial-networks,1,0,1
2127,7986,1,,2018-09-15T09:45:40.717,-1,208,"i 'm a web developer and have been doing web development for years . now , i need to lean ai development . i want to build a machine learning application . for example , i want to create an application which alerts people of high blood pressure , especially when they are physically exercising . i 'm not sure where to start and what to study . i am looking for a complete guide . what should i learn ?",18257,2444,2019-05-03T12:42:48.533,2019-05-03T12:42:48.533,a complete guide to learn ai,ai-basics getting-started,4,0,1
2128,7992,1,8002,2018-09-15T15:17:47.943,1,66,"currently , i am interested in how nns or any other ai models can be used for composing music . but there are many other interesting applications too , like language processing . i am wondering that : nns generally need a cost function for learning . but for example , for composing music , what would be an appropriate cost function ? i mean , algorithms ca n't ( yet ) really ' calculate ' how good music is , right ?",17769,16920,2018-09-15T21:04:59.220,2018-09-16T12:57:02.357,how to find a cost function for human data,neural-networks,1,1,
2129,7993,1,8003,2018-09-15T16:14:09.657,1,116,"at the time when the basic building blocks of machine learning ( the perceptron layer and the convolution kernel ) were invented , the model of the neuron in the brain taught at the university level was simplistic . back when neurons were still just simple computers that electrically beeped untold bits to each other over cold axon wires , spikes were not seen as the hierarchical synthesis of every activity in the cell down to the molecular scale that we might say they are today . in other words , spikes were just a summary report of inputs to be integrated with the current state , and passed on . in comprehending the intimate relationships of mitochondria to spikes ( and other molecular dignitaries like calcium ) we might now more broadly interpret them as synced messages that a neuron sends to itself , and by implication its spatially extended inhabitants . synapses weigh this information heavily but ultimately , but like the electoral college , fold in a heavy dose of local administration to their output . the sizes and positions within the cell to which mitochondria are deployed can not be idealized or anthropomorphized to be those metrics that the neuron decides are best for itself , but rather what is thermodynamically demanded . 1 notice the reference to summing in the first bolded phrase above . this is the astronomically oversimplified model of biology upon which contemporary machine learning was built . of course ml has made progress and produced results . this question does not dismiss or criticize that but rather widen the ideology of what ml can become via a wider field of thought . notice the second two bolded phrases , both of which denote statefulness in the neurons . we see this in ml first as the parameters that attenuate the signals between arrays of artificial neurons in perceptrons and then , with back - propagation into deeper networks . we see this again as the trend in ml pushes toward embedded statefulness by integrating with object oriented models , the success of lstm designs , the interrelationships of gan designs , and the newer experimental attention based network strategies . but does the achievement of higher level thought in machines , such as is needed to ... fly a passenger jet safely under varying conditions , drive a car in the city , understand complex verbal instructions , study and learn a topic , provide thoughtful ( not mechanical ) responses , or write a program to a given specification ... requiring from us a much more radical is the transition in thinking about what an artificial neuron should do ? scientific research into brain structure , its complex chemistry , and the organelles inside brain neurons have revealed significant complexity . performing a vector - matrix multiplication to apply learning parameters to the attenuation of signals between layers of activations is not nearly a simulation of a neuron . artificial neurons are not very neuron - like , and the distinction is extreme . a little study on the current state of the science of brain neuron structure and function reveals the likelihood that it would require a massive cluster of gpus training for a month just to learn what a single neuron does . are artificial networks based on the perceptron design inherently limiting ? references [ 1 ] fast spiking axons take mitochondria for a ride , by john hewitt , medical xpress , january 13 , 2014 , https://medicalxpress.com/news/2014-01-fast-spiking-axons-mitochondria.html",4302,4302,2018-09-20T05:17:24.117,2018-09-20T05:17:24.117,are artificial networks based on the perceptron design inherently limiting ?,neural-networks research hardware architecture long-short-term-memory,2,1,1
2130,7996,1,,2018-09-15T18:31:48.160,2,64,"i am trying to understand the dimensionality of the outputs of convolution operations . suppose a convolutional layer with the following characteristics : input map a bank of $ f$ filters , each of dimension a stride of $ & lt;s_x , s_y&gt;$ for the corresponding x and y dimensions of the input map either valid or same padding ( explain for both if possible ) what should be the expected dimensionality of the output map expressed in terms of $ h , w , d , f , h ' , w ' , s_x , s_y$ ?",18267,,,2018-12-17T13:38:37.943,dimensionality of convolutional layers & convolution operations,neural-networks deep-learning convolutional-neural-networks,2,1,1
2131,7998,1,,2018-09-15T18:55:27.197,3,165,"keras ' convolutional and deconvolutional layers are designed for square grids . is there was a way to adapt them for use in hexagonal grids ? for example , if we were using axial coordinates , the input of the kernel of radius 1 centered at ( x , y ) should be : [ ( x-1,y ) , ( x-1,y+1 ) , ( x , y-1 ) , ( x , y+1 ) , ( x+1,y-1 ) , ( x+1 , y ) ] one option is to fudge it with a 3 by 3 box , but then you are using cells at different distances . some ideas : modify kera 's convolutional layer code to use those inputs instead of the default inputs . the problem is that kera calls its backend instead of implementing it itself , which means we need to modify the backend too . use a 3 by 3 box , but set the weights at ( x-1,y-1 ) and ( x+1,y+1 ) to zero . unfortunately , i do not know how to permanently set weights to a given value in kera . use cube coordinates instead of axial coordinates . in this case , a 3 by 3 by 3 box will only contain the central hex 's neighbors and inputs set to 0 . the problem is that it makes the input array much bigger . even more problematic , some coordinates that correspond to non - hexes ( such as ( 1,0,0 ) ) will be assigned non - zero outputs ( since ( 0,0,0 ) falls within its 3 by 3 by 3 box ) . are there any better solutions ?",18006,1581,2018-09-16T10:49:22.067,2019-04-16T07:44:43.477,convolutional layers on a hexagonal grid in keras,neural-networks machine-learning convolutional-neural-networks python keras,1,1,1
2132,7999,1,,2018-09-15T20:16:07.100,0,33,we have data in text format as sentences . the goal is to detect rules which exist in this set of sentences . i have a limited set of contextless sentences that fit a pattern and want to find the pattern . i might not have sentences that do n't fit the pattern . what should be an approach to do that ?,18271,18271,2018-09-15T20:46:27.200,2018-09-17T13:34:21.757,what is the approach to deduce formal rules based on data ?,machine-learning learning-algorithms unsupervised-learning,1,1,0
2133,8000,1,8012,2018-09-15T22:29:39.477,5,163,"usually neural networks consist from layers , but is there research effort that tries to investigate more general topologies for connections among neurals , e.g. arbitrary directed acyclic graphs ( dags ) . i guess there can be 3 answers to my question : every imaginable dag topology can be reduced to the layered dags already actively researched , so , there is no sense to seek for more general topologies ; general topologies exist , but there are fundamental restrictions why they are not used , e.g. maybe learning is not converging in them , maybe they generate chaotic osciallations , maybe they generate bifurcations and does not provide stability ; general topologies exist and are promising , but scientists are not ready to work with them , e.g. maybe they have no motivation , standard layered topologies are good enough . but i have no idea , which answer is the correct one . reading the answer on https://stackoverflow.com/questions/46569998/calculating-neural-network-with-arbitrary-topology i start to think that answer 1 is the correct one , but there is no reference provided . if answer 3 is correct , then big revolution can be expected . e.g. layered topologies in many cases reduces learning to the matrix exponentiation and good tools for this are created - tensorflow software and dedicated processors . but there seems to be no software or tools for general topologies is they have some sense indeed .",8332,10135,2018-10-17T09:08:13.607,2018-10-17T09:08:13.607,neural networks of arbitrary / general topology ?,neural-networks topology,1,2,
2134,8007,1,,2018-09-16T14:55:08.080,1,19,"if one uses one of the open source implementations of the wavenet generative speech synthesis design , such as https://r9y9.github.io/wavenet_vocoder/ , and trains using something like the cmu 's arctic corpus , now can one add a voice that sounds younger , older , less professional , or in some other way distinctive . must the entire training begin from scratch , or is there a more resource and time friendly way ?",9203,,,2018-09-16T14:55:08.080,adding voices to voice synthesis corpuses,training generative-model speech-synthesis,0,0,
2135,8013,1,8020,2018-09-17T04:44:46.687,1,57,"i 'm training a language model with 5000 vocabularies using a single m60 gpu ( w/ actually usable memory about 7.5 g ) . the number of tokens per batch is about 8000 , and the hidden dimension to the softmax layer is 512 . so , if i understand correctly , fully - connected ( softmax ) layer theoretically consumes 5000 * 8000 * 512 * 4=81.92 gb for a forward pass ( 4 is for float32 ) . but the gpu performed the forward and backward passes without any problem , and it says the gpu memory usage is less than 7 gb in total . i used pytorch . what 's causing this ? edit : to be clearer , the input to the final fc layer ( 256x5000 matrix ) is of size [ 256 , 32 , 256].",18298,18298,2018-09-17T17:22:59.283,2018-09-17T18:35:57.343,calculation of gpu memory consumption on softmax layer does n't match with the empirical result,neural-networks natural-language-processing,1,1,
2136,8016,1,,2018-09-17T11:36:42.407,4,205,"i have a very imbalanced dataset of two classes : 2 % for the first class and 98 % for the second . such imbalance does not make training easy and so balancing the data set by undersampling class 2 seemed like a good idea . however , as i think about it , should not the machine learning algorithm expect the same data distribution in nature as in its training set ? i know , for sure , that the distribution of data in nature matches my imbalanced dataset . does that mean that the balanced dataset will negatively affect the neural net performance with testing ? when it assumed a different distribution of data caused by my balanced data set .",17582,,,2018-09-17T12:42:20.777,does balancing the training data set distribution for a neural network affect its understanding of the original distribution of data ?,neural-networks machine-learning deep-learning,1,0,2
2137,8024,1,8071,2018-09-18T09:32:20.113,1,91,"i have a mixed image database(unstructured data ) . in the database there are some images that i am interested in and i want to discard the rest by using cnn . i am not looking for specific objects in the images like dogs , cats etc . in the database i have photos and non photo images like infamous logos , scanned documents etc . i want to find photos and discard the others . all the examples and online courses i found are based on object recognition . in my case which method can i use to classify my images as just ' relevant ' and ' irrelevant ' ?",18283,1581,2018-09-19T00:37:22.357,2018-09-21T01:58:47.920,how can i classify relevant and irrelevant images from the database ?,convolutional-neural-networks ai-basics getting-started,1,4,
2138,8026,1,,2018-09-18T15:22:32.103,5,97,ai algorithms involving neural networks can use tensor specific hardware . are there any other artificial intelligence algorithms that could benefit from many tensor calculations in parallel ? are there any other computer science algorithms ( not part of ai ) that could benefit from many tensor calculations in parallel ? have also a look at tensorapplications and application theory .,18344,2444,2019-05-05T23:52:19.933,2019-05-05T23:52:19.933,which artificial intelligence algorithms could use tensor specific hardware ?,algorithm applications hardware,1,6,
2139,8027,1,,2018-09-18T20:10:36.710,0,131,"is there research work that uses neural network as the ( bdi ) agent ( or even full - scale cognitive architecture like soar , opencog ) - that continuously receives information from the environment and act in an environment and modifies its base of belief in parallel ? usually nn are trained to do only one task and tensorflow / pytorch supports batch mode only out of the box . also nn algorithms and theory are constructed assuming that training and inference phases are clearly separated and they have each own algorithms . so - completely new theory and software can be required for this - are there efforts in this direction ? if no , then why not ? it is so self - evident that such systems can be of benefit . https://arxiv.org/abs/1802.07569 is good review about incremental learning and it contains chapters of implemented systems , but all of them still separates learning phase from inference phase . symbolic systems and symbolic agents ( like json agentspeak ) can have updating belief / knowledge base and they can also act during receiving new information or during forming new beliefs . i am specifically seeking research about nns which do learning and inference in parallel . as far as i sought then this separation still persists in self - organizing incremental nns that are gaining some popularity . i can image the construction of chained nns in tensorflow - there is some controller network that receives input ( possibly preprocessed by hierarchically lower networks ) and that decides what to the : s.c . mental actions are the ouput of this controller , these actions determine whether some subordinated network is required to undergo additional learning or whether it can be temporary used for the processing of some information . central network itself , of course , can decide to move into temporary learning phase from time to time to improve its reasoning capabilities . such pipeline of master - slave networks is indeed possible in tensorflow but still tensorflow will have one central clock , not distributed , loosely connected processing . but i do n't know whether existence of central clock is any restriction on the generality of capabilities of such system . well , this hierarchy of networks maybe can be realized inside the one large network as well - maybe this large network can allow separate parts ( subsets of neurons ) to function in somehow independent and mutually controlling mode , maybe such regions of large neural network can emerge indeed . i am interested in this kind of research - maybe there are available some good papers for this ?",8332,8332,2018-09-18T21:33:16.977,2018-09-19T11:06:08.753,neural network as ( bdi ) agent - running in continuous mode ( that do inference in parallel with learning ) ?,neural-networks intelligent-agent,1,0,
2140,8030,1,,2018-09-19T05:25:17.373,1,117,"can neural network take decision about its own weights ( update of weights ) during training phase or during the phase of parallel training and inference ? when one region of hierarchical nn takes decision about weights of other region is the special case of my question . i am very keen to understand about self - awareness , self - learning , self - improvement capabilities of neural networks , because those exactly those self- * capabilities are the key path to the artificial general intelligence ( e.g. goedel machine ) . neural networks are usually mentioned as examples of special , single - purpose intelligence but i can not see the reason for such limitation if nn essentially trys to mimic human brains , at least in purpose if not in mechanics . well - maybe this desired effect is already effectively achieved / emerges in the operation of recurrent anns as the effect of collective behavior ?",8332,8332,2018-09-19T05:34:34.000,2018-10-15T23:28:55.473,can neural network take decision about its own weights ( update of weights ) ?,neural-networks artificial-consciousness superintelligence,1,7,
2141,8031,1,,2018-09-19T05:46:55.393,1,648,"the problem of multi - goal path planning was introduced in an icra paper in the year 2011 : “ multi - goal planning is a task which arises in many robotics applications . it combines the challenging requirements of planning feasible point - to - point trajectories in obstacle - filled — and possibly high - dimensional -- state spaces with the complexity of combinatorial optimization . ” brendan englot : multi - goal feasible path planning using ant colony optimization , 2011 ( page 1 ) the difficulty of solving this complex task is described at page 3 : “ constructing a graph which describes feasible paths over all goal - to - goal pairings is a costly task . [ .. ] in obstacle - filled and high - dimensional configuration spaces with many goals , joining all goals into a single connected component may be very costly , and especially challenging if we are undertaking a kinodynamic planning task . ” ( page 3 ) usually , using the manhattan distance is enough when we do an a * search with one target . however , it seems like for multiple goals , this is not the most useful way . which heuristic do we have to use when we have multiple targets ?",18359,11571,2018-09-20T13:19:47.183,2018-09-20T13:19:47.183,what heuristic to use when doing a * search with multiple targets ?,search heuristics,2,4,
2142,8034,1,,2018-09-19T07:21:47.930,2,46,"in physics , there are a lot of graphs , such as ' velocity vs time ' , ' time period vs length ' and so on . let 's say i have a sample set of points for a ' velocity vs time ' graph . i draw it by hand , rather haphazardly , on a canvas . this drawn graph on the canvas is then provided to the computer . by computer i mean ai . i want it to sort of beautify my drawn graph , such as straightening the lines , making the curves better , adding the digits on axes and so on . in other words , i want it to give me a better version of my drawn graph which i can readily use in , say , a word document for a report . a ) is it possible / plausible to do this ? b ) are there any apis available that can already do this ? ( do n't want to reinvent the wheel ) c ) any recommendations / suggestions to make the idea possible by altering it somehow ?",18361,1581,2018-09-19T18:53:19.690,2018-09-19T18:53:19.690,how can i use a.i / image processing to construct mathematical graphs from drawing ?,image-recognition,0,2,
2143,8038,1,,2018-09-19T11:22:14.860,0,49,"i 'm trying to use a cnn to analyse statistical images . these images are not ' natural ' images ( cats , dogs , etc ) but images generated by visualising a dataset . the idea is that these datasets hopefully contain patterns in them that can be used as part of a classification problem . most cnn examples i 've seen have one of more pooling layers , and the explaination i 've seen for them is to reduce the number of training elements , but also to allow for some locational independance of an element ( e.g. i know this is an eye , and can appear anywhere in the image ) . in my case location is important and i want my cnn to be aware of that . ie . the presence of a pattern at a specific location in the image means something very specific compared to if that feature or pattern appears elsewhere . at the moment my network looks like this ( taken from an example somewhere ) : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ layer ( type ) output shape param # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = conv2d_1 ( conv2d ) ( none , 196 , 178 , 32 ) 896 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_1 ( activation ) ( none , 196 , 178 , 32 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ max_pooling2d_1 ( maxpooling2 ( none , 98 , 89 , 32 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ conv2d_2 ( conv2d ) ( none , 96 , 87 , 32 ) 9248 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_2 ( activation ) ( none , 96 , 87 , 32 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ max_pooling2d_2 ( maxpooling2 ( none , 48 , 43 , 32 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ conv2d_3 ( conv2d ) ( none , 46 , 41 , 64 ) 18496 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_3 ( activation ) ( none , 46 , 41 , 64 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ max_pooling2d_3 ( maxpooling2 ( none , 23 , 20 , 64 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ flatten_1 ( flatten ) ( none , 29440 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dense_1 ( dense ) ( none , 32 ) 942112 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_4 ( activation ) ( none , 32 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dropout_1 ( dropout ) ( none , 32 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dense_2 ( dense ) ( none , 3 ) 99 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_5 ( activation ) ( none , 3 ) 0 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = total params : 970,851 trainable params : 970,851 non - trainable params : 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the ' images ' i 'm training on are 180 x 180 x 3 pixels and each channel contains a different set of raw data . what strategies are there to improve my cnn to deal with this ? i have tried simply removing some of the pooling layers , but that greatly increased memory and training time and did n't seem to really help .",18372,,,2018-11-19T00:01:50.360,cnn pooling layers unhelpful when location important ?,deep-learning convolutional-neural-networks keras,2,2,
2144,8039,1,8047,2018-09-19T12:35:57.423,2,141,"i 'm finding it hard to understand the relationship between chaotic behavior , the human brain , and artificial networks . there are a number of explanations on the web , but it would be very helpful if i get a very simple explanation or any references providing such simplifications .",18375,4302,2018-09-20T19:04:34.220,2018-12-08T23:30:47.427,what is chaotic behavior and how it is achieved in non - linear regression and artificial networks ?,deep-learning philosophy human-inspired convergence chaos,3,1,
2145,8042,1,,2018-09-19T14:18:14.023,1,18,"i implemented and deployed with flask an xgboost model for a classification problem . but being aware that features importance can change over time to predict probability of label for new data , i implemented a cron so that the model can be retrained every two weeks . but i do n't know how i can handle new features since i would have to wait a great volume of data to retrain the model to take into account this new feature ? is there an alternative of model deployment to this problem ?",18192,,,2018-09-19T14:18:14.023,how to handle feature changes in a model deployed ?,machine-learning ai-design models,0,0,
2146,8048,1,8062,2018-09-19T19:01:45.640,3,35,"i want to implement sparse extended information slam . there is four step to implement it . the algorithm is available in probabilistic robotics book at page 310 , table 12.3 . in this algorithm line no:13 is not very clear to me . i have 15 landmarks . so will be a vector of ( 48 * 1 ) dimension where ( 3 * 1 ) for pose . now $ h_t^i$ is a matrix whose columns are dynamic as per the algorithm it is ( 3j-3 ) and 3j . j is the values of landmarks 1 to 15 . now how could i multiply a dynamic quantity with a static one . there must be a error that matrix dimension mismatch when implement in matlab . please help me to understand the algorithm better .",18384,,,2018-09-20T12:19:25.623,seif motion update algorithm doubt,research intelligent-agent self-driving probabilistic robotics,1,2,
2147,8049,1,,2018-09-19T20:37:49.937,1,40,"this is not a soft question . neither is this question related to singularity conjecture or wars with robots . this question seeks a mathematical formulation of what is currently only qualitative and thus not clearly understood . it relates to servitude , dominance , and what measure of control species of biological or artificial entities exert over others . dominance relationships quantified we do know and rarely doubt or argue about the following somewhat self - evident statement . control implies dominance . this question focuses on how we can evaluate quantitatively whether humans are dominant over artificial systems or whether those artificial systems now dominate humans . this may seem esoteric or philosophical to some , but it is not . the balance of power between humans and artificial systems is a concrete phenomenon that may be accurately represented as a function of discrete events . we see artificial systems , with varying degrees of automation , adaptability , intelligence , and other qualitative features succumbing to the controlling forces of humans that deploy them to serve humanity without question . this is the focus of technophiles . we also see an ever increasing number of articles about game addiction , social net addiction , and texting addiction on the web , which , at its current trend will possibly surpass the volume of heroin addiction articles . we see the number of hours humans in industrialized countries interact with display devices with an ever increasing proportion of the visual content being generated artificially . this is the focus of technophobes . what is the balance of this equilibrium ? in biological systems , we see that termites are highly adaptive and can eat human habitats , yet humans can build with insect resistant materials and apply insecticides . those methods of control are greater than the control exhibited over wood , as remarkable as those who study termites say it is . an example mathematical model the above statement of inference , "" control implies dominance , "" can be represented in many formal ways . this is an example mathematical model that exhibits some features of importance but is not fully developed as a model . $ o_{e\epsilon}$ is the obedience exhibited by entity $ e$ to commands given by entity . $ m_{e\epsilon}$ is the mechanical compliance exhibited by entity $ e$ to manipulations instrumented by entity . $ i_{e\epsilon}$ is the concession of entity $ e$ to influences created by entity . $ u_{e\epsilon}$ is the unconscious purposeful behavior exhibited by entity $ e$ in response to hidden manipulations instrumented by entity . $ t$ is the measurement time period . $ d_{e\epsilon}$ is the dominance of entity $ e$ over entity . the sum , over any given measurement period , of forms of control of $ a$ over $ b$ , when greater than that sum in the opposite direction , implies that $ a$ is dominant over $ b$. inclusion of non - adversarial interaction similarly , symbiosis implies collaboration . this may directly relate to the question because not all interaction between entities , types of entities , species , or artificial systems are adversarial . in fact , it is highly probable that there is more collaboration than dominance in the world . this may be a basic fact about economics . let 's examine this related inference using the same mathematical strategy . $ c_{e\epsilon}$ is the conscious symbiotic tie of entity $ e$ to entity . $ b_{e\epsilon}$ is the mechanical binding of entity $ e$ to entity . $ q_{e\epsilon}$ is the asymmetry in an equilibrium based tie between entity $ e$ and entity . $ c_{e\epsilon}$ is the collaboration between entity $ e$ and entity . the sum , over any given measurement period , of forms of symbiosis between $ a$ and $ b$ , implies that there is positive collaboration between entities $ a$ and $ b$. returning to the focal question in what way can we measure control between humans and machines ? the below questions are not the question . the above one is . however , these may elucidate the relevance of the primary question . exactly how much are humans and artificial systems collaborating symbiotically ? how much are they adversarial in some way , and , in that respect , which side is dominant and to what degree ? are there classes of artificial systems that dominate over classes of humans , as in technology enthusiasts that have quantifiable debt resulting from technology purposes ? are there classes of humans that dominate over artificial systems , like government entities that monitor and can regulate the packets of information over the internet between nations ? most if not all of this is measurable , yet no commonly known body of theory has emerged that measures it so that public awareness of its state relative to artificial systems can be known rather than discussed without any basis for knowledge . there should be .",4302,,,2019-04-05T06:26:16.433,in what way can we measure control between humans and machines ?,ai-basics security emergence risk-management quantification,1,4,
2148,8055,1,,2018-09-20T01:27:25.590,1,8,"i am trying to reproduce this paper 's model , i.e. stacking two u - nets to yield one final prediction . the paper mentions that : the deconvolution features of the first u - net and the intermediate prediction y1 are concatenated together as the input of the second u - net . my question is : what does it mean by concatenating deconvolution features and the prediction ( which is an array ? cm ) ? the next paragraph says that : the second u - net finally gives a refined prediction y2 , which we use as the final output of our network . we apply the same loss function to both y1 and y2 during training . it leads to the next question : does it mean that i have to train u - net twice ?",18389,,,2018-09-20T01:27:25.590,how do we stack two u - nets to yield one final prediction ?,neural-networks convolutional-neural-networks,0,0,
2149,8057,1,,2018-09-20T06:06:06.213,2,50,"i have created a game based on this game here . i am attempting to use deep q learning to do this , and this is my first foray into neural networks ( please be gentle ! ! ) i am trying to create a nn that can play this game . here are some relevant facts about the game : player 1 ( the fox ) has 1 piece that he can move diagonally 1 step in any direction player 2(the geese ) has 4 pieces that they can move only forward diagonally ( either diagonal left or diagonal right ) 1 step . the fox wins if he reaches the other end of the board , the geese win if they trap the fox so it can not move . i am trying to work on the agent first for the geese as it seems to be the harder agent with more pieces and restrictions . here is the important sections of code i have so far : this is where i setup the game board , and set the total actions for the geese def _ _ init__(self ) : self.state_size = ( length , length ) # # length is 8 so ( 8,8 ) # ... # other dqn variables that are n't important to question # ... self.action_size = 8 # # 4 geese , each can potentially make 2 moves self.model = self.build_model ( ) and here is where i create my model def build_model(self ) : # builds the nn for deep - q model model = sequential ( ) # establishes a feed forward nn model.add(dense(64,input_shape = ( length , ) , activation='relu ' ) ) model.add(dense(64 , activation='relu ' ) ) model.add(dense(self.action_size , activation = ' linear ' ) ) model.compile(loss='mse ' , optimizer='adam ' ) this is where i perform an action def act(self , state , env ) : # get the list of allowed actions for the geese actions_allowed = env.allowed_actions_geese_agent ( ) if np.random.rand(0,1 ) & lt;= self.epsilon : # # do a random move return actions_allowed[random.randint(0 , len(actions_allowed)-1 ) ] act_values = self.model.predict(state ) print(act_values ) return np.argmax(act_values ) my question : since there are 4 geese and each can make 2 possible moves , am i correct in thinking that my action_size should be 8 ( 2 for each goose ) or should it be maybe 2 ( for diagonal left or right ) or something else entirely ? the reason why i am at a loss is because on any given turn , some of the geese may have an invalid move , does that matter ? my next question : even if i have the right output layer for the geese agent , when i call model.predict(state ) where i pick my action ... how do i interpret the output ? and how would i map that action it selects to a valid action that can be made ? here is a picture of the result of using model.predict(state ) , as you can see it returns a ton of data and then when i call return np.argmax(act_values ) i get 59 back ... not sure how to utilize that ( or if it 's even correct based on my output layer ) ... and finally i included a drawing of the board . f is the fox and 1,2,3,4 are the different geese . i apologize for the massive post , but i am just trying to provide as much information that is helpful .",18244,,,2018-09-20T06:06:06.213,mapping actions to the output layer in keras model for a board game,machine-learning game-ai python keras,0,0,1
2150,8058,1,,2018-09-20T06:46:46.007,-2,215,"problem statement i have 4 main input features . this is a small snippet of the data for clearer understanding . gate name - > for example and gate index_1 - > [ 0.001169 , 0.005416 , 0.01391 , 0.03037 , 0.06381 , 0.1307 , 0.2645 , 0.532 ] index_2 - > [ 7.906e-05 , 0.001123 , 0.00321 , 0.007253 , 0.01547 , 0.03191 , 0.06478 , 0.1305 ] values - > [ [ 11.0081 , 14.0303 , 18.8622 , 27.3426 , 43.8661 , 76.7538 , 142.591 , 274.499 ] , [ 11.3461 , 14.3634 , 19.1985 , 27.6827 , 44.2106 , 77.0954 , 142.926 , 274.879 ] , [ 12.258 , 15.2816 , 20.1095 , 28.5856 , 45.1057 , 77.9778 , 143.8 , 275.758 ] , [ 13.665 , 16.7457 , 21.5835 , 30.0545 , 46.5581 , 79.4212 , 145.252 , 277.192 ] , [ 15.6636 , 18.9526 , 23.9051 , 32.4281 , 48.9011 , 81.7052 , 147.477 , 279.371 ] , [ 17.8838 , 21.5839 , 26.8957 , 35.7103 , 52.3901 , 85.2132 , 150.89 , 282.714 ] , [ 19.3338 , 23.6933 , 29.7184 , 39.1212 , 56.4053 , 89.9721 , 155.913 , 287.637 ] , [ 18.7856 , 23.9999 , 31.1794 , 41.7549 , 60.0043 , 95.0488 , 162.951 , 295.005 ] ] my task is to predict this values matrix , given that i have index_1 and index_2 . originally this values matrix is propagation delay , calculated using a simulator called spice . where i am facing problem there is no written relation between index_1 , index_2 or values since simulator calculates this value using it 's own models . i have made a csv file which contains the data in separate columns . another approach that i thought . if i can give index_1 , index_2 and any 5 * 5 sub - matrix to the model , and the model can predict the values of whole 8 * 8 matrix . but the problem is again , which machine learning model do i use . approaches tried so far i have tried a cnn model for this but it is giving me very low accuracy . used one dense fully connected neural network but it is over - fitting the data and not giving me any values for matrix . i am still stuck at how to predict the matrix values given this data . what are other strategies can be used ?",18392,4302,2018-09-21T03:31:39.870,2018-10-24T06:00:47.040,machine learning to predict 8 * 8 matrix values using three independent matrices,machine-learning deep-learning,1,2,
2151,8060,1,8075,2018-09-20T08:14:08.480,1,307,"the wumpus world proposed in book of stuart russel and peter norvig , is a game which happens on a 4x4 board and the objective is to grab the gold and avoiding the threats that can kill you . the rules of game are : you move just one box for round start in position ( 1,1 ) , bottom left you have a vector of sensors for perceiving the world around you . when you are next to another position ( including the gold ) , the vector is ' activated ' . there is one wumpus ( a monster ) , 2 - 3 pits ( feel free to put more or less ) and just one gold pot you only have one arrow that flies in a straight line and can kill the wumpus entering the room with a pit , the wumpus or the gold finishes the game scoring is as follows : +1000 for grabbing the gold , -1000 for dying to the wumpus , -1 for each step , -10 for shooting an arrow . fore more details about the rules , chapter 7 of the book explains them . well now that game has been explained , the question is : in the book , the solution is demonstrated by logic and searching , does there exist another form to solve that problem with neural networks ? if yes , how to do that ? what topology to use ? what paradigm of learning and algorithms to use ? 1 * : my english is horrible , if you can send grammar corrections , i 'm grateful . 2 * : i think this is a bit confusing and a bit complex . if you can help me to clarify better , please do commentary or edit !",18391,4709,2018-09-20T12:41:44.133,2018-09-21T13:28:38.530,does a solution for wumpus world with neural networks exist ?,neural-networks deep-learning reinforcement-learning game-ai learning-algorithms,1,0,
2152,8061,1,,2018-09-20T08:51:24.263,1,427,"i am trying to implement this paper . in this paper , the author uses the forward derivative to compute the jacobian matrix df / dx using chain rule where f is the probability got from the last layer and x is input image . my model is given below . kindly let me know how to go about doing that ? class lenet5(nn.module ) : def _ _ init__(self ) : self.derivative= none # store derivative super(lenet5 , self).__init _ _ ( ) self.conv1= nn.conv2d(1,6,5 ) self.relu1= nn.relu ( ) self.maxpool1= nn.maxpool2d(2,2 ) self.conv2= nn.conv2d(6,16,5 ) self.relu2= nn.relu ( ) self.maxpool2= nn.maxpool2d(2,2 ) self.conv3= nn.conv2d(16,120,5 ) self.relu3= nn.relu ( ) self.fc1= nn.linear(120,84 ) self.relu4= nn.relu ( ) self.fc2= nn.linear(84,10 ) self.softmax= nn.softmax(dim= -1 ) def forward(self , img , forward_derivative= false ) : output= self.conv1(img ) output= self.relu1(output ) output= self.maxpool1(output ) output= self.conv2(output ) output= self.relu2(output ) output= self.maxpool2(output ) output= self.conv3(output ) output= self.relu3(output ) output= output.view(-1,120 ) output= self.fc1(output ) output= self.relu4(output ) output= self.fc2(output ) f= self.softmax(output ) # want to comput the jacobian df / dimg jacobian= computejacobian(f , img)#how to write this function return f , jacobian",17372,,,2018-09-21T19:59:50.607,compute jacobian matrix of deep learning model ?,deep-learning feedforward,1,6,
2153,8063,1,,2018-09-20T13:14:32.060,9,1885,"i 'm training an auto - encoder network with adam optimizer ( with amsgrad = true ) and mse loss for single channel audio source separation task . whenever i decay the learning rate by a factor , the network loss jumps abruptly and then decreases until the next decay in learning rate . i 'm using pytorch for network implementation and training . following are my experimental setups : setup-1 : no learning rate decay , and using the same adam optimizer for all epochs setup-2 : no learning rate decay , and creating a new adam optimizer with same initial values every epoch setup-3 : 0.25 decay in learning rate every 25 epochs , and creating a new adam optimizer every epoch setup-4 : 0.25 decay in learning rate every 25 epochs , and not creating a new adam optimizer every time rather using pytorch 's "" multisteplr "" and "" exponentiallr "" decay scheduler every 25 epochs i am getting very surprising results for setups # 2 , # 3 , # 4 and am unable to reason any explanation for it . following are my results : setup-1 results : here i 'm not decaying the learning rate and i 'm using the same adam optimizer . so my results are as expected . my loss decreases with more epochs . below is the loss plot this setup . plot-1 : optimizer = torch.optim.adam(lr=m_lr,amsgrad=true , ........... ) for epoch in range(num_epochs ) : running_loss = 0.0 for i in range(num_train ) : train_input_tensor = .......... train_label_tensor = .......... optimizer.zero_grad ( ) pred_label_tensor = model(train_input_tensor ) loss = criterion(pred_label_tensor , train_label_tensor ) loss.backward ( ) optimizer.step ( ) running_loss + = loss.item ( ) loss_history[m_lr].append(running_loss / num_train ) setup-2 results : here i 'm not decaying the learning rate but every epoch i 'm creating a new adam optimizer with the same initial parameters . here also results show similar behavior as setup-1 . because at every epoch a new adam optimizer is created , so the calculated gradients for each parameter should be lost , but it seems that this doesnot affect the network learning . can anyone please help on this ? plot-2 : for epoch in range(num_epochs ) : optimizer = torch.optim.adam(lr=m_lr,amsgrad=true , ........... ) running_loss = 0.0 for i in range(num_train ) : train_input_tensor = .......... train_label_tensor = .......... optimizer.zero_grad ( ) pred_label_tensor = model(train_input_tensor ) loss = criterion(pred_label_tensor , train_label_tensor ) loss.backward ( ) optimizer.step ( ) running_loss + = loss.item ( ) loss_history[m_lr].append(running_loss / num_train ) setup-3 results : as can be seen from the results in below plot , my loss jumps every time i decay the learning rate . this is a weird behavior . if it was happening due to the fact that i 'm creating a new adam optimizer every epoch then , it should have happened in setup # 1 , # 2 as well . and if it is happening due to the creation of a new adam optimizer with a new learning rate ( alpha ) every 25 epochs , then the results of setup # 4 below also denies such correlation . plot-3 : decay_rate = 0.25 for epoch in range(num_epochs ) : optimizer = torch.optim.adam(lr=m_lr,amsgrad=true , ........... ) if epoch % 25 = = 0 and epoch ! = 0 : lr * = decay_rate # decay the learning rate running_loss = 0.0 for i in range(num_train ) : train_input_tensor = .......... train_label_tensor = .......... optimizer.zero_grad ( ) pred_label_tensor = model(train_input_tensor ) loss = criterion(pred_label_tensor , train_label_tensor ) loss.backward ( ) optimizer.step ( ) running_loss + = loss.item ( ) loss_history[m_lr].append(running_loss / num_train ) setup-4 results : in this setup , i 'm using pytorch 's learning - rate - decay scheduler ( multisteplr ) which decays the learning rate every 25 epochs by 0.25 . here also , the loss jumps everytime the learning rate is decayed . as suggested by @dennis in the comments below , i tried with both relu and 1e-02 leakyrelu nonlinearities . but , the results seem to behave similar and loss first decreases , then increases and then saturates at a higher value than what i would achieve without learning rate decay . plot-4 shows the results . plot-4 : scheduler = torch.optim.lr_scheduler.multisteplr(optimizer=optimizer , milestones=[25,50,75 ] , gamma=0.25 ) scheduler = torch.optim.lr_scheduler.exponentiallr(optimizer=optimizer , gamma=0.95 ) scheduler = ......... # defined above optimizer = torch.optim.adam(lr=m_lr,amsgrad=true , ........... ) for epoch in range(num_epochs ) : scheduler.step ( ) running_loss = 0.0 for i in range(num_train ) : train_input_tensor = .......... train_label_tensor = .......... optimizer.zero_grad ( ) pred_label_tensor = model(train_input_tensor ) loss = criterion(pred_label_tensor , train_label_tensor ) loss.backward ( ) optimizer.step ( ) running_loss + = loss.item ( ) loss_history[m_lr].append(running_loss / num_train ) edits : as suggested in the comments and reply below , i 've made changes to my code and trained the model . i 've added the code and plots for the same . i tried with various lr_scheduler in pytorch ( multisteplr , exponentiallr ) and plots for the same are listed in setup-4 as suggested by @dennis in comments below . trying with leakyrelu as suggested by @dennis in comments . any help . thanks",8720,8720,2018-10-03T07:35:43.577,2018-10-10T18:24:52.940,loss jumps abruptly when i decay the learning rate with adam optimizer in pytorch,machine-learning deep-learning optimization autoencoders loss-functions,1,1,1
2154,8068,1,8072,2018-09-20T21:33:32.960,2,84,"many have examined the idea of modifying learning rate at discrete times during the training of an artificial network using conventional back propagation . the goals of such work have been a balance of the goals of artificial network training in general . minimal convergence time given a specific set of computing resources maximal accuracy in convergence with regard to the training acceptance criteria maximal reliability in achieving acceptable test results after training is complete the development of a surface involving these three measurements would require multiple training experiments , but may provide a relationship that itself could be approximated either by curve fitting or by a distinct deep artificial network using the experimental results as examples . epoch index learning rate hyper - parameter value observed rate of convergence the goal of such work would be to develop , via manual application of analytic geometry experience or via deep network training the following function , where is the ideal learning rate for any given epoch indexed by $ i$ , is the loss function result , and is a function the result of which approximates the ideal learning rate for as large an array of learning scenarios possible within a clearly defined domain . the development of arriving at as a closed form ( formula ) would be of general academic and industrial value . has this been done ?",4302,,,2018-09-21T08:31:26.947,is a calculus or ml approach to varying learning rate as a function of loss and epoch been investigated ?,deep-learning optimization topology convergence hyper-parameters,1,0,
2155,8076,1,,2018-09-21T13:49:52.590,1,146,"i have trouble implementing back propogation for multi class classification of cifar10 dataset my neural network has 2 layers forward propagation x - > l1 - > l2 weights w are initialized as random np.random.randn(this_layer_units , previous_layer_units ) * 0.01 x is input of size ( no_features * number of examples ) z1 = ( w1 * x ) + b1 a1 = relu(z1 ) l1 has relu activation z2 = ( w2 * a1 ) + b2 a2 = softmax(z1 ) l2 has softmax activation cost is caluclated using this equation cost = -(1 / m)*np.sum((y * np.log(a2 ) ) + ( ( 1 - y)*np.log(1-a2 ) ) ) back propagation derivative of cost is calculated da2 = -(1 / m)*(np.divide(y , a2 ) - np.divide(1 - y , 1 - a2 ) ) da2 = derivative of a2 y = one hot encoded true values softmax is np.exp(z)/ np.sum(np.exp(z ) ) now how do i proceed from here how do i find dz2 ( derivative of z2 ) using da2 and update weights link to entire jupyter notebook code",18428,18428,2018-09-21T16:19:19.587,2018-09-21T17:19:46.957,how to find partial derivative of softmax w.r.t logits in python,neural-networks machine-learning deep-learning python backpropagation,1,0,
2156,8079,1,,2018-09-21T21:03:26.313,1,15,"the idea that come to my mind is called value based model for ann . we use simple dcf formula to calculate kind of q value : rewards / discount rate . discount rate is a risk of getting the reward on the information that agent know about . of course if you have many factors you just sum that . so , we calculate fv for every cell that agent know information about and this is a predicted data . we put predicted - actual and teach model how to run using loss function . rephrased , does increase in output layer actually train the model to be better ? the human logic is that if i took course i have a bigger value that helps me to live . what about nn ? does it actually more precise if we with time increase output ?",18436,,,2018-09-21T21:03:26.313,does inflation should occur in output layer when i do artificial neural network to increase smartness of the model ?,reinforcement-learning,0,0,
2157,8080,1,8082,2018-09-21T22:38:36.357,4,84,"let ’s say i have a neural net doing classification and i ’m doing stochastic gradient descent to train it . if i know that my current approximation is a decent approximation , can i conclude that my gradient is a decent approximation of the gradient of the true classifier everywhere ? specifically , suppose that i have a true loss function , $ f$ , and an estimation of it , $ f_k$. is it the case that there exists a $ c$ ( dependent on $ f_k$ ) such that for all $ x$ and if $ |f(x)-f_k(x)|&lt;\epsilon$ then $ |\nabla f(x ) - \nabla f_k(x)|&lt;c\epsilon$ ? this is n’t true for general functions , but it may be true for neural nets . if this exact statement is n’t true , is there something along these lines that is ? what if we place some restrictions on the nn ? the goal i have in mind is that i ’m trying to figure out how to calculate how long i can use a particular sample to estimate the gradient without the error getting too bad . if i am in a context where resampling is costly , it may be worth reusing the same sample many times as long as i ’m not making my error too large . my long - term goal is to come up with a bound on how much error i have if i use the same sample $ k$ times , which does n’t seem to be something in the literature as far as i ’ve found .",12732,12732,2018-09-25T23:49:40.093,2018-09-25T23:49:40.093,do good approximations produce good gradients ?,neural-networks gradient-descent,1,0,1
2158,8083,1,8108,2018-09-22T05:59:03.060,1,38,"while we train a cnn model we often experiment with number of filters , number of convolutional layers , fc layers , filter size , sometimes stride , activation function , etc . more often than not after training the model once , it is just a trial & amp ; error process . is there a way that helps me to architect my model fundamentally before training ? once i train model how do i know which among these variables ( number of filters , size , number of convolutional layers , fc layers ) should be changed - increased or decreased ? p.s . this question assumes that data is sufficient in volume and annotated properly and still accuracy is not up to the mark . so , i 've ruled out the possibility of non - architectural flaws for the question .",17980,9947,2018-09-22T11:08:40.063,2018-09-23T19:39:49.210,"fundamentally choosing number & size of filters , convolution layers in deep learning",deep-learning convolutional-neural-networks,1,3,
2159,8088,1,,2018-09-22T15:37:23.323,1,205,"i am working on a prototype for an ev3 neural network . because for competitions , we are not allowed to use bluetooth or wifi connections , the neural network must be made with the ev3 block - based programming system ( labview for lego mindstorms ) . i am currently working on a feed - forward neural network that uses a genetic algorithm to learn . i will now explain the specifics . the neural network has a simple job : learn the difference between blue and red . the network has three layers ( input , hidden , output ) . there are two inputs . both inputs are reflected light intensity , measured one after another with the same sensor . the hidden layer has three neurons which perform a summation and output the value with a sigmoid function . the output layer has one neuron . values above or equal to 0.5 are outputted as blue while the rest are outputted as red . since there are no calculus blocks in labview for mindstorms , the summation is performed as a series of multiplication and addition problems . e is estimated as 2.71828182846 in the sigmoid function . every neuron has sigmoid rectification except the input neurons . the reason i chose to differentiate red and blue is because it is a good place to start and labview for mindstorms has a block that already knows the difference between blue and red ( color - color sensor block ) . i can use this to tell the program if its guesses were right or wrong . because the mindstorm is a feed - forward neural network , it has both weights and biases . the weights and biases are randomly selected between the values -5 to 5 . ( these were arbitrally chosen , i am not sure what to choose ) . using this network , the program generates 10 ( arbitrarily chosen number ) different "" species "" ( i am not sure what to call these ) each with 12 different weights and biases . i make each "" species "" take a test of 10 ( arbitrarily chosen number ) questions on which they are given a grade ( # of right/ total # ) based on their guess and the real answer . the program then generates a list of all the grades . using a bubble sort program ( which i have created , but have n't been successful ) there are 90 comparisons that are made to sort the grades from greatest to least . the top two grades are chosen and their associated "" species "" have 10 offspring generated by randomly selecting the weights and biases of the two . then the whole process is then repeated and theoretically , the best list of weights and biases should be generated . not having any schooling in deep learning or programming , i am wondering if i am doing anything wrong . so far , i have completed the randomization of weights / biases , the structure of the neural network , and the bubble sorting of the test scores ( which still has not worked ) . i am suspicious that my inputs both being reflected light intensity and my weight / bias constrain to -5 to 5 may prevent my network from performing optimally . please provide your guidance on what i should fix or if more information is necessary . thank you for your time .",18451,,,2019-04-20T18:02:15.563,neural network on ev3 mindstorm without 3rd party software,neural-networks deep-learning feedforward mindstorms,1,0,
2160,8089,1,,2018-09-22T15:58:25.997,4,95,"i 'm an embedded systems designer , with a little background in fuzzy logic . i 'm developing a device for machine diagnostics based on a series of sensors data . my question is whether through fuzzy logic i can develop some kind of diagnostic indicator of machine parts failure . let 's say that i have the sensor a and sensor b near three parts . i am wondering if it is possible to get probabilities that part 1 , part 2 , or part 3 are in failure . i can collect all physical parameters as temperature , pressure , speed , vibration , etc . related to each of those parts . also i have the expert person who can give me info about relation among data collection and failures . however , i do n't have interest in accomplishing this with an expert system .",18452,4302,2018-10-22T11:36:33.243,2018-12-21T15:22:36.027,machine diagnostics with fuzzy logic,getting-started detecting-patterns fuzzy-logic embedded-design,3,0,
2161,8091,1,8092,2018-09-22T18:18:21.100,2,95,"depending on the source i find people using different variations of the "" squared error function "" , how come that be ? variation 1 variation 2 notice that it is being devided by 1 over m as opposed to variation 1 ( 1/2 ) the stuff inside the ( ) ^2 is simply notation i get that , but dividing by 1 / m and 1/2 will cleary get a different result . which version is the "" correct "" one , or is there no such thing as a correct or "" official "" squared error function ?",11814,,,2018-09-22T18:55:34.987,variations of the squared error function,neural-networks,1,1,1
2162,8093,1,,2018-09-23T01:14:07.010,1,65,"so i trained an ai to generate shakespeare , which it did somewhat well . i used this 10,000 character sample . next i tried to get it to generate limericks using these 100,000 limericks . it generated garbage output . when i limited it to 10,000 characters , it then started giving reasonable limerick output . how could this happen ? i thought more data was always better . the ai was a neural network with some lstm layers , implemented in keras .",18006,,,2018-09-25T06:31:49.260,why would giving my ai more data make it perform worse ?,python recurrent-neural-networks datasets keras lstm,1,1,
2163,8094,1,8136,2018-09-23T03:24:56.677,4,67,"i 'm learning the logistic regression and l2 regularization . the cost function looks like below . $ $ j(w ) = -\displaystyle\sum_{i=1}^{n } ( y^{(i)}\log(\phi(z^{(i)})+(1-y^{(i)})\log(1-\phi(z^{(i)})))$$ and the regularization term is added . ( is a regularization strength ) $ $ j(w ) = -\displaystyle\sum_{i=1}^{n } ( y^{(i)}\log(\phi(z^{(i)})+(1-y^{(i)})\log(1-\phi(z^{(i ) } ) ) ) + \frac{\lambda}{2}\| w \|$$ intuitively , i know that if becomes bigger , extreme weights are penalized and weights become closer to zero . however , i 'm having a hard time to prove this mathematically . $ $ $ $ $ $ this does n't show the reason why incrementing makes weight become closer to zero . it is not intuitive .",18427,,,2018-09-25T15:13:33.483,how does l2 regularization make weights smaller ?,regularization,1,0,2
2164,8097,1,8098,2018-09-23T10:29:05.220,0,185,"i 'm trying to implement an algorithm that would choose the optimal next move for the game of connect 4 . as i just want to make sure that the basic minimax works correctly , i am actually testing it like a connect 3 on a 4x4 field . this way i do n't need the alpha - beta pruning , and it 's more obvious when the algorithm makes a stupid move . the problem is that the algorithm always starts the game with the leftmost move , and also during the game it 's just very stupid . it does n't see the best moves . i have thoroughly tested methods makemove ( ) , undomove ( ) , getavailablecolumns ( ) , iswinningmove ( ) and islastspot ( ) so i am absolutely sure that the problem is not there . here is my algorithm . nextmove.java private static class nextmove { final int evaluation ; final int moveindex ; public nextmove(int eval , int moveindex ) { this.evaluation = eval ; this.moveindex = moveindex ; } int getevaluation ( ) { return evaluation ; } public int getmoveindex ( ) { return moveindex ; } } the algorithm private static nextmove max(c4field field , int moveplayed ) { // moveindex previously validated // 1 ) check if moveindex is a final move to make on a given field field.undomove(moveplayed ) ; // check if ( field.iswinningmove(moveplayed , c4symbol.blue ) ) { field.playmove(moveplayed , c4symbol.red ) ; return new nextmove(blue_win , moveplayed ) ; } if ( field.iswinningmove(moveplayed , c4symbol.red ) ) { field.playmove(moveplayed , c4symbol.red ) ; return new nextmove(red_win , moveplayed ) ; } if ( field.islastspot ( ) ) { field.playmove(moveplayed , c4symbol.red ) ; return new nextmove(draw , moveplayed ) ; } field.playmove(moveplayed , c4symbol.red ) ; // 2 ) moveindex is not a final move // --&gt ; try all possible next moves final list&lt;integer&gt ; possiblemoves = field.getavailablecolumns ( ) ; int besteval = integer.min_value ; int bestmove = 0 ; for ( int moveindex : possiblemoves ) { field.playmove(moveindex , c4symbol.blue ) ; final int currenteval = min(field , moveindex).getevaluation ( ) ; if ( currenteval & gt ; besteval ) { besteval = currenteval ; bestmove = moveindex ; } field.undomove(moveindex ) ; } return new nextmove(besteval , bestmove ) ; } private static nextmove min(c4field field , int moveplayed ) { // moveindex previously validated // 1 ) check if moveindex is a final move to make on a given field field.undomove(moveplayed ) ; // check if ( field.iswinningmove(moveplayed , c4symbol.blue ) ) { field.playmove(moveplayed , c4symbol.blue ) ; return new nextmove(blue_win , moveplayed ) ; } if ( field.iswinningmove(moveplayed , c4symbol.red ) ) { field.playmove(moveplayed , c4symbol.blue ) ; return new nextmove(red_win , moveplayed ) ; } if ( field.islastspot ( ) ) { field.playmove(moveplayed , c4symbol.blue ) ; return new nextmove(draw , moveplayed ) ; } field.playmove(moveplayed , c4symbol.blue ) ; // 2 ) moveindex is not a final move // --&gt ; try all other moves final list&lt;integer&gt ; possiblemoves = field.getavailablecolumns ( ) ; int besteval = integer.max_value ; int bestmove = 0 ; for ( int moveindex : possiblemoves ) { field.playmove(moveindex , c4symbol.red ) ; final int currenteval = max(field , moveindex).getevaluation ( ) ; if ( currenteval & lt ; besteval ) { besteval = currenteval ; bestmove = moveindex ; } field.undomove(moveindex ) ; } return new nextmove(besteval , bestmove ) ; } the idea is that the algorithm takes in the arguments of a currentfield and the lastplayedmove . then it checks if the last move somehow finished the game . if it did , i just return that move , and otherwise i go in - depth with the subsequent moves . blue player is max , red player is min . in each step i first undo the last move , because it 's easier to check if the "" next "" move will finish the game , than check if the current field is finished ( this would require to analyze for all possible winning options in the field ) . after i check , i just redo the move . from some reason this does n't work . i am stuck with that for days ! i have no idea what 's wrong ... any help greatly appreciated ! edit i 'm adding the code how i 'm invoking the algorithm . @override public int nextmove(c4game game ) { c4field field = game.getcurrentfield ( ) ; c4field tmp = c4field.copyfield(field ) ; int moveindex = tmp.getavailablecolumns().get(0 ) ; final c4symbol symbol = game.getplayertomove().getsymbol().equals(c4symbol.blue ) ? c4symbol.red : c4symbol.blue ; tmp.droptocolumn(moveindex , symbol ) ; nextmove mv = symbol .equals(c4symbol.blue ) ? max(tmp , moveindex ) : min(tmp , moveindex ) ; int move = mv.getmoveindex ( ) ; return move ; }",18470,18470,2018-09-23T12:26:05.070,2018-09-23T12:55:11.693,connect 4 minimax does not make the best move,game-ai minimax java,1,0,
2165,8099,1,8101,2018-09-23T13:54:20.620,1,35,"from what i understand , the value function estimates how ' good ' it is for an agent to be in a state and a policy is a mapping of actions to state . so if i have understood value function and policies correctly , why does the value of a state change with the policy with which an agent gets there ? i guess i 'm having difficulty grasping the concept that the goodness of a state changes depending on how an agent got there ( different policies may have different ways , and hence different values , for getting to a particular state ) . if there can be a concrete example ( perhaps on a gridworld or on a chess board ) , that might make it clear why that might be the case .",18468,,,2018-09-23T16:20:12.523,dependance of value function of an mdp on policy,reinforcement-learning ai-basics getting-started,1,0,
2166,8100,1,,2018-09-23T15:20:15.643,2,41,"i want to understand what the gamma parameter does in svm , according to this page . http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html intuitively , the gamma parameter defines how far the influence of a single training example reaches , with low values meaning ‘ far ’ and high values meaning ‘ close ’ . the gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors . i do n't understand this part "" of a single training example reaches "" , does it refer to the training dataset ?",18464,,,2018-09-24T06:00:33.820,parameter gamma in svm,svm,2,0,
2167,8104,1,,2018-09-23T17:17:40.693,1,125,"i 'm considering a gpu system for deep learning applications , mainly for training models with large datasets . so i 'm not sure whether it makes sense to choose nvidia nv - link over more tesla v100 graphics cards ? is nv - link the bottleneck that a system with two tesla v100 and nv - link is "" faster "" compared to four tesla v100 without nv - link ? as i have seen , the nvidias gpu system dgx-1 and dgx-2 uses nvlink .",13295,13295,2018-09-23T17:36:47.857,2018-09-30T06:21:36.227,"is pcie the bottleneck in a deep learning gpu system , so it makes sense to choose nvidia nv - link over more tesla v100 graphics cards ?",deep-learning hardware hardware-evaluation,0,0,
2168,8105,1,,2018-09-23T18:06:33.120,5,41,i am trying to think of some marketing related classification challenges that a feed forward neural network would be suited for . any ideas ?,18453,,,2019-04-24T21:01:20.270,which marketing related classifictation challenges is a feed forward neural network suited to sovle ?,neural-networks classification feedforward,1,0,
2169,8106,1,,2018-09-23T19:00:34.053,5,146,"some have said , "" two heads are better than one . "" that 's true if they are collaborating . if not , the two together may be worse than zero . although that 's a rhetorical opening paragraph , this is a mathematical question . what are the algebraic properties of intelligence ? is intelligence additive ? is intelligence additive and under what conditions ? if we have a software component containing some ai and we duplicate it and aggregate the two , is the new aggregated component twice as smart ? twice as fast ? twice as reliable ? twice as versatile ? twice as accurate ? under realistic computing conditions , none of these are true in the general or even common case . yet we imagine that larger systems will be smarter . why ? is it subtractive ? is intelligence subtractive ? if we create intelligence on earth in one location , does that decrease the total intelligence everywhere else ? is there a law of conservation of intelligence . probably not . is it conserved ? before we consider the idea of conservation of intelligence , consider conservation of information . do we have more information now , or just massive redundancy because of easy and fast ability to copy data now ? when humanity discovers something , does it forget something else ? how would we know if that were true , since we forgot ? this paradox is important to artificial intelligence . some things we know it is important to know exactly what kind of hill we think we are climbing . here 's what we do know . we know that accuracy and reliability can be in conflict . a guess is sometimes more reliable and less accurate , where as an answer with six significant digits ca n't always be trusted , as in the case of applying newtonian physics to the orbit of mercury . we intuitively know , when we encounter success , to keep it and perhaps replicate it . we manufacture designs that have been proven to work . we think intelligence is useful , and we can show examples that are convincing . i 'm convinced . we want to manufacture that too , yet we do n't know if twice the intelligence is twice as good or even what twice the intelligence means in the concrete realm of a computer program . comparative intelligence although we do n't know what , "" more intelligent , "" means in every condition of comparison or agree on who or what is more intelligent in every circumstance , there is more that we do know . some of what we know can help us move toward understanding the algebraic properties of intelligence . a smart entity in one place is not smart in another . this is true of learning . many write and speak of general intelligence , yet universal intelligence is n't even realized in humans . one would not ask linus torvalds to write a song for a television commercial , and one would n't ask christopher nolan to find a better way to automatically taper the stochastic component in a stochastic gradient descent strategy . we know that a person placed in one job will get it immediately and if placed in another many not in their life time ever do it well . that 's one key . intelligence appears to be linked to the type of environmental challenge . as much as general intelligence is discussed as something we hope ai research to achieve , it may never be realized . some have said , "" you can not prove the existence of god , "" which is just as easy to say about general intelligence . there are sometimes no proofs for infinite cases and they can not be measured for the purpose of proof of concept . omniscience and omnipotence may forever be outside the scope of human endeavors . parallelism is proven we do know that if we have a large set of data to process and we detect a bottleneck in the process , we can scale the components of the bottleneck , and in some cases , if done with an understanding of the process and the cause of the bottleneck , produce close to double the throughput by intelligently doubling the computing resources . in that case , data throughput can scale . however , that may or many not be the case with intelligence . that brings us back to the question of whether intelligence is additive . potentially important hint we know this too . if we have adversarial intelligence , the competition may , under certain circumstances , lead to improvements . the intelligence of adversaries may tune one another . although the competition is adversarial , there is a symbiotic element , and two competitors can be friends outside of the competition . we see this in school as students compete for the high gpas . this has been demonstrated in the success of simple generative adversarial networks too . we also know that adversarial relationships are inherently limited . we know that without collaboration , the formation of partners and teams , some things do n't occur at all or become mutually destructive . john nash broadly defined a mathematics of economic equilibria that form in what morgenstern and von neumann defined as non - zero sum games . we see in history the effectiveness , the legitimacy , and sometimes the elegance of collaboration . what would george westinghouse have accomplished without tesla ? would china and the u.s . be largely on the same page today ( in spite of their grossly different view of political structure ) had nixon and kissinger not collaborated and then initiated more collaboration with mao tse - tung and chou en - lai , who had also been collaborating ? competitive interaction within the biosphere helped e. coli evolve . it helped jackals evolve too , but symbiosis had much to do with the progress of both species . jackals could n't digest their food efficiently without the e. coli bacteria that benefits from the hunting skills of the jackals . both organisms may have benefited from survival of the fittest , yet without symbiotic relationships , both life forms would have been considerably diminished . what conditions determine the algebraic properties ? when does what algebraic operation on intelligence model reality ? perhaps intelligence is not a property at all . perhaps intelligence is an umbrella term for a set of much more precise quantities that behave reliably . in scientific history , it would not be the first time . aristotle spoke of attraction , but physicists now know about gravity , electrostatic and electromagnetic forces , bonding , and other effects that can be modeled with precision . the work of google and caltech on pac learning is one of the systematic approaches to the phenomena of learning . such work steps toward the ability to work with ai metrics algebraically . quantifying intelligence in when we talk about more intelligence , we infer that intelligence is a quantity . here and elsewhere , i 've drawn attention to the dysfunctional side of representing intelligence as a scalar in . i 've proposed that intelligence must , at the very least , be represented as vector in , where n is at least twenty . whether intelligence is a vector with dozens of features ( dimensions ) or not , how can we work with it as a quantity ? or are we wrong to think of it as quantitative ? if it is a quantity , must it always be relative to a specific set of problems no matter how large we grow a set of intelligent capabilities through continued ai development ? what are the algebraic properties of intelligence ? response to comments although perlovsky 's work is interesting , the wikipedia blogging in general and in this case is not . perlovsky does delve into the nature of intelligence , but does not consider its algebraic properties ( as in abstract algebra ) such as morgenstern and von neumann did with game theory , which is the central theme in this question .",4302,4302,2019-03-06T06:21:00.540,2019-04-05T08:02:30.500,what are the algebraic properties of intelligence ?,learning-algorithms math collaboration,3,1,3
2170,8109,1,8110,2018-09-24T04:06:26.310,1,40,"i have a course named "" evolutionary algorithm "" . but , our teacher is always mentioning the word "" optimization "" in his lectures . i am confused . is he actually teaching optimization ? if yes , why is the name of the course not "" optimization "" ? what is the difference between the study of evolutionary algorithm and optimization ?",,,,2018-09-24T05:18:06.820,what is the difference between the study of evolutionary algorithm vs. optimization ?,evolutionary-algorithms optimization,1,1,
2171,8117,1,8123,2018-09-24T09:56:38.203,3,86,"let 's assume i want to teach a cnn some physics . starting with a u - net , i input images a and b as separate channels . i know that my target ( produced by a very slow monte - carlo code ) represents a signal such as f(g(a ) * h(b ) ) , where f , g and h are fairly "" convolutional "" operations -- meaning , involving mostly blurring and rescaling operations . i feel safe to state that this problem would not be too difficult for the case of f(g(a ) + h(b ) ) -- but what about f(g(a ) * h(b ) ) ? can i expect a basic cnn such as the u - net to be able to represent the * ( multiplication ) operation ? or should i expect to be forced to include a multiply layer in my network , somewhere where i expect that the part before can learn the g and h parts , and the part after can learn the f part ?",18486,,,2018-09-24T20:22:29.943,"can a basic cnn ( conv2d , maxpooling2d , upsampling2d ) find a good approximation of a product of its input channels ?",convolutional-neural-networks,1,0,
2172,8121,1,,2018-09-24T14:52:52.543,2,45,"i have trouble finding material ( blog , papers ) about this issue , so i 'm posting here . taking a recent well known example : musk has tweeted and warned about the potential dangers of ai , saying it is "" potentially more dangerous than nukes "" , referring the issue of creating a superintelligence whose goals are not aligned with ours . this is often illustrated with the paperclip maximiser though experiment . let 's call this first concern "" ai alignment "" . by contrast , in a recent podcast , his concerns seemed more related to getting politicians and decision makers to acknowledge and cooperate on the issue , to avoid potentially dangerous scenarios like an ai arms race . in a paper co - authored by nick bostrom : racing to the precipice : a model of artificial intelligence development , the authors argue that developing agi in a competitive situation incentivises us to skim on safety precautions , so it is dangerous . let 's call this second concern "" ai governance "" . my question is about the relative importance between these two issues : ai alignment and ai governance . it seems that most institutions trying to prevent such risks ( miri , fhi , fli , openai , deepmind and others ) just state their mission without trying to argue about why one approach should be more pressing than the other . how to assess the relative importance of those two issues ? and can you point me any literature about this ?",1741,4302,2018-09-25T00:24:10.640,2018-09-25T06:59:42.090,should we focus more on societal or technical issues with ai risk,strong-ai social neo-luddism value-alignment risk-management,1,1,1
2173,8126,1,8134,2018-09-24T21:35:10.663,1,47,"within a piece of text , i 'm trying to detect who did what to whom . for instance , in the following sentences : cv hit iv . cv was hit by iv . i 'd like to know who hit whom . i ca n't remember what this technique is called . thanks for any help !",18501,,,2018-09-25T09:03:37.953,looking for nlp technique and drawing a blank,natural-language-processing computational-linguistics,1,0,
2174,8128,1,,2018-09-25T01:08:12.727,3,333,"i have difficulty understanding the following paragraph in the below excerpts from page 4 to page 5 from the paper dueling network architectures for deep reinforcement learning . the author said "" we can force the advantage function estimator to have zero advantage at the chosen action . "" for the equation $ ( 8)$ below , is it correct that $ a - maxa$ is at most zero ? ... lack of identifiability is mirrored by poor practical performance when this equation is used directly . to address this issue of identifiability , we can force the advantage function estimator to have zero advantage at the chosen action . that is , we let the last module of the network implement the forward mapping $ $ q(s , a ; \theta , \alpha , \beta ) = v(s ; \theta , \beta ) + \left ( a(s , a ; \theta , \alpha ) - \max_{a ' \in | \mathcal{a } | } a(s , a ' ; \theta , \alpha ) \right ) . \tag{8}$$ now , for $ a^∗ = \text{arg max}_{a ' \in \mathcal{a } } q(s , a ' ; \theta , \alpha , \beta ) = \text{arg max}_{a ' \in \mathcal{a } } a(s , a ' ; \theta , \alpha)$ , we obtain $ q(s , a^∗ ; \theta , \alpha , \beta ) = v ( s ; \theta , \beta)$ . hence , the stream $ v(s ; \theta , \beta)$ provides an estimate of the value function , while the other stream produces an estimate of the advantage function . i would like to request further explanation on equation 9 , when the author wrote what is bracketed between the red parentheses below . an alternative module replaces the max operator with an average : $ $ q(s , a ; \theta , \alpha , \beta ) = v ( s ; \theta , \beta ) + \left ( a(s , a ; \theta , \alpha ) − \frac { 1 } { |a| } \sum_{a ' \in \mathcal{a } } a(s , a ' ; \theta , \alpha ) \right ) . \tag{9}$$ on the one hand this loses the original semantics of $ v$ and $ a$ because they are now off - target by a constant , but on the other hand it increases the stability of the optimization : with ( 9 ) the advantages only need to change as fast as the mean , instead of having to compensate any change to the optimal action ’s advantage in ( 8) . in the paper , to address identifiability issue , there are two equation used . my understanding is both equations are trying to fix advantage part - the last module . for equation $ ( 8)$ , are we trying to make $ v(s ) = q*(s)$ , as the last module is zero . and for equation $ ( 9)$ , the resulting $ v(s)$ = true $ v(s)$ + mean $ ( a)$ ? - as the author said ‘ on the one hand this loses the original semantics of $ v$ and $ a$ because they are now off - target by a constant ’ . and the constant refers to mean $ ( a)$ ? is my understanding correct ?",18504,22916,2019-03-28T22:13:02.037,2019-03-28T22:13:02.037,"difficulty in understanding identifiability in the "" dueling network architectures for deep reinforcement learning "" paper",reinforcement-learning q-learning,3,0,1
2175,8131,1,8133,2018-09-25T06:36:42.460,-1,289,what is the best book for learn artificial intelligence ?,18510,1671,2018-09-25T23:21:40.430,2018-09-25T23:21:40.430,what is the best book for learn artificial intelligence,ai-basics getting-started reference-request,1,6,
2176,8135,1,,2018-09-25T14:15:59.553,3,44,"in my daily machine learning / deep learning workflow , i often want to interact with a dataset in my code . specifically , i would like to be able to load some module / package which can make sure that i have the dataset in a specific folder represent the structure of the dataset on a higher , possible object oriented level is homogeneous across datasets of the same class , e.g. image classification ( set of images with labels , possibly bounding boxes ) open source , such that i could contribute a specification to a previously uncovered dataset that let 's me specify a set of commonly used licenses to which i can comply , to filter all compatible datasets currently i mainly use python , but other languages are also in the scope of this question ( matlab , java ) . does such an api exist ? if not , which ones come closest to the requirements stated above ? to give you an example of how i would expect it to feel like , see the following python code import dataset_api as da in_2012 = da.get('imagenet ' , version='2012 ' ) in_2017 = da.get('imagenet ' , version='2017 ' ) coco = da.get('coco ' ) image_classification = da.merge([in_2012 , in_2017 , coco ] ) images = image_classification.images image = images[0 ] image.path # the absolute file path to the image file on my disk image.objects # an array of object that are visible in this image note : this question is also posted on datascience.stackexchange.com . i was n't sure which exchange better fits the question .",12345,,,2018-09-25T17:49:35.960,python api for publicly available datasets,machine-learning python datasets structured-data,1,0,2
2177,8142,1,,2018-09-26T08:50:58.333,2,31,"i have read about auto encoder . understood what is encoding part , and decoding part , and the latent space . now , i tried to implement this in keras . below is the code . ilayer = input ( ( 784 , ) ) layer1 = dense(128 , activation='relu ' ) ( ilayer ) layer2 = dense(64 , activation='relu ' ) ( layer1 ) layer3 = dense(28 , activation = ' relu ' ) ( layer2 ) layer4 = dense(64 , activation='relu ' ) ( layer3 ) layer5 = dense(128 , activation='relu ' ) ( layer4 ) layer6 = dense(784 , activation='softmax ' ) ( layer5 ) model = model ( ilayer , layer6 ) model.compile(loss='binary_crossentropy ' , optimizer='adam ' ) ( trainx , trainy ) , ( testx , testy ) = mnist.load_data ( ) print ( "" shape of the trainx "" , trainx.shape ) trainx = trainx.reshape(trainx.shape[0 ] , trainx.shape[1 ] * trainx.shape[2 ] ) print ( "" shape of the trainx "" , trainx.shape ) model.fit ( trainx , trainx , epochs=5 , batch_size=100 ) as simple as that , i have a 6 dense layers . i am still able see that the output image is closer to the input image . from blurred to deblurred conversion , i can have few dense layers like the above ( basically a simple neural network ) and get the inputs de - blured . in that cause , why autoencoder is very famous for de - blurring ? am i missing any core part of this auto encoder ? .",17994,18663,2018-10-18T03:16:19.397,2018-10-18T03:16:19.397,autoencoder why it is special for image decoding ?,neural-networks machine-learning convolutional-neural-networks keras autoencoders,0,0,
2178,8143,1,,2018-09-26T08:00:54.500,1,33,i am doing a research on above cited topic but i am stuck with how to actually start the project on this . what tools are required for this kind of project ? what resources are required to do project on this ?,22150,4709,2018-11-04T20:25:55.313,2018-11-04T20:25:55.313,a binarization method for degraded document image using artificial neural network and interpolation inpainting,neural-networks resource-request,0,2,
2179,8145,1,,2018-09-26T11:52:51.907,-1,71,"i want to compare the time complexity of two deep neural networks , but i have no idea how to go about it . how do i graphically achieve that with respect to the number of iterations , accuracy or any other metric ?",18542,,,2018-09-27T13:18:05.417,how to visualize the time complexity for training a neural network,deep-learning tensorflow python,1,2,
2180,8147,1,,2018-09-26T14:57:39.903,1,367,"i have a simple question about the choice of activation function for the output layer in feed - forward neural networks . i have seen several codes , where the choice of activation function for the output layer is linear . now , it might well be that i am wrong about this , but is n't that simply equivalent to a rescaling of the weights connecting the last hidden layer to the output layer ? and following this point , are n't you just as well off with just using the identity function as your output activation function ?",18546,,,2018-09-26T16:00:28.673,using linear activation function in output layer,neural-networks,1,1,1
2181,8160,1,8175,2018-09-27T22:17:08.487,-1,103,"i am currently looking to use a neural network to classify gestures . i have a series of dx , dy , dz readings that represent the differences across the three axes made during the gesture . about 10 movements for each example of the gesture . basically a 10x3 matrix and then classify the training data into about 15 classes . i plan to use a cnn classifier to do this because , while the time domain is relevant this problem the difference in the movements can be differentiated when presented with as a discrete matrix . i 'm used to using images with a neural net so i instinctively want to just convert the matrices into a 2d tensor and feed them into a cnn , but i was wondering if there was a better way to do this ? for example , i have seen 1d tensors passed to a fully connected neural network for classification which seems like it could be more appropriate for this data input type ? any tips on general architecture would be really appreciated as well ! thanks !",18577,18577,2018-09-28T19:54:52.090,2018-09-29T14:32:19.580,using 3d points as inputs to a neural net,convolutional-neural-networks classification,1,2,
2182,8161,1,,2018-09-28T04:29:35.277,1,67,i know implication ( — > ) is used for conditions like if x is true then b will be true but sometimes implication is used in other than these type of sentences for example : all a 's are bs : ∀x ( a(x ) ⇒ b(x ) ) i do n't understand why implication is used here ? and if implication is necessary to use here then why implication is not used in the example written below ? some a 's are b 's : ∃x ( a(x ) & amp ; b(x ) ),18582,1641,2018-09-28T12:08:28.910,2018-09-28T12:08:28.910,what is the correct way to use implication in first order logic ?,logic,1,1,
2183,8168,1,,2018-09-29T06:36:21.413,1,130,i recently learned about genetic algorithms and i solved the 8 queens problem using a genetic algorithm but i do n't know how to optimize any functions using a genetic algorithm . i want a guide on how to find chromosomes and fitness function for such a function ? and i do n't want code .,18604,3726,2018-10-01T13:48:49.427,2018-10-19T10:37:07.983,how to optimize a function using a genetic algorithm ?,genetic-algorithms optimization,1,1,
2184,8177,1,,2018-09-29T21:28:09.700,1,55,"i am starting a project to predict the generation of urban waste . i have found very little information on this topic on the internet . i would be very useful advice on how to approach this topic , and what techniques you would use . i have found academic articles that make predictions with feedforward neural networks . but they seem basic or old . i would expect the use of recurrent networks , but all the examples i find on the internet are about climate prediction , or economics . and they are very different topics . it would be very useful to make the prediction of waste generation based on an existing model , and not something invented by me . on what topic would you recommend that i take as a base ? i appreciate any advice . regards",18621,,,2018-10-01T16:37:37.763,predict waste generation,neural-networks recurrent-neural-networks prediction feedforward,1,1,1
2185,8181,1,8183,2018-09-30T14:20:21.857,3,127,"i am trying to design a neural network on python . instead of the sigmoid function which has a limited range , i am thinking of using the cube root function which has the following graph : is this suitable ?",18640,,,2018-09-30T16:08:06.890,is the cube root function suitable as a n activation function ?,neural-networks activation-function,1,0,1
2186,8184,1,,2018-09-30T16:15:45.267,0,43,how do support vector machines ( svms ) differentiate between a glass and a bottle or between a malignant and a benign tumor when it dealing with it for the first time ? what will be the analysis mechanism involved in this ?,18643,3726,2018-10-01T13:49:06.093,2018-10-31T15:00:54.777,how does an svm work ? how does it perform comparisons between malignant and benign tumor,image-recognition classification svm,1,2,
2187,8188,1,8191,2018-09-30T17:52:52.947,4,3431,"i made my first neural net in c++ without any libraries . it was a net to recognize numbers from the mnist dataset . in a 784 - 784 - 10 net with sigmoid function and 5 epochs with each 60000 samples , it took about 2 hours to train . it was probably slow anyways , because i trained it on a laptop and i used classes for neurons and layers . to be honest , i 've never used tensor flow , so i wanted to know how the performance of my net would be compared to the same in tensor flow . not to specific but just a rough aproximation .",17103,,,2018-10-01T09:51:46.657,how fast is tensorflow compared to self written neural nets ?,neural-networks tensorflow,2,0,2
2188,8190,1,9287,2018-09-30T18:55:45.547,4,802,"i was able to find the original paper on lstm , i was not able to find the paper that introduced "" vanilla "" rnns . where can i find it ?",18649,2444,2018-12-01T04:14:13.163,2018-12-01T04:14:13.163,where can i find the original paper that introduced rnns ?,recurrent-neural-networks lstm papers,1,0,0
2189,8193,1,,2018-09-30T21:25:40.257,1,76,"to me it seems to be ill defined . partially because of absence of knowledge which points are to be considered outliers in the first place . the problem which i have in mind is "" bad market data "" detection . for example if a financial data provider is good only most of the time , but about 7 - 10 % of data do not make any sense . the action space is binary : either take an observation or reject it . i am not sure about the reward , because the observations would be fed into an algorithm as inputs and the outputs of the algo would be outliers themselves . so the outliers detection should prevent outputs of the algorithm going rouge . it is necessary to add that if we are talking about the market data ( stocks , indices , fx ) , there 's no guarantee that the distributions are stationary and there might be trends and jumps . if a supervised classifier is trainer based on historical data , how and how often should it be adjusted to be able to cope with different modes of the data .",18653,18653,2018-10-01T14:50:02.347,2018-10-01T15:25:42.310,is it possible to state an outliers detection problem as a reinforcement learning problem ?,reinforcement-learning,1,3,
2190,8194,1,,2018-09-30T23:47:00.477,4,183,"in the circumstances of two perfect ai 's playing each other , will white have an inherent advantage ? or can black always play for a stalemate by countering every white strategy ?",18656,10135,2018-10-18T08:32:19.143,2018-10-18T08:32:19.143,"if two perfect chess ai 's played each other , would it always be a stalemate or would white win for an inherent first - move advantage ?",game-theory chess,4,0,1
2191,8196,1,,2018-10-01T05:30:04.247,0,90,i 'm currently working on tumor detection project using dicom images as i 'm beginner in it currently having a difficulty in segmenting each part in image and giving each segment a new colour .,18661,,,2018-10-01T06:53:25.823,how image segmentation actually works ?,image-recognition matlab,1,2,
2192,8203,1,8351,2018-10-01T13:12:52.117,5,208,"can anyone suggest reference books to start with ai ? preferably , i am looking for books that provide source code in java or python .",18677,2444,2019-05-03T15:13:43.133,2019-05-03T15:13:43.133,what are examples of reference books to start with ai ?,ai-basics python ai-community reference-request java,4,1,4
2193,8206,1,,2018-10-01T14:03:16.267,2,131,"i have programmed my first network for the mnist dataset . i was wondering what the first approach would be to recognize certain movements . i have read about that the time dimension should be considered for solving such problems , but that 's where i am stuck .",18681,1581,2018-10-01T19:40:59.467,2018-11-12T15:02:20.447,using convolutional neural networks for movement classification,neural-networks deep-learning convolutional-neural-networks image-recognition classification,4,0,
2194,8212,1,,2018-10-02T07:15:40.557,2,269,"i recently watched the video on proximal policy optimization ( ppo ) . now , i want to upgrade my actor - critic algorithm written in pytorch with ppo , but i'am not sure how the new parameters / thetas are calculated . in the paper proximal policy optimization algorithms ( at page 5 ) , the pseudocode of the ppo algorithm is shown : it says to run , compute advantage estimates and optimize the objective . but how can we calculate for the objective ratio , since we have not updated the yet ?",18696,2444,2019-02-16T02:38:55.663,2019-02-16T02:38:55.663,how do i calculate the policy in the proximal policy optimization algorithm ?,reinforcement-learning actor-critic proximal-policy-optimization,1,0,
2195,8215,1,,2018-10-02T08:07:45.853,0,263,"( cross - posting here from the data science stack exchange , as my question did n't get any replies . i hope it 's okay ! ) i 've been playing around with yolov3 and obtaining some good results on the ~20 custom classes i trained . however , one or two classes look like they can use some additional training data ( not a lot , say about 10 % more data ) , which i can provide . what is the most efficient way to train my model now ? do i need to start training from scratch ? can i just throw in my additional data ( with the appropriate changes to the config files etc . ) and run the training based on the weight matrix i already acquired , but for a small number of iterations ? ( 1000 ? ) or is this more like a transfer learning problem now ? thanks for all tips !",18699,,,2019-05-03T08:01:55.587,add training data to yolo post - training,deep-learning training object-recognition,1,0,
2196,8217,1,,2018-10-02T08:42:29.030,1,82,"i implemented a lstm neural network in pytorch . it worked but i want to know if it worked the way i guessed how it worked . say there 's a 2-layer lstm network with 10 units in each layer . the inputs are some sequence data xt1 , xt2 , xt3 , xt4 , xt5 . so when the inputs are entered into the network , xt1 will be thrown into the network first and be connected to every unit in the first layer . and it will generate 10 hidden states/10 memory cell values/10 outputs . then the 10 hidden states , 10 memory cell values and xt2 will be connected to the 10 units again , and generate another 10 hidden states/10 memory cell values/10 outputs and so on . after all 5 xt 's are entered into the network , the 10 outputs from xt5 from the first layer are then used as the inputs for the second layer . the other outputs from xt1 to xt4 are not used . and the the 10 outputs will be entered into the second layer one by one again . so the first from the 10 will be connected to every unit in the second layer and generate 10 hidden states/10 memory cell values/10 outputs . the 10 memory cell values/10 hidden states and the second value from the 10 will be connected and so forth ? after all these are done , only the final 10 outputs from the layer 2 will be used . is this how the lstm network works ? thanks .",18268,,,2018-10-02T08:42:29.030,structure of a multilayered lstm neural network ?,lstm,0,1,
2197,8219,1,8346,2018-10-02T11:15:13.087,1,315,"a more formal implication of this question is whether intelligence requires a context . on topic this question may have little value to the fields of data science or statistics , however it is of central importance to the field of artificial intelligence . the aim of the ai field has been and will continue to be the simulation of human intelligence and possibly develop types of intelligence for which the human brain is not well equipped . such does not require understanding data set training requirements or postmodern thought . it requires knowing , in a more mathematically formal way , what intelligence is . the proclamation , "" we know it when we see it , "" is not science and will not help develop the underdeveloped areas within the ai field . narrowness of inquiry when norbert wiener , alonso church , claude shannon , alan turing , marvin minsky , and others laid the foundations for artificial intelligence , they considered this question and others like it to be mathematical questions . although they may have approached these questions with thought experiments like turing 's immitation game , they also developed those ideas mathematically , before they tried to embody their ideas in computers . not all these questions have a definitive answer in the literature , and the further investigation to reach them is of paramount importance to the further development of the artificial intelligence field . the turing challenge to cartesian thought turing proposed at the end of the description of his famous test , "" will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman ? these questions replace our original , ' can machines think ? ' "" 1 turing effectively challenged the 1641 statement of rené descartes in his discourse on method and meditations on first philosophy : "" it never happens that [ an automaton ] arranges its speech in various ways , in order to reply appropriately to everything that may be said in its presence , as even the lowest type of man can do . "" descartes and turing , when discussing automatons achieving human abilities , shared a single context through which they perceived intelligence . those that have been either the actor or the administrator in actual turing tests understand the context : dialog . intelligence contexts other than dialog 2 the context of dialog is distinct from other contexts such as writing a text book , running a business , or raising children . if you apply the principle of comparing machine and human intelligence to automated vehicles ( whether jet airliners , cars , trucks , drones , or trains ) an entirely different context becomes immediately apparent . then the question becomes , does the distribution of fatalities , maiming , disfigurements , and property losses from automatic control match or do better than the same distributions of human control . we see not only the difference in context , but two other differences . the statistical comparison proposed by turing is a single dimension . either the computer is as indistinguishable from the human as the man is from the woman or not . in piloting or driving scenarios , the question of how to compare a maiming to a disfigurement arises . as in law and government , how much property loss is equal to a loss of one human life becomes part of the criteria . validation of the automaton by inequality , where the control of the vehicle being distinguishably better than human control is still success . this in contrast to validation through rough equality , where the automaton 's ability to keep up in a dialog with a human is renders it effectively indistinguishable from another human . ( a dialog where the computer is too smart would make it distinguishable , thwarting the spoof . ) range of contextualization we have two questions at the extremes in set theory . q1 . what does intelligence then mean with no context ? q2 . what does intelligence then mean with all possible contexts ? these questions seem easy if taken one at a time . a1 . that 's the same as any context . a2 . that 's what we 've been calling general intelligence . but are those two mathematically equal ? can we project that , if an automaton performs as well as or outperforms humans in a hundred contexts , it can surely do so with ten more contexts ? furthermore , do we select a low functioning , average functioning , or best functioning human for comparison ? what about contexts we do n't have on earth yet but will have as time progresses ? returning to embodiment and definition can a baby artificial mind grow into an adult by churning internet data , or does it need to be placed in the context of a robotic entity so it can move around and experience interaction with the physical world ? can a brain be intelligent without a body ? more generally and more formally ... does intelligence require a context ? references [ 1 ] chapter 1 ( "" imitation game "" ) of computing machinery and intelligence , 1951 . [ 2 ] multiple intelligences theory",4302,4302,2018-11-02T14:49:40.877,2018-11-02T14:49:40.877,can a brain be intelligent without a body ?,terminology concepts turing-test robotics intelligence,4,3,1
2198,8220,1,8266,2018-10-02T11:54:03.637,3,356,"i will be undertaking a project over the next year to create a self learning ai to play a racing game , currently the game will be mario kart 64 . i have a few questions which will hopefully help me get started : what aspects of ai would be most applicable to creating a self learning game ai for a racing game ( q - learning , neat etc ) could a ann or neat that has learned to play mario kart 64 be used to learn to play another racing game ? what books / material should i read up on to undertake this project ? what other considerations should i take throughout this project ? thank you for your help !",18209,,,2018-10-05T08:54:33.797,creating a self learning mario kart game ai ?,neural-networks game-ai python self-driving neat,2,3,1
2199,8221,1,,2018-10-02T12:43:52.323,1,22,"i was going through the wikipedia page on ontology components and noticed something that i had been hoping to find , for a long time . in components ' overview it mentioned : function terms : complex structures formed from certain relations that can be used in place of an individual term in a statement i tried digging deeper into function terms but could n't find any other references . is this a standard component of an ontology ? references would be nice . ultimately , i 'm looking for something like this in the dbpedia ontology . would have been nice if i could add ontology and dbpedia as tags to the question , but i do n't have the requisite reputation .",4707,,,2018-10-02T12:43:52.323,about function terms in an ontology,knowledge-representation world-knowledge,0,0,
2200,8223,1,8234,2018-10-02T14:18:40.637,4,69,"i 've been told this is how i should be preprocessing audio samples , but what information does this method actually give me ? what are the alternatives and why should n't i use them .",18709,,,2018-10-03T05:39:22.023,why is short - time fourier transform used for preprocessing audio samples ?,voice-recognition,1,3,
2201,8227,1,8229,2018-10-02T17:58:58.710,2,3261,"so guys , i 've been seeing a lot of tutorials on the internet about ai that are mostly done with python . apart from these , i 've seen c # being used in ai topics but in things like for example "" self - driving cars "" , i 've seen python and not c # or any other languages . i wanted to ask , do you recommend that i learn python ? because i know c # and i wanted to become more professional in it , but , now that i see that python is being used a lot , i 'm getting intrigued in it . do you recommend python or other languages or should i keep up with c # ? just to mention , i 'm 14 years old and i have enough time to learn more and it does n't really matter what i love to do , because , i love coding and ai specially , so , it does n't really matter . if it 's not a waste of time , i should get started , right ? if you recommend python , please tell me which compiler i should use . i do n't really know if it has a compiler , but i want to know where i should start from . thanks .",18595,,,2019-02-05T06:52:46.887,c # or python for ai ?,getting-started programming-languages,4,8,
2202,8238,1,,2018-10-03T09:44:28.433,1,115,"gist : should i use lisp for a part of the following project . what are the other options . me and a friend are planning to create a 3d modelling agent where a designer can : - specify constrains on how the model can be ( eg : model can not be more than 8 inches wide , model has to have 2 holes on top , etc ) . provide 2d images for "" inspiring "" the agent to prefer certain forms . from what i can fathom , the first part seems to pertain to symbolic ai ( deterministic constraints satisfaction ) and the second part pertains to fuzzy ai ( some form of 3dgan ? ? ? ) . i have studied fol and planning systems , although i have never implemented one . i am learning ml and nn on the side . we plan on using the unity game engine since it has a lot of tools related to 3d environments built in , like vertex modelling , a rendering pipeline and even ml agents . coming to my question , should i use lisp for the symbolic ai part ? what about prolog ? or should i prefer to use user - developed fol and csp libraries for c # ( since unity game engine uses c # ) ?",18726,,,2018-10-03T09:44:28.433,stack for automatic 3d mesh generation,neural-networks generative-model symbolic-computing,0,0,
2203,8239,1,,2018-10-03T11:06:25.020,1,14,"i want to build a model to support decision making in order to propose or not loan insurance to clients . because sometimes clients asking loan and loan insurance have less chance to have their loan accepted by a bank and sometimes more chances . there are three actors in the problem : a bank , a loaner applicant ( someone who ask for a loan ) and a counselor . the counselor studies the loaner application and if it has a good profile it will propose to him loan from banks that fits his profile . then the application is sent to the bank but the bank could refuse the applicant ( based on criteria we do n't know ) . the counselor has also to decide whether or not he will propose to the loaner applicant a loan insurance . i want to build a model for that decision . the risk is that some banks reject loan applicant who accepts a loan insurance and other banks accept more applicants with a loan insurance . but there are n't rules regarding banks since some banks accept or reject applicants with loan insurance according of the profile of the applicants and the type of acquisition . thus , the profile of the applicant and the bank he is applying to can matter in their rejection from banks but all criteria influencing the decision are quite uncertain . even though it is a classification problem , in my dataset i do n't have a good label for loan insurance proposal . i have a features that says if the insurance was proposed or not and to less than 1 % the insurance was proposed . i have a label that says if the clients application for a loan was accepted or not . thus , the data i have is former applicants profile - and banks who propose loan according to what the applicant wants - and if they were accepted or not by the bank and if they wanted a loan insurance or not . i thought of combining the label with the information but i do n't really know how or maybe doing multi - label classification but also i do n't really know if it does fit to the problem .",18192,,,2018-10-03T11:06:25.020,merge one label with one information for classification problem or multi - label classification,classification decision-theory,0,0,
2204,8240,1,8247,2018-10-03T11:45:23.277,1,50,"i am trying to dissect paper about weight normalization : https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf unfortunately , because my math is little bit rusty , i got little bit stuck with the proof ... could you provide me with some clarification about proof of the topic ? what i understand is that we introduce , instead of weight vector w scalar g ( magnitude of original w ? ) and v/||v|| ( direction of original w ? ) of the vector . what i am not really sure about is : if gradients are noisy ( does this means that in some dimension we have small and in some high curvature or that error noise differs for very similar values of w ? ) the value will quickly increase here and effectively limit the speed of descent by decreasing value of ( g/||v|| ) . this means that we can choose larger learning rates and it will somehow adjust effect of the learning rate during the training . and what i completely miss is : it should somehow explain the reasoning behind the idea about final effects , unfortunately , i do n't really understand this chapter in the paper , i probably lack some knowledge about linear algebra . can you verify that my understanding of the paper is correct ? can you recommend some sources ( books / videos ) to help me to understand second part of proof ( related to second set of formulas ? ) thank you , tom",18729,18729,2018-10-03T14:44:14.897,2018-10-03T18:03:24.430,weight normalization paper,neural-networks deep-learning gradient-descent math linear-algebra,1,0,
2205,8241,1,8242,2018-10-03T15:27:01.740,2,27,"imagine a system that is trained to manipulate dampers to manage air flow . the training data includes damper state and flow characteristics through a complex system of ducts . the system is then given an objective ( e.g. maintain even flow to all outputs ) and set loose to manage the dampers . as it performs those functions there are anomalies in the results which the system is able to detect . the algorithm continues to learn from its own empirical data , the result of implemented damper configurations , and refines its algorithm to improve performance seeking the optimum goal of perfectly even flow at all outputs . what is that kind of learning or ai system called ?",17061,,,2018-10-03T16:37:50.783,"what is the ai discipline where an algorithm learns from an initial training set , but then refines its learning as it uses that training ?",training,1,1,
2206,8243,1,8252,2018-10-03T16:44:19.617,3,276,"imagine a system that controls dampers in a complex vent system that has an objective to perfectly equalize the output from each vent . the system has sensors for damper position , flow at various locations and at each vent . the system is initially implemented using a rather small data set or even a formulaic algorithm to control the dampers . what if that algorithm were programmed to "" try "" different configurations of dampers to optimize the air flows , guided broadly by either the initial ( weak ) training or the formula ? the system would try different configurations and learn what improved results , and what worsened results , in an effort to reduce error ( differential outflow ) . what is that kind of ai system called ? what is that system of learning called ? are there systems that do that currently ?",17061,1671,2018-10-03T19:53:50.893,2018-10-03T23:52:03.940,what is the name of an ai system that learns by trial and error ?,machine-learning training ai-basics terminology,3,4,1
2207,8250,1,8256,2018-10-03T22:29:20.213,2,54,"i 've been reading about expert systems and noticed started reading about mycin . i was astonished to find that mycin diagnosed patients better than the infectious diseases physicians . http://www.aaaipress.org/classic/buchanan/buchanan33.pdf since , it had such a good success rate , why did it fail ?",18748,,,2018-10-04T08:51:16.990,why did mycin fail ?,expert-system,1,0,
2208,8251,1,,2018-10-03T23:11:36.607,1,81,"i have created a game on an 8x8 grid and there are 4 pieces which can move essentially like checkers pieces ( forward left or forward right only ) . i have implemented a dqn in order to pull this off . here is how i have mapped my moves : self.actions = { "" 1fl "" : 0 , "" 1fr "" : 1,""2fl "" : 2 , "" 2fr "" : 3,""3fl "" : 4 , "" 3fr "" : 5,""4fl "" : 6 , "" 4fr "" : 7 } essentially i assigned each move to an integer value from 0 - 7 ( 8 total moves ) . my question is : during any given turn , not all 8 moves are valid , how do i make sure when model.predict(state ) the resulting prediction will be a move that is valid ? here is how i am currently handling it . def act(self , state , env ) : # get the allowed list of actions actions_allowed = env.allowed_actions_for_agent ( ) # do a random move if random # greater than epsilon if np.random.rand(0,1 ) & lt;= self.epsilon : return actions_allowed[random.randint(0 , len(actions_allowed)-1 ) ] # get the prediction from the model by passing the current game board act_values = self.model.predict(state ) # check to see if prediction is in list of valid moves , if so return it if np.argmax(act_values[0 ] ) in actions_allowed : return np.argmax(act_values[0 ] ) # if prediction is not valid do a random move instead .... else : if len(actions_allowed ) & gt ; 0 : return actions_allowed[random.randint(0,len(actions_allowed)-1 ) ] i feel like if the agent predicts a move , and if that move is not in the actions_allowed set i should punish the agent . but because it does n't pick a valid move i make it do a random one instead , but i think this a problem . because its bad prediction may ultimately end up still winning the game since the random move may have a positive outcome . i am at a total loss . the agent trains .... but it does n't seem to learn .... i have been training it for over 100k games now , and it only seems to win 10 % of it games .... ugh . other helpful information : - i am utilizing experience replay for the dqn which i have based on the code from here : here is where i build my model as well : self.action_size = 8 length = 8 def build_model(self ) : # builds the nn for deep - q model model = sequential ( ) # establishes a feed forward nn model.add(dense(64,input_shape = ( length , ) , activation='relu ' ) ) model.add(dense(64 , activation='relu ' ) ) model.add(dense(self.action_size , activation = ' linear ' ) ) model.compile(loss='mse ' , optimizer='adam ' )",18244,16565,2018-10-04T12:31:15.300,2018-10-04T12:31:15.300,using a dqn with a variable amount of valid moves per turn for a board game,reinforcement-learning game-ai python keras dqn,1,1,1
2209,8254,1,,2018-10-04T06:25:15.123,1,73,"as i 'm beginner in image processing , i am having difficulty in segmenting all the parts in dicom image . currently , i 'm applying watershed algorithm , but it segments only that part that has tumour . i have to segment all parts in the image . which algorithm will be helpful to perform this task ? the image below contains the tumour . this image is the actual dicom image",18661,2444,2019-05-02T13:33:54.870,2019-06-01T14:02:06.037,how do i segment each part of a dicom image ?,computer-vision matlab image-segmentation,1,0,
2210,8258,1,8544,2018-10-04T14:41:25.100,4,2219,"i 'm looking for annotated dataset of traffic signs . i was able to find belgium , german and many more traffic signs datasets . the only problem is these datasets contain only cropped images , like this : while i need ( for yolo -- you only look once network architecture ) not - cropped images . i 've been looking for hours but did n't find dataset like this . does anybody know about this kind of annotated dataset ? edit : i prefer european datasets .",18760,18760,2018-10-05T08:22:10.423,2018-10-21T14:19:19.097,traffic signs dataset,neural-networks convolutional-neural-networks datasets,3,0,2
2211,8259,1,8287,2018-10-04T17:54:45.610,0,55,"is theta supposed to be updated in a perceptron , like the weights , and if so , what is the formula for this ? i 'm trying to make the perceptron learn and and or , but without updating theta , i do n't feel like it 's possible to learn the case where both inputs are 0 . they will of course be independent of the weights , and therefore the output will be step(-theta ) , meaning theta ( which has a random value ) alone will determine the output . thanks .",17488,,,2018-10-06T23:11:16.973,update theta in a perceptron ?,neural-networks perceptron,1,2,1
2212,8261,1,8285,2018-10-04T18:07:47.730,3,497,"as an electronics & amp ; communication engineering student i 've heard some stories and theories about "" the math we have is not enough to complete a thinker - learner ai . "" what is the truth ? is humankind waiting for another newton to make new calculus or another einstein - hawking to complete the quantum mechanics ? if so , what exactly do we need ? what will we call it ?",18764,3217,2018-10-06T23:01:33.980,2018-12-30T20:51:45.460,is known math really enough for ai,machine-learning agi math,2,2,2
2213,8267,1,,2018-10-05T13:09:59.060,2,32,"following up on my previous questions using a hypothetical ai system to manage air flow using dampers to achieve an optimal target of exactly equal airflow at a number of vents ; ( thank you for all the responses to that question ) i was directed to "" reinforcement learning . "" in reinforcement learning , the system sets some controllable variables and then determines the quality of the result of the dependent variable(s ) ; using that "" quality "" to update the algorithm . in simple games this works fine because for each setting there is a single result . but , for the real world ( e.g. air flow system ) the result takes some time to develop and there is no single precise "" pair "" result to the conditions set . the flow change takes time and even oscillates a bit as flow stabilizes to a steady state . in practical systems how is this "" lag "" accounted for ? how are the un - settled ( false ) results ignored ? how is this noise distinguished from exogenous factors ( un - controlled system inputs e.g. an open window exposed to wind ) ?",17061,,,2018-10-05T16:45:01.470,dealing with lags in reinforcement learning,machine-learning reinforcement-learning,1,0,
2214,8269,1,,2018-10-05T15:25:10.763,1,20,"i 'm a student of artificial intelligence . this semester i 've computer vision course . we should select a topic related to intelligent vehicles and read several papers and implement one of those paper . i searched and i saw different topics related to intelligent vehicles like detection , tracking , semantic segmentation , ... but i do n't have enough knowledge to select some papers that have these properties : - published recently [ 2015 - 2018]. - kinda related to intelligent vehicles like autonomous cars and drones . - not hard to implement . i think papers that use very deep learning or are needed to run simulations are hard for implementing . after hours of searching , i got confused . so i decided to get help from you .",10051,,,2018-10-05T15:25:10.763,good papers for implementing as project of computer vision course,computer-vision autonomous-vehicles,0,0,
2215,8270,1,,2018-10-05T16:47:38.873,1,221,"i 'd like to implement a partially connected neural network with ~3 to 4 hidden layers ( a sparse deep neural network ? ) where i can specify which node connects to which node from the previous / next layer . so i want the architecture to be highly specified / customized from the get - go and i want the neural network to optimize the weights of the specified connections , while keeping everything else 0 during the forward pass and the backpropagation ( connection does not ever exist ) . i am a complete beginner in neural networks . i have been recently working with tensorflow & amp ; keras to construct fully connected deep networks . is there anything in tensorflow ( or something else ) that i should look into that might allow me to do this ? i think with tf , i should be able to specify the computational graph such that only certain connections exist but i really have no idea yet where to start from to do this ... i came across papers / posts on network pruning , but it does n't seem really relevant to me . i do n't want to go back and prune my network to make it less over - parameterized or eliminate insignificant connections . i want the connections to be specified and the network to be relatively sparse from the initialisation and stay that way during the back - propagation .",18788,16920,2019-04-09T10:07:12.413,2019-05-29T13:13:32.600,how to create partially connected nns with prespecified connections using tensorflow ?,neural-networks machine-learning deep-learning deep-network feedforward,3,0,
2216,8273,1,,2018-10-05T19:59:52.837,1,154,we are currently working on developing a 3d modeling software that allows designers to set spatial constraints to models . the computer then should generate a 3d mesh conforming to these constraints . why should or should n't we use lisp for the constraint satisfaction part ? will prolog environment be any better ? or should we stick to c / c++ libraries ? one requirement we have is that we want to use the unity game engine as it has a lot of 3d tools built in,18726,,,2019-05-04T00:03:05.447,what are the advantages and disadvantages of using lisp for constraint satisfaction in 3d space,optimization programming-languages lisp prolog,1,1,
2217,8274,1,,2018-10-05T21:52:46.623,2,67,i implemented an image segmentation pipeline and i trained it on the dicom dataset . i compared the results of the model with manual segmentation to find the accuracy . is there other methods for evaluation ?,18661,2444,2019-05-04T15:53:28.127,2019-05-04T15:53:28.127,which evaluation methods can i use for image segmentation ?,computer-vision datasets image-segmentation,1,0,
2218,8279,1,,2018-10-06T08:46:15.770,1,94,"i was reading a machine learning book that uses probabilities like these : $ p(x;y ) , p(x;y , z ) , p(x , y;z)$ i could n't find what they are and how can i read and understand them ? apart from the context , i saw one of these probabilities on it here :",9941,16565,2018-11-07T15:57:29.540,2019-05-06T17:04:40.953,"what are the meanings of these ( p(x;y ) , p(x;y , z),p(x , y;z ) ) ?",machine-learning probabilistic,1,2,
2219,8281,1,,2018-10-06T11:49:18.957,2,192,"with reference to the research paper entitled sentiment embeddings with applications to sentiment analysis , i am trying to implement its sentiment ranking model in python for which i am required to optimize the following hinge loss function : unlike the usual mean square error , i can not find its gradient to perform backpropagation . please help with a solution . thanks .",18804,,,2019-01-17T02:02:10.430,gradient of hinge loss function,neural-networks sentiment-analysis loss-functions,1,1,
2220,8283,1,8286,2018-10-06T15:40:41.847,0,38,"example : texas holdem poker vs texas holdem poker with the same rounds , just with no public cards dealt . would algorithms like cfr approximate nash - equilibrium more easily ? could ai that does not look at public cards achieve similar performance in normal texas holdem as ai that looks at public state tree ?",18808,18808,2018-10-08T14:56:26.247,2018-10-08T14:56:26.247,what 's the difference between poker with public cards and without them ?,ai-design game-ai ai-community poker,1,0,
2221,8284,1,8401,2018-10-06T18:29:31.960,6,210,"i have a neural network with the following structure : i am expecting specific outputs from the neural network which are the target values for my training . let 's say the target values are 0.8 for the upper output node and -0.3 for the lower output node . the activations function used for the first 2 layers are relu or leakyrelu while the last layer uses atan as an activation function . for back propogation , instead of adjusting values to make the network 's output approach 0.8,-0.3 . is it suitable if i use the inverse function for atan -which is tan itself- to get "" the ideal input to the output layer multiplied by weights and adjusted by biases "" . the tan of 0.8 and -0.3 is 0.01396 and -0.00524 approximately . my algorithm would then adjust weights and biases of the network so that the "" pre - activated output "" of the output layer -which is basically ( sum(output_layer_weight*output_layer 's inputs)+output_layer_biases)- approaches 0.01396 and -0.00524 . is this suitable",18640,,,2018-10-14T05:10:07.937,is it suitable to find inverse of last layer 's activation function and apply it on the target output ?,neural-networks activation-function,4,6,
2222,8290,1,,2018-10-07T02:03:59.597,0,34,"the subject matter is to count the number of people in a large room , wherein a camera is placed in a very high ceiling : an example would be grand central station . faces are not visible : the scalp ( top of the head ) is visible to the camera as shown in the link 's video . the goal : i would like to perform a google literature search to assess the work that has been performed on overhead head recognition , however , i am not sure what the best keyword pairs : ( scalp ? head ? people ? ) to describe the object that is to be recognized from a camera positioned in the ceiling ( overhead ? bird 's eye ? satellite ? ) . i 'd like the search to return leading - edge ( ai ) techniques that benchmark results",18819,18819,2018-10-08T20:56:26.840,2018-10-08T20:56:26.840,keywords to describe people counting from a camera ?,object-recognition,1,2,
2223,8291,1,8343,2018-10-07T02:15:21.373,1,33,"i have written my own basic convolutional neural network in java as a learning exercise . i am using it to analyze the mit cbcl face database image set . they are a set of 19x19 pixel greyscale images . network specifications are : single convolution layer with 1 filter : filter size : 4x4 . stride size : 1 single pooling layer 2x2 max pooling 3 layer mlp(input , 1 hidden and output ) input = 64 neurons hidden = 15 neurons output = 2 neurons learning rate = 0.1 now i am getting reasonable accuracy(92.85 % ) , but my issue is that it is being achieved at very different points in the epoch count across network runs : epochs training accuracy test accuracy validation accuracy run 1 415 93.13 92.44 93.35 run 2 515 92.44 93.18 92.84 run 3 327 93.83 92.05 92.38 i am using the java random class with the same seed for every run to initialize the kernel , the mlp weights and break the input data into 3 sets.(training is being done using the 33 - 33 - 33 method ) i am a loss as to what is causing this variation in epoch count to achieve the highest point in validation accuracy . can anybody explain this ?",18818,10135,2018-10-18T10:44:36.693,2018-10-18T10:44:36.693,huge variations in epoch count for highest generalized accuracy in cnn,convolutional-neural-networks java,1,0,
2224,8293,1,8295,2018-10-07T09:55:50.223,6,822,"i 've been looking at reinforcement learning , and specifically playing around with creating my own environments to use with the openai gym ai . i am using agents from the stable_baselines project to test with it . one thing i 've noticed in virtually all rl examples is that there never seems to be any dropout layers in any of the networks . why is this ? i have created an environment that simulates currency prices and a simple agent , using dqn , that attempts to learn when to buy and sell . training it over almost a million timesteps taken from a specific set of data consisting of one month 's worth of 5-minute price data it seems to overfit a lot . if i then evaluate the agents and model against a different month 's worth of data is performs abysmally . so sounds like classic overfitting . but is there a reason why you do n't see dropout layers in rl networks ? is there other mechanisms to try and deal with overfitting ? or in many rl examples does it not matter ? e.g. there may only be one true way to the ultimate high score in the ' breakout ' game , so you might as well learn that exactly , and no need to generalise ? or is it deemed that the chaotic nature of the environment itself should provide enough different combinations of outcomes that you do n't need to have dropout layers ?",18372,,,2018-10-09T17:32:58.447,why do you not see dropout layers on reinforcement learning examples ?,machine-learning reinforcement-learning overfitting dropout,1,3,2
2225,8296,1,,2018-10-07T14:48:28.470,4,68,"naturally everyone knows most common advantages of autonomous ( not autopilot - controlled ) cars , e.g. safety , efficiency , parking , etc . but are there any advantages for the law ? normally you only hear about problems like having to change a lot of the legal aspects . i would have thought that there was no hit and run , drunk driving and generally no deliberately caused car accidents . i would be very interested if someone would come up with further advantages . i hope this is the right page for a question like that .",18839,,,2019-05-07T02:02:50.563,legal advantages of autonomous vehicles,self-driving legal autonomous-vehicles,1,1,
2226,8297,1,,2018-10-07T17:37:00.000,1,32,"i 'm programming on connect6 with mcts . monte carlo tree search is based on random moves . it counts up the number of wins in certain moves . ( whether it wins in 3 turns or 30 turns ) is the move with less turns more powerful than the move with more turns?(as mcts just sees if it 's win or not -- not considering the number of turns it took to win ) and if so , is it meaningful to give bigger weight to the one with less turn win ?",18844,1641,2018-10-08T19:35:36.270,2018-11-07T20:00:03.040,is it meaningful to give more weight to the result of monte carlo search with less turn win ?,game-ai monte-carlo-tree-search,1,0,
2227,8298,1,,2018-10-07T17:42:25.997,1,704,"i have a saved keras model . how can i get back the labels from the model ? because right now , i can use the predict method to get back the probability for a sample to belong to a certain class e.g. class 1 , 2 , 3 . but how can i know what class 1 , 2 , 3 correspond to in the model e.g. cat , dog , bird ?",18845,,,2018-10-11T12:57:35.170,keras : get back labels from a model,classification keras,2,2,
2228,8303,1,8308,2018-10-07T23:58:40.180,1,52,"i want to implement a neural network on a big dataset . but training time is long ( ~1h30 per epoch ) . i 'm still in the development process , so i do n't want to wait such long time just to have poor results at the end . this and this suggest that overfitting the network on a very small dataset ( 1 ~ 20 samples ) and reach a loss near 0 is a good start . i did it and it works great . however , i am looking for the next step of validating my architecture . i tried to overfit my network over 100 samples , but i ca n't reach a loss near 0 in reasonable time . how can i ensure the results given by my nn will be good ( or not ) , without having to train it on the whole dataset ?",18852,,,2018-10-08T14:43:59.020,how to detect a neural network will work with the whole dataset ?,neural-networks machine-learning deep-learning,3,1,
2229,8309,1,8587,2018-10-08T15:15:23.527,5,90,"i 'm learning about multilayer perceptrons , and i have a quick theory question in regards to hidden layer neurons . i know we can use two hidden layers to solve a non - linearly seperable problem by allowing for a representation with two linear seperators . however , we can solve a non - linearly seperable problem using only one hidden layer . this seems fine , but what kind of representation does one hidden layer add ? my question is how is the dimensionality of the output affected ? i 've drawn a diagram of a multilayer perceptron with one hidden layer neuron . i used this same layout to solve a non - linearly seperable problem . the single hidden layer node is inside the red square . forgive my poor ms - paint skills .",18870,18870,2018-10-08T15:23:04.950,2018-10-23T10:54:16.193,how does a single hidden layer neuron affect output ?,hidden-layers perceptron,1,2,1
2230,8310,1,,2018-10-08T15:55:59.107,1,458,"i 'm trying to apply the world models architecture to the sonic game ( using the gym - retro library ) . my problem concerns the evolutionnary algorithm part that i use as the controller ( worldmodels = auto encoder + rnn + controller ) . i 'm using a genetic algorithm called neat ( i use the neat - python library ) . i am searching for someone who can help me with the neat - python implementation . here is the method that runs a generation : python best_genome = pop.run(popevaluator.evaluate_genomes , 1 ) currently , all the individuals of the population are evaluated on the first level of sonic the hedgehog . the "" run "" method should return the best genome of the population based on their performance on this level . then , i use this best genome to re - create the associated neural network in order to run it in the same level . i was expected to see the exact same run as the best individual , but this is not the case . sometimes it does , sometimes not . there are not a lot of examples with neat and i based my code on this one from the official documentation . here is my own implementation , if you want to check . if anybody has already used neat , help would be welcome !",18872,,,2018-10-08T15:55:59.107,neat + keras : reproducibility problem ( world models implementation ),neural-networks genetic-algorithms keras evolutionary-algorithms neat,0,0,1
2231,8320,1,,2018-10-09T02:19:31.433,3,305,can the recurrent neural network input come from short time fourier transform in matlab ? i mean the input is not from time series domain .,18884,17221,2018-10-09T10:12:53.510,2018-10-09T10:35:17.490,fourier transform inputs ( frequency ) for rnn,recurrent-neural-networks,1,0,1
2232,8323,1,,2018-10-09T12:43:29.380,1,320,"almost all the neural network architecture i have come across have a square input size of an image . like 32x32,64x64,128x128 , ....... ideally we might not have a square image for all kind of scenarios . example : 384x256 my question is how to we handle such images during training development test of a neural network ? do we force the image to resize to the input of the neural network or just crop the image to the required input size ? ps : have asked the same on coursera",17763,17763,2018-10-09T13:08:37.123,2018-10-14T17:35:47.283,how to handle rectangle images in neural network ?,neural-networks convolutional-neural-networks,2,4,
2233,8326,1,8342,2018-10-09T14:59:58.643,1,71,"i am using tensorflow cnn to build an image classification / prediction model . currently all the images in the dataset are each about 1 mb in size . most examples out there use very small images . the image size seems large , but i not too sure . any thoughts on the feasibility of 1 mb images ? if not what can i do to compress programmatically ?",18896,,,2018-11-09T11:01:00.173,is 1 mb an acceptable memory size for images being trained in a cnn ?,convolutional-neural-networks image-recognition training tensorflow datasets,1,6,
2234,8327,1,,2018-10-09T15:20:53.440,0,38,"according to the original paper on page 4 , 224x224x3 image is reduced to 112x112x64 using a filter 7x7 and stride 2 after convolution . n x n = 224x224 f x f = 7x7 stride = s = 2 padding = p = 0 output of convolution is ( ( ( n+2p - f)/s)+1 ) according to which ( n+2p - f ) = ( 224 + 0 - 7 ) = 217 , divide by stride = 217/2=108.5 ( taking the lower value ) , adding 118 + 1=119 . how do we get a output image of 112 now ?",17763,,,2018-11-08T17:00:58.083,inception neural network input layer confusion,neural-networks convolutional-neural-networks,1,0,
2235,8328,1,,2018-10-09T15:58:33.040,1,66,"i have a project , which is the keyboard biometrics of users . suppose i have 3 users , i do not know how to label in two types of class , ( + 1 , -1 ) . if i want to verify the identity to user1 , my idea of ​​class designation would be : times label user 1 9.4 9.2 1.0 3.4 0.5 1 9.4 9.2 1.0 3.4 0.5 1 9.4 9.2 1.0 3.4 0.5 1 9.4 9.2 1.0 3.4 0.5 1 9.4 9.2 1.0 3.4 0.5 1 user 2 0.1 3.2 1.0 1.2 1.7 -1 3.4 1.2 3.0 1.1 2.8 -1 2.4 2.2 3.0 1.6 2.9 -1 1.4 3.2 2.0 2.6 3.6 -1 3.4 0.2 3.0 2.7 3.5 -1 user n 0.2 1.4 4.5 3.7 2.9 -1 9.2 1.5 7.6 2.6 2.6 -1 9.3 1.6 7.5 2.9 3.4 -1 9.8 3.8 6.6 2.8 2.5 -1 9.8 2.8 1.7 3.8 1.6 -1 but as my system has more and more users classes -1 will be too many compared to classes +1 , how should i label the classes ?",18464,18464,2018-10-09T16:04:55.987,2018-10-11T03:20:23.150,how should i label the classes in rna ?,neural-networks,1,2,
2236,8330,1,8332,2018-10-09T16:24:19.857,1,34,"as stated in the title , i 'm wondering if it would be possible to "" outperform "" the master in the apprenticeship learning . i 'm aware that the question might be not clear enough ; but hopefully , someone might have done something on it before . more precisely , i 'm actually asking a clear way to define the target of learning : if the master 's behavior is defined as standards , then of course the answer should be no . however , consider a real example : in the case of autonomous vehicles , if we design the algorithm to mimic a human master , then would it be possible for the algorithm to outperform the master ( consider ideal case , and neglect physical condition , e.g. tired , un - focused ... ) , especially in new situations , if we well - define a new reasonable standard ?",18900,,,2018-10-09T16:47:52.523,"in apprenticeship learning , is it possible to outperform the master ?",machine-learning reinforcement-learning,1,0,
2237,8331,1,,2018-10-09T16:32:10.183,1,52,"i 've a grid of rectangles acting as blocks . the robot traverses through the inter - spaces between these consecutive blocks . now i have sensor data streaming in representing right and left wheel speeds . based on the differences in the speeds of left and right wheels , i infer the robot 's position and path it has threaded . i get the associated individual segments of the total distance when it travels straight , left or right . these distances are a function of the actual speed of the robot and the time interval elapsed before the end of that activity . these computed distances for the segments though do n't map and fit - in well when projected on the grid layout of the environment . the segments are rather not adhering to the boundary limitations . i wanted to know if i can use rl to force the calculated distances to fit in with the layout given certain knowledge ( or conditions , if you will):the start and end position of the robot and the inter - space distances . if not rl , do you know how can i solve this problem . i suspect my function computing the distances is off and wondering if rl can help me figure out the right mapping of sensor data to the path traveled adhering to the grid layout dimensions . if you consider the illustration above you will notice s , d and d ' signifying the starting position , the true destination , and the destination location computed by adding together the calculated distances for each of the segments representing right(r ) , left(l ) and straight(s ) along the path towards the destination . inter - space length is given 7 m and dimensions of the blocks are ( 27 m x 15 m ) . if you look at the data presented on the left side you will notice 18 m left and consecutive 24 m right represents the activity , in the grid , as the passage through the blocks . granted -- perhaps the car negotiates the edges and corners through this passage in a protracted left(l ) and right(r ) movements , without necessarily going straight(s ) straddling and linking the turns as one would expect . the question arises , however , when taken into account these individual segment lengths and stitch them together you end up in a destination not in the ballpark range of the expected value . how can we design this problem so as to employ rl methods to , sort of , impose these grid dimensional constraints on this distance calculation methodology to yield better results . or , probably best to re - imagine the whole problem so it is amenable to the application of rl . any advice/ insights would be appreciated .",16551,16551,2018-10-09T19:39:33.480,2018-10-09T19:39:33.480,reinforcement learning for segmenting the robot path to reflect the true distances,machine-learning reinforcement-learning robotics,0,5,
2238,8333,1,,2018-10-09T18:47:38.883,1,104,"i ( mis?)understood the neat algorithm has the following steps : create a genome pool with n random genomes calculate each genome fitness assign each genome to a species calculate the adjusted fitness and the number of offspring of each species breed each species through mutation / crossover from the stronger genomes go to step 2 . step 3 is tricky : speciation is made placing each genome g in the first species in which it is compatible with the representative genome of that species , or in a new species if g is not compatible with any existing species . compatible is meant as having compabilitity distance below a certain threshold . regarding representative genome neat paper says : each existing species is represented by a random genome inside the species from the previous generation somewhere i 've found that keeping the number of species stable is good , and this is achieved automatically with dynamic thresholding . however , dynamic thresholding makes hard to evaluate species behaviour across generations . let me give one example : assume that in generation 20 , species 1 has genome a as representative and species 2 has genome b as representative . assume elitism is implemented . as the representative genome is taken from previous generation , assume that in generation 21 , genome a and b are still representatives for species 1 and 2 , however assume compatibility threshold has changed ( i.e. bigger ) in order to reach the target species number . with this change , a and b have now a compatibility distance lower than threshold and should be placed in the same species , however they are representatives of different species . how to solve this issue ? more in general , with dynamic thresholding , how to make sure species management across generations is consistent ? e.g. neat paper also says : if the maximum fitness of a species did not improve in 15 generations , the networks in the stagnant species were not allowed to reproduce . how to make sure that across all 15 generations , we are still considering that same single species and this has not drastically changed ( so that they are actually different ' objects ' ? ) . e.g. in the example above , if a and b are both placed in species 1 in generation 21 , species 2 no longer represents what it represented in generation 20 .",13087,,,2018-10-09T20:19:01.987,neat - managing species across generations,neat,0,1,1
2239,8335,1,,2018-10-09T23:56:19.440,1,20,"when a human looks at a page . he notices the sets of letters are grouped together separated by white space . if the white space was replaced by another character say z , it would be harder to distinguish words . for a neural network , spaces are "" just another character "" . how can we set up an rnn so it gives special importance to the difference between certain characters like white spaces and letters so that it will train faster ? assume the input is just a sequence of ascii characters .",4199,,,2018-10-09T23:56:19.440,pre priming a network for white space,neural-networks machine-learning natural-language,0,0,
2240,8338,1,8340,2018-10-10T05:45:15.603,1,56,"as shown below , my deep neural network is overfitting : where the blue lines is the metrics obtained with training set and red lines with validation set is there anything i can infer from the fact that the accuracy on the training sets is really high ( almost 1 ) ? from what i understand , it means that the complexity of my model is enough / too big . but does it means my model could theoretically reach such a score on validation set with same dataset and appropriate hyperparameters ? with same hyperparameters but bigger dataset ? my question is not how to avoid overfitting .",18852,,,2018-10-10T08:32:38.880,interpretation of a good overfitting score,neural-networks machine-learning deep-learning overfitting,1,3,
2241,8339,1,,2018-10-10T07:02:05.440,4,47,"i have a dataset of images belonging to $ n$ classes , $ a_1 , a_2 ... a_n , b_1,b_2 ... b_m$ and i want to train a cnn to classify them . the classes can be considered as subclasses of two broader classes $ a$ and $ b$ , therefore the confusion between $ a_i$ and $ a_j$ is much less problematic than the confusion between $ a_i$ and $ b_j$ . therefore i want the cnn to be trained in such a way that the difference between $ a_i$ and $ b_j$ is considered as more relevant . 1 ) are there any loss functions that take this requirement into account ? could a weighted cross entropy work in this case ? 2 ) how this loss would change if the classes were unbalanced ?",16671,16671,2018-10-11T07:46:19.437,2018-10-11T13:33:48.357,how to define a loss function for a classifier where the confusion between some classes is more important than the confusion between others ?,convolutional-neural-networks classification loss-functions,1,0,
2242,8344,1,,2018-10-10T18:00:40.413,2,66,"i 'm trying to create a simple dyna - q agent to solve small mazes , in python . for the q function , q(s , a ) , i 'm just using a matrix , where each row is for a state value , and each column is for one of the 4 actions ( up , down , left , right ) . i 've implemented the "" real experience "" part , which is basically just straightforward sarsa . it solves a moderately hard ( i.e. , have to go around a few obstacles ) maze in 2000 - 8000 steps ( in the first episode , it will no doubt decrease with more ) . so i know that part is working reliably . now , adding the part that simulates experience based on what it knows of the model to update the q values more , i 'm having trouble . the way i 'm doing it is to keep an experiences list ( a lot like experience replay ) , where each time i take a real action , i add its ( s , a , r , s ' ) to that list . then , when i want to simulate an experience , i take a random ( s , a , r , s ' ) tuple from that list ( david silver mentions in his lecture ( # 8 ) on this that you can either update your transition probability matrix p and reward matrix r by changing their values , or just sample from the experience list , which should be equivalent ) . in my case , with a given s and a , since it 's deterministic , r and s ' are also going to be the same as the ones i sampled from the tuple . then i calculate q(s , a ) and max_a'(q(s',a ' ) ) , to get the td error ( same as above ) , and do stochastic gradient descent with it to change q(s , a ) in the right direction . but it 's not working . when i add simulated experiences , it never finds the goal . i 've tried poking around to figure out why , and all i can see that 's weird is that the q values continually increase as time goes on ( while , without experiences , they settle to correct values ) . does anyone have any advice about things i could try ? i 've looked at the sampled experiences , the q values in the experience loop , the gradient , etc ... and nothing really sticks out , aside from the q values growing . edit : here 's the code . the first part ( one step td learning ) is working great . adding the planning loop part screws it up . def dynaq(self , n_steps=100 , n_plan_steps=5 ) : self.initepisode ( ) for i in range(n_steps ) : # get current state , next action , reward , next state s = self.getstatevec ( ) a = self.epsgreedyaction(s ) r , s_next = self.iterate(a ) # get q values , q_next is detached so it does n't get changed by the gradient q_cur = self.q[s , a ] q_next = torch.max(self.q[s_next]).detach().item ( ) td0_error = ( r + self.params['gamma']*q_next - q_cur).pow(2).sum ( ) # sgd self.optimizer.zero_grad ( ) td0_error.backward ( ) self.optimizer.step ( ) # add to experience buffer e = experience(s , a , r , s_next ) self.updatemodel(e ) for j in range(n_plan_steps ) : xp = self.experiences[randint(0,len(self.experiences)-1 ) ] q_cur0 = self.q[xp.s , xp.a ] q_next0 = torch.max(self.q[xp.s_next]).detach().item ( ) td0_error0 = ( xp.r + self.params['gamma']*q_next0 - q_cur0).pow(2).sum ( ) self.optimizer.zero_grad ( ) td0_error0.backward ( ) self.optimizer.step ( )",18920,18920,2018-10-10T19:49:58.080,2018-10-10T19:49:58.080,"dyna - q algorithm , having trouble when adding the simulated experiences",reinforcement-learning,0,7,
2243,8345,1,,2018-10-10T20:38:33.697,1,30,"i 'm trying to build a chat analysis that could identify the intent of check price . has any kind of preset intent list to train my chatbot already been done ? data : a data set for "" check price "" intent 's . like "" how much are _ _ _ _ "" context : i looking for a data set to train a intent recognization for check price at chat bot . exemple : if some one ask for a product price . i want to tag this is as check price intent . for that i am trainning a intent recognition . region : prefirencal the whole globe , but for start could be only at english license : could both payed or free non - answers : i did nt find nothing like a dataset for intent",18925,18925,2018-10-15T11:21:24.453,2018-10-15T11:21:24.453,preset intent list,natural-language-processing datasets chat-bots,0,2,
2244,8348,1,8472,2018-10-10T23:45:00.113,3,637,"when extending reinforcement learning to the continuous states , continuous action case , we must use function approximators ( linear or non - linear ) to approximate the q - value . it is well known that non - linear function approximators , such as neural networks , diverge aggressively . one way to help stabilize training is using reward clipping . because the temporal difference q - update is a bootstrapping method ( i.e. , uses a previously calculated value to compute the current prediction ) , a very large previously calculated q - value can make the current reward relatively minuscule , thus making the current reward not impact the q - update , eventually leading the agent to diverge . to avoid this , we can try to avoid the large q - value in the first place by clipping the reward between [ 1 , -1]. but i have seen some other people say that instead of clipping the reward itself , we can instead clip the q - value between an interval . i was wondering which method is better for convergence , and under what assumptions / circumstances . i was also wondering if there are any theoretical proofs / explanations about reward / q - value clipping and which one being better .",17706,,,2018-10-16T19:14:20.127,should the reward or the q value be clipped for reinforcement learning,machine-learning reinforcement-learning value-iteration reward-clipping,1,1,
2245,8350,1,,2018-10-11T05:13:14.063,2,37,"would this work at all ? idea is to start training a neural net with some number of nodes . then , add some new nodes and more layers and start training only the new nodes ( or only modifying the old nodes very slightly ) . ideally , we would connect all old nodes to the new layer added since we might have learned many useful things in the hidden layers . then repeat this many times . intuition is that if the old nodes give bad information the new layer of nodes will weight the activations of old nodes close to zero and learn new / better concepts in the new nodes . the benefit is that we will keep old knowledge forever . caveat is that the network can still temporarily "" forget "" concepts if a new layer weights old information close to zero , but it can potentially remember it again too . if this completely fails , i 'm curious if there 's some known way to prevent a neural network from forgetting concepts it learned .",18936,,,2018-10-14T14:22:00.670,"would this work to prevent forgetting : train a neural net with n nodes . then , add more nodes and stop training the original nodes",neural-networks,1,5,
2246,8360,1,,2018-10-11T19:13:45.250,1,97,"i 'm working on a project , whereby ; one of the actors is "" teacher "" and it 's role will be , insert the course name , outcomes of skills , then insert the question , student degree and automatically will be connected with appropriate skill through the natural language processing ( nlp ) analysis student degree . what algorithm should i used in natural language processing to connect the question to the appropriate skill ( text comparison ) ?",18955,1581,2018-10-30T17:09:23.763,2018-10-30T17:09:23.763,question in nlp,natural-language-processing algorithm,1,4,
2247,8362,1,,2018-10-11T22:19:06.240,2,30,i am interested in understanding how to choose data - acquisition parameters for the subject matter : frame resolution frame rates ( fps ) the goal is to have ' enough ' ( preferably the minimal ) resolution and frames to enable ai to identify people . questions are there any published rules of thumb or processes to select video parameters ? is there a term or label for the selection of video parameters for ai projects ?,18819,,,2018-10-11T22:19:06.240,people counting video analytics : data acquisition parameters,datasets,0,0,
2248,8364,1,8365,2018-10-11T23:09:12.837,2,47,"i am a newbie in the deep learning and am looking for advice on predicting traffic congestion events . i have a table for vehicles travel times data , another table with the roads length segmented based on stop locations . i am thinking to derive the time - wise route specific speed details based on stop locations . after initial data cleansing and messaging , my input parameters are the time and stop location with actual speed details . i train my model with the training dataset and validate as per the deep learning recommended approach . so my questions are : is this approach correct or how can i improve it ? i am not sure if the number of inputs can be increased for better results . which activation method will be best to utilize to get a range of conditions / event types rather than binary 1 or 0 ? this will require dealing with a bigger dataset of at least over a few gbs . this will evolve into around 200gbs in the final product . can i use my professional grad laptop to process this data or if i should consider going to big data processing power ? please advise . thanks in advance for your help .",18959,,,2018-10-12T00:16:21.363,deep learning model training and processing requirement for traffic data,deep-learning,1,0,
2249,8367,1,,2018-10-12T06:04:40.683,2,59,"there are some predefined categories ( overview , data architecture , technical details , applications etc ) . the requirement is to classify the input text of paragraphs into their resp . category . i ca nt use any pretrained word embeddings ( word2vec , glove ) because the data entered is not in general english ( talking about dogs , environment etc ) but pure technical ( how does a particular program orks , steps to download anaconda etc ) . do n't have any data available in internet to train as well . anything that understands semantic - surface - level of a sentence will work",18963,,,2018-10-16T12:24:44.130,how to find the category of a technical text on a surface - semantic - level,machine-learning deep-learning natural-language-processing python semantics,1,1,
2250,8368,1,,2018-10-12T06:43:28.877,1,90,"i 'm try to train a rnn with a chunk of audio data , where x and y are two audio channels loaded into numpy arrays . the objective is to experiment with different nn designs to train them to transform single channel ( mono ) audio into a two channel ( stereo ) audio . my questions are : do i need a stateful network type , like lstm ? ( i think yes . ) how should i organize the data , considering that there are millions of samples and i ca n't load into memory a matrix of each window of data in a reasonable time - span ? for example if i have an array with : [ 0 , 0.5 , 0.75 , 1 , -0.5 , 0.22 , -0.30 ... ] and i want to take a window of 3 samples , for example . i guess i need to create a matrix with every sample shift like this , right ? [ [ 0.00 , 0.50 , 0.75 ] [ 0.50 , 0.75 , 1.00 ] [ 0.75 , 1.00,-0.50 ] [ 1.00,-0.50 , 0.22 ] ] where is my batch_size ? should i make the matrix like this per each sample shift ? per each window ? this may be very memory consuming if i intend to load a 4 min song . is this example matrix a single batch ? a single sample ?",18964,4302,2018-10-14T01:15:08.080,2019-04-20T03:02:35.447,difficulty understanding keras lstm fitting data,keras lstm audio-processing,1,0,
2251,8370,1,,2018-10-12T18:14:51.050,1,46,"sparse linear system are normally solved by using solvers like minres , conjugate gradient , gmres . efficient preconditioning , i.e. , finding a matrix p such that pax = pb is easier to solve then the original problem , can drastically reduce the computational effort to solve for x. however , preconditioning is normally problem specific and there is not one preconditioner that works well for every problem . i thought this would be an interesting problem to apply rl , since there are certain norms ( e.g. condition number of matrix pa ) to measure if p is a good preconditioner , but i could not find any research in this field . is there a specific problem why rl could not be applied ?",18975,18975,2018-10-13T18:23:13.027,2018-10-13T20:09:19.973,using reinforcement learning to find a preconditioner for linear systems of the form ax = b,reinforcement-learning linear-algebra,1,2,
2252,8371,1,8376,2018-10-12T19:45:40.173,1,29,"i have created a classifier for some simple gestures using an input layer , a hidden layer with tanh activation and an output softmax layer , i 'm also using the adam optimiser . the network classifies perfectly with validation data , however i 'd like it to be able to take in random noise that looks nothing like the shapes and not be able to classify it confidently . for example : one gesture input looks like this and is correctly classified as gesture ' a ' : however , when i pass this ' noise ' , which is clearly differentiable to the human eye , as input it still classifies it with 100 % confidence that it is the same gesture ' a ' . i assume it 's because the inputs are still very close to 0 ? my instinct is to scale up the inputs perhaps to increase the differentiation between the noise and the input . however , in real operation the noise will all be on a similar scale to the inputs and i wo n't know what is noise and what is n't so i will still have to apply the same scaling to that noise . will i run into the same problem ? on a more general note is there a teaching approach to prevent misclassifications , particularly if we know what they might look like ? for example , in this case i thought i could perhaps generate some noise and use it at training time to create an extra noise class , or is it just best to come up with such a well - trained network that you can use some sort of confidence threshold ? for example , if the network only produces 50 % confidence on an input then i can discard it as noise . any suggestions much appreciated !",18577,,,2018-10-13T07:52:31.447,recognising noise in simple classification,convolutional-neural-networks classification,1,0,
2253,8380,1,8381,2018-10-13T11:47:35.950,2,341,this survey of artificial intelligence with prolog provides a short perspective on the prolog programming language and its position in the history of ai . it proposes that a return to prolog research and development as begun in the 1980s when japan began work on a fifth generation computer system . has the reduction in cost and the increase in speed of computing platforms re - opened a door that was closed primarily because of technology immaturity ?,4302,1581,2018-10-16T18:53:54.493,2018-10-16T20:43:57.280,can strong ai now be achieved with prolog ?,strong-ai prolog,2,1,
2254,8382,1,,2018-10-13T12:17:13.547,2,783,"quick draw is a google experiment using user generated online doodles and machine learning to play a game of "" guess what i 'm drawing "" similar to the board game pictionary . i 'm interested if anyone happens to know what algorithms are in play in this example ?",11893,,,2018-10-13T23:18:28.857,what models is google 's quick draw using ?,machine-learning reinforcement-learning google,2,1,1
2255,8391,1,,2018-10-13T17:48:31.180,2,41,maxpooling is performed as one of the steps in inception which yields same output dimension as that of the input . can anyone explain how this max pooling is performed ?,17763,,,2018-10-14T04:44:03.950,maxpooling in inception ?,neural-networks convolutional-neural-networks,1,0,
2256,8392,1,8394,2018-10-13T18:28:08.137,4,103,"machine learning and data science are mainly made for processing large amounts of data nowadays , for example - a multitude of pictures . but do these fields have some applications in the decision making ? i mean - do at least some of the companies make a decision making systems as a part of their products ? and do they hire dms specialists ? is there any difference between dms and "" regular "" ds ?",7014,,,2018-10-14T08:40:01.340,decision making systems applications,machine-learning decision-theory data-science decision-tree,2,0,
2257,8398,1,,2018-10-14T01:06:17.617,2,30,"a fixed video camera records people moving through its field of view . the goal is to detect and track the head , in real - time as it moves through the video . the norm is there are many heads , which often are sometimes partially obscured . this example video boxes heads and provides a head count . there seems to be many different models . examples include : adaboost - haar head detection mask r - cnn lbp cascade given the context of the video , what is the thought process that you would use to choose a model ?",18819,10135,2018-10-18T10:44:45.367,2019-04-18T21:05:15.850,choosing instance semantic detection,neural-networks semantics,1,0,
2258,8403,1,8404,2018-10-14T09:05:08.993,3,78,"i 'm making a connect four game using the typical minimax + alpha - beta pruning algorithms . i just implemented a transposition table , but my tests tell me the tt only helps 17 % of the time . by this i mean that 17 % of the positions my engine comes across in its calculations can be automatically given a value ( due to the position being calculated previously via a different move order ) . for most games , is this figure expected ? to me it seems very low , and i was optimistically hoping for the tt to speed up my engine by around 50 % . it should be noted though that on each turn in the game , i reset my tt ( since the evaluation previously assigned to each position is inaccurate due to lower depth back then ) . i know that the effectiveness of tt 's are largely dependent on the game they 're being used for , but any ballparks of how much they speed up common games ( chess , go , etc ) would be helpful . edit - after running some more tests and adjusting my code , i found that the tt sped up my engine to about 133 % ( so it took 75 % as much time to calculate ) . this means those 17 % nodes were probably fairly high up in the tree , since not having to calculate the evaluation of these 17 % sped up things by 33 % . this is definitely better , but my question still remains on whether this is roughly expected performance of a typical tt .",16917,16917,2018-10-14T13:25:32.393,2018-10-14T13:29:50.357,transposition table is only used for roughly 17 % of the nodes - is this expected ?,game-ai search gaming minimax efficiency,1,0,
2259,8407,1,,2018-10-14T18:25:30.217,6,91,"so let 's say you had a really nice day in a flight simulator and you are getting videos of this type of quality : this is full hd ( 1080p ) , but heavily compressed . you can literally see the pixels . now i tried to use something like raisr , and this python implementation , but it only scales the image up and does not ' fix the thicc pixels ' . so is there a type of ai that does fix this kind of video / photo into a reasonable quality video ? i just want to get rid of those pixels and image artefacts that was generated during the compression .",19013,19013,2018-10-14T18:40:04.567,2019-02-01T22:01:30.487,can ai ' fix ' heavily compessed videos / photos ?,machine-learning deep-learning,2,5,1
2260,8410,1,8464,2018-10-14T20:35:35.353,-2,42,"i am preparing a binary classifier . initially i used the the following parameters based on the well known cat and dog classifier example ; train_datagen = imagedatagenerator(rescale=1./255 ) validation_datagen = imagedatagenerator(rescale=1./255 ) test_datagen = imagedatagenerator(rescale=1./255 ) # data generator for training data train_generator = train_datagen.flow_from_directory(train_dir , target_size=(224 , 224 ) , batch_size=32 , class_mode='binary ' ) # data generator for validation data validation_generator = validation_datagen.flow_from_directory(validation_dir , target_size= ( 224 , 224 ) , batch_size=16 , class_mode='binary ' , shuffle = false ) # data generator for test data test_generator = validation_datagen.flow_from_directory(test_dir , target_size=(224 , 224 ) , batch_size=16 , class_mode='binary ' , shuffle = false ) # compile the model model.compile(loss='binary_crossentropy ' , optimizer = optimizers.rmsprop(lr=1e-4 ) , metrics=['acc ' ] ) and the model was ; # create a sequential model model = models.sequential ( ) # add the vgg convolutional base model model.add(vgg_conv ) # add new layers model.add(layers.flatten ( ) ) model.add(layers.dense(1024 , activation='relu ' ) ) model.add(layers.dropout(0.5 ) ) model.add(layers.dense(1 , activation='sigmoid ' ) ) i use vgg16 with imagenet weights by adding these layers at the end of the model . cat and dog classifier works with a good accuracy and i was sure this model is going to give me very good results for the initial steps . however , when i checked the confusion matrix i saw that all the images are classified they belong to category 0 . then i replaced all "" binaries "" with "" categorical "" and used softmax activator with 2 units . all the rest is the same and now i obtain very good results . i do not understand how this happens ? why the first configuration work very well with another dataset but gives me garbage results with my own dataset ? it is better to try as much as possible but it takes a bit less than a day with my dataset . i also do not understand how a binary classifier works with only one unit at the output layer since the actual categories are two .",18283,,,2018-10-16T15:16:45.387,number of units of the last layer,keras,1,1,
2261,8412,1,8421,2018-10-15T04:27:41.923,1,167,"in keras , when we use an lstm / rnn model , we need to specify the node [ i.e. , lstm(128)]. i have a doubt regarding how it actually works . from the lstm / rnn unfolding image or description , i found that each rnn cell take one time step at a time . what if my sequence is larger than 128 ? how to interpret this ? can anyone please explain me ? thank in advance .",18795,2444,2019-02-16T02:32:25.597,2019-02-16T02:32:25.597,what should i do when i have a variable - length sequence when instantiating an lstm in keras ?,machine-learning recurrent-neural-networks long-short-term-memory,2,0,
2262,8413,1,,2018-10-15T06:02:47.483,0,24,"little help here "" i do n't want to put unknown encodings of group image in to a classifier "" what i did was find matches for each of the encoding and trying to remove encodings with matches=0 for encoding in encodings : matches = np.count_nonzero(face_recognition.compare_faces(data[""encodings "" ] , encoding ) ) d[""encods""]=[matches , encoding ] for k , v in list(d.items ( ) ) : if v[0]==0 : del d[k ] print(""dict"",d ) i am not getting what i want.please help",16770,16770,2018-10-15T06:34:35.073,2018-10-15T07:49:35.153,remove encoding array from list of arrays,machine-learning image-recognition,1,10,
2263,8414,1,8507,2018-10-15T06:04:36.427,4,146,"following - up my question about my over - fitting network my deep neural network is over - fitting : i have tried several things : simplify the architecture apply more ( and more ! ) dropout data augmentation but i always reach similar results : training accuracy is eventually going up , while validation accuracy never exceed ~70 % . i think i simplified enough the architecture / applied enough dropout , because my network is even too dumb to learn anything and return random results ( 3-classes classifier = > 33 % is random accuracy ) , even on training dataset : my question is : this accuracy of 70 % is the best my model can reach ? if yes : why the training accuracy reach such high scores , and why so fast , knowing this architecture seems to be not compatible ? my only option to improve the accuracy is then to change my model , right ? if no : what are my options to improve this accuracy ? i'v tried a bunch of hyperparameters , and a lot of time , depending of these parameters , the accuracy does not change a lot , always reaching ~70 % . however i ca n't exceed this limit , even though it seems easy to my network to reach it ( short convergence time ) edit here is the confusion matrix : i do n't think the data or the balance of the class is the problem here , because i used a well - known / explored dataset : snli dataset and here is the learning curve : note : i used accuracy instead of error rate as pointed by the resource of martin thoma it 's really ugly one . i guess there is some problem here . maybe the problem is that i used the result after 25 epoch for every values . so with little data , training accuracy do n't really have time to converge to 100 % accuracy . and for bigger training data , as pointed in earlier graphs , the model overfit so the accuracy is not the best one .",18852,18852,2018-10-15T13:35:12.087,2018-10-24T00:12:52.170,how to improve testing accuracy when training accuracy is high ?,neural-networks machine-learning deep-learning overfitting,2,3,
2264,8417,1,8420,2018-10-15T07:46:52.850,0,295,"the densenet architecture can be summarize with this figure : why there is transition layers between each blocks ? in the papers , they justify the use of transition layers as follow : the concatenation operation used in eq . ( 2 ) is not viable when the size of feature - maps changes . however , an essential part of convolutional networks is pooling layers that change the size of feature - maps . to facilitate pooling in our architecture we divide the net- work into multiple densely connected dense blocks but , if i understand what they means : the problem is that the feature map size can change , thus we ca n't concatenate . but how adding transition layer change this problem ? and how can several dense blocks connected like this are more efficient that one single bigger dense block ? optional question : why all standard densenet are made of 4 dense blocks ? i guess i will have the answer to this question if i understood better the previous questions ...",18852,18852,2018-10-15T07:54:22.720,2018-10-15T08:24:02.407,why is there transition layers in densenet ?,neural-networks machine-learning deep-learning convolutional-neural-networks,1,0,
2265,8422,1,,2018-10-15T10:10:12.483,1,121,is the laptop asus zenbook pro 90nx0152-m02980 enough to do deep learning model ? specs : processeur intel core i7 - 7500u ( dual - core 2.7 ghz / 3.5 ghz turbo - cache 4 mb ) 8 gb memory ssd m.2 sata de 256 gb,19030,7800,2018-10-18T03:11:54.760,2018-12-17T17:02:00.420,asus zenbook with deep learning,deep-learning hardware-evaluation,2,3,
2266,8424,1,,2018-10-15T11:24:32.070,1,252,"i want to make a kind of robotic brain i.e. a big neural network , which includes nlp model ( for understanding human voice ) , real - time object recognition ( so it can identify particular object ) , face recognition model ( for identifying faces ) .. is possible to build a huge neural networks in which we can combine all these separate models togetherly so we can use all 3 model 's capabilities at same time in parallel ? ( i.e. if i ask robot / chatbot using mic , can you can see that table/ that boy ? , robot start recognizing object & amp ; faces and reply me back by speech if he could identify or not ! ) if it 's possible then kindly share your idea how can i implement this ? or is there any better to make such ai ? ( in python , tensorflow )",14527,,,2018-10-15T12:10:28.000,can we combine multiple different neural networks in one ?,neural-networks deep-learning python tensorflow robots,1,0,
2267,8427,1,8437,2018-10-15T12:38:11.573,9,592,what exactly are ontologies in ai ? how should i write them and why are they important ?,18123,2444,2019-04-30T17:39:47.410,2019-04-30T17:39:47.410,what are ontologies in ai ?,ai-design terminology definitions ontology,2,2,3
2268,8429,1,,2018-10-15T14:07:42.130,2,37,the process revolves around a child 's drawing . each part of each drawing corresponds to a score as in the draw a person test conceived by dr . florence goodenough in 1926 . the goal of the machine is to measure a child 's mental age through a figure drawing task .,18027,4302,2018-10-15T23:01:19.387,2018-10-15T23:01:19.387,"how to use machine learning to create a "" draw - a - person test """,machine-learning ai-design image-recognition intelligence-testing,0,0,
2269,8430,1,,2018-10-15T15:28:58.157,1,57,i have coded an ai checkers game but would like to see how good it is . some people have informed me to use the chinook ai opensource code . but i am having trouble trying to integrate that software into my ai code . how do i integrate another game engine in checkers with the ai i have coded ?,16906,,,2018-10-15T15:28:58.157,checkers ai game engines,game-ai checkers,0,11,
2270,8432,1,8434,2018-10-15T17:55:35.607,6,409,"my background is in electrical engineering ( bs , ms ee / signal processing ) and i have a good grasp of cs foundations ( data structures , algorithms , os , discrete math ) and software engineering . i have option of enrolling in a ms program in applied math at a good school . my objective is to switch to ai from my current career . what areas of applied math is relevant to ai ? and do you think it 's a good preparation instead of enrolling in a cs program .",19040,19040,2018-10-16T03:08:07.730,2018-10-17T18:50:40.647,applied math relevant to ai,ai-basics getting-started math academia,3,0,4
2271,8435,1,,2018-10-15T19:47:42.013,4,68,"i am new to the object recognition community . here i am asking about the broadly accepted ways to calculate the error rate of a deep cnn when the network produces different results using the same data . 1 . problem introduction recently i was trying to replicate some classic deep cnns for the object recognition tasks . inputs are some 2d image data including objects and the output are the identification / classification results of the object . the implementation involves the use of python and keras . the problem i was facing is that , i may get different validation results among multiple runs of the training even using the same training / validation data sets . to me , that made it hard to report the error rate of the model since every time the validation result may be different . i think this difference is because of the randomness involved in different aspects of deep cnn , such as random initialization , the random ‘ dropout ’ used in the regulation , the ‘ shuffle ’ process used in the choosing of epochs , etc . but i do not know yet the “ right ” ways to deal with this difference when i want to calculate the error rate in object recognition field . 2 . my exploration – online search i have found some answers online here . the author proposed two ways , and he / she recommended the first one shown below : the traditional and practical way to address this problem is to run your network many times ( 30 + ) and use statistics to summarize the performance of your model , and compare your model to other models . the second way he / she introduced is to go to every relevant aspect of the deep cnn , to "" freeze "" their randomness nature on purpose . this kind of approach has also been introduced from keras q&amp;a here . they call this issue the “ making reproductive results ” . 3 . my exploration – in academia community ( no result yet , need your help ! ) since i was not sure whether the two ways mentioned above are the “ right ” ones broadly accepted , i was going further exploring in the object recognition academia community . now i just begin to read from imagenet website . but i have not found the answer yet . maybe you could help me knowing the answer easier . thanks ! daqi",19042,19042,2018-10-30T02:39:54.697,2019-05-30T06:03:05.407,"what are the ways to calculate the error rate of a deep convolutional neural network , when the network produces different results using the same data ?",deep-learning convolutional-neural-networks python keras object-recognition,1,0,1
2272,8436,1,8438,2018-10-15T20:41:39.347,0,46,"in the early 20th century , anything princeton staff represented related to computing and the simulation of human thought was assumed to be flawless . in the late 20th century , it was mit 's ai lab that was trusted to define what was feasible and what were the next steps . is google the new source of legitimacy in ai pronouncements ? because of the connection between that corporation and the daily life of people on earth , has that publication honor that was given to the primary ai centers of research in the 20th century reached a level in the early 21st century that rivals deity ? it sometimes seems that , when people do n't understand something in a paper with the word google anywhere in the authorship block , it is automatically assumed that the reader is wrong because the paper is of universal authority . they seem to jump to the assumption that their mind must adjust to whatever the paper states . is this equivalent to religious scripture ? carl jung wrote , contemporary man is blind to the fact that , with all his rationality and efficiency , he is possessed by "" powers "" that are beyond his control . his gods and demons have not disappeared at all ; they have merely got new names . this is an ai stackexchange , so the proposal in this question is that perhaps the same brain machinery has been used for the following things an ancient egyptian gathering with others to worship the sun god re casiodoris using the resources of the roman empire to send holy scripture out on horseback to the far reaches of the empire to save it from the invading barbarian hordes ai researchers finding some unassailable source of truth in a university or a multinational corporation is carl jung correct , and , if so , is this human need to find legitimacy in an institution now manifesting in the ai community 's absolute trust in a corporation that automated the 18th century library card catalog and built that automaton into a large multinational corporation ? lastly , if both those questions can be answered in the affirmative , what risks can exist when the human mind treats a university or a corporation as a god from an economic or social risk management perspective ?",4302,,,2018-10-16T18:01:56.787,has google 's view of ai risen to the status of universal truth ?,social risk-management,2,0,
2273,8458,1,8545,2018-10-16T06:20:22.073,3,48,"this is a question about pattern recognition and feature extraction . i am familiar with hough transforms , the fast radial transform and variants ( e.g. , gfrs ) , but these highlight circles , spheres , etc . i need an image filter that will highlight the centroid of a series of spokes radiating from it , such as the center of a asterix or the spokes of a bicycle wheel ( even if the round wheel is obscured . does such a filter exist ?",19049,,,2018-10-23T05:40:00.017,how to recognize non - circular radial symmetry in images ?,pattern-recognition feature-selection,2,3,
2274,8461,1,8462,2018-10-16T09:49:12.310,3,44,"i would like to get a simple example running in matlab that will use a neural net to learn an arbitrary function from input output data ( basically model identification ) and then be able to approximate that function from just the input data . as means of training this net i have implemented a simple back propagation algorithm in matlab but i was not able to get anywhere close to satisfactory results . i would like to know what i may be doing wrong and also what approach i may use instead . the goal is to have the network represent an identified function f(x ) which takes a series x as input and outputs the learned mapping from x - > y. here is the gnu octave code i have so far : pkg load control signal function r = sigmoid(z ) r = 1 ./ ( 1 + exp(-z ) ) ; end function r = linear(z ) r = z ; end function r = grad_sigmoid(z ) r = sigmoid(z ) . * ( 1 - sigmoid(z ) ) ; end function r = grad_linear(z ) r = 1 ; end function r = grad_tanh(z ) r = 1 - tanh(z ) .^ 2 ; end function nn = nn_init(n_input , n_hidden1 , n_hidden2 , n_output ) nn.w2 = ( rand(n_input , n_hidden1 ) * 2 - 1 ) ' nn.w3 = ( rand(n_hidden1 , n_hidden2 ) * 2 - 1 ) ' nn.w4 = ( rand(n_hidden2 , n_output ) * 2 - 1 ) ' nn.lambda = 0.005 ; end function nn = nn_train(nn_in , state , action ) nn = nn_in ; [ out , nn ] = nn_eval(nn , state ) ; d4 = ( nn.a4 - action ) . * grad_linear(nn.w4 * nn.a3 ) ; d3 = ( nn.w4 ' * d4 ) . * grad_tanh(nn.w3 * nn.a2 ) ; d2 = ( nn.w3 ' * d3 ) . * grad_tanh(nn.w2 * nn.a1 ) ; nn.w4 -= nn.lambda * ( d4 * nn.a3 ' ) ; nn.w3 -= nn.lambda * ( d3 * nn.a2 ' ) ; nn.w2 -= nn.lambda * ( d2 * nn.a1 ' ) ; end function [ out , nn ] = nn_eval(nn_in , state ) nn = nn_in ; nn.z1 = state ; nn.a1 = nn.z1 ; nn.a2 = tanh(nn.w2 * nn.a1 ) ; nn.a3 = tanh(nn.w3 * nn.a2 ) ; nn.a4 = linear(nn.w4 * nn.a3 ) ; out = nn.a4 ; end nn = nn_init(1 , 100 , 100 , 1 ) ; t = 1:0.1:3.14 * 10 ; input = t ; output = sin(input ) ; learned = zeros(1 , length(output ) ) ; for j = 1:500 for i = 1 : length(input ) nn = nn_train(nn , [ input(i ) ] , [ output(i ) ] ) ; end j end for i = 1 : length(input ) learned(i ) = nn_eval(nn , [ input(i ) ] ) ; end plot(t , output , ' g ' , t , learned , ' b ' ) ; pause here is the result : the result is not even close to where i want it to be . has it got something to do with my implementation of back propagation ? what changes do i need to do to the code to get a better approximation going ?",19056,,,2018-10-16T10:58:03.343,learning an arbitrary function using a feedforward net,feedforward,1,0,
2275,8463,1,,2018-10-16T13:34:12.010,1,88,"assistive subsystems consider an automated vacuum cleaner with the following subsystems under the command of an ai system to be designed . these subsystems limit the ai complexity to just intelligent guidance of the vacuum , much like the behavioral components of the nervous systems of any arthropods , vertebrates , or other bilaterally symmetric animals . accurate and reliable vacuum body position and orientation control through four 8 "" diameter rubber wheels with independent suspension and radial positioning via a separate compass and floor plan positioning system & mdash ; the ai can simply command that system to go to position $ ( x , y)$ in room $ r_i$ and face "" nnw "" ( north - north - west ) . accurate and reliable tool control & mdash ; the ai can simply command "" brush "" , "" crevice "" , "" rectangle "" , "" rug "" , "" right - angle "" , or a coordinate in with a tool approach angle for the tube onto which tools are attached , given as a compound angle $ ( \theta , \phi)$ relative to the orientation of the vacuum , where is the yaw and is the pitchdirection from which the tool access to a tactile interference an event queue containing a time series of significant changes in force vectors . the queue contains elements that include the force vector in and whether the force change is being reported for force on the vacuum or on the tool . product limitations assume the following as product limitations . ( whether or not these are the best limitations , these are the directives from the board of directors . ) the vacuum is blind . ( a vision system cost to train across multiple households , manufacture , test , and update , and the associated impact on consumer price tags was evaluated and a tactile only sensory system is specified as a device engineering constraint . ) the room is free from foot traffic , children playing , and other such human motive activity . the floor of the room is free from balls , toys , spills , cloth items , and other such artifacts that would frustrate a vacuuming operation . requirements of maintaining two maps an accessibility map must be maintained to make use of past information about where the vacuum can go without bumping into something . it is obviously easier to maintain an accessibility map than to rediscover a room upon every room vacuuming . this map must include tool positions also . a coverage map must be initialized at the beginning of every room vacuuming and continuously updated to ensure that all the floor area is covered with appropriate overlapping of tool paths . distinct question note that this challenge is distinct from the how do autonomous robotic vacuum cleaners perceive the environment for navigation ? question in that it is not about discovery of room and furnishings location but rather assumes floor plans are installed upon product deployment ( which is more likely to be a highly effective short term product strategy ) . the problem of adapting to changes in the accessibility map is the challenge here . the question how can a fully automated vacuum cleaner use and update room information ? see above two maps .",4302,,,2018-10-16T13:34:12.010,how can a fully automated vacuum cleaner use and update room information ?,reinforcement-learning ai-design path-planning autonomous-vehicles consumer-product,0,2,
2276,8465,1,,2018-10-16T15:43:07.630,0,48,"i already know the basics of the basic of machine learning . e.g. : backpropagation , convolution , etc . first of let me explain reinforcement learning to make sure i grasped the concept correctly . in reinforcement learning a random - initialized network will first "" play""/""do "" a sequence of moves in an environment . ( in this case a game ) . after that , it will receive a reward $ r$ . furthermore a q - value gets defined by the engineer / hooby coder . this reward times the q - value $ q$ to the power of the position $ n$ of the action will be feeded back using bp . so how do i know how slight chances in are changing $ rq^n$ ?",19062,1581,2018-10-16T19:10:37.147,2018-10-16T19:10:37.147,how do i know how changes in the weights are changing the reward in reinforcement learning,reinforcement-learning game-ai backpropagation,1,0,
2277,8467,1,,2018-10-16T17:14:28.100,2,227,"given infinite resources / time one could create agis by writing code to simulate infinite worlds . in some of the worlds agis would be created . detecting them would be another issue . since we do n't have infinite resources the most probable way to create an agi is to write some bootstrapping code which would reduce the resources / time to reasonable values . in that agi code ( that would make it reasonable to create with finite resources / time ) is it required to have a part that deals with time / space estimation of possible actions taken ? or should that be outside of the code and be something the agi discovers by itself after it starts running ? any example of projects targeting agi that are using time / space estimation might be useful for reaching a conclusion . clarification , by time / space i mean time / space complexity analysis for algorithms , see : measures of resource usage and analysis of algorithms the question formulation might lead people to think that the time / space estimation can only apply to some class of actions called algorithms . to clarify my mistake , i mean the estimation to apply to any action plan . imagine you are an agi and you have to make a choice between different set of actions to pursue your goals . if you had 2 goals and one of them used less space and less time then you would always pick it over the other algorithm . so time / space estimation is very useful since intelligence is about efficiency . there is at least 1 exception though , imagine in the example before that the goal of the agi is to pick the set of actions that leads to the most expensive time / space set of actions ( or any non - minimal time / space cost ) then obviously because of the goal constraint you would pick the most time / space expensive set of actions . in most other cases though , you would just pick the most time / space efficient algorithm .",10826,4302,2018-10-20T11:20:57.150,2018-10-20T11:20:57.150,is time / space estimation of possible actions required for creating agi ?,strong-ai agi,2,4,
2278,8476,1,8477,2018-10-17T07:42:21.090,4,964,what is an agent in reinforcement learning ( rl ) ? i think it is not the neural network behind . what does the agent in rl exactly do ?,19062,2444,2018-11-19T18:21:35.957,2018-11-19T18:21:35.957,what does the agent in reinforcement learning exactly do ?,reinforcement-learning intelligent-agent,2,0,1
2279,8480,1,,2018-10-17T10:54:29.347,4,319,scanned documents could be from - news papers / book / magazine with complex alignments for text(text could be in any angle w.r.t . the page ) . i can do lot of processing for different features extraction right ! but i want to know some robust methods which does not need much features . so i thought machine learning could help me . it should be like doing less processing and more machine learning . thank you .,18459,,,2018-10-19T01:05:13.263,how to do scanned document analysis to detect text and non - text regions,detecting-patterns,2,0,
2280,8482,1,8486,2018-10-17T12:00:49.070,6,83,"after doing some exercices on q - learning for maze solving , i wondered : my q - learning algorithms solve only one maze . the ai does n't learn how to solve mazes , so how can i achieve it ? for instance learn "" follow one wall until the end of the maze "" instead of "" turn left left right ... "" ? i guess it is the same approach for self driving cars ?",19094,10135,2018-10-18T10:44:42.947,2018-10-18T10:44:42.947,q - learning the generic maze solution,reinforcement-learning ai-design training q-learning challenges,1,1,2
2281,8487,1,,2018-10-17T20:16:58.747,1,33,"i am reading article https://allenai.org/paper-appendix/emnlp2017-wt/ http://ai2-website.s3.amazonaws.com/publications/wikitables.pdf about training neural network and the loss function is mentioned on page 6 chapter 3.4 - this loss function o(theta ) is expressed as marginal loglikelihood objective function . i simply does not understand this . the neural network generates logical expression ( query ) from some question in natural language . the network is trained using question - answer pairs . one could expect that simple sum of correct-1 / incorrect=0 result could be good loss function . but there is strange expression that involves p(l|qi , ti ; theta ) that is not mentioned in the article . what is meant by this p function ? as i understand , then many logical forms l are generated externally for some question qi . but further i can not understand this . the mentioned article largely builds on other article http://www.aclweb.org/anthology/p16-1003 from which it borrows some terms and ideas . it is said that l is treated as latent variable and p seems to be some kind of probability . of course , we should assign the greated probability to the right logical form l , but where can i find this assignment . does training / supervision data should contain this probability function for training / supervision data ?",8332,8332,2018-10-17T20:26:36.093,2018-10-17T20:26:36.093,how to understand marginal loglikelihood objective function as loss function ( explanation of an article ) ?,neural-networks natural-language-processing lstm loss-functions,0,0,
2282,8490,1,,2018-10-18T05:34:51.540,1,23,"in the abstract section of the paper network in network , what does the authors actually mean to say ?",18017,1671,2018-10-21T22:58:19.273,2018-10-21T22:58:19.273,"what is meant by "" model discriminability for local patches within the receptive field "" ?",deep-learning convolutional-neural-networks models,0,0,
2283,8491,1,,2018-10-18T07:30:20.910,5,67,"while working through some example from github i 've found this network ( it 's for fashionmnist but it does n't really matter ) . pytorch forward method ( my query in upper case comments with regards to applying softmax on top of relu ? ) : def forward(self , x ) : # two conv / relu + pool layers x = self.pool(f.relu(self.conv1(x ) ) ) x = self.pool(f.relu(self.conv2(x ) ) ) # prep for linear layer # flatten the inputs into a vector x = x.view(x.size(0 ) , -1 ) # does it make sense to apply relu here * * x = f.relu(self.fc1(x ) ) # and then softmax on top of it ? x = f.log_softmax(x , dim=1 ) * * # final output return x",19116,1847,2018-10-18T12:16:23.473,2018-10-18T12:56:46.763,does it make sense to apply softmax on top of relu ?,convolutional-neural-networks python,1,1,
2284,8493,1,8494,2018-10-18T10:38:39.577,2,93,"i am new to rl and i am trying to work through the book reinforcement learning : an introduction i ( sutton & amp ; barto , 2018 ) . in chapter 3 on finite markov decision processes , the authors write the expected reward as $ $ r(s , a ) = \mathbb{e}\left[r_t|s_{t-1}=s , a_{t-1}=a\right]=\sum_{r\in \mathcal{r}}r\sum_{s'\in \mathcal{s}}p(s',r|s , a)$$ i am not sure if the authors mean $ $ r(s , a ) = \mathbb{e}\left[r_t|s_{t-1}=s , a_{t-1}=a\right]=\sum_{r\in \mathcal{r}}\left[r\sum_{s'\in \mathcal{s}}p(s',r|s , a)\right]$$ or $ $ r(s , a ) = \mathbb{e}\left[r_t|s_{t-1}=s , a_{t-1}=a\right]=\left[\sum_{r\in \mathcal{r}}r\right]\cdot\left[\sum_{s'\in \mathcal{s}}p(s',r|s , a)\right].$$ if the authors mean the first , is there any reason why it is not written like the following ? $ $ r(s , a ) = \mathbb{e}\left[r_t|s_{t-1}=s , a_{t-1}=a\right]=\sum_{r\in \mathcal{r}}\sum_{s'\in \mathcal{s}}\left[r\,p(s',r|s , a)\right]$$",19123,2444,2019-02-16T02:29:38.477,2019-02-16T02:29:38.477,understanding the notation in the definition of the expected reward,reinforcement-learning rl-an-introduction notation,1,0,1
2285,8496,1,8525,2018-10-18T13:36:09.240,2,242,"in the book reinforcement learning : an introduction ( sutton & amp ; barto , 2018 ) . the authors ask exercise 3.2 : is the mdp framework adequate to usefully represent all goal - directed learning tasks ? can you think of any clear exceptions ? i thought maybe a card game would be an example if the state does not contain any pieces of information on previously played cards . but that would mean that the chosen state leads to a system that is not fully observable . hence , if i track all cards and append it to the state ( state vector with changing dimension ) the problem should have the markov property ( no information on the past states is needed ) . this would not be possible if the state is postulated as invariant in mdp . if the previous procedure is allowed , then it seems to me that there are no examples where the mdp is not appropriate . i would be glad if someone could say if my reasoning is right or wrong . what would be an appropriate answer to this question ?",19123,,,2018-10-19T19:06:36.580,when is markov decision process ( mdp ) not adequate for goal - directed learning tasks,reinforcement-learning markov-chain,1,5,
2286,8497,1,,2018-10-18T15:33:57.757,1,20,"we have multiple computers and the ability to ssh between them . what are options using either java , c / c++ , javascript , or python to distribute our learning tasks ? we will be using dcnn , dqn , and lstm in different combinations .",19127,,,2018-10-18T15:33:57.757,"if there are several computers on a subnet , can training time be reduced by distributing the work across them ?",deep-learning convolutional-neural-networks python lstm java,0,0,
2287,8499,1,,2018-10-18T18:54:44.450,0,65,"introduction exhaustive search is a method in ai planning to find a solution for so called constraint satisfaction problems . ( csp ) . that are problems which have some conditions to fulfill and the solver is trying out all the alternatives . an example csp problem is the 8-queens problem which has geometrical constraints . the standard method in finding a solution for the 8-queens problem is a backtracking solver . that is an algorithm which generates a tree for the state space to search inside inside the graph . apart from practical applications of backtracking search there are some logic - oriented discussions available which are asking on a formal level which kind of problems have a solution and which not . for example to find a solution for the 8-queen problem many millions of iterations of the algorithm are needed . the question is now : which problems are too complex to find a solution . the second problem is , that sometimes the problem itself has no solution , even the complete state space was searched fully . let us make an example . at first we construct a problem in which the constraints are so strict that even a backtracking search wo n't find a solution . one example would be to prove that “ 1 + 1=3 ” another example would be to find a chess sequence , if the game is lost or it is also funny to think about how to arrange nine ! queen on a chess table so that they does n't hurt . is there any literature available which is describing constraint satisfaction problems on a theoretical basis in which the constraints of the problem are too strict ? original posting just wondering - like with a 8-queens problem . if we change it to a 9-queens problem and do a exhaustive search , we will see that there is no solution . is there a problem in which the search fails to show that a solution does not exist ?",19137,11571,2018-10-19T08:48:42.697,2018-11-09T14:29:28.160,any problems / games / puzzles in which exhaustive search can not show that a solution does not exist ?,search,2,2,
2288,8509,1,,2018-10-19T07:15:05.473,1,247,"i 've created a neural net using the convnetsharp library which has 3 fully connected hidden layers . the first having 35 neurons and the other two having 25 neurons each , each layer with a relu layer as the activation function layer . i 'm using this network for image classification - kinda . basically it takes inputs as raw grayscale pixel values of the input image and guesses an output . i used stochastic gradient descent for the training of the model and a learning rate of 0.01 . the input image is a row or column of omr "" bubbles "" and the network has to guess which of the "" bubble "" is marked i.e filled and show the index of that bubble . i think it is because its very hard for the network to recognize the single filled bubble among many . here is an example image of omr sections : using image - preprocessing the network is given a single row or column of the above image to evaluate the marked one . here is an example of a preprocessed image which the network sees : here is an example of a marked input : i 've tried to use convolutional networks but i 'm not able to get them working with this . basically , my question is that what type of neural network and network architecture should i use for this kind of task ? . an example of such network with code would be greatly appreciated . i have tried many preprocessing techniques such as background subtraction using the absdiff function in emgucv and also using mog2 algorithm and i 've also tried threshold binary function but there still remains enough noise in the images which makes it difficult for the neural net to learn . i think this problem is not specific to using neural nets for omr but for others too . it would be great if there could be a solution out there that could store a background / template using a camera and then when the camera sees that image again , it perspective transforms it to match exactly to the template i 'm able to achieve this much - and then find their difference or do some kind of preprocessing so that a neural net could learn from it . if this is not quite possible , then is there a type of neural network out there which could detect very small features from an image and learn from it . i have tried convolutional neural network but that also is n't working very well or i 'm not applying them efficiently .",19144,1581,2018-10-31T16:41:02.807,2019-04-29T19:01:55.817,neural network for optical mark recognition ?,neural-networks machine-learning deep-learning convolutional-neural-networks computer-vision,1,0,2
2289,8510,1,,2018-10-19T07:59:55.483,1,15,"while dealing with image data at very large scale , there are different sources where data is coming from . often , we do not have any control over quality of labels/ annotations . i already do use sampling quality checks method to manually check the quality of annotations but as the volume of data has increased , even sampling qc become an inefficient job . are there other methods to automate / simplify this task for data at large scale ?",17980,17980,2018-10-19T09:47:54.260,2018-10-19T09:47:54.260,input annotations quality check for large scale image data,deep-learning computer-vision datasets,0,0,
2290,8511,1,8513,2018-10-19T08:04:29.920,1,51,my neural network is simple enough and does not overfit . dropout is a regularization technique for reducing overfitting in neural networks from wikipedia adding dropout in a non - overfitting network can increase accuracy ? even if i increase the complexity of the network ?,18852,,,2018-10-19T08:36:34.967,usefulness of dropout for non - overfitting network,neural-networks machine-learning deep-learning dropout,1,0,
2291,8514,1,8520,2018-10-19T09:34:21.753,1,69,"i am referring to eq . 3.6 ( p / g 49 ) based on sutton 's online book and can be found in an image below . i could not make sense of the final derivation of the equation $ r(s , a , s')$ . my question is actually how do we come to that final derivation ? surprisingly , the denominator of $ p(s'|s , a)$ can literally be replaced by $ p(s ' , r|s , a)$ as eq . 3.4 suggests , then it will end up with "" $ r$ "" term only due to cancellation of numerator $ p(s ' , r|s , a)$ and denominator $ p(s'|s , a)$ . any explanation on that would be appreciated .",19147,1641,2018-10-19T15:29:00.243,2018-10-19T17:36:11.363,reward - related formulation in reinforcement learning,reinforcement-learning math,1,0,
2292,8518,1,,2018-10-19T11:55:43.517,2,170,"imagine a "" simple "" feedforward , fully connected neural network , with some input size , some number of hidden layers , and some # of neurons .... etc but with a fixed number of output size ( that is saying , even if we change and optimize the whole structure of the neural network through through cross validation , the output size must be constant ) , then what are some ways of performing neural network given that the l1 norm of the output values is less than or equal to 1 ? then , given such methodology , how would i go about performing backpropagation ? i was thinking there must be some "" penalty "" method just like how in mathematical optimization problem , you can introduce log barrier function as the "" penalty function """,19150,,,2019-04-19T21:01:45.347,how to perform neural network with output constraint ?,neural-networks machine-learning backpropagation,1,0,
2293,8519,1,8521,2018-10-19T14:50:54.090,1,13,"as you can see in the title , i 'm trying to program an ai in java that would help someone optimize his storage . the user has to enter the size of his storage space ( a box , a room , a warehouse etc ... ) and then enter the size of the items he has to store in this space . ( note that everything must be a rectangular parallelepiped ) and the ai should find the best position for each item such that the space is optimized . here is a list of what i started to do : i asked the user to enter the size of the storage space ( units are trivial here except for the computing cost of the ai later on i 'm guessing ) , telling him that the values will be rounded down to the unit i started by creating a 3-dimensional array of integers representing the storage space 's volume , using the 3 values taken earlier . filling it with 0s , where 0s would later represent free space and 1s occupied space . then , store in another multidimensional array the sizes of the items he has to store and that 's where the ai part should be starting . first thing the ai should do is check whether the addition of all the items ' volumes does n't surpass the storage space 's volume . but then there are so many things to do and so many possibilities that i get lost in my thoughts and do n't know where to start ... in conclusion , can anyone give me the proper terms of this problem in ai literature , as well as a link to an existing work of this kind ? thanks",19155,,,2018-10-19T16:59:44.100,ai that maximizes the storage of rectangular parallelepipeds in a bigger parallelepiped,optimization storage,1,0,
2294,8522,1,,2018-10-19T18:13:35.587,1,43,"for example , consider an agent concerned with predicting the weather , with variable r indicating whether or not it is likely to rain , variable c indicating whether or not it is cloudy , and variable l indicating low pressure . given knowledge base k : l ( pressure is low ) c ( it is cloudy ) c ∧ l ⇒ r , ( clouds and low pressure imply rain ) the agent may conclude r ; thus , the agent ’s knowledge implies that r is true , because k |= r. similarly , given knowledge base l : ¬l ( pressure is high ) c ( it is cloudy ) c ∧ l ⇒ r , ( clouds and low pressure imply rain ) the agent can not conclude that r is true ; l 6|= r deriving a truth table : l c r ( ( l ∧ c ) → r ) f f f t f f t t f t f t f t t t t f f t t f t t t t f f t t t t but this does not make sense .",19164,,,2018-10-20T18:19:25.740,how do i use truth tables to prove entailment ?,ai-basics,0,1,
2295,8523,1,,2018-10-19T18:13:38.887,3,57,"the question , in short , is : what the possibility of emerging of the new branch of the psychology - the psychology of an artificial intelligence ? possibly as a new branch of an engineering psychology , due to the fact that engineering psychology handles the realms of interactions between human and the machine , as well as human factor in general . is the creation of this new field of study really necessary ? if so , what questions it will address and will try to solve ?",7014,,,2019-05-30T10:03:55.613,the possibility of emerging of the psychology of an artificial intelligence,ai-basics strong-ai theory artificial-consciousness,2,0,
2296,8527,1,,2018-10-19T19:53:17.767,1,414,"when am i supposed to update my weights ? after each forward- , and backpropagation ; and or after each completed batch ? furthermore , if i am supposed to update the weights both after each forward- , and backpropagation as well as afte each batch , when am i then supposed to divide by all training examples ?",11814,11814,2018-10-21T15:33:53.673,2018-10-21T15:33:53.673,when are weights updated ? ( feed - forward neural network ),neural-networks feedforward,3,0,
2297,8528,1,8538,2018-10-19T22:18:13.773,1,64,"i am reviewing a statement on the website for es regarding structured exploration . https://blog.openai.com/evolution-strategies/ structured exploration . some rl algorithms ( especially policy gradients ) initialize with random policies , which often manifests as random jitter on spot for a long time . this effect is mitigated in q - learning due to epsilon - greedy policies , where the max operation can cause the agents to perform some consistent action for a while ( e.g. holding down a left arrow ) . this is more likely to do something in a game than if the agent jitters on spot , as is the case with policy gradients . similar to q - learning , es does not suffer from these problems because we can use deterministic policies and achieve consistent exploration . where can i find sources showing that policy gradients initialize with random policies , whereas q - learning uses epsilon - greedy policies ? also , what does "" max operation "" have to do with epsilon - greedy policies ?",19167,,,2018-10-20T20:56:10.753,"some rl algorithms ( especially policy gradients ) initialize with random policies , which often manifests as random jitter on spot for a long time ?",reinforcement-learning evolutionary-algorithms q-learning,1,1,
2298,8533,1,,2018-10-20T11:16:20.773,1,18,"there are several levels of abstraction involved in piloting and driving . signals representing the state of the vehicle and its environment originating from multiple transducers 1 latched sample vectors / matrices boundary events ( locations , spectral features , movement , appearance and disappearance of edges , lines , and sounds ) objects object movements object types ( runways , roads , aircraft , birds , cars , people , pets , screeches , horns , bells , blinking lights , gates , signals , clouds , bridges , trains , buses , towers , antennas , buildings , curbs ) trajectory probabilities based on object movements and types behaviors based on all the above hints intentions based on behavior sequences and specific object recognition collision risk detection moving from interpretation to control execution ... preemptive collision avoidance reaction horn sounding plan adjustment alignment of plan to state trajectory control skid avoidance skid avoidance reaction steering , breaking , and signalling notifications to passengers what , if any , levels of higher abstraction can be sacrificed ? humans , if they are excellent pilots or drivers , can use all of these levels to improve pedestrian and passenger safety and minimize expense in time and money . footnotes [ 1 ] optical detectors , microphones , strain gauge bridges , temperature and pressure gauges , triangulation reply signals , voltmeters , position encoders , key depression switches , flow detectors , altimeters , radar transducers , tachometers , accelerometers",4302,,,2018-10-20T11:16:20.773,to what level of abstraction must fully automated vehicles build their driving model before safety can be maximized ?,path-planning autonomous-vehicles ai-safety collision-avoidance,0,0,
2299,8534,1,,2018-10-20T12:58:12.103,1,53,"i struggle to find rosenblatts perceptron training algorithm in any of his publications from 1967 - 1951 , namely : [ 1 ] principles of neurodynamics : perceptrons and the theory of brain mechanisms [ 2 ] the perceptron : a probabilistic model for information storage and organization in the brain . [ 3 ] the perceptron — a perceiving and recognizing automaton does somebody know where to find the original learning formula ?",19177,1581,2018-10-31T16:34:54.363,2018-10-31T16:34:54.363,first perceptron learning algorithm,neural-networks machine-learning reference-request perceptron,0,0,
2300,8546,1,8557,2018-10-21T07:24:15.437,4,44,suppose that i have a model m that overfits a large dataset s such that the test error is 30 % . does that mean that there will always exist a model that is smaller and less complex than m that will have a test error less than 30 % on s ( and does not overfit s ) .,19190,,,2018-10-21T16:08:07.117,does overfitting imply an upper bound on model size / complexity ?,machine-learning overfitting,1,4,
2301,8548,1,8552,2018-10-21T08:17:40.687,1,90,what is the best and easiest programming language to learn to implement genetic algorithms ? c++ or python or any other ?,19192,9947,2018-10-21T09:34:24.130,2018-10-21T23:15:05.787,learning genetic algorithm for beginners,genetic-algorithms programming-languages genetic-programming,2,0,
2302,8550,1,,2018-10-21T09:15:40.760,6,53,"what are the future prospects in near future from a theoretical investigation of description logics , and modal logics in the context of artificial intelligence research ?",19193,1581,2018-10-30T17:00:49.623,2019-04-28T18:02:56.103,what are the current trends / open questions in logics for knowledge representation ?,logic knowledge-representation,1,0,1
2303,8551,1,,2018-10-21T09:34:21.847,2,61,"i have a data input vector ( no image classification ) which size varys from 2 to 7 entrys . every one of them belongs to a class out of 7 . so i have a variable input size and a variable output size . how can i deal with the variable input sizes ? i know zero padding is a option but maybe there are better ways ? seconds : is multi label classification possible in one network ? what i mean : the first entry has to bei classified in one of the seven classes , the second entry ... and so on . i am also open to other classification techniques , if there is a better one that suits the problem . best regards , gesetzt",19195,,,2018-10-21T09:34:21.847,variable sized input - multi label classification with neural network,neural-networks classification,0,0,0
2304,8553,1,,2018-10-21T12:44:38.280,1,18,i have read quite a lot about capsule networks but can not understand how the squashed vector would also rotate in response to rotation or translation of the image.a simple example would be helpful.i understand how routing by agreement works .,19201,,,2018-10-21T12:44:38.280,how exactly is equivariance achieved in capsule networks ?,machine-learning deep-learning computer-vision,0,0,
2305,8554,1,,2018-10-21T13:35:49.953,1,84,"i am trying to study the book reinforcement learning : an introduction ( sutton & amp ; barto , 2018 ) . in chapter 3.1 the authors state the following exercise exercise 3.5 give a table analogous to that in example 3.3 , but for $ p(s',r|s , a)$ . it should have columns for $ s$ , $ a$ , $ s'$ , $ r$ , and $ p(s',r|s , a)$ , and a row for every 4-tupel for which $ p(s',r|s , a)&gt;0 $ . the following table and graphical representation of the markov decision process is given on the next page . i tried to use $ p(s'\cup r|s , a)=p(s'|s , a)+p(r|s , a)-p(s ' \cap r|s , a)$ but without a significant progress because i think this formula does not make any sense as $ s'$ and $ r$ are not from the same set . how is this exercise supposed to be solved ? edit : maybe this exercise intends to be solved by using $ $ p(s'|s , a)=\sum_{r\in \mathcal{r}}p(s',r|s , a)$$ and $ $ r(s , a , s')=\sum_{r\in \mathcal{r}}r\dfrac{p(s',r|s , a)}{p(s|s , a)}$$ and $ $ the resulting system is a linear system of 30 equation with 48 unknowns . i think i am missing some equations ...",19123,19123,2018-10-21T16:52:48.470,2019-04-16T15:49:36.083,"reinforcement learning ( rl ) how to obtain $ p(s',r|s , a)$",reinforcement-learning probabilistic,2,0,
2306,8559,1,,2018-10-21T16:38:29.787,2,121,"take a look at section 2.2.2 of this book ( from page-15 to 16 ) . 2.2.2 representation and evaluation $ $ max f ( x)= x sin(10πx)+2.0 ... ... ... ( 2.8)$$ $ $ s.t . −1 ≤ x ≤ 2$$ we can use a real number , in the range $ [ −1,2]$ , to represent a solution in eq . $ 2.8 $ directly . many operators can handle real number representation . but we use the binary code or binary representation here for two reasons . gas were originally proposed to be binary code to imitate the genetic encoding of natural organisms . on the other hand , binary code is good for pedagogy . a binary chromosome is necessary to represent a solution x in the scale $ [ −1,2]$ . the same holds for the binary representation of real numbers in a computer . in binary code , we can not represent a real number completely correctly , so a trade - off is necessary . a tolerance needs to be deﬁned by the user , which means the errors below the tolerance are extraneous . if we divide the deﬁnition domain into $ 2 ^ 1 = 2 $ parts evenly and select the smallest number in the parts to represent any number in the division , we can only represent $ −1 $ and $ 0.5 $ by $ 0 $ and $ 1 $ respectively . $ 2 ^ 2 = 4 $ divisions make the $ 00 $ , $ 01 $ , $ 10 $ , and $ 11 $ represent $ −1 $ , $ −0.25 $ , $ 0.5 $ , and $ 1.25 $ , respectively . the larger division number we select , the less error there is in representing a real number on binary code . suppose we use $ 100 $ binary codes to represent a real number in the range $ [ −1,2]$ ; the maximum error is , which would be satisfactory for most users . in this way , we can represent a real number with any accuracy requirements . in this problem , we use $ l = 12 $ binary codes to represent one real number as follows , which constitutes a chromosome to be evolved . actually , i have n't understood this text . i know that binary numbers are already able to represent fractions . so , what are they talking about ?",,,2018-10-21T17:42:47.167,2019-02-26T23:03:30.740,representation of real numbers in genetic algorithm,genetic-algorithms,2,0,
2307,8560,1,,2018-10-21T17:09:04.533,1,1299,"batch size is a term used in machine learning and refers to the number of training examples utilised in one iteration . the batch size can be one of three options : batch mode : where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent mini - batch mode : where the batch size is greater than one but less than the total dataset size . usually , a number that can be divided into the total dataset size . stochastic mode : where the batch size is equal to one . therefore the gradient and the neural network parameters are updated after each sample . how do i go about choosing the optimal batch size for my network ? if you hypothetically did n't have to worry about computational issues , what would the optimal batch size be in this case ?",11814,11814,2018-10-21T17:59:53.440,2018-10-23T05:16:14.437,batch mode vs mini - batch mode vs stochastic mode,neural-networks machine-learning gradient-descent,3,0,3
2308,8567,1,,2018-10-22T00:26:09.770,1,36,"suppose there are sensors which supply numerical metrics . if a metric goes above or below healthy threshold , an event ( alert ) is raised . metrics depend on each other in one way or another ( we can learn the dependencies via ml algorithms ) so when the system is in alerting state only one or a few metrics will be a root cause and all others will be simply consequences . we can assume there is enough historical metric data available , to learn dependencies but there are just a few historical malfunctions . also , when malfunction happens there is no one to tell what was the root cause , the algorithm should learn how to detect root causes by itself . which algorithms can be used to detect root cause event in the situation above ? are there any papers available on the subject ?",19218,,,2018-10-22T00:26:09.770,detect root cause across many event occurrences,machine-learning,0,1,
2309,8569,1,,2018-10-22T01:27:58.513,0,35,"( maybe related : usefulness of dropout for non - overfitting network ) my neural network does not overfit . using data augmentation in a non - overfitting network can increase its accuracy ? note : i 'm asking this question in the nlp area , where data augmentation is not trivial . i 'm thinking about back - translation . the augmented data might not be of similar quality , which can hurt the accuracy ( i guess ? ) .",18852,1581,2018-11-01T17:42:18.447,2018-11-01T17:42:18.447,usefulness of data augmentation for non - overfitting network [ nlp ],natural-language-processing datasets,0,0,
2310,8570,1,8574,2018-10-22T05:15:20.537,3,147,why in every aspect are we now considering neural networks as an artificially intelligent entity / program ?,19220,1581,2018-10-22T19:54:03.040,2018-10-22T19:54:03.040,why are neural networks considered artificially intelligent ?,neural-networks research artificial-consciousness,1,0,
2311,8578,1,,2018-10-22T09:31:20.077,3,82,"the levenshtein algorithm and some ratio and proportion may handle this use case . based on the pre - defined sequence of statements , such as "" i have a dog "" , "" i own a car "" and many more , i must determine if an another input statement such as "" i have a cat "" is the same or how much percentage does the input statement is most likely equal to the pre - defined statements . for example : predefined statements : "" i have a dog "" , "" i own a car "" , "" you think you are smart "" input statements and results : i have a dog - 100 % ( because it has exact match ) , i have a cat - ~75 % ( because it was almost the same except for the animal , think - ~10 % ( because it was just a small part of the third statement ) , bottle - 0 % ( because it has no match at all ) the requirement is that tensorflow be used rather than java , which is the language i know , so any help with what to look at to get started would be helpful . my plan was to use the predefined statements as the train_data , and to output only the accuracy during the prediction , but i do n't know what model to use . please , guide me with the architecture and i will try to implement it .",19229,2444,2019-04-25T10:21:19.467,2019-04-25T10:21:19.467,which model should i use to determine the similarity between predefined sentences and new sentences ?,machine-learning natural-language-processing tensorflow,1,0,
2312,8580,1,,2018-10-22T13:29:12.913,1,33,"consider i have a 3 layers neural network . input layer containing 784 neurons . hidden layer containing 100 neurons . output layer containing 10 neurons . my objective is to make an ocr and i used mnist data to train my network . suppose i gave the network an input taken from an image , and the values from the output neurons are the next : $ 0 : 0.0001 $ $ 1 : 0.0001 $ $ 2 : 0.0001 $ $ 3 : 0.1015 $ $ 4 : 0.0001 $ $ 5 : 0.0002 $ $ 6 : 0.0001 $ $ 7 : 0.0009 $ $ 8 : 0.001 $ $ 9 : 0.051 $ when the network returns this output , my program will tell me that he identified the image as number 3 . now by looking at the values , even though the network recognized the image as 3 , the output value of number 3 was actually very low : $ 0.1015 $ . i am saying very low , because usually the highest value of the classified index is as close as 1.0 , so we get the value as 0.99xxx . may i assume that the network failed to classify the image , or may i say that the network classified the image as 3 , but due to the low value , the network is not certain ? am i right thinking like this , or did i misunderstand how does the output actually works ?",7983,7983,2018-10-22T13:34:53.473,2018-12-21T19:02:15.453,how much extra information can we conclude from a neural network output values ?,neural-networks ocr,1,0,
2313,8586,1,,2018-10-23T07:48:10.637,4,316,"i am training a deep neural network . there is a constraint on an output value of the network . ( e.g. output has to be between 0 and 180 ) i think some possible solutions are using sigmoid , tanh activation at the end of the layer . i wonder if there are better ways to put constraints on the output value of a neural network .",19245,10135,2018-10-28T18:42:20.733,2018-10-30T01:33:45.730,putting constraints on output of deep neural network,neural-networks deep-learning keras,1,1,
2314,8588,1,8657,2018-10-23T13:28:43.277,2,131,"i am creating a dataset made of many images which are created by preprocessing a long time series . each image is an array of ( 128,128 ) and the there are four classes . i would like to build a dataset similar to the mnist in scikit-learn.database but i have no idea how to do it . my aim is to have something that i can call like this : ( x_train , y_train ) , ( x_test , y_test ) = my_data ( ) should i save them as figures ? or as csv ? which is the best way to implement this ?",19251,,,2018-10-28T22:58:38.247,best way to create an image dataset for cnn,convolutional-neural-networks datasets,1,0,
2315,8591,1,,2018-10-23T22:22:35.900,2,62,"i am developing an image search engine . the engine is meant to retrieve wrist watches based on the input of the user . i am using sift descriptors to index the elements in the database and applying euclidean distance to get the most similar watches . i feel like this type of descriptor is not the best since watches have a similar structure and shape . right now , the average difference between the best and worst matches is not big enough ( 15 % ) i 've been thinking of adding colour to the descriptor , but i 'd like to hear other suggestions .",19266,2444,2019-04-23T23:08:14.800,2019-05-24T00:02:55.470,what is a good descriptor for similar objects ?,image-recognition computer-vision feature-selection,1,1,
2316,8594,1,,2018-10-24T06:48:54.563,2,192,"the intelligence of the human brain is said to be a strong factor leading to human survival . the human brain functions as overseer for many functions the organism requires . like that , the most important thing behind artificial intelligence is computers . robots can employ artificial intelligence systems , just as humans employ brains . when it comes to the human brain , we are prone to make mistakes . where as artificial intelligence is sometimes represented to the public as perfect . what is true about ai and why ? why was our brain not created or evolved in a way that it does not make any errors ? we tend to make mistakes . why ?",19275,4302,2018-11-28T23:01:18.090,2018-11-28T23:01:18.090,do intelligent systems make mistakes ?,philosophy genetic-algorithms reliability fallibility,3,8,
2317,8595,1,,2018-10-24T07:21:53.587,4,206,can artificial intelligence be hacked or not by using enough computing skills or by other cryptography methods ? so are we able to say that ai is one of in its secured form ?,19220,1671,2018-10-24T20:02:08.100,2018-10-24T22:44:26.587,can artificial intelligence be hacked or not ?,ai-design ai-basics security,3,9,2
2318,8596,1,,2018-10-24T08:45:56.553,1,35,"assume that we have a labeled dataset with inputs and outputs , where the output range is , but the majority of outputs is in . should one adopt some kind of over- or undersampling approach after compartmentalising the output space to make the dataset more balanced ? that would usually be done in classification , but does it apply to regression problems , too ? thanks in advance !",16901,16901,2018-10-24T09:41:14.460,2018-10-24T09:41:14.460,unbalanced dataset in regression rather than classification,machine-learning,0,0,
2319,8597,1,,2018-10-24T10:32:36.837,1,149,"i am writing an app , where when a ball is shot from a canon it is supposed to land in a hole which is on a given distance . the ball is supposed to land between the distance of the begining of the hole and the end of the hole . the size of the hole is 4 m and the size of the ball is 0.4 m . my problem is that i am not sure how to write the fitness function for this . the place where the ball falls should be close to this interval of [ d , d+3.6 ] , where d is the distance of the hole . if anyone could give me a hint on how to approach this problem , i would be grateful .",19283,,,2018-10-24T15:07:37.783,fitness function in genetic algorithm based on an interval,genetic-algorithms java fitness-functions,1,2,
2320,8604,1,,2018-10-24T17:37:16.983,1,30,"i understand the concept of convolution . let 's say that my input dimension is 3 x 10 x 10 and if i say that i will have 20 activation maps and a filter size of 5 , i will end up with 20 different filters for my layer , each with the dimension of ( 3 x 5 x 5 ) my output will therefor be ( 20 x ? x ? ) . i put a "" ? "" there , because it obviously depends on the filter stride etc . now i wanted to implement deconvolution but i am stuck at the following point : for the following questions , let 's assume that the input size for the deconvolution is ( 5 x 8 x 8) , if we think about a filter in 3 dimensions . can i choose any depth for the filter ? how would the effect of the amount of filters ( amount of activation maps ) work with deconvolution ? do i only have one filter ? how does the input depth ( 5 ) come into play . would the output depth be equal to ( filter depth ) * ( input depth ) ? i am trying to find the symmetry to forward convolution but i do not understand how to use the amount of activation maps in deconvolution . i am very thankful for any help .",16353,,,2018-10-24T17:37:16.983,neural network deconvolution filters,convolutional-neural-networks,0,1,
2321,8605,1,8636,2018-10-24T18:29:00.283,2,282,"i am applying a double dqn algorithm to a highly stochastic environment where some of the actions in the agent 's action space have very similar "" true "" q - values ( i.e. the expected future reward from either of these actions in the current state is very close ) . how can i still ensure that the algorithm gets these values ( and their relative ranking ) right ? edit 1 to provide a little more info : what happens currently is that the loss function on the q - estimator decreases rapidly in the beginning , but then starts evening out . the q - values also first converge quickly , but then start fluctuating around . i 've tried increasing the batch size which i feel has helped a bit . what did not really help , however , was decreasing the learning rate parameter in the loss function optimizer . which other steps might be helpful in this situation ? edit 2 if this question is misleading ( since reinforcement learning is an approximate dynamic programming method ) , please just let me know . i mean the algorithm usually does find an only slightly sub - optimal solution to the mdp . edit 3 to answer some questions brought up in the comments : ( 1 ) i do indeed have full control over the mdp including the reward function which in my case is sparse ( 0 until the terminal episode ) . the "" true "" q - values i know from an analytical solution to the problem i am trying to solve using reinforcement learning . ( 2 ) the rewards are the same for identical transitions . however , the rewards vary for any given state and action taken therein . ( 3 ) the environment is only stochastic for a part of the actions in the action space . i.e. the action chosen by the agent influences the stochasticity of the rewards .",18109,18109,2018-10-27T17:04:09.417,2018-10-27T18:22:29.393,ensure convergence of ddqn if true q - values are very close,reinforcement-learning dqn,1,2,
2322,8607,1,8632,2018-10-24T20:05:07.130,5,968,"i am training a multilayer neural nets with 146 samples ( 97 for training set , 20 for validation set and 29 for testing set ) . i am using : automatic differentiation , sgd method , fixed learning rate + momentum term , logistic function , quadratic cost function , l1 and l2 regularization technique , adding some artificial noise 3 % . when i used l1 or l2 regularization technique my problem ( overfitting problem ) got worst . i tried different values for lambdas ( the penalty parameter 0.0001 , 0.001 , 0.01 , 0.1 , 1.0 and 5.0 ) . after 0.1 i just killed my ann . the best result that i took it was using 0.001 ( but it is worst comparing the one that i did nt use regularization technique ) . the graph represent the error functions for different penalty parameters and also a case without using l1 . and the accuracy what can be ? thanks ! !",19268,,,2018-10-26T15:28:36.860,why l1 / l2 regularization technique did not improve my accuracy ?,neural-networks deep-learning training overfitting regularization,1,5,
2323,8609,1,,2018-10-24T22:46:13.930,1,125,"i am so much curious about how do we see(with eyes ofc ) and detect things and their location so quick . is the reason that we have huge gigantic network in our brain and we are trained since birth to till now and still training . basically i am saying , are we trained on more data and huge network ? is that the reason ? or what if there 's a pattern for about how do we see and detect object . please help me out , maybe my thinking is in wrong direction . what i wanna achieve is an ai to detect object in picture in human ways.thanks .",18459,,,2018-11-07T16:21:49.540,can we make object detection as human eyes+brain do ?,human-like human-inspired,2,1,
2324,8610,1,,2018-10-24T23:39:02.690,1,15,"i am preparing the bus movement dataset for deep learning ( ann / cnn / rnn ) analysis for congestion events detection . this is an extension to my original question , which can be located at ' deep learning model training and processing requirement for traffic data ' for the general approach on this topic , and this question is for preparing the dataset and need your kind advice on it . in simple words , i would like to know the state of congestion for a bus route at a specific point in time ( year ) . here are my entities : routes bus_scheduled_routes bus_route_stops bus_trips ( operational_date , vehicle_id , trip_id , vehicle_position_update , trip_stop_id , passenger_loaded , velocity , direction , scheduled_arrival_time , actual_arrival_time ) events ( human and non - human induced ) points of interests ( pois ) if i have these entities based data and i create a view that gives me a time reference based view comprising of week(52 ) , day(7 ) , vehicle_id , trip_id , stop / position_update_interval , speed , acceleration , velocity , scheduled_arrival_time , actual_arrival_time . will this view be recommended to start training the model ? secondly , how can i integrate the human / non - human induced events and points of interests ( pois ) data into this view so my model can predict better results ? to generalize the model data will be ' time segment / trips time ( seasons ) , location component ( bus routes and stops ) , arrival time / trip completion time ' . i am thinking to add an attribute for human / non - human induced events as type tying with the ' time segment ' and adding the pois as type and vicinity to the stop points . what are your recommendation about it ? thanks in advance for your help .",18959,,,2018-10-24T23:39:02.690,data / model preparation for spatio - temporal deep - learning analysis for traffic congestion events detection,deep-learning time,0,0,
2325,8611,1,,2018-10-24T23:42:44.743,4,295,"fuzzy logic seemed like an active area of research in machine learning and data mining back when i was in grad school ( early 2000s ) . fuzzy inference systems , fuzzy c - means , fuzzy versions of the various neural network and support vector machine architectures were all being taught in grad courses and discussed in conferences . since i 've started paying attention to ml again ( ~2013 ) , fuzzy logic seems to have dropped off the map completely and its absence from the current ml landscape is conspicuous given all the ai hype . was this a case of a topic simply falling out of fashion , or was there a specific limitation of fuzzy logic and fuzzy inference that led to the topic being abandoned by researchers ?",2306,,,2018-10-26T17:15:20.210,why did fuzzy logic fall out of fashion ?,machine-learning fuzzy-logic,2,1,1
2326,8613,1,,2018-10-25T07:50:28.110,1,303,"i saw when browsing we can use data augmentation for creating a dataset for face recognition . the augmented images may include inverted , tilted or distorted faces . do the model detect the face from the inverted image . when i tried my model ca nt able to detect any inverted or tilted faces .",17058,17058,2018-10-26T06:32:22.170,2018-10-26T07:40:16.527,how can we use data augmentation for creating data set for face recognition and will the inverted faces on augmented images detected ?,image-recognition facial-recognition,1,2,
2327,8615,1,,2018-10-25T10:12:34.790,2,61,which possibilities exist to evaluate the visual reasoning capabilities of neural networks in the field of image recognition ? are there methods to measure the ability of machine reasoning ? or something more specific : is it possible to measure if a network understood the concept of a car / a cat / a human without using the classification accuracy .,16634,17221,2018-10-26T19:14:17.280,2018-10-26T21:43:52.173,how to measure the reasoning capabilities of neural networks,convolutional-neural-networks image-recognition reasoning,1,3,2
2328,8616,1,,2018-10-25T13:28:23.067,2,40,"i loaded a neural network model trained with caffe by other people in opencv . the model should detect the presence of a car in a single parking spot outputting the probability of it being free / occupied . the model was trained with images all belonging to the same parking area , taken at different hours of day and with different light conditions . images were taken by different cameras but the cameras are all of the same model ( raspberry cameras ) . i tried to run the model with a few images some of them taken from their dataset and other downloaded from google . the images taken from their dataset are correctly classified while the ones taken from google are not correctly classified . my question is : is it possible to deploy a nn model trained with images all coming from a single parking area in another parking area ? is not such a model for parking detection occupancy supposed to generalize independently from the location where training images have been taken ? if you know about an already existing trained model that works good please let me know .",12959,12959,2018-10-26T07:54:35.637,2018-10-26T07:54:35.637,influence of location on a neural network trained for parking detection occupancy,neural-networks image-recognition classification training,1,2,
2329,8619,1,,2018-10-25T16:53:16.913,8,208,"one of the corner stones of the selfish gene ( dawkins ) is the spontaneous emergence of replicators , i.e. molecules capable of replicating themselves . has this been modeled in silico in open - ended evolutionary / artificial life simulations ? systems like avida or tierra explicitly specify the replication mechanisms ; other genetic algorithm / genetic programming systems explicitly search for the replication mechanisms ( e.g. to simplify the von neumann universal constructor ) links to simulations where replicators emerge from a primordial digital soup are welcome .",19319,1671,2018-10-29T00:53:39.447,2018-12-02T02:00:55.160,spontaneous emergence of replicators in artificial life,genetic-algorithms theory genetic-programming self-replication,4,8,2
2330,8620,1,,2018-10-25T17:02:23.657,3,338,"i am trying to figure out how to use a reinforcement learning algorithm , if possible , as a "" black box "" to play a game . in this game , a player has to avoid flying birds . if he wants to move , he has to move the mouse on the display , which controls the player position by applying a force . the player can choose any position on the display for the mouse . a human can play this game . to get a sense what needs to be done , have a look at this youtube video . i thought about using an ann , which takes as input the information of the game ( e.g. positions , speeds , radius , etc . ) and outputs a move . however , i am unlikely to ever record enough training data to train the network properly . therefore , i was thinking that a rl algorithm , like q - learning , would be more suited for this task . however , i have no clue how to apply q - learning to this task . for example , how could the coder possibly know what future reward another move will bring ? i have a few questions : in this case , the player has infinitely many actions . how would i apply rl to this case ? is rl a good approach to solving this task ? is there a handy java library which would allow me to use a rl algorithm ( as a block - box ) to solve this problem ? are there alternatives ?",19062,2444,2019-02-15T16:41:41.513,2019-02-15T16:53:25.833,how do i apply reinforcement learning to a game with infinitely many actions ?,reinforcement-learning game-ai applications java,1,4,1
2331,8624,1,,2018-10-25T21:56:45.493,2,46,"the bacon algorithm , introduced by pat langley and herbert simon etc . were meant to automate scientific discovery -- producing causal explanation to variations in given data . it was found , in particular , to have been able to "" discover "" the laws of planetary motion , but there were criticism concerning whether or not its achievement could be consider true scientific discovery . the one i remember was that -- the data given to the algorithm are pre - processed by human operators so as to include mostly relevant factors of the world . for example , the distances of planets from the sun was given , showing that the human operators implicitly understood the importance of this variable for the laws of planetary motion . i was wondering if there were other significant criticism of the algorithm -- either in general as a automaton of discovery or concerning its particular feat with planetary motion .",1321,,,2018-10-25T21:56:45.493,what were the criticisms of the bacon algorithm ?,cognitive-science automation,0,1,
2332,8625,1,,2018-10-26T01:45:21.197,1,126,"i have been successfully figured out how the minimax algorithm works for a game like chess . were by a game tree is used and you assign a value to the terminal nodes and propagate that value up the tree . but checking if its the maximizer player turn or the minimizing players turn . my question is , is there a way to represent this algorithm mathematically ? if so how would i go about showing it ?",16906,,,2018-10-26T05:59:07.247,minimax algorithm,chess minimax,1,0,
2333,8626,1,8630,2018-10-26T05:35:11.980,2,962,"i 've to train a neural network using microphone data ( wav files ) , accelerometer sensor data and light sensor data . right now the approach i thought was to convert all data into images and combine them into a single image and train my neural network . another approach was to convert wav files into arrays and combine them along with sensor data and train my neural net . are my approaches correct or is there a better way to do this ? any suggestions / ideas are welcome .",19332,,,2018-10-26T12:36:09.363,how to combine input from different types of data sources ?,neural-networks deep-learning convolutional-neural-networks ai-design audio-processing,1,0,
2334,8631,1,,2018-10-26T13:14:25.750,1,27,"it is said that the number of possible sequences of game play in the game go is greater than the number of atoms in the universe . whether or not that is true , imagine the number of possible sequences of risks , starts , stops , turns , or other events or actions that can occur when driving to the store . if one shops at a 24-hour store at 3 am , the number of conditions are small . if one shops in a city at 5 pm , the driving sequence set may be far larger than the go game sequence set . more importantly , when go is misplayed , no dog is run over , no child is killed , and no passenger is carted away in an ambulance . we have , for automated vehicle control systems , the goal of recognizing risk and responding appropriately in action . the moves are not timed . a wide array of unpredictable circumstances can emerge at any time . when ai designers first approach this situation , a basic design choice must be made between two polar opposite approaches or some midpoint . a sequence of serially arranged system components and layers that recognize all the elements of these astronomically varied conditions , recognizes the combinations of the elements that present risk , and decide what to do with horn , breaks , steering , clutch , transmission , and fuel rate . ( fuel rate is what the accelerator pedal approximately controls . ) an array of system components that read the same sensor input and each optimally detect a particular class of risks . each parallel detector would then need to provide to a central system a set of system responses intended to avoid those risks based on each component 's learned responses . prioritization would follow in a more centralized component so that the correct decisions can be regarding horn , breaks , and the others . here are a few important external events for an automated vehicle system to recognize . each is given a letters which , if the second choice above is chosen , may or may not be a reasonable division of responsibility for separate independent parallel detector components . trajectories of the vehicle being controlled & mdash ; a trajectories of pedestrians & mdash ; b trajectories of dogs and cats & mdash ; b trajectories of wheeled vehicles & mdash ; c trajectories of trains & mdash ; c trajectories of balls & mdash ; c trajectories of utos ( unidentified terrestrial objects & mdash ; d locations of stationary objects in the road & mdash ; e locations of road endings & mdash ; e locations of curbs & mdash ; e locations of bridge abutments & mdash ; e locations of rocks of size greater than 2 cm & mdash ; e traffic signs & mdash ; f traffic lights & mdash ; f train signals & mdash ; f locations of wire gates & mdash ; g states of horizontal raise - able gates & mdash ; h indications of many pedestrians & mdash ; i is it best to train for trajectories of type b with the same system as training for signal recognition of type f ? will the saving of additional components be more important than specialization ? will such generalizations work well for the full array of surprises that may occur during real routes of vehicles ? or is the diversity in system architecture a better a better choice for an av embedded ai system ? in support of either position , why ?",4302,,,2018-10-26T13:14:25.750,recognition and response generalizations for autonomous vehicles or not ?,ai-design topology autonomous-vehicles action-recognition embedded-design,0,0,
2335,8637,1,8642,2018-10-27T13:19:19.173,2,842,"i 'm new to ai and and would like to understand the difference between learning agents and other types of agents , in what ways learning agents can be applied , and if they differ from deep learning .",18950,1671,2018-10-27T22:05:48.027,2018-10-29T00:14:58.720,what is a learning agent and how are they applied ?,deep-learning ai-basics intelligent-agent theory applications,2,0,1
2336,8640,1,,2018-10-27T15:45:28.137,4,39,"this is my first post here . our problem setting : we have to do a binary classification of data given a training - dataset d , where the majority of items belongs to class a and some items belong to class b. the classes are heavily imbalanced . our approach : we wanted to use a gan to produce more samples of class b so that our final classification model has a nearly balanced set to train . our problem : let‘s say that the data from both class a and class b are very similar . given that we want to produce synthetic class - b - samples with the gan , we feed real class - b - examples into the discriminator alongside with generated samples . but wait .. a and b are similar . it could happen that the generator produces an item x , that would naturally belong to class a. but since the discriminator has never seen a class - a - item before and both classes are very close , the discriminator could say that this item x is part of the original data that was fed into the discriminator . so the generator successfully fooled the discriminator in believing that an item x is part of the original data of class b , while x is actually part of class a. if the gan keeps producing items like this , the produced data is useless since it would add heavy noise to the original data , if combined . at the same time , let‘s say before we start training the generator , we show the discriminator our class a and class b samples while giving information , that the class - a - items are not part of class b ( through backprop ) . the discriminator would learn to reject class - a - items that are fed to it . but wouldn‘t this mean that the discriminator has just become the classification model we wanted to build in the first place to distinguish between class a and class b ? do you know any solution to the above stated problem or can you refer to some paper / other posts on this ? thanks a lot !",19378,19378,2018-10-27T16:51:20.350,2018-12-02T03:02:20.387,class restriction in generative adversarial networks,neural-networks machine-learning classification generative-model generative-adversarial-networks,1,4,2
2337,8641,1,,2018-10-27T19:17:15.913,1,104,i 'm in high school but hoping to have a career in artificial intelligence . what should i be pursuing educationally to get into this field ?,19382,1671,2018-10-27T21:22:46.437,2019-05-27T14:20:50.943,what are the skills and disciplines i need to learn to get a job in artificial intelligence ?,ai-basics getting-started math academia,0,9,
2338,8646,1,,2018-10-28T01:27:58.637,1,22,"i am interested in creating the neural network architecture described in this recent paper . however , i am not sure how to get started . does tflearn or keras allow us to wire the lstm in such ways ?",19167,,,2018-10-28T01:27:58.637,how to wire up bi - directional lstms ?,lstm,0,1,
2339,8650,1,,2018-10-28T09:37:14.010,4,42,"in error based learning using gradient descent , if i give you a training dataset then can you find the minimum error after training ? and the minimum error should be true for all architectures of neural network . consider you will use mse for calculating the error . and you can choose anything you want other than my specified condition . it 's like no matter how you change your network you can never cross the limit .",18459,,,2018-10-28T09:37:14.010,is there a limit of minimum error for a particular training dataset in artificial neural network ?,neural-networks,0,1,1
2340,8651,1,8655,2018-10-28T13:30:51.453,7,854,"what is main difference between goal - based agent and utility - based agent ? please , give a real world example .",18950,2444,2018-10-29T19:47:41.220,2018-10-31T21:20:37.183,what is the difference between goal - based agent and utility - based agent ?,ai-basics intelligent-agent concepts goal-based utility-based,2,2,3
2341,8653,1,,2018-10-28T15:29:07.570,1,25,"the nvidia 2080 gpus command a $ 500 premium for 3 gb of incremental ram ( 8 gb - > 11 gb ) . what are the relevant questions and thought process to determine the incremental improvement by moving from the 8 gb to the 11 gb gpu ? assume these contexts for image segmentation ( people counting ) matlab tensorflow i am inclined to think that the platform affects the ram evaluation , however , i can not be certain . update thanks for the good responses that sharpen the question . updates to questions are provided below . the plan is to train an image segmenter with recorded images and try different algorithms ( yolo r - cnn etc . ) . the end goal is to develop a model that will process a live stream . i am in the planning stage and expect to build ( if not buy ) a gpu dl server .",18819,18819,2018-10-29T02:29:33.793,2018-10-29T02:29:33.793,evaluating utility of incremental ram,tensorflow matlab hardware-evaluation,0,3,
2342,8658,1,8772,2018-10-28T23:25:41.620,3,86,"in one of his lectures levine describes the objective of reinforcement learning as : $ $ j(\tau ) = e_{\tau\sim p_\theta(\tau)}[r(\tau)]$$ where refers to a single trajectory and $ p_\theta(\tau)$ is the probability of having taken that trajectory so that $ p_\theta(\tau ) = p(s_1)\prod_{t = i}^t \pi_{\theta}(a_t , s_t)p(s_{t+1}|s_t , a_t))$ . starting from this definition , he writes the objective as $ j(\tau ) = \sum_{t=1}^t e_{(s_t , a_t)\sim p_\theta(\tau)}[r(s_t , a_t)]$ and argues that this sum can be decomposed by using conditional expectations , so that it becomes : $ $ j(\tau ) = e_{s_1 \sim p(s_1)}[e_{a_1 \sim \pi(a_1|s_1)}[r(s_1 , a_1 ) + e_{s_2 \sim p(s_2|s_1 , a_1)}[e_{a_2 \sim \pi(a_2|s_2)}[r(s_2 , a_2 ) ] + ... |s_2]|s_1,a_1]|s_1]]$$ can anyone explain this last step ? i guess the law of total expectation is involved , but i can not figure out how exactly .",19212,,,2018-12-07T07:32:35.470,reinforcement learning objective as conditional expectations,machine-learning reinforcement-learning statistical-ai,2,1,1
2343,8659,1,,2018-10-29T00:03:36.203,0,10,"i 'm not sure this question is in the right stack exchange website , but i thought this place is a good starting point , because of the experience of users . this question is not about artificial intelligence , but about time - management . i am a new machine learning engineer , and i am working with neural networks . however , i have difficulties to manage my time : the training time of my neurals networks are long ( an hour for the simplest model , up to more than 24 hours ) . i find it difficult to find good hyperparameters in these conditions . another problem is that often , i find myself having nothing to do while training time . i am looking for methods / advices to optimize my working time considering the training time for my network can be very long . what i have tried : organizing my work so i have to do all the little tasks ( code cleaning , documentation , etc ... ) while training . use this free - time to learn more . meanwhile , implement other models , to use as a baseline .",18852,,,2018-10-29T00:03:36.203,time - management while training network : need advices,neural-networks training time,0,0,
2344,8662,1,,2018-10-29T02:52:21.983,1,39,"i am working on a rl project , but got stuck at one point : the task is continuous ( non - episodic ) . following some suggestion from sutton 's rl book , i am using a value function approximation method with average reward ( differential return instead of discount return ) . for some state ( represented by some features ) , only one action is legal . i am not sure how to design reward for such action . is it ok to just assign the reward in the previous step ? could anyone tell me the best way to decide the reward for the only legal action . thank you ! update : to give more details , i added one simplified example : let me explain this by a simplified example : the state space consists of a job queue with fix size and a single server . the queue state is represented by the duration of jobs and the server state is represented by the time left to finish the current running job . when the queue is not full and the server is idle , the agent can schedule a job to server for execution and see a state transition(taking next job into queue ) or the agent can take next job into queue . but when the job queue is full and server is still running a job , the agent can do nothing except take a blocking action and witness a state transit ( time left to finish running job gets decreased by one unit time ) . the blocking action is the only action that the agent can take in that state .",19405,19405,2018-10-29T03:44:29.427,2018-10-29T03:44:29.427,how to design the reward for an action which is the only legal action at some state,reinforcement-learning,0,1,2
2345,8664,1,,2018-10-29T15:02:46.970,1,33,"i 'm a bit confused about this . assume i have a cnn network with two branches : top bottom the top branch outputs a feature vector of shape 1x1x1x10 ( batch , h , w , c ) the bottom branch outputs a feature vector of shape ( 1 , 10 , 10 , 10 ) . i want to use the top feature vector as a convolutional filter , and convolve it with the bottom feature vector . i can do this in pytorch with the "" functional.conv2d "" function , the problem is , i do n't know how back - prop works in this case ( will it be unstable ? ) since the output feature is a now a parameter as well , do i need to stop gradients or do something else in this case to backprop correctly ?",19408,,,2018-10-29T15:02:46.970,using features extracted from a cnn as convolutional filter,convolutional-neural-networks backpropagation,0,0,
2346,8667,1,8673,2018-10-29T16:09:10.950,1,58,"how do i define a reward function for my pomdp model ? in literature it is common to use one simple number as a reward , but i am not sure if this is really how you define a function . because this way you have do define a reward for every possible action - state combination . i think that the examples in literature might be not practical in reality , but only for purpose of explanation .",19413,1847,2018-10-29T18:36:10.410,2018-10-30T16:33:05.633,how to define reward function in pomdp,reinforcement-learning,2,0,
2347,8668,1,,2018-10-29T16:50:36.387,4,105,"i need to be able to detect and track humans from all angles , especially above . there are , obviously , quite a few well - studied models for human detection and tracking , usually as part of general - purpose object detection , but i have n't been able to find any information that explicitly works for tracking humans from above .",19414,,,2019-05-03T07:06:38.700,are there any pretrained models for human recognition from all angles ?,object-recognition models,2,4,
2348,8676,1,,2018-10-29T21:33:51.103,1,205,"i just wanted to confirm that my understanding of the different markov decision processes are correct , because they are the fundamentals of reinforcement learning . also , i read a few literature sources , and some are not consistent with each other . what makes most sense to me is listed below : markov decision process all the states of the environment is known so the agent has all the information required to make the optimal decision in every state . we can basically assume that the current state has all information about the current state and all the previous states ( i.e. , the markov property ) semi markov decision process the agent has enough information to make decisions based on the current state . however , the actions of the agent may take a long time to complete , and may not be completed in the next time step . therefore , the feedback and learning portion should wait until the the action is completed before being evaluated . because the action takes many time steps , "" mini rewards "" obtained from those time steps should also be summed up . example : boiling water . state 1 : water is at 23 c action 1 : agent sets stove top at 200 c reward ( 30 seconds after , when water started to boil ) : +1 for the fact that water boiled in the end , but -0.1 reward for each second it took for the water to start boiling . so total reward was -1.9 ( -2.9 because the water did not boil for 29 seconds , then +1 for water boiling on 30th second ) partially observable markov decision process the agent does not have all information regarding the current state , and has only an observation , which is a subset of all the information in a given state . therefore , it is impossible for the agent to truly behave optimally because of lack of information . one way to solve this is to use belief states , or to use rnns to try to remember previous states to make better judgements on future actions ( i.e. , we may need states from the previous 10 time steps to know exactly what 's going on currently ) . example : we are in a room that is pitch black . at any time instant , we do not know exactly where we are , and if we only take our current state , we would have no idea . however , if we remember that we took 3 steps forward already , we have a much better idea of where we are . are my above explanations correct ? and if so , is n't it possible to also have partially observable semi markov decision processes ?",17706,2444,2019-02-16T02:25:02.007,2019-02-16T02:25:02.007,"is my understanding of the differences between mdp , semi mdp and pomdp correct ?",reinforcement-learning definitions mdp pomdp semi-mdp,0,4,
2349,8680,1,,2018-10-30T10:39:08.117,1,31,"i would like to create an npc engine for games which are in a fantasy context like lord of the rings , warcraft , skyrim , dragon age etc . this engine should be able to determine the polarity of the given sentence ( positive / negative / neutral ) and should also be able to tag the words ( part - of - speech tagging , but that 's not relevant now ) . the problem is that i can not find a dataset which i can teach my ai with , so that fits into the fantasy context . as an example for lord of the rings , there are characters like sauron which are negative , but there 's a ton of expressions too in a fantasy world which can not be found in a general dataset . therefore , i would like to ask if you could suggest to take dataset(s ) which contain expressions , even better names in a fantasy context . it does n't matter which fantasy universe does it take . thank you !",19441,19441,2018-10-30T10:57:50.280,2018-11-03T02:20:27.260,"searching for dataset in specifix context for nlp , sentiment analysis",natural-language-processing datasets sentiment-analysis,1,3,
2350,8681,1,8688,2018-10-30T12:57:21.523,1,2651,greedy best first search algorithm take the history and then check value ans then reach to goal . in a * search algorithm take history and also cost then calculate value then reach to goal . when we use greedy best first search algorithm then start from initial then check value that taken by history then go to next node and so on . when using a * search algorithm then start from initial then calculate value by adding cost and history value then check value and no next . in this we go back also if we see that in first level there is a small value then all node in first level or sec or so on . my question : am confuse that we call a * search is best then greedy best first search when we use a * search then if we go back then we stuck in loop so how we say its best then greedy,19442,1671,2018-11-13T22:36:23.803,2018-11-13T22:36:23.803,greedy best first search algorithm vs a * search algorithm,algorithm ai-basics terminology search concepts,2,4,1
2351,8682,1,8684,2018-10-30T13:02:59.023,-1,57,"i was using pca in my whole dataset ( and after split to training , validation and test ) , but after some researchs i found out that is wrong way to do . then i have few questions : -are there some articles / references explain why is the wrong way ? -how can i transform the validation / test set ? steps to do pca ( from https://www.sciencedirect.com/science/article/pii/s0022460x0093390x ): zero mean $ $ where x is my training set centering ( variance ) $ $ s^{2 } = \frac{1}{m}\sum_{i=1}^{m } ( x_{i}-\mu)^{t}(x_{i}-\mu)$$ use ( 1 ) and ( 2 ) to transform my original training dataset $ $ x_{new } = \frac{1}{\sqrt{m } } \frac{(x_{i } - \mu)}{s}$$ calculate covariance matrix ( actually correlation matrix ) $ $ c= x_{new}^t x_{new}$$ take the k -eigenvectors ( /phi ) from covariance matrix and defined the new space for my new dimension training set ( where k are the principal components that i choose acording my variance ) $ $ x_{new dim } = x_{new}\phi$$ ok , then i have my new dimensional training dataset after pca ( till here its right according to other papers that i have read ) . the question is : what i have to do now for my validation / testing set ? just the equation below ? $ $ y_{new dim } = y\phi $ $ where y is my ( for exemple ) validation original dataset ? can someone explain the right thing to do ? thanks ! ! ! :)",19268,19268,2018-10-30T15:59:54.110,2018-10-30T15:59:54.110,how to perform pca in the validation / test set ?,neural-networks machine-learning training datasets dimensionality,1,0,
2352,8683,1,,2018-10-30T13:05:20.243,1,282,"i 'm having trouble implementing ac for continuous action space . as far as i can tell , my code does n't seem to have any bugs ! the agent is learning "" something "" as its behaviour seems to vary dramatically after several episodes , but it never seems to ever approach a type of behaviour which i 'd think is reasonable . i 've used very similar code and things have gone smoothly in discrete space and little has changed other than changes to the output ( mean and variance ) . below is the relevant code : class actor(object ) : def _ _ init__(self , sess , s_size , h_size , a_size , env , lr=1e-3 ) : mu = tf.layers.dense(self.hidden_1 , self.a_size , activation = tf.nn.tanh , bias_initializer = none ) sigma = tf.layers.dense(self.hidden_1 , self.a_size , activation = tf.nn.softplus , bias_initializer = none ) sigma = sigma + 1e-10 self.normal_dist = tf.contrib.distributions.normal(mu , sigma ) self.action = tf.clip_by_value(self.normal_dist.sample(1 ) , env.action_space.low[0 ] , env.action_space.high[0 ] ) self.adv = tf.placeholder(dtype=tf.float32 ) # get log prob of the actions taken in _ samples self.acts = tf.placeholder(shape=[none , a_size ] , dtype = tf.float32 ) self.log_prob = self.normal_dist.log_prob(self.acts ) self.loss = -self.log_prob * self.adv the environment i 'm using is the lunarlandercontinuous - v2 . i 've tested ddpg in this same environment and the agent learns incredibly quickly in comparison with the same learning rate and model size which is making me very confused . if anyone has any input it would be very much appreciated . thanks class critic(object ) : def _ _ init__(self , sess , s_size , h_size , env , gamma=0.99 , lr=1e-3 ) : self.gamma = gamma self.replay_buffer = [ ] self.input = tf.placeholder(shape=[none , s_size ] , dtype = tf.float32 ) self.hidden_1 = tf.layers.dense(self.input , h_size , activation = tf.nn.relu , bias_initializer = none ) self.hidden_2 = tf.layers.dense(self.hidden_1 , h_size , activation = tf.nn.relu , bias_initializer = none ) self.value = tf.layers.dense(self.hidden_2 , 1 , activation = none , bias_initializer = none ) self.q_value = tf.placeholder(shape=[none , ] , dtype = tf.float32 ) self.advantage = self.q_value - self.value self.loss = tf.reduce_mean(tf.square(self.advantage ) ) self.lr = lr optimizer = tf.train.adamoptimizer(self.lr ) self.update = optimizer.minimize(self.loss )",7858,7858,2018-10-31T15:32:08.597,2018-11-06T13:21:02.417,continuous advantage actor critic implementation,reinforcement-learning tensorflow,1,7,
2353,8686,1,,2018-10-30T15:45:41.537,1,39,"in language models , cnns can extract different n - gram features from the input . from my current understanding , these models are called "" multi - channel cnns "" . i 'm referring to these materials : https://medium.com/@sharaf/a-paper-a-day-19-a-multichannel-convolutional-neural-network-for-cross-language-dialog-state-bb00f5328163 https://arxiv.org/pdf/1701.06247.pdf what is the difference between channels in convolutional networks and the "" multi - channel "" model ? are these two types of channels similar or do they have different functions ?",19403,1581,2018-10-30T16:39:45.610,2018-10-30T16:39:45.610,multi - channel cnns and channels,convolutional-neural-networks natural-language-processing,0,0,
2354,8689,1,,2018-10-30T20:38:16.187,5,57,"how much can the addition of new features improve the performance of the model during the optimization process ? let 's say i have a total of 10 features . suppose i start the optimisation process using only 3 features . can the addition of the 7 remaining ones improve the performance of the model ( which , you can assume , might already be quite high ) .",16474,2444,2019-05-06T16:47:09.670,2019-05-06T16:49:59.693,how much can the addition of new features improve the performance ?,machine-learning deep-learning optimization theory feature-selection,2,0,
2355,8694,1,,2018-10-31T12:12:25.677,0,76,"i am building 2 models using xgboost , one with x number of parameters and the other with y number of parameters of the data set . it is a classification problem . a yes - yes , no - no case is easy , but what should i do when one model predicts a yes and the other model predicts a no ? model a with x parameters has an accuracy of 82 % and model b with y parameters has accuracy of 79 % .",19465,19466,2018-11-01T19:51:41.880,2019-05-01T13:01:52.837,ensemble models - xgboost,data-science,2,2,
2356,8697,1,,2018-10-31T16:08:38.630,4,121,"i have a dataset of millions of chat messages from different discussions . some of the messages are written by people who lack understanding or relevant language skills . these messages almost always contain little to no thought and are almost always irrelevant to the main topic . some of the messages are written by highly educated people . the lexical structure of these messages is more complex . what popular algorithm can yield results of good precision for this task - measuring or classifying human intelligence ? let 's say the goal is classifying the data in these 3 categories : "" dumb "" , "" average "" , "" intelligent "" . the exact measuring of people 's intelligent quotient by the way they write is a bit more interesting topic but i expect this would require far more research and fiddling . i found almost no resources on prior research on this topic . has anyone come across a labeled dataset that might be useful for training a classifier on this task ?",19471,19471,2018-10-31T22:36:55.263,2019-05-30T13:04:05.470,measuring and classifying human intelligence ?,classification human-inspired intelligence,2,2,2
2357,8701,1,,2018-11-01T00:42:18.080,7,927,"in a cnn , the receptive field is the portion of the image used to compute the filter 's output . but one filter 's output ( which is also called a "" feature map "" ) is the next filter 's input . what 's the difference between a receptive field and a feature map ?",19485,2444,2018-11-02T18:03:05.300,2018-11-06T15:37:56.630,what is the difference between a receptive field and a feature map ?,convolutional-neural-networks terminology difference,2,0,3
2358,8704,1,,2018-11-01T07:28:53.387,1,91,"i 'm doing my thesis on reinforcement learning . my focus on partially observable environments like 3d games . i want to choose a 3d platform for testing and doing research . i know some of them . deepmind lab and openai universe . but my question is that which of these environments is good for me ? is there any environment for this purpose that is benchmark and reliable ? i want a platform that accepted in academia and reliable . for example deepmind is not a standard or open source friendly , is it rational to use their platform for research in academia ? what i have to do ?",19493,,,2018-11-01T16:36:24.790,3d environment for rl research in academia,machine-learning reinforcement-learning research deepmind open-ai,2,0,
2359,8707,1,8826,2018-11-01T10:49:37.647,7,268,"what is a learning agent , and how does it work ? what are examples of learning agents ( e.g. , in the field of robotics ) ?",19442,1581,2018-11-07T17:54:46.100,2018-11-07T18:01:24.147,what is a learning agent ?,terminology intelligent-agent robotics,2,1,1
2360,8709,1,,2018-11-01T11:45:55.017,1,43,what 's the difference between simple reflex and model bases reflex agent ?,19442,1581,2018-11-01T17:17:47.953,2018-11-01T18:05:21.893,difference between simple reflex and model bases reflex agent,intelligent-agent simple-reflex-agents,0,0,
2361,8716,1,,2018-11-01T16:56:50.253,0,19,"i have trained a convolutional neural network on images to detect emotions . now i need to use the same network to extract features from the images and use the features to train an lstm . the problem is : the dimensions of the top layers are : [ none , 4 , 4 , 512 ] or [ none , 4 , 4 , 1024 ] . therefore , extracting features from this layer will result in a 4 x 4 x 512 = 8192 or 4 x 4 x 1024 = 16384 dimensional vector for each image . clearly , this is not what i want . therefore , i would like to know what to do in this case and how to extract features that are of reasonable size . should i apply gloabl average pooling to the activation or what ? any help is much appreciated ! !",16150,,,2019-05-01T08:03:30.737,problem extracting features from convolutional layer where the dimensions are big for feature maps,convolutional-neural-networks feature-selection dimensionality,1,0,
2362,8719,1,,2018-11-01T18:21:36.677,1,23,"it is an easy matter to make paper look old , for example , using any of the techniques explained on this page of wikihow : https://www.wikihow.com/make-paper-look-old this raises the obvious and interesting question : is present - level ai sufficient to distinguish fake old paper from real ?",19510,1671,2018-11-01T20:10:28.740,2018-11-01T20:10:28.740,can present - level ai distinguish fake old paper from real ?,applications strong-narrow-ai,1,2,
2363,8721,1,8730,2018-11-01T20:08:02.730,2,148,"in a gym environment , the action space is often a discrete space , where each action is labeled by an integer . i can not find a way to figure out the correspondence between action and number . for example , in frozen lake , the agent can move up , down , left or right . the code : import gym env = gym.make(""frozenlake-v0 "" ) env.action_space returns discrete(4 ) , showing that four actions are available . if i env.step(0 ) , which direction is my agent moving ?",19516,2444,2019-02-16T02:19:16.093,2019-02-16T02:19:16.093,what is the mapping between actions and numbers in openai 's gym ?,reinforcement-learning gym,1,0,
2364,8735,1,,2018-11-02T07:23:32.690,2,1188,"when should the iterative deepening search ( ids ) , also called iterative deepening depth - first search ( iddfs ) , and the depth - limited search be used ?",19442,2444,2018-11-11T08:01:28.930,2018-12-12T13:51:09.003,when should the iterative deepening search and the depth - limited search be used ?,algorithm ai-basics search,2,0,
2365,8738,1,8750,2018-11-02T08:06:45.560,3,54,"from the paper human level control through deeprl , the correlation in the data causes instability in the network and may causes the network to diverge . i wanted to understand what does this instability and divergence mean ? and why correlated data causes this instability .",19523,,,2018-11-03T08:21:03.843,reason for issues with correlation in the dataset in dqn,q-learning dqn,1,0,
2366,8744,1,,2018-11-02T15:41:12.840,0,74,"assume one is using transfer learning via a model which was trained on imagenet . assume that the pre - processing which was used to achieve the pretrained model contained z - score standardization using some mean and std which was calculated on the training data . should one apply the same transformation on their new data ? should they apply z - score standardization using a mean and std of their own training data ? assume that the pre - processing now did not contain any standardization . should one apply no standardization on their new data as well ? or should one apply z - score standardization , using the mean and std of their new data , and expect better results ? for example i 've seen that the inception v3 model which was trained by keras did not use any standardization , and i 'm wondering if using z - score standardization on my new data could yield better results .",19531,,,2019-05-01T18:01:45.983,how should one standardize input when transfer learning,neural-networks training ai-basics,1,0,
2367,8745,1,,2018-11-02T16:39:28.633,0,79,"this is a very basic question . i 'm running a faster rcnn trainer on a dataset for object recognition . my images range from 200x200 to 7360x4912 in resolution . there are only 2 classes being trained ( both are very similar , but slightly different ) . 2291 total images , 727 labels for one class and 917 labels for the second . after about 400k steps this is what my loss curve looks like . it has n't completely plateaued just yet , but i 'm not convinced it will get to where i need it . my question is about the jaggedness of the results . i have smoothing set to 0.99 to get an overall view of the progress , but the loss actually fluctuates very wildly . is this to be expected or normal ? if not then what should i be looking for to improve the results ? is this mostly a dataset cleanup issue or could the network settings use some fine tuning ? thanks for any help . loss graph",19532,,,2019-05-02T12:02:30.610,what should a good loss curve look like ?,convolutional-neural-networks tensorflow object-recognition,1,0,
2368,8747,1,,2018-11-02T17:19:51.197,1,106,"i am working on a project for price movement forecasting and i am stuck with poor quality predictions . at every time - step i am using an lstm to predict the next 10 time - steps . the input is the sequence of the last 45 - 60 observations . i tested several different ideas , but they all seems to give similar results . the model is trained to minimize mse . for each idea i tried a model predicting 1 step at a time where each prediction is fed back as an input for the next prediction , and a model directly predicting the next 10 steps(multiple outputs ) . for each idea i also tried using as input just the moving average of the previous prices , and extending the input to input the order book at those time - steps . each time - step corresponds to a second . these are the results so far : 1- the first attempt was using as input the moving average of the last n steps , and predict the moving average of the next 10 . at time t , i use the ground truth value of the price and use the model to predict t+1 .... t+10 this is the result predicting moving average on closer inspection we can see what 's going wrong : prediction seems to be a flat line . does not care much about the input data . 2 ) the second attempt was trying to predict differences , instead of simply the price movement . the input this time instead of simply being x[t ] ( where x is my input matrix ) would be x[t]-x[t-1]. this did not really help . the plot this time looks like this : predicting differences but on close inspection , when plotting the differences , the predictions are always basically 0 . plot of differences at this point , i am stuck here and running our of ideas to try . i was hoping someone with more experience in this type of data could point me in the right direction . am i using the right objective to train the model ? are there any details when dealing with this type of data that i am missing ? are there any "" tricks "" to prevent your model from always predicting similar values to what it last saw ? ( they do incur in low error , but they become meaningless at that point ) . at least just a hint on where to dig for further info would be highly appreciated . thanks ! update here is my config { "" data "" : { "" sequence_length"":30 , "" train_test_split "" : 0.85 , "" normalise "" : false , "" num_steps "" : 5 } , "" training "" : { "" epochs"":200 , "" batch_size "" : 64 } , "" model "" : { "" loss "" : "" mse "" , "" optimizer "" : "" adam "" , "" layers "" : [ { "" type "" : "" lstm "" , "" neurons "" : 51 , "" input_timesteps "" : 30 , "" input_dim "" : 101 , "" return_seq "" : true , "" activation "" : "" relu "" } , { "" type "" : "" dropout "" , "" rate "" : 0.1 } , { "" type "" : "" lstm "" , "" neurons "" : 51 , "" activation "" : "" relu "" , "" return_seq "" : false } , { "" type "" : "" dropout "" , "" rate "" : 0.1 } , { "" type "" : "" dense "" , "" neurons "" : 101 , "" activation "" : "" relu "" } , { "" type "" : "" dense "" , "" neurons "" : 101 , "" activation "" : "" linear "" } ] } } notice the last layer with 101 neurons . it is not an error . we just want to predict the features as well as the price . in other words , we want to predict the price for time t+1 and use the features predicted to predict the price and new features at time t+2 , ...",18765,18765,2018-11-04T17:26:24.793,2019-03-05T18:22:42.810,price movement forecasting issue,neural-networks lstm dropout,2,2,
2369,8748,1,8835,2018-11-02T17:42:52.453,5,99,"“ funding artificial intelligence is real stupidity . ” -- john r. pierce was this computer pioneer way off the mark ? – or was there important sub - text there ? pierce was an expert for machine translation in the 1960s . he coauthored the following paper pierce , john r. , and john b. carroll . "" language and machines : computers in translation and linguistics . "" ( 1966 ) . that means , he worked on the domain of ai but explained his subject as useless . perhaps marvin minsky , rodney brooks and sebastian thrun would agree to him ?",19510,1671,2018-11-08T00:21:18.267,2018-11-08T01:17:04.373,was pierce way off the mark ?,philosophy history soft-question mythology-of-ai quotes,1,11,
2370,8749,1,,2018-11-02T18:19:26.173,1,161,"i use pytorch , bauces allennlp is built on it and good libraries are for it . but can autokeras be used for pytorch based ml pipelines , or am i required to switch to keras ? google is quite silent when asked for tutorials for this combination . maybe pytorch is lacking automated ml framework ?",8332,,,2018-11-03T14:29:08.817,can autokeras be used for neural networks of pytorch,neural-networks machine-learning automation,1,1,1
2371,8751,1,,2018-11-02T19:52:14.770,1,61,"until now , i always thought that genetic algorithm can be used for problems of which the solution space can be encoded ( modeled ) as a chromosome of a specific length . however , some people claim that they used ga for this game and this game . they are basically games in which we control an agent on a 2-dimensional area . obviously , the length of the genome sequence depends on how fast the game is finished . so , how is ga used for such games ? if you think ga is not the most suitable method for this kind of problems can you explain why and give better alternatives ?",19539,,,2019-03-06T08:16:22.553,how to use genetic algorithm for varying lengths of solutions,game-ai genetic-algorithms,1,2,1
2372,8755,1,8758,2018-11-03T10:13:52.817,4,2517,"what is the uniform - cost search algorithm ? how does it work ? i would appreciate to see a graphical execution of the algorithm . how does the "" frontier "" evolve in the case of ucs ?",19442,2444,2018-11-11T08:03:25.883,2018-11-11T08:03:25.883,how does the uniform - cost search algorithm work ?,algorithm ai-basics search,2,0,
2373,8761,1,8762,2018-11-03T14:41:31.623,2,51,"i am implementing an actor - critic reinforcement learning algorithm for winning a two player tic - tac - toe like game . the agent is trained against a min - max player and after a number of episodes is able to learn a set of rules which lead it to winning a good majority of games . however , as soon as i play against the trained agent by using even a slightly different playing style , it looses miserably . in other words , it is evident the agent overfitted with respect to the deterministic behaviour of the min - max player . it is clear to me what are the roots of the problem , but i would like to get an overview of the different methodologies which can be applied to overcome ( or mitigate ) this issue . the two solutions i would like to try are the following : 1 . training the agent with different opponents for fixed amounts of episodes ( or time ) each . so that for example i train the agent by using a depth 2 min - max player for the first 10000 episodes , then i use a random playing agent for the next 10000 episodes , then i use a depth 4 min - max player for other 10000 episodes and repeat the process . 2 . starting episodes from different initial configurations . in this way the agents will play a much wider set of sampled games and will be more difficult for the agent to overfit . are these two reasonable approaches ? are there other tricks / good practices to try out ?",19212,,,2018-11-03T17:01:24.127,how to overcome overfitting to single player styles in reinforcement learning ?,reinforcement-learning overfitting,1,0,1
2374,8766,1,9284,2018-11-04T06:11:43.757,2,105,"i am a student and i study algorithms like tree search algorithm graph search algorithm then apply bfs dfs ucs ids dls where are all these algorithms are used ? i know there is a long list of algorithms , but i do n't know about all those algorithms . they all have little difference , i just want to know which algorithm is mostly used in robots . explain with example .",19442,17892,2018-11-04T10:55:50.040,2018-12-01T03:24:23.907,which algorithm is best for robots ?,algorithm search robots robotics,2,1,
2375,8768,1,8769,2018-11-04T08:20:46.963,5,260,which algorithm is used in robot sophia to understand and answer the questions ?,19448,17892,2018-11-04T14:34:19.810,2018-11-10T01:34:55.990,which algorithm is used in robot sophia to understand and answers the questions ?,algorithm robotics,1,1,
2376,8773,1,8774,2018-11-04T13:34:36.077,1,59,"ben goertzel , the developer of sophia the robot , is not only interested in the research of artificial general intelligence ( agi ) , but is discussing ethical aspects of artificial intelligence too . law firms like gowling wlg advise the public and other companies in the risks of artificial intelligence . the question is : why robot sophia , s developer not use wig and is that true she has ability of thinking ?",18950,11571,2018-11-04T14:44:53.803,2018-11-08T03:35:16.830,"why robot sophia , s developer not use wig and is that true she has ability of thinking ?",ethics emotional-intelligence robotics,2,1,
2377,8778,1,,2018-11-05T00:10:41.387,1,50,"with a team , we are studying how it is possible to predict the price movement with high - frequency . instead of predicting the price directly , we have decided to try predicting price difference as well as the features . in other words , at time t+1 , we predict the price difference and the features for time t+2 . we use the predicted features from time t+1 to predict the price at time t+2 . we got very excited , because we thought getting good results with the following graph we got problems in production and we was n't known the problem till we plot the price difference . here is the content of the config file { "" data "" : { "" sequence_length"":30 , "" train_test_split "" : 0.85 , "" normalise "" : false , "" num_steps "" : 5 } , "" training "" : { "" epochs"":200 , "" batch_size "" : 64 } , "" model "" : { "" loss "" : "" mse "" , "" optimizer "" : "" adam "" , "" layers "" : [ { "" type "" : "" lstm "" , "" neurons "" : 51 , "" input_timesteps "" : 30 , "" input_dim "" : 101 , "" return_seq "" : true , "" activation "" : "" relu "" } , { "" type "" : "" dropout "" , "" rate "" : 0.1 } , { "" type "" : "" lstm "" , "" neurons "" : 51 , "" activation "" : "" relu "" , "" return_seq "" : false } , { "" type "" : "" dropout "" , "" rate "" : 0.1 } , { "" type "" : "" dense "" , "" neurons "" : 101 , "" activation "" : "" relu "" } , { "" type "" : "" dense "" , "" neurons "" : 101 , "" activation "" : "" linear "" } ] } } prices do n't change very fast . therefore , the next price is almost always very close to the last price . in other words , p_{t+1 } - p_{t } is very often close to zero or zero directly . if there is too many zeros then the network will only recognize the zeros . the model has picked up on that . i guess the model learned almost nothing except the very simple relationship that the next price is close to the last price . there is not necessarily anything wrong with the model . predicting stock prices should be a very hard problem . so a straightforward improvement should be of taking the features as a whole instead of their difference . i want to keep working with price difference instead of the price in itself because we are making the series potential more stationary . what might be a good solution to deal with the repetitive zeros related to our "" price difference "" problem ? does applying the log - return is a better idea than applying price differences ? does a zero inflated estimators is a good idea ? first predict whether it 's gon na be a zero . if not predict the value . https://gist.github.com/fonnesbeck/874808 ?",18765,18765,2018-11-06T16:51:06.133,2019-04-05T13:01:25.187,price difference predictions curve almost vanished,neural-networks machine-learning deep-learning lstm,1,0,
2378,8779,1,,2018-11-05T00:24:30.483,1,182,"i was lately curious about a reinforcement learning approach that would solve maths equations . for example , if i have the following equation : $ $ f(g(h(w ) ) ) = 0 , with \ w = \begin{matrix } a_{11 } & amp ; 0 & amp ; \ldots & amp ; a_{1n}\\ 0 & amp ; a_{22 } & amp ; \ldots & amp ; a_{2n}\\ \vdots & amp ; \vdots & amp ; \ddots & amp ; \vdots\\ 0 & amp ; 0 & amp;\ldots & amp ; a_{nn } \end{matrix } $ $ along additional constraints on the 3 functions like $ f & lt ; g $ ; $ h & gt ; 2 * g $ ; and $ f , g , h $ not constant the goal is to find the 3 functions expressions given a specific matrix $ w$ and the constraints . can i use reinforcement learning to find a solution or solution s to this problem ? thank you",19579,19579,2018-11-06T22:09:44.857,2018-11-06T22:09:44.857,solving equations using reinforcement learning,reinforcement-learning math,0,5,
2379,8780,1,,2018-11-05T04:48:07.907,5,171,i 'm interested in different approaches to study artificial intelligence that would open up career opportunities . what resources are available ? what disciplines are required ?,19545,1671,2018-11-05T21:42:29.427,2019-05-23T05:22:40.087,what are the paths toward a career in artificial intelligence ?,getting-started profession,3,3,1
2380,8781,1,,2018-11-05T08:32:43.027,1,45,"i 'd like to explore the possibilities of applying artificial intelligence to ocr reading . basic ocr invoices processing let me convert 30 % of them only . main pourpose is defining invoices areas by training an ai , then process those areas with ocr . so i am looking into ai to define and recognize a document topology first , then apply ocr locally . from a brief search it is classified as zonal or template ocr . any chance of a premade open source library ?",19589,4302,2018-12-12T03:19:10.013,2018-12-12T03:19:10.013,zonal or template ocr invoices reading,neural-networks topology ocr,0,0,
2381,8783,1,8784,2018-11-05T09:10:34.903,3,140,"in examples and tutorial about dqn , i 've often noticed that during the experience replay ( training ) phase people tend to use stochastic gradient descent / online learning . ( e.g. link1 , link2 ) # sample minibatch from the memory minibatch = random.sample(self.memory , batch_size ) # extract informations from each memory for state , action , reward , next_state , done in minibatch : # if done , make our target reward target = reward if not done : # predict the future discounted reward target = reward + self.gamma * \ np.amax(self.model.predict(next_state)[0 ] ) # make the agent to approximately map # the current state to future discounted reward # we 'll call that target_f target_f = self.model.predict(state ) target_f[0][action ] = target why ca n't they use mini batches instead ? i 'm new to rl , but in deep learning people tends to use mini - batches as they would result in a more stable gradient . does n't the same principle apply to rl problems ? is the randomness / noise introduced actually beneficial to the learning process ? am i missing something , or are these sources all wrong ? note : not all the sources rely on stochastic gradient descent : e.g. keras - rl seems to rely on minibatches ( https://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py )",19592,,,2018-11-05T10:09:37.380,deep q - learning : why do n't we use mini - batches during experience reply ?,deep-learning reinforcement-learning gradient-descent,1,0,
2382,8790,1,,2018-11-05T15:39:08.287,2,37,"i am trying to understand the proof of theorem 2.1 from this paper : ross , stéphane , and drew bagnell . "" efficient reductions for imitation learning . "" proceedings of the thirteenth international conference on artificial intelligence and statistics . 2010 . the cost - to - go is given as $ $ j(\pi ) = \sum_{t=1}^{t}\mathbb{e}_{s\,\sim\ , d^t_{\pi}(s)}\left[c_\pi(s)\right].$$ in the paper they use for the learned policy and for the expert policy . in the derivation they write $ $ j(\pi)\leq \sum_{t=1}^{t}\ { p_{t-1}\mathbb{e}_{s\ , \sim \ , d_t(s)}\left[c_\pi(s ) \right]+(1-p_{t-1})\}$$ $ $ in which $ p_{t-1}$ is the probability of not not making an error with policy up to the time $ t-1 $ . and is the surrogate 0 - 1 loss . the following steps are easy to follow , but how did they come up with these steps ?",19123,2444,2019-02-13T02:35:09.013,2019-02-13T02:35:09.013,"understanding the proof of theorem 2.1 from the paper "" efficient reductions for imitation learning """,reinforcement-learning proofs,0,0,
2383,8791,1,8792,2018-11-05T15:49:30.983,5,174,"i 'm interested in the differences in scope between statistical ai and classical ai ( gofai ) . what are the different qualities , and the differences in capability and application ? real world examples would be appreciated .",19448,1671,2018-11-05T21:23:49.157,2018-11-05T21:23:49.157,difference in scope of statistical ai and classical ai ?,artificial-consciousness statistical-ai classical-ai,1,1,1
2384,8793,1,8797,2018-11-05T17:42:12.927,1,101,i am interested in making a simple chess engine using neural networks . i already have a fairly good value network but i ca n't figure out how to train a policy network . i know that leela chess zero outputs the probability of any of the about 1800 possible moves . but how do you train such a network ? how do you calculate the loss when you only have the 1 move that was played in the game to work with ?,19604,,,2018-11-06T10:18:36.773,chess policy network,deep-learning deep-network chess loss-functions,1,0,
2385,8794,1,,2018-11-05T17:57:00.960,1,17,"for example haar cascade can be trained using only positive and negative examples , you do n't need any bounding box annotations . but it not a deep learning approach . another example can be the most straight forward image recognition model + sliding window . but it is very slow",19605,,,2018-11-05T17:57:00.960,is there any deep learning object detection algorithms that can work without bounding boxes annotated data ?,deep-learning computer-vision,0,0,
2386,8796,1,11779,2018-11-05T20:18:14.980,2,47,"we all know information filter is a dual representation of kalman filter . the main difference between information filter and kalman filter is the way the gaussian belief is represented . in kalman filter , the gaussian belief is represented by their moments , whereas in information filter , the gaussian belief is represented by the canonical form . canonical form is comprised of an information matrix ( ) and an information vector ( ) . as we all know , the name kalman filter is after rudolf e. kálmán , one of the primary developers of this theory . now i want to know where the name information filter came from ? why it is called information filter ? what type of information is attached to this filter ? is there any significance behind the nomenclature ? i have the same question about information matrix and information vector . is there any significance behind the nomenclature ? i already read probabilistic robotics by sebestian thrun . chapter 3 gaussian filter , in the subsection 3.4 the information filter . there are many equations and theories but that does not tell us about the nomenclature . probabilistic robotics",18384,17892,2018-11-06T21:12:36.760,2019-04-24T05:14:12.243,why is information filter called information filter ?,machine-learning ai-design computer-vision robotics bayes,1,0,2
2387,8798,1,,2018-11-06T10:52:15.843,0,171,"i created a very simple bot to learn how to use chatterbot . this library already comes with a training , but i wanted extra training with an import of a corpus in portuguese that i found in github . from chatterbot import chatbot bot = futaba ( "" terminal "" , storage_adapter=""chatterbot.storage.sqlstorageadapter "" , logic_adapters= [ "" chatterbot.logic.mathematicalevaluation "" , "" chatterbot.logic.timelogicadapter "" , "" chatterbot.logic.bestmatch "" ] , input_adapter=""chatterbot.input.terminaladapter "" , output_adapter=""chatterbot.output.terminaladapter "" , database_uri="" .. /database.db "" ) print(""type something to begin ... "" ) while true : try : bot_input = bot.get_response(none ) except ( keyboardinterrupt , eoferror , systemexit ) : break that 's all i have . how can i import this corpus into my chatbot ?",18233,,,2018-11-06T16:23:21.993,how to add external training in chatterbot ?,training python chat-bots,1,4,
2388,8807,1,,2018-11-06T18:50:04.937,6,120,"i 'm building a weather station , where i 'm sensing temperature , humidity , air pressure , brightness , $ co_2 $ , but i do n't have a raindrop sensor . is it possible to create an ai which can say if it 's raining or not , with the help of the given data above and maybe analyzing the slope from the last hour or something ? which specific technology should i use and how can i train it ?",19634,2444,2018-11-13T18:45:19.773,2018-11-13T18:45:19.773,how do i predict if it is rainy or not ?,training prediction models,1,2,2
2389,8813,1,8827,2018-11-07T00:34:19.327,4,88,"let me compare two textbooks : ( 1 ) "" artificial intelligence : a modern approach "" by stuart j. russell and peter norvig and ( 2 ) "" artificial intelligence : structures and strategies for complex problem solving "" by george f. luger ( sixth edition ) . i have an impression that the former ( 1 ) is biased towards symbolic ai ( especially logic - based ) and the latter ( 2 ) is biased towards statistical methods . do you think the same ? or , in wider sense , is ( 1 ) more rational and ( 2 ) more empirical ? do you know other ai books with a strong emphasis on statistical methods ?",17643,1671,2018-11-07T22:25:45.920,2018-11-07T22:25:45.920,methodology bias in ai textbooks,statistical-ai academia symbolic-ai,1,2,2
2390,8814,1,8815,2018-11-07T01:38:17.627,-1,24,"currently , i found the right recipe for a time series regression problem to finally get acceptable to good results . here is the config file { "" data "" : { "" sequence_length"":45 , "" train_test_split "" : 0.85 , "" normalise "" : false , "" num_steps "" : 10 } , "" training "" : { "" epochs"":30 , "" batch_size "" : 32 } , "" model "" : { "" loss "" : "" mse "" , "" optimizer "" : "" adam "" , "" layers "" : [ { "" type "" : "" lstm "" , "" neurons "" : 161 , "" input_timesteps "" : 45 , "" input_dim "" : 161 , "" return_seq "" : true , "" activation "" : "" relu "" } , { "" type "" : "" dropout "" , "" rate "" : 0.1 } , { "" type "" : "" lstm "" , "" neurons "" : 161 , "" activation "" : "" relu "" , "" return_seq "" : false } , { "" type "" : "" dense "" , "" neurons "" : 128 , "" activation "" : "" relu "" } , { "" type "" : "" dense "" , "" neurons "" : 1 , "" activation "" : "" linear "" } ] } } here is the results i got what can be good improvements i can bring to my model so that i can get better results ? there are a lot of small spikes and other places where the curve is rather constant that i should get a rise or fall of the curve .",18765,18765,2018-11-07T01:59:47.427,2018-11-07T04:25:15.633,getting better results in improving the configuration,neural-networks recurrent-neural-networks,1,1,
2391,8821,1,8909,2018-11-07T13:22:16.307,3,712,the iterative deepening a * search is an algorithm that can find the shortest path between a designated start node and any member of a set of goals . the a * algorithm evaluates nodes by combining the cost to reach the node and the cost to get from the node to the goal . how is iterative deepening a * better than the a * algorithm ?,19652,2444,2019-04-16T09:41:23.100,2019-04-16T09:41:23.100,how is iterative deepening a * better than a * ?,search difference a-star iterative-deepening-a-star,1,0,
2392,8823,1,,2018-11-07T15:13:22.593,1,26,my understanding is that the conjugate gradient method is faster than gradient descent because it does less zig zags while descending . how come the state of the art papers i see all use gradient descent for backpropagation and not conjugate gradient descent ?,19167,19167,2018-11-07T22:15:42.273,2018-11-07T22:15:42.273,neural network backpropagation gradient descent better than conjugate gradient descent ?,neural-networks gradient-descent,0,2,
2393,8825,1,,2018-11-07T17:04:09.217,1,70,"is there a good reference / tutorial for using rnn / lstm to determine lag interval for 2 time series ? e.g. i have { x_n } , { y_n } and i want to figure out by how much does { x_n } typically lags behind { y_n } ?",11478,,,2018-11-08T04:05:13.020,rnn and lstm for discovering time lag,deep-learning,0,0,
2394,8828,1,8842,2018-11-07T18:24:47.230,4,209,"it seems to me that the way neural networks are trained is similar to the way we "" train "" or educate a child ( or a person , in general ) . can an ai eventually think like a human ?",19660,2444,2018-11-14T13:59:10.290,2018-11-14T15:01:16.487,can an artificial intelligence eventually think like a human ?,neural-networks human-like cognition,2,0,6
2395,8831,1,,2018-11-07T21:28:31.757,0,18,"i am trying to read https://arxiv.org/abs/1701.01727 about generalisation of hopfield neural networks and i like the clear ideas that physics and hamiltoanian framework can be used for modeling such networks and for deducing lot of properties . my question is - how hopfield networks are connected to the standard networks ? i see at least 3 differences : hopfield neurons has binary values on / off ( +1 , -1 ) but machine learning neurons have real ( are at least approximately ( within machine limits ) real ) values . threshold function is the only activation function used in hopfield neurons , machine learning has lot of more complex , real - valued functions connectivity patterns in machine learning networks ( lstm , gru cells ) are far more richer . so - maybe hopfield neural networks can be generalized up to the level of machine learning networks and at the same time preserving the use of hamiltonian framework . is it possible , is there any work in this direction ?",8332,,,2018-11-07T21:28:31.757,"how hopfield neural networks are connected with "" industrial "" neural networks used in machine learning ?",neural-networks,0,0,
2396,8844,1,8847,2018-11-08T04:10:14.600,3,1588,"i am a cs student , our professor gave an assignment that each group of our class should watch at least 5 movies based on artificial intelligence , then discuss and share with the whole class . can someone help me with searching of the movies about ai ?",19657,1671,2018-11-08T22:15:07.500,2019-01-28T20:28:01.317,best movies about artificial intelligence ?,ai-basics philosophy theory social mythology-of-ai,6,4,1
2397,8846,1,,2018-11-08T05:34:22.210,2,72,"there are some fields of computer vision that are similar to artificial intelligence . for example , pattern recognition and path tracking . based on these similarities , can we say that the computer vision is a part of artificial intelligence ?",19656,1847,2018-11-08T10:00:00.910,2018-11-08T10:00:00.910,are computer vision and digital image processing part of artificial intelligence ?,machine-learning image-recognition computer-vision,1,0,
2398,8854,1,8862,2018-11-08T14:38:04.963,2,108,"i 'm aware of those ai programmes which can play games and neural networks which can identify pictures . but are they really thinking . do they think like humans ? do they have consciousness ? or are they just obeying a bunch of codes ? for example , when an ai learns to play pacman , is it really learning that it should not touch the ghosts or are they just following a mechanical path which will make them win the game ?",19192,,,2018-11-08T16:08:13.863,do genetic algorithm and neural networks really think ?,neural-networks genetic-algorithms genetic-programming,3,0,
2399,8860,1,8874,2018-11-08T15:25:25.307,0,41,what are ways of determining the number of trees to be generated in a random forest algorithm ?,19442,2444,2019-05-01T17:10:12.560,2019-05-01T17:10:12.560,how many trees should be generated in a random forest ?,machine-learning decision-tree hyper-parameters random-forest,1,0,
2400,8867,1,,2018-11-08T22:03:32.067,2,37,"there are two models for the same task : model_1 : 98 % accuracy on training set , 54 % accuracy on test set . model_2 : 48 % accuracy on training set , 47 % accuracy on test set . from the statistics above we can say that model_1 overfits training set . q1 : can we say that model_2 underfits ? q2 : why model_1 is bad choice if it performs better than model_2 on test set ?",19713,,,2019-05-08T02:00:55.650,overfitted model performs better in test set,machine-learning deep-learning overfitting,1,0,
2401,8868,1,,2018-11-08T22:11:56.110,1,58,"one of the somewhat subliminal and entirely unsubstantiated assumptions we hear is that more computing power will allow us to approximate human intelligence & mdash ; that quantitative augmentation will lead to qualitative improvements in automation . no new techniques are proposed & mdash ; just the scaling of existing ones . when we study functional mris , we see that much of the brain is at rest at any given time . we read pop psychology articles about awakening the untapped intelligence , as if the brain volume at rest is a flaw . we notice what is lit up , but we tend to ignore the possibility of purpose behind inactivity . here 's the question these assumptions trigger . can not thinking be a form of intelligence ? many can see the intuitive attractiveness of the buddhist idea that clearing the mind can lead to enlightenment . we wish we could be more logical and more intuitive . it 's clear that there is no consensus about how to balance these intellectual goals . if we admit this intellectual agnosticism , we are more likely to move toward the greater understanding of what intelligence can be at its best . is it possible that more computing power is necessary only for a small segment of problem types when approached under the assumption that power is necessary ? will these current approaches be later seen as primitive and limiting ? might a more enlightened approach we have n't yet considered lead to excellent performance on low end mobile phones for entire classes of problems ? what new ideas show promise along these lines ?",4302,,,2019-01-08T12:02:15.440,is the proposal that more power leads to more intelligence correct ?,ai-design theory,1,1,
2402,8871,1,,2018-11-09T04:49:30.127,1,81,"set aside networks , image classification , gradients , and the strength of intelligence for a moment and consider the world before people lit fires . fires were started periodically just as they are now , when lightning struck dry deciduous matter . people probably ran for water . perhaps some smart people learned that fire created warmth at night after a blaze and discovered how to preserve it . they could n't let it go out . if they did , they 'd have to wait through many cold nights until the next forest fire , which could take decades , and then try again . how did someone invent how to start one without lightning ? the first plow , the first written law , the first bow , the first coin , the first water wheel , the first mechanical clock , the first circuit , the first logo , the first transistor , the first web site . now try to imagine it from the other end . there is something important . it has no name . it has no design . it has no method of procurement . if you create one , no one will know what it does or why it is there . but you do . you 're its inventor . it began in your brain . you saw it before it existed . it was vision without the involvement of your eyes . you then must bring it into being with your speech or make it with your hands . this is the rarest , most human , and most precious form of intelligence . what kind of algorithm can invent ? genetic algorithms have not produced new designs of things that meant nothing to anyone when they appeared but mean something to the computer running the algorithm . something is missing in the way we perceive intelligence . maybe it is a kind of negative model , where the empty space represents the thing that needs inventing . how would one create such a model ? is there something like a virtual die , where the empty space in the universe of utility is highlighted ? what kind of algorithm can detect the absence of something without it ever having first existed ? once that ability is understood , we can then approach the problem of running through a wide array of approaches to create this previously un - invented thing and check each for feasibility , but first we must learn how to artificially envision nameless things that fill previously unimagined niches . what kind of algorithm can invent ? addenda in response to excellent comments this question is dear to me because i 've invented things , some for corporations , some for my own laboratory , and some that i failed to push hard enough and someone else invented something very close and developed it first . this last case is interesting , and i 've seen it happen many times . it 's also common in scientific history , where two people who do n't communicate directly simultaneously come up with some scientific or industrial invention . reading alonso church 's interview we find that alan turing did n't study under him as most historical accounts state . according to church , he developed his lambda calculus and turing developed his machine in parallel and without direct consultation . there is an environmental aspect to invention , as if the world around the inventors are subconsciously searching without a clear objective . there is an accidental appearance to invention , but everyone i know or read about who invented something was poking around in the area of the invention in their mind , and not just casually . we obsessed over some imaginary search space , hunting for something novel and purposeful . it is like a rat in a maze that smells cheese but does not know the path . we ca n't just try all the passageways marking each path to avoid duplicate trials to get to the cheese . we do n't know it yet but there is a hole in the ceiling covered with a thin veil . until we realize there is another level to the maze , we can not find the cheese . the veil represents the discovery . the passage into the second level where the cheese resides is the novelty . we could find the hole by accidentally hitting the veil when arbitrarily jumping up and down or with a stray ball when playing a game of toss with another rat , but it would sure be faster if we realized something . we smell cheese but failed to find it . when we doubt our method , we start looking for fissures in the surfaces we have n't yet stepped on . doubt has something to do with it . yes , there is a requirement of some kind of understanding or model of the world that can be altered and tested in imaginary space . this is most obvious in the writings containing the thought experiments of archemedes ( buoyancy principle which led to the relative incompressability of liquids , the first conception of a screw , ... ) , the thought experiments of isaac newton ( the two prisms in series , the cannon ball blasted into orbit which led to the entire field of newtonian physics , ... ) , and the thought experiments of turing ( the imitation game , computability and the punch tape machine ) .",4302,4302,2018-11-09T15:02:01.910,2018-11-09T15:54:50.957,what kind of algorithm can invent ?,ai-design genetic-algorithms invention feasibility,1,2,
2403,8878,1,8890,2018-11-09T14:10:34.167,1,773,"we seem to be experiencing an ai revolution , spurring advancement in many fields . please explain , at a high level , the uses of artificial intelligence and why we might need it .",19220,1671,2019-05-22T20:54:29.987,2019-05-22T21:00:51.460,why do we need artificial intelligence ?,ai-basics applications,2,0,1
2404,8879,1,,2018-11-09T14:17:08.837,1,456,mode collapse is a common problem faced by gans . i am curious why does n't vae suffer mode collapse ? some source,19738,,,2018-11-09T16:24:36.010,why does n't vae suffer mode collapse,generative-adversarial-networks,1,0,
2405,8883,1,8899,2018-11-09T17:46:04.760,0,31,"if an antecedent in a rule involves $ m$ two - state features and results in consequences from a set of $ n$ possible ones , we have $ 2^{m+n}$ permutations , which are , in a sense , be categories . if features and actions are identified in advance , along with a set of labelled example data for which the rules of labelling are unknown , can a rule could be created by an artificial network with $ m+n$ binary outputs ? if so , can the rule created be excluded by the loss function in an independent application of the same or a similar artificial network , and can this process be recursed until a rule set for labelling examples has been created ?",4302,,,2018-11-10T03:38:21.960,can an artificial network create a rule from rule components ?,neural-networks ai-design logic feature-selection rule-acquisition,1,0,
2406,8885,1,,2018-11-09T18:08:51.777,2,606,i observed in several papers that the variational autoencoder 's output is blurred while gans output is crisp and has sharp edges . can someone please give some intuition why that is the case ? i did think a lot but could n't find any logic .,19738,,,2019-01-15T06:23:41.407,why are variational autoencoder 's output is blurred while gans output is crisp and has sharp edges ?,generative-model generative-adversarial-networks autoencoders,2,1,
2407,8894,1,,2018-11-10T02:38:22.483,5,297,"my guess is that they come under supervised learning , as we have labelled dataset of images , but i am not sure as there maybe other aspects in gans which might come into play in the determination of the class of algorithms gan falls under .",18956,9947,2018-11-10T07:39:05.590,2018-11-28T23:36:13.520,do gan 's come under supervised learning or unsupervised learning ?,neural-networks unsupervised-learning generative-adversarial-networks,2,2,3
2408,8898,1,,2018-11-10T03:38:04.040,1,33,"i need to map from a vector space representation onto a tree structure . a possible solution : given a word vector as input , produce a path in the tree from the root down to the node that most closely matches that vector . the path would be a variable length string composed from a finite set of symbols . the variable length output leads me towards long short - term memory ( lstm ) models . but i 've never built a complete lstm model before . the understanding gap : my vector inputs are already dense representations . specifically , i 'm working with glove right now . but the output path symbols would require a different encoding , correct ? how can i structure / train the necessary encoder - decoder pair so that it can handle both word vectors and these path symbols as input while still being able to produce path symbols as output ? edit : after more research , i think the correct term would be a "" one to many "" model using an rnn architecture ( of which lstm is one type ) . so i would n't need an encoder for the one input vector because it 's already a dense vector . but i would need to train an autoencoder for decoding the multiple output vectors that describe the tree path .",19703,2444,2019-04-16T22:41:05.633,2019-04-16T22:41:05.633,how could i learn tree paths given word embeddings ?,neural-networks natural-language-processing lstm word-embedding glove,0,0,1
2409,8902,1,8907,2018-11-10T08:22:32.563,3,3235,"what are the differences between the a * algorithm and the greedy best - first search algorithm ? which one should i use ? which algorithm is the better one , and why ?",19657,9947,2018-11-11T19:09:02.967,2018-11-11T19:09:02.967,what are the practical differences between a * and greedy best - first search ?,algorithm search,1,1,
2410,8906,1,,2018-11-10T12:24:29.683,3,199,"i understand a * and dijkstra for avoiding obstacles , they require that points are traversable there are points that are not traversable thus the algorithms wo nt bump into the obstacles because the obstacles ca nt be traversed or maybe if cost is a factor the relationships between the point is of such a high weight that the algos wo nt take that path . i ve been using graphs this way with good results so a * and dijkstra are great candidates and i have them working sweet , but they always require me to have points in place to traverse , i m the one who puts the points in and creates the relationships between the points , its not ai or any type of learning , its just an algo traversing points on a map . lets say i have a white image , a green blob in the middle and points at either side of the blob , i need to get from a - > b i do n't have points to traverse i just have this image , is it a case of machine learning to learn an agent to get from point a - > b then apply that learning to more complex maps ? if so what what should i be looking at ? or is that the wrong route to take pardon the pun . if any of my google queries contain "" ai "" all i get back is deep mind this and deep mind that and a lot of game developer answers that include sending rays out in front etc , but again that 's not ai or learning . edit after answers were posted : ok thanks for the responses , i do n't have 50 reputation so i ca nt answer to your posts both answers seem to come back to graphs though as well as the paper referenced in the first answer which is what i m using just now . ok so images is definitely out as your right its a super amount of work and would be very complicated . ill try and explain it in a different way , take this image of a route created by my graph , the route is fine , it adheres to directional traffic separation schemes and is perfectly usable , zoomed out it looks a bit jaggy but zoomed in the route is fine are you saying there is no way an agent could be trained to navigate around a map going from a - > b without the use of graphs ? in the image above the underlying dataset takes into account ocean points ( low resolution ) , areas where there are many island ( high resolution ) , canals , tss , port approaches and harbour navigation etc so there is a lot of under lying data and your route is only as good as the data you put in , there s also a lot of other concerns especially in the ocean portions where you do n't wants to just connect to your neighbour , you want your route to take longer jumps . if you had an array 640 millions point for latitude longitude 3 decimal precision with high values for land and low values for water , could an agent be trained to go from a - > b ? by keeping the agent on water and if it crashes into land then do the simulation again but learn from its mistakes ? i know its a long shot but i m just trying to get a handle on what s being done or if there is anything out there to look at . great responses thanks .",19761,1641,2018-12-15T16:56:08.750,2019-01-14T17:01:15.920,point a to b avoidance,machine-learning computer-vision path-planning,2,0,
2411,8908,1,8918,2018-11-10T14:13:41.093,1,72,when and where the basic concept of artificial intelligence begin ? the first impact of artificial intelligence on human society ?,19657,,,2018-11-10T20:11:31.060,the first impact of artificial intelligence on human society ?,ai-basics philosophy,2,1,1
2412,8910,1,,2018-11-10T15:27:45.823,5,156,"problem my problem is the following : given 1000 wins , losses , and ties from a chess simulation i am using , what shape should each game be ( i.e. , sequence of moves leading to win / loss / tie ) in order to build an deep neural network on it ? literature review kind of got me in the right direction . great theoretical way of thinking about it , but left me wanting detail . gave a good high level tutorial , but not enough detail . alpha beta pruning python example , but no neural network . neural network using reinforcement learning , but i ca nt figure out what the ( x , y ) shapes are and their meaning from just digging into the source code . current situation i currently have a sample of 1000 wins , 1000 losses , and 1000 ties from the python - chess api , so if i is the index of a game , then this is the structure of the current dataset i am working with : game_i -&gt ; ( num_moves_i,8,8,16 ) so each game_i where i in { 1 .. 3000 } and num_moves_i is variable depending on the game ( e.g. , 14 for a good winning game , or 765 moves for a tie game ) . the 16 represents a one_hot_encoding for one of the 16 unique board pieces . the data set is also an alternating board state , so : game_i[0 ] = = board state of whites first move game_i[1 ] = = baord state of blacks first move furthermore , i also have alpha - beta pruning and maximin working , so for each move i have an intrinsic value associated with it using recursion three levels deep . leading my current approach to a regression of a given move , essentially leading me to believe the ai would simply learn the heuristic and predict a value the heuristic would give . summary clearly my proof of concept 1000 winning games is not enough to make a meaningful ai , but that is n't my goal . i want to learn the techniques , not produce an enterprise scale chess ai . does the tensor shape make sense ? is this a reinforcement learning problem ? if so , how can i shape my current framing into that type of thinking ? theory in this area would be greatly appreciated as i am less familiar with it . is this a rnn / lstm problem ? ( e.g. , predict the next board state ) . is this a regression problem ? is this a sequence mining problem ? what is the standard approach to framing this problem , once you have data falling through the pipeline . your support is more than appreciated . * update ( still in research ) * with further research , a candidate label for the training data is the tensor and the item from the state - space that was selected with that board state . then only keep games containing sequences of moves which obtain a cumulative value > = epsilon . this would require a one_hot_encoding of all moves played in all games we wish to train on , as labels . e.g. , ( game_i , board_ij , e2e6 )",19764,19764,2018-11-10T21:55:51.217,2019-04-09T23:00:55.527,building ai from chess - data shape from simulation,reinforcement-learning ai-design chess structured-data,1,0,1
2413,8915,1,,2018-11-10T18:23:31.287,2,33,"there 's the need to design a horizontal plane cleaning system that is controlled by positioning servos . two in two of three floor rollers and three in the x , y , and z positioning of a wiping device . cleaning fluid may be dispensed or vacuumed away , and the rollers can be concurrently locked so that the position of the cleaning system relative to the floor can be held steady or unlocked . to keep device cost low , primitive lidar is used for ranging at each extremity most exposed by positioning extremes . for instance , ranging is detected pointing upward so that the position in z is sensitive to headroom , and ranging is detected on the front , left , and right sides of the wiping device to ensure it does n't hit walls at full speed or knock over items on the horizontal plane surface . there is no pixel based input to the ai . ranges are reported as two 32 bit integers approximately representing two measurements in millimeters . one is the distance to the closest point of reflection and the other is the mean distance of reflection . only by comparing them can walls can be distinguished from objects placed on the surface . positioning targets for the entire system rolling on the floor and for the wiping device relative to the entire system are provided as 32 bit integers to the driving systems of each servo . what the system does to clean is roll over to a location , given a programmed of approach and line of movement in front of the horizontal surface , clean the surface area , constrained by the fixed parameters below , and roll back to the original system position . objects on the surface may change from cleaning to cleaning , but not walls . there may be a need to lock the rollers , wipe in several motions , unlock the rollers , move over , lock again , wipe some more , and do this a few times . there are three things we need to minimize that could be used in a loss function . time to complete the cleaning of a horizontal surface cleaning fluid consumption variance in cleaning fluid distribution on the surface what kind of network could learn to send the right positions to the servos to perform the cleaning ? is q - learning best ? how is the example data to be collected ? is there a way to learn without training , allowing performance to be poor at first but still get the job done and then improve each time afterward ? how can ai be used to make this a practical cleaning system ? technical details there are eight numbers programmed into the firmware . the overlap of wiping in mm the minimum time cleaning fluid remains on surface before vacuuming it up the rate of cleaning fluid use in microns ( one milliliter is one cubic centimeter , so one milliliter of fluid per square meter horizontal surface has the unit cm cubed over meters square , which is one cm over 10,000 square cm per square meter , which is one micron . ) the maximum speed and acceleration of wiping motion changes relative to the horizontal surface the maximum speed and acceleration of movement of the device center relative to the floor the maximum permissible speed when the wiping device reaches a wall the minimum clearance to maintain between the wiping device from items on the horizontal surface to avoid knocking one over",19168,,,2019-04-09T20:02:35.497,way to control movement and coverage in an embedded ai cleaning system ?,machine-learning optimization embedded-design consumer-product mapping-space,1,0,
2414,8916,1,,2018-11-10T19:38:48.130,2,109,how is a feed - forward neural network with few hidden layers and lots of nodes in those hidden layers different from a network with a lot of hidden layers but relatively lesser nodes in those hidden layers ?,19769,75,2018-11-14T22:51:28.573,2018-11-14T22:51:28.573,significance of depth of a deep neural network,neural-networks machine-learning deep-learning deep-network feedforward,1,0,1
2415,8920,1,,2018-11-10T20:40:17.907,0,39,"one disadvantage or weakness of artificial intelligence today the slow nature of learning or training success . for instance , an ai agent might require a 100,000 samples or more to reach an appreciable level of performance with a specific task . but this is unlike humans who are able to learn very quickly with a minimum number of samples . humans are also able to teach one another , or in other words , transfer knowledge acquired . my question is this : are artificial intelligence learnings or trainings transferable from one agent to the other ? if yes , how ? if no , why ?",17892,,,2018-11-13T09:04:55.203,are artificial intelligence learnings or trainings transferable from one agent to the other ?,machine-learning reinforcement-learning training intelligent-agent,1,0,
2416,8924,1,8925,2018-11-11T04:11:48.587,0,256,how are artificial neural networks different from normal computer programs ( or software ) ?,19220,2444,2019-05-01T17:24:06.433,2019-05-01T17:24:06.433,how are artificial neural networks different from normal computer programs ?,neural-networks machine-learning difference,3,1,
2417,8927,1,,2018-11-11T11:04:13.450,2,91,"how does recurrent neural network updates its weights and bias through backpropagation ? is time taken into account while updating the weights of a rnn using backpropagation through time(bptt ) ? """,19780,1671,2018-11-19T18:43:39.417,2018-11-19T18:43:39.417,update of weights in recurrent neural network through back propagation,ai-basics recurrent-neural-networks backpropagation concepts,0,1,
2418,8929,1,,2018-11-11T13:47:45.863,1,70,"i have a question about convolutional neural newtork . consider this image : conv example we have a part of an input matrix and a filter . ok , now we can do the convolution and the result is a scalar , if it is a large number the future was found otherwise no . so , the features map is a matrix where each number indicates the points where a feature was found . i understood this . the output of this convolution is a features map ( after activation function ) . my misunderstanding start here . the next convolution , will find another feature . i do n't understand how this filter , can find a new feature from a representation of where the previous feature was found . the feature maps , output of the first convolution is a rappresentation of indicates only where the features were found . should n't be the features found instead of a number that indicates how much was found this feature ? ? how exactly does this work ?",19786,,,2018-11-11T17:41:41.427,features map convolutional neural network,neural-networks convolutional-neural-networks,3,0,
2419,8932,1,8942,2018-11-11T14:39:08.293,1,78,"my ai ( for the card game schnapsen ) currently calculates every possible way the game could end and then evaluates the percentage of winning for every playable card / move . the calculation is done recursively using a tree . if a game could move on in three different ways the percentage of winning on this node would be mean * ( 1 - ( standarddeviation * f ) ) * 100 where f is between 0 and 2 . when the game ca n't move on and the ai wins the percentage is 100 , when lost 0 . i 'm including the standard deviation in this formula to prevent the ai from risking too much . in other words : i 'm using a mcts that uses percentages . is there a better formula or way of calculating the next move to maximize the chance of winning ? does including the standard deviation make sense ?",19783,19783,2018-11-11T20:45:58.053,2018-11-12T17:50:20.777,weighted move rating for ai,algorithm monte-carlo-tree-search,1,4,
2420,8938,1,,2018-11-12T05:21:00.323,1,28,"every neural network updates its weights through back - propagation . how is back - propagation used for updating weights in a combination of 2 or more neural networks ( e.g.:cnn - lstm , gan - cnn , etc . ) . for instance a cnn - lstm model is a cnn model stacked on top of an lstm model . when cnn model is stacked on top of an lstm model , do we consider hidden layer of both model or hidden layer of outer model(lstm ) ?",19780,,,2018-11-12T05:21:00.323,use of backpropagation for weight updates in a combination of 2 neural networks,neural-networks deep-learning backpropagation,0,1,
2421,8940,1,,2018-11-12T10:11:25.020,1,151,cisco and other companies are using tara ai — a matching tool that connects it projects with freelancers who have the exact skills required to complete them . looking for an explanation of how tara ai works . ( i read about it on forbes but can not find info about the ai methods utilized via google search . ),19108,1671,2018-11-13T22:14:17.967,2019-04-28T20:01:51.620,how does tara ai work ?,theory applications architecture,3,4,1
2422,8943,1,,2018-11-12T17:00:20.057,2,325,what are the differences between the uniform - cost search ( ucs ) and greedy best - first search ( gbfs ) algorithms ? how would you convert a ucs into a gbfs ?,19254,2444,2018-11-12T17:20:31.330,2019-04-16T01:56:50.377,what are the differences between uniform - cost search and greedy best - first search ?,algorithm ai-basics difference,1,0,
2423,8944,1,,2018-11-12T17:00:32.700,0,37,"grammatical induction is the art of learning a grammar from training data . we have a domain - specific - language for example a python dialect and should create for that dialect a grammar which is able to parse the language . parsing means , to identify for new sourcecode samples if they are valid and which tokens has which meaning . to make things more complicated the language is not a programming language but the input in a textadvanture . that means , the grammar should learn what the internal parser is doing . additionally , the language has a meaning . that means , the user types in a sentence and at as result the avatar on the screen moves in a direction . in the literature such task is called grammar induction , because the grammar is not there and must be find from scratch . additional the link between the language and the actions on the screen are needed . i know this is a very complicated task and it is not possible to explain this in detail . so my question goes only into the direction of a submodul of the overall system . before it is possible to iterate a grammar some datastructure is needed to store the grammar . how can i do this ? do i need the antlr syntax for a grammar , can i store a grammar as a graph , as a lisp list tree or in a sql database ? in every case , the idea is not to program the grammar by hand in software , but to iterate with a solver over possible grammars according to the input stream which is an unknown language .",11571,,,2018-11-12T17:00:32.700,datastructure for grounded grammar induction,natural-language-processing genetic-programming,0,0,1
2424,8951,1,,2018-11-13T12:44:36.330,2,55,what is the difference between ids * and a * algorithm . and how ids * is better than a * algorithm . please explain your answer with pros and cons . thanks,19254,,,2018-11-24T14:27:01.607,what is the difference between ids * and a * algorithm ?,algorithm difference,0,2,
2425,8954,1,,2018-11-13T15:45:28.540,0,57,"i found the below image of how a cnn works but i do n't really understand it . i think i do understand cnns , but i find this diagram very confusing . my simplified understanding : features are selected convolution is carried out so that to see where these features fit ( repeated with every feature , in every position ) pooling is used to shrink large images down ( select the best fit feature ) . relu is used to remove your negatives fully - connected layers contribute weighted votes towards deciding what class the image should be in . these are added together , and you have your % chance of what class the image is . confusing points of this image to me : why are we going from one image of $ 224 \times 224 \times 3 $ to two images of $ 224 \times 224 \times 64 $ ? why does this halving continue ? what is this meant to represent ? it continues on to $ 56 \times 56 \times 256 $ . why does this number continue to halve , and the number , at the end , the $ 256 $ , continues to double ?",19860,2444,2019-04-12T18:48:29.457,2019-05-12T19:01:25.750,explanation of this convolution neural network diagram,machine-learning deep-learning convolutional-neural-networks,1,0,
2426,8958,1,,2018-11-13T17:22:08.943,1,36,"( un - original ) idea : would n't it be cool if we could fact - check using an algorithm that could understand a whole bunch of documents ( e.g. scientific papers ) as higher - order logic ? question : what work has been done on this to date ? what i 've got so far : ( 1 ) i seem to recall there being prior work to create a subset of english ( i think intended for use in scientific writing ) that could be easily interpreted by an algorithm . this does n't quite get us to the algorithm described above ( as it 's restricted to a subset of english ) - but seems pertinent . ( 2 ) once parsed , i guess a resolution algorithm like that in prolog could be used to check wether a fact ( presumably also inputted as a logical statement ) contradicts the logic of the documents ?",19864,,,2018-11-14T09:29:04.163,possible to translate generic english - language document into higher - order logic ?,natural-language-processing logic,2,2,
2427,8960,1,,2018-11-13T19:49:51.370,1,33,"as the title says , i want to train a jordan network ( i.e. a particular kind of recurrent neural network ) using a certain number of time series . let 's say that $ x_1 , x_2 , \ldots x_n$ are $ n$ input time series ( i.e. $ x_i = [ x_{i,1 } , x_{i,2 } , \ldots , x_{i , t}]$ , where $ t$ is the length of the time series ) and $ y_1 , y_2 , \ldots y_n$ ( i.e. $ x_i = [ y_{i,1 } , y_{i,2 } , \ldots , y_{i , t}]$ ) are the corresponding target time series . more specifically , the target time series are just sequences of "" $ 0 $ s "" , which may end with sequences of "" $ 1 $ "" s. here i show you some example : $ $ y_i = [ 0 ~ 0 ~ 0 \ldots 0 ~ 0 ~ 1 ~ 1 ~ 1 \ldots 1 ~1 ] , $ $ $ $ y_i = [ 0 ~ 0 ~ 0 \ldots 0 ~ 0]. $ $ this means that i want that my machine "" learn to raise "" under some situations related to the corresponding inputs $ x_i$ . indeed , the objective of my network is to "" raise "" an alarm if "" something "" happens . at the moment , my training strategy is the following . i create a new time series which corresponds the concatenation of all the available $ x_i$ and $ y_i$ . let 's call the concatenated series $ x$ and $ y$ . then i use $ x$ and $ y$ to train a network . here is my problem . if i concatenate , then i also teach to my machine to "" drop "" , since i can have situation like this : $ $ y = [ \ldots 1 ~ 1 ~ 0 ~ 0 \ldots].$$ is this really a problem ? are there other "" training strategies "" to be employed so that i avoid this kind of unwanted behaviors ?",19868,19868,2018-11-13T19:55:34.103,2018-11-13T19:55:34.103,train a recurrent neural network by concatenating time series . is it safe ?,neural-networks machine-learning training recurrent-neural-networks,0,3,
2428,8962,1,,2018-11-13T23:49:24.553,4,302,"what is the actual learning algorithm : back - propagation or gradient descent ( or , in general , the optimization algorithm ) ? i am reading through chapter 8 of parallel distributed processing hand book and the title of the chapter is "" learning internal representation by error propagation "" by pdp research group . https://web.stanford.edu/class/psych209a/readingsbydate/02_06/pdpvolichapter8.pdf if there are no hidden units , no learning happens . if there are hidden units , they learn internal representation by propagating error back . does this mean back propagation[delta rule ] is the learning rule and gradient descent is an optimization algorithm used to optimize cost function ?",18195,18195,2018-11-17T01:01:56.193,2018-11-19T07:35:17.367,what is the actual learning algorithm : back - propagation or gradient descent ?,machine-learning backpropagation terminology optimization,4,2,
2429,8971,1,,2018-11-14T11:47:31.657,1,80,"i am confused by how her learns from unsuccessful trajectories . i understand that from failed trajectories it creates ' fake ' goals that it can learn from . ignoring her for now , if in the case where the robotic arm reaches the goal correctly , then the value functions ( v ) and action - value functions ( q ) that correspond to the trajectories that get to the goal quicker will increase . these high q and v values are ultimately important for getting the optimal policy . however if you create ' fake ' goals from unsuccessful trajectories - that would increase the q and vs of the environment that lead to getting the ' fake ' goal . those new q and vs would be unhelpful and possibly detrimental for the robotic arm to reach the real goal . what am i misunderstanding ?",19895,19895,2018-11-14T12:13:55.137,2018-11-14T12:13:55.137,how does hindsight experience replay learn from unsuccessful trajectories,deep-learning reinforcement-learning q-learning,0,3,
2430,8974,1,,2018-11-14T21:10:49.883,1,68,"i am working on the blockchain technology and i am not very familiar with the ai concept . the proposal of this web page : ( http://www.euraxess.lu/jobs/349354 ) opens a discussion about use of nature inspired artificial intelligent methods for traceability chain decision in blockchain technology as an alternative to current consensus mechanisms . it continues as follows : "" it has been demonstrated that using traceability chain is a more effective method . in traceability chain , since the mechanism has to trace related information among participant ’s nodes across the entire chain , the extraction and recognition of the data features plays a crucial role in improving the efficiency of the process . "" however , it does not give any example to demonstrate an instance of this approach . so , i searched in google.scholar and any ordinary web pages to find only an instance similar to this approach since it has mentioned : "" it has been demonstrated that using traceability chain is a more effective method . "" ( please read the web page ) is someone here familiar with this approach ? and if yes , is there any article / example to explain a more about this approach of consensus in blockchain ? and what does exactly mean "" traceability chain "" ? and also ingeneral , can we call this approach as a consensus ? the text is not really clear to me . please not that i have no idea about this proposal and just i 'd like to know if it 's practicable ? or it 's buzzwords ? also , may this approach related to the approach that has been mentioned in this answer ( swarm intelligence ) : https://ai.stackexchange.com/a/1315/19910 or it is a different concept ? thanks for your help",19910,19910,2018-11-15T10:08:47.340,2018-11-15T10:08:47.340,nature - inspired artificial intelligent methods for blockchain ?,algorithm swarm-intelligence,0,0,
2431,8975,1,,2018-11-14T21:18:26.050,0,33,"question : express each of the following tasks in the framework of learning from data by specifying the input space x , output space y , target function f : x->y and the specifics of the data set that we will learn from . a)medical diagnosis : a patient walks in with a medical history and some symptoms , and you want to identify the problem . * my answer : input space : medical history . output space : symptoms . target function : identify problem : medical history - > symptoms * b)handwritten digit recognition(for example postal zip code recognition for mail sorting ) my answer : input space : postal zip code . output space : handwritten digit recognition . target function : mail sorting c ) determining if an email is spam or not . my answer input space : email , output space : spam or not . target function determining d)predicting how an electric load varies with price , temperature , and day of the week . my answer : input space : price , temperature and day of the week . output space : electric load . target function : prediction . e)a problem of interest to you for which there is no analytic solution , but you have data from which to construct an empirical solution . no answer this is my question and i provide answer of those question except the last one . my question is that my answers are correct or not ?",18384,,,2018-11-15T07:32:07.733,doubts at basic step of learning problem,machine-learning classification training datasets problem-solving,1,0,
2432,8976,1,,2018-11-15T00:34:44.840,2,19,"i have three equations that relates five variables { a , b , c , r , s } with a sum and two ratios . eq . 1 : a = b + c ; eq . 2 : s = b / a ; eq . 3 : r = b / c. given two values for any of the five variables i get a solution . but , this is not the automation problem i want to solve . i can have the solution of variable r by simply knowing s . this is solved by a "" human algorithm "" as follows . substitute a of eq . 1 in eq . 2 . divide the second term of the new eq . 2 by the variable c. replace b / c by the expression of eq . 3 . that means s = r / ( r+1 ) . the questions is -how can an ai algorithm solve this ? , i.e. the machine should recognize that given the variable r she can obtain directly the variable s and do no require another variable .",19913,,,2018-11-15T06:56:07.867,"machine mathematical reasoning by clever substitutions , how to do with ai",reasoning,1,0,
2433,8978,1,,2018-11-15T03:46:46.673,-2,48,"if the number of input neurons and output neurons does n't change , what will change if i have one hidden layer , but first with 1 neuron , then with 4 neurons ? taking into consideration the fact that each perceptron is able to linearly separate points on an unknown / unwritten linear function , would this then be able to , theoretically , instead of simply linearly separate points , separate points into those that occur inside a square , and those that occur outside ? this is , of course , without a bias neuron present .",19917,2444,2019-05-01T17:07:49.387,2019-05-01T17:07:49.387,basic functions and results,ai-basics,1,3,
2434,8983,1,8993,2018-11-15T12:58:17.873,1,807,"in hill climbing methods , at each step , the current solution is replaced with the best neighbour ( that is , the neighbour with highest / smallest value ) . in simulated annealing , "" downhills "" moves are allowed . what are the advantages of simulated annealing with respect to hill climbing approaches ? how is simulated annealing better than hill climbing methods ?",19652,2444,2019-03-02T10:59:40.133,2019-03-02T10:59:40.133,how is simulated annealing better than hill climbing methods ?,search comparison hill-climbing simulated-annealing,1,0,
2435,8986,1,8991,2018-11-15T15:03:08.543,9,3885,what are the limitations of the hill climbing algorithm ? how can we overcome these limitations ?,19254,2444,2019-02-25T21:32:00.887,2019-02-25T21:32:00.887,what are the limitations of the hill climbing algorithm and how to overcome them ?,algorithm search optimization problem-solving hill-climbing,2,0,2
2436,8987,1,8988,2018-11-15T15:18:27.533,-1,98,i am new in artificial intelligence and want to getting start with deep learning in python . i know about the basics of python . what should i have to learn more ?,19254,3217,2018-11-15T16:54:27.510,2018-11-15T20:40:19.203,what are the prerequisites for starting out in deep learning ?,deep-learning python,2,0,1
2437,8990,1,8995,2018-11-15T16:27:40.773,2,39,"i am not sure the name of this kind of problem , but anyway , the situation is as below . assign teachers into groups and consider on each of their workload , availability etc . there are some other soft / hard constraint ( equality / inequality ) like each group should have at least 2 teachers everyone in the group have similar workload total workload in the group is below a certain value all are in different expertise and more ... i am trying to build a sub - optimal solution to solve this problem . linear / non - linear programming seems not working for grouping problems . i am thinking of genetic algorithm or reinforcement learning . can this problem solve by using rl or drl ? i am trying to define the groups as state , and actions include "" assigntogroup "" and "" removefromgroup "" . and any kind of idea or suggestion of how to solve this problem ? many thanks",19930,19930,2018-11-15T17:30:40.980,2018-11-15T20:49:19.087,reinforcement learning to grouped scheduling optimisation problem,reinforcement-learning optimization,1,0,
2438,8996,1,,2018-11-15T21:34:42.037,1,26,"is there neural machine translation methods , that for one input sentence outputs multiple alternative output sentences in that target language . it is quite possible , that sentence in source language have multiple meanings and it is not desirable that neural network discards some of the meanings if there is no context for disambiguation provided . how multiple outputs can be acommodated into encode - decoder architecture , or different architecture is required ? i am aware of only one work https://arxiv.org/abs/1805.10844 ( and one referen herein ) but i am still digesting whether their network outputs multiple sentences or whether it just acommodates variations during training phase .",8332,8332,2018-11-15T21:46:50.920,2018-11-15T22:06:03.700,"neural machine translation , that outputs multiple alternative , ambiguous translations ?",neural-networks computational-linguistics,1,0,
2439,8999,1,,2018-11-16T11:47:11.507,2,43,"there are 4 kinds of adverbs : adverbs of manner . for example , slowly , quietly adverbs of place . for example , there , far adverbs of frequency . for example , everyday , often adverbs of time . for example , now , first , early nltk , spacy and textblob only tag a token as an adverb without specifying which kind it is . are there any libraries which tag including the type of adverb ?",19962,1847,2018-11-16T12:16:36.087,2018-11-16T15:09:19.593,how to know which kind of adverb in nlp parts of speech ( pos ) tagging ?,natural-language-processing,2,1,
2440,9001,1,9003,2018-11-16T13:02:07.620,2,81,"i am reading the book titled "" reinforcement learning : an introduction "" ( by sutton and barto ) . i am at chapter 5 , which is about monte carlo methods , but now i am quite confused . there is one thing i do n't particularly understand . why do we need the state - transition probability function when calculating the importance sampling ratio for off - policy prediction ? i understood that one of the main benefits of mc over dynamic programming ( dp ) is that one does not need to have a model of the state - transition probability for a system . or is this only the case for on - policy mc ?",18266,2444,2019-02-25T15:27:10.067,2019-02-25T15:30:36.080,do we need the transition probability function when calculating the importance sampling ratio ?,reinforcement-learning on-policy monte-carlo importance-sampling dynamic-programming,1,0,
2441,9005,1,9009,2018-11-16T15:40:10.037,3,873,"local search algorithms are useful for solving pure optimization problems , in which the aim is to find the best state according to an objective function . my question is what is the objective function ?",19254,1847,2018-11-16T16:18:19.263,2018-11-18T16:40:38.893,what is an objective function ?,algorithm search problem-solving,2,0,1
2442,9007,1,9223,2018-11-16T17:34:30.133,3,79,"i am building a generative model chatbot as a research and learning project . one of the most important parts of my project is to research ways in which i can make this chatbot work in a consistently ethical fashion . this chatbot is simply a single seq2seq network running on my local machine . it ca n't be interacted with over the internet ( yet ) , although i may end up creating a way to do that . it has no feedback loops of any kind as of right now , though reinforcement learning with a loop might be helpful . the idea is that there would be some sort of unchanging knowledgebase for the chatbot to use that has hard - coded ethical statements and values that the bot has no ability to change . whenever a question is asked of the chatbot , before being inputted to the network , the knowledgebase is searched and relevant facts will be appended to the input ( separated from regular inptu by tokens . my question is , will this even be effective at allowing it to generate its own responses yet still be confined to the ethical standards given it ? my main concern is that it may begin to ignore these "" facts "" over time , and they may become irrelevant . another ( possibly much better ) approach might be to use deep reinforcement learning . however i may find it difficult to implement with my existing sequence - to - sequence network . so which would likely be better ? or perhaps i should try a combination of the two ?",17432,17432,2018-11-16T18:50:53.837,2018-11-27T21:33:43.380,what is the best way to integrate unchangeable ethics into a chatbot,reinforcement-learning chat-bots ethics,2,5,
2443,9010,1,,2018-11-16T22:59:49.967,0,36,"let 's say there are two types of cancer(type 1 and type 2 ) . say we want to see if one of pour friends has cancer type 1 or 2 . we can treat this as a classification problem . but what if we use unsupervised learning ( clustering ) to separate the data into to 2 different groups and see each whether each item in group 1 belongs to a person with cancer type 1 or 2 . we will then see whether our friend belongs to group 1 or 2 . i know it is stupid to do this and we have to do extra work but can we even do this ? let 's say that the features are only the age and the height ( i know it 's really dumb but just bear with me ) . the data associated with people with cancer type 1 is [ 10 , 150 ] , [ 12 , 153 ] , [ 9 , 143 ] , [ 13 , 160 ] and for people with type 2 cancer : [ 20 , 175 ] , [ 23 , 180 ] , [ 19 , 174 ] . let 's say we plot the data on a graph ( without labelling the data ) and the unsupervised program ( clustering ) just separates the two groups ( say group 1 for type 1 ) . we then can see that to whom each data in group 1 belongs . we see those people have cancer type 1 . so given new data , we see what group our friend belongs to . if she / he belonged to group 1 , he 's got cancer type 1 and if not , she / he has cancer type 2 .",17894,1847,2018-11-17T08:50:47.707,2019-04-16T10:03:25.723,using unsupervised learning for classification problems,classification unsupervised-learning,1,2,
2444,9011,1,9037,2018-11-17T06:03:54.540,0,54,"with the advancement of deep learning and a few others automated features learning techniques , manual feature engineering started becoming obsolete . any suggestion on when to use manual feature engineering , feature learning or a combination of the two ?",17980,17980,2018-11-18T07:16:25.223,2018-11-18T11:27:03.863,feature learning vs feature engineering vs feature learning and engineering,deep-learning image-recognition feature-selection,1,0,
2445,9014,1,9015,2018-11-17T11:39:38.267,1,184,"i 'm trying to implement a deep q - network in keras / tf that learns to play minesweeper ( our stochastic environment ) . i have noticed that the agent learns to play the game pretty well with both small and large board sizes . however , it only converges / learns when the layout of the mines is the same for each game . that is , if i randomize the mine distribution from game to game , the agent learns nothing - or near to it . i tried using various network architectures and hyperparameters but to no avail . i tried a lot of network architectures including : the input to the network is the entire board matrix , with the individual cells having values of -1 if unrevealed , or 0 to 8 if revealed . the output of the network is also the entire board representing the desirability of clicking each cell . tried fully connected hidden layers ( both wide and deep ) tried convolutional hidden layers ( tried stacked them , using different kernel sizes , padding .. ) tried adding dropout after hidden layers too is dqn applicable for environments that change every episode or have i approached this from the wrong side ? it seems no matter the network architecture , the agent wo n't learn . any input is greatly appreciated . please let me know if you require any code or further explanations . thank you .",19981,,,2018-11-17T12:48:41.930,deep q - learning poor convergence on stochastic environment,reinforcement-learning deep-network keras q-learning convergence,1,2,
2446,9016,1,9201,2018-11-17T13:00:53.227,3,103,"the domain of emergency call for clogged pipelines has to do with taking a call and managing the reaction of plumber departments . it is mostly a group oriented communication situation between the caller , the first level call taker , the second level dispatcher and external stations in the back office . from a linguistic point of view , there are different kind of speech acts available . for example paraphrasing which is the repetition of previous speech with own words , or counter - speech which is criticizing something said before . modeling all the different social roles , their usage of speech acts and make the overall decision process transparent is a difficult task . i 've searched a bit for existing papers about the subject , but it seems that the domain was n't explored yet . my question is : is it possible to create some kind of chatbot population which talks to each other back and forth and is able to simulate an emergency dispatching task which includes conflicts between the operators and contrasting point of views about how to handle a certain situation under resource limitations ?",11571,1847,2018-11-17T14:32:14.960,2018-11-27T10:11:07.590,how to model a plumber dispatcher with artificial intelligence ?,natural-language-processing game-ai,1,1,
2447,9017,1,9043,2018-11-17T13:22:49.690,0,141,what is the difference between breadth first search and recursive best first search ? how can i describe the key difference between them ?,19657,19657,2018-11-17T15:12:25.037,2018-11-18T18:00:08.380,breadth first search vs recursive best first search ?,algorithm search breadth-first-search,1,2,
2448,9019,1,9021,2018-11-17T13:46:26.643,2,491,"recently , i have come across the information ( lecture 8 and 9 about mdps of this uc berkeley ai course ) that the time complexity for each iteration of the value iteration algorithm is , where $ |s|$ is the number of states and $ |a|$ the number of actions . here is the equation for each iteration : $ $ v_{k+1}(s ) \gets \max_a \sum_{s ' } t(s , a , s ' ) [ r(s , a , s ' ) + \gamma v_k(s ' ) ] $ $ i could't understand why the time complexity is . i searched the internet , but i did n't find any good explanation .",19984,2444,2019-02-16T02:15:23.207,2019-02-16T02:15:23.207,what is the time complexity of the value iteration algorithm ?,reinforcement-learning algorithm time-complexity value-iteration,1,0,
2449,9022,1,,2018-11-17T15:47:15.667,0,588,"i 'm new to machine learning and i was watching a video about gradient descent.it said that we want our cost function(mean squared error ) to have the minimum value but that minimum value shown in the graph was n't 0;it was a negative number!how can our cost function which is mean squared error have a value under 0?the square of a real number is always positive.even if it is possible , do n't we want our error to be 0 ? can someone explain it please ? p.s : the video was prof.ng's machine learning course . thank you in advance for any help !",19988,9203,2018-11-19T17:21:42.100,2018-11-19T17:21:42.100,can the value of a cost function be negative ?,machine-learning gradient-descent loss-functions,2,0,
2450,9024,1,,2018-11-17T16:23:25.537,1,153,"apparently , in the q - learning algorithm , the q values are not updated according to the "" current policy "" , but according to a "" greedy policy "" . why is that the case ? i think this is related to the fact that q - learning is off - policy , but i am also not familiar with this concept .",19984,2444,2019-02-13T02:36:41.727,2019-02-13T22:13:05.417,why are q values updated according to the greedy policy ?,reinforcement-learning q-learning,1,0,
2451,9025,1,9027,2018-11-17T17:36:55.853,0,117,"i have done a lot of research on the internet about reinforcement learning and i found encountered methods of reinforcement learning : q - learning and deep q - learning . and i have developed a vague idea of how these two work . before i knew anything about reinforcement learning this is how i thought it would work : suppose i have 2 virtual players in a game who can shoot each other , one of them is a decent playing hard - coded / pre - coded ai , and the other one is the player i want to train ( to shoot the other player and dodge his bullets ) , the aim of the game would be to get the greatest net score ( shots you hit minus shots you took ) within 1 minute ( a session ) , and you only have 20 bullets . you have 3 actions , move left - right ( 0 = maxleftspeed , 1 = maxrightspeed ) , jump ( 0 = don't jump , 1 = jump ) , shoot ( 0 = do n't shoot , 1 = shoot ) . what i thought was , you could create a basic feed - forward neural network use the enemy 's position and his bullet(s ) 's position(s ) and bullet direction(s ) for the input layer and the action being taken will be given by the ( 3 nodes in the ) output layer . the untrained player starts off with a randomized algorithm , then ( for back - propagation ) at the end of each session it modifies one of the parameters by a bit in the neural network , and a new session is started with the slightly modified nn . if this session ends with more points than the previous session , it keeps the changes and makes more changes towards that direction , otherwise , the changes are redone , or possibly reversed . i would visualise this as gradient descent similar to that of supervised learning . so my questions are : is something like this already out there ? what is it called ? if nothing like this is out there , could you give me any tips to optimize this method or point out any key points i should keep in minds while carrying this out ? since i have written this game , i have control over the speed of the actions , but if i did not , i know this ai would take ages to learn , so is there any way to make the learning faster while still keeping the basic idea in mind ? how exactly is this different from deep q - learning ( if it is ) ? thanks in advance !",18711,2444,2018-11-18T10:57:59.313,2018-11-18T10:57:59.313,is it possible to use a feed - forward neural network to predict the actions in reinforcement learning ?,deep-learning reinforcement-learning game-ai q-learning,1,0,
2452,9026,1,,2018-11-17T18:43:24.567,1,40,"first of all i 'm very new to the field . maybe my question is a bit too naive of even trivial .. i 'm currently trying to understand how can i go about recognizing different faces . here is what i tried so far and the main issues with each approach : 1 ) haar cascade - > hog - > svm : the main issue is that the algorithm becomes very indecisive when more than 4 people are trained .. the same occurs when we change haar cascade for a pre - trained cnn to detect faces .. 2 ) dlib facial landmarks - > distance between points - > svm or simple neural network classification : this is the current approach and it behaves very well when when 4 people are trained .. when more people are trained it becomes very messy , jumping from decision to decision and never resolves to a choice . i 've read online that triplet loss is the way to go .. but i very confused as to how i d go about implementing it .. can i use the current distance vectors found using dlib or should i scrap everything and train my own cnn ? if i can use the distance vectors how would i pass the data to the algorithm ? is triplet loss a trivial neural network only with it 's loss function altered ? i 've took the liberty to show exactly how the distance vectors are being calculated : the green lines represent the distances being calculated a 33 float list is returned which is then fed to the classifier here is the relevant code for the classifier ( keras ) : def fit_classifier(self ) : x_train , y_train = self._get_data(self.train_data_path ) x_test , y_test = self._get_data(self.test_data_path ) encoding_train_y = np_utils.to_categorical(y_train ) encoding_test_y = np_utils.to_categorical(y_test ) model = sequential ( ) model.add(dense(10 , input_dim=33 , activation='relu ' ) ) model.add(dense(20 , activation='relu ' ) ) model.add(dense(30 , activation='relu ' ) ) model.add(dense(40 , activation='relu ' ) ) model.add(dense(30 , activation='relu ' ) ) model.add(dense(20 , activation='relu ' ) ) model.add(dense(10 , activation='relu ' ) ) model.add(dense(max(y_train)+1 , activation='softmax ' ) ) model.compile(loss='mse ' , optimizer='adam ' , metrics=['accuracy ' ] ) model.fit(x_train , encoding_train_y , epochs=100 , batch_size=10 ) i think this is a more theoretical question than anything else .. if someone with good experience in the field could help me out i 'd be very happy !",19992,,,2018-11-17T18:43:24.567,machine learning approach to facial recognition,neural-networks convolutional-neural-networks classification facial-recognition,0,0,1
2453,9030,1,,2018-11-17T19:53:45.473,0,18,"lets say we have a data - set of all cats and we have to identify the cat breed based on given test image . as , the two different cat breeds have visual similarity can we use existing networks ( vgg , imagenet , googlenet ) to solve this problem ? should facenet be applied here ? as , the problem is similar to face detection where face characteristics of two different people are same yet it can correctly recognize a person . what if with visual similarity in data - set we have only few example of each class ? like for a problem ( random ) we have good amount of data but for each class we have only few examples . is there any model that can be applied here ?",19966,,,2019-04-16T22:24:41.920,image prediction model when data - set classes have visual similarity,neural-networks convolutional-neural-networks image-recognition,1,0,
2454,9032,1,9036,2018-11-17T21:49:50.503,-2,57,"this question isn´t about a problem , so i apologize for that . i want to learn everything related to ai but i do nt know where to start . i am studying computer science , first year of the career . any advice , or a good book , or something that can help to start from 0 ? thanks for your time .",19998,,,2018-11-18T12:03:48.067,help . i want to learn about this field,ai-community,1,2,
2455,9033,1,,2018-11-17T22:06:42.443,1,59,"in q - learning , during training , it does n’t matter how i select actions . the algorithm always converges to optimal optimal policy . why does this happen ?",19984,2444,2019-02-13T02:36:27.970,2019-02-13T02:36:27.970,why does q - learning converges to optimal policy even if i am acting suboptimally ?,reinforcement-learning q-learning proofs,0,4,1
2456,9039,1,,2018-11-18T13:40:03.443,1,127,"are there examples of applications in blockchain consensus using swarm intelligence , as opposed to classical consensus mechanisms like pow or pbft ? please note that recent classical consensuses , including lottery - based such as pow in which the winner of lottery creates the new block , or voting - based such as pbft or paxos in which the entities achieve a consensus through a voting process ; both approaches have problems of efficiency , latency , scalability , performance etc . and emerging a new alternative approach seems necessary . in this way , can nature - inspired algorithms ( such as evolutionary algorithms ( ea ) , particle swarm optimization ( pso ) , ant colony optimizatio n ( aco ) etc ) be employed as an alternative to classical consensus algorithms ?",19910,19910,2018-11-18T21:40:03.827,2018-11-18T21:40:03.827,how swarm intelligence can empower blockchain ?,terminology evolutionary-algorithms applications swarm-intelligence,0,0,
2457,9046,1,,2018-11-18T19:47:26.853,2,62,"i am reading the paper on regularization of dnns by louizos , welling and kingma ( 2017 ) ( link to arxiv ) . in section 2.1 the authors define the cost function as follows : $ $ \mathcal{r}\left ( \tilde{\theta } , \pi \right ) = \mathbb{e}_{q(z|\pi)}\left [ \frac{1}{n } \left(\sum_{i=1}^n \mathcal{l}\left(h\left ( x_i , \tilde{\theta}\circ z\right ) , y_i \right ) \right)\right ] + \lambda\sum_{i=1}^{|\tilde{\theta}|}\pi_i . $ $ in the above display , are the weights , $ z$ is a random vector of the same dimension as consisting of independent bernoulli components $ q(z_i|\pi ) \sim bernoulli ( \pi_i)$ , and is the element - wise product . the authors then state the following : the first term is problematic for due to the discrete nature of $ z$ , which does not allow for efficient gradient based optimization . i am not sure i understand this . denoting the first term by ( $ r_i$ defined below ) , and using the notation and for the set of all possible values of $ z$ , we should have $ $ r_i : = \mathbb{e}_{q(z|\pi)}\left [ \mathcal{l}\left ( h\left(x_i , \tilde{\theta}\circ z\right ) , y_i \right ) \right ] = \sum_{z \in \mathcal{z}}\pi_z\mathcal{l}\left ( h\left(x_i , \tilde{\theta}\circ z\right ) , y_i \right ) $ $ so , it seems to me that the gradient of $ r_i$ with respect to can be obtained as $ $ \frac{d r_i}{d\pi_j } = \sum_{z \in \mathcal{z}}\mathcal{l}\left ( h\left(x_i , \tilde{\theta}\circ z\right ) , y_i \right ) \frac{d\pi_z}{d\pi_j } $ $ and if $ z_j=1 $ and $ -\frac{\pi_z}{1-\pi_j}$ if $ z_j=0 $ . so , it appears that we can obtain the derivative of the first term with respect to as well . my question is the following : if my above calculation is correct , then the derivatives can be computed , and we can perform sgd on the cost function . but the authors claim that it can not be obtained and hence they introduce the ` hard concrete ' distribution etc . to construct a differentiable cost function .",20016,20016,2018-11-22T20:03:07.243,2018-11-22T20:03:07.243,"regarding l0 sparsification of dnns proposed by louizos , kingma and welling",deep-learning training regularization,0,1,
2458,9047,1,9051,2018-11-18T22:00:47.657,1,32,"apparently , one can buy a special - purpose integrated circuit ( an ic like this one , for instance ) to host a convolutional neural network . question is such a circuit digital ? except for digital random - number generation , is the circuit 's behavior deterministic ? what kind of hardware is this ? reference already asked and answered : "" if digital values are mere estimates , why not return to analog for ai ? "" further details what i mean is this : suppose that you and i each acquired a special - purpose neural ic , same make , same model . suppose that you and i configured and trained our circuits alike . except for digital random - number generation , would our neural nets behave identically ? or would analog effects cause our nets to turn oppositely on some closely balanced edge case ? if it is plain to you that a misconception has caused me to pose the question improperly , then a correction to the question along with an answer would be appreciated . if you wish to know my background , i am an electrical power engineer in my 50s .",20018,20018,2018-11-18T22:30:24.880,2018-11-19T00:35:27.110,are commercially available neural ics digital ?,convolutional-neural-networks hardware,1,0,
2459,9050,1,9052,2018-11-19T00:11:07.600,4,204,"i did some self - study to learn neural network , object detection , and deep learning . now i started implementing yolov3 . i am looking for some website that i can communicate with people and make friends who are learning and implementing deep learning algorithms like me . it will speed up my learning since self - studying gets boring sometimes . i was wondering if you could introduce some website that i can find peoples like myself .",20025,1671,2018-11-19T17:16:52.537,2018-11-28T23:18:00.907,where can i discuss with deep learning beginners ?,neural-networks machine-learning deep-learning online-resources,5,3,1
2460,9057,1,,2018-11-19T10:41:46.293,2,143,"is ai limited by the fact that it requires us to give it a task or goal to achieve ? it has all the capability to get to that goal in ways we might not think of but it still only gets to a goal we can imagine , how do we get ai to think of goals or tasks that go beyond us ? do we create a sense of ' passion ' in the ai to drive it to better than the goal it is given , if so how do we quantify a goal we see as 100 % of our need when in fact it could be only a fraction of that ?",20033,4302,2018-12-08T05:24:46.693,2019-05-30T08:02:05.690,is the imagination of ai limited to our own imagination ?,ai-design automation feasibility,4,3,
2461,9059,1,,2018-11-19T14:37:27.787,2,15,"hebb 's postulate attempts to explain associative learning via the processes of sampling ( using sensors ) , emitting responses and receiving feedback . this is a form of control orientated architecture using an environment and biological systems . is this the progenitor of ai ?",19204,1671,2018-11-19T18:04:33.333,2018-11-19T18:04:33.333,is hebbian learning the progenitor of ai ?,machine-learning architecture biology,0,1,
2462,9062,1,9143,2018-11-19T16:06:10.987,3,70,"i read about the hill climbing algorithms , the simulating annealing algorithm , but i am confused . what is the basic purpose of local search methods ?",19442,2444,2019-03-02T11:11:48.373,2019-03-02T11:12:18.327,what is the basic purpose of local search methods ?,optimization search hill-climbing simulated-annealing local-search,1,0,1
2463,9065,1,,2018-11-19T20:03:25.013,2,47,"i was watching a video which tells a bit about reinforcement learning , and i learnt that if the robot makes wrong movement then they train the network with negative learning rate . from this method , something came to my mind . my question is "" can i use a wrong data to train a neural network ? "" . to illustrate the method , i 'll be using the eye tracker project that i 'm working on right now . in my project there are photos and the points that corresponds the locations that i m looking to at that photo . its like grid ( 9 , 16 ) . if i look to the middle of the screen , it means the output is ( 4 , 7.5 ) . if i look left up side of the screen it means ( 0,0 ) . normally for a photo that i 'm looking into the middle , we use that photo as input and ( 4 , 7.5 ) as output to train network using positive learning rate . now let me rephrase the question . can i train a model giving a photo that i 'm looking into the middle as input and ( 0,0 ) as output(label ) using negative learning rate ? thank you , if i made a mistake against the rules of stackoverflow , i 'm so sorry . i 'll be waiting your valuable answers . edit : this is a conversation between me and someone from stackoverflow , i 'll let you read , hope you get a point . - > yes , you can . but , what would be the reason of passing a wrong ground truth to your training process ? – neb 14 hours ago - > if i have no various data to train , i can create more data via this method to increase the certainty when i use squared error loss . but i have doubts about this method . for example lets assume we have a photo named ' x ' and its label is ( 5,5 ) . at first epoch , let the model gives ( 2,2 ) for photo ' x ' . if i try to train network with a photo x and label - > ( 4,4 ) using negative learning rate , it might send away the point from ( 2,2 ) to ( 1,1 ) whereas we expect it to send the point ( 2,2 ) to ( 5,5 ) . did you get what i meant ? – faruk nane 14 hours ago - > you are right . using a negative learning rate and a wrong ground truth will not necessarly make the learning process converge to the optimal value for your net 's parameters – neb 13 hours ago - > so can i say that "" when i 'm sure that the absolute error for each case is less than 2 , i can use this method using points away 2 units . "" so it 'll make the outputs closer to the target point . i do n't really know if we can easily say that . because we consider this method as if there are only 2 parameters which is the output point . however a model has many parameters so it might affect so differently . my brain is so confused . i think this might be an academic work , right ? – faruk nane 13 hours ago - > well , it is difficult to suggests you the path to follow without knowing the exact specifics of your problem . in any case , if you 're trying to solve this problem for fun or self - improvement , i 'd suggest you to experiment with the solutions you came up with and see if they works . – neb 13 hours ago //edit : up up",16864,16864,2018-11-20T16:10:10.293,2018-11-20T16:10:10.293,setting learning rate as negative number for wrong train cases,deep-learning,0,1,
2464,9076,1,9079,2018-11-20T15:25:44.293,4,861,"in the context of evolutionary computation , in particular genetic algorithms , there are two stochastic operations "" mutation "" and "" crossover "" . what are the differences between them ?",19254,2444,2019-02-25T21:30:47.363,2019-02-25T21:30:47.363,"what is the difference between "" mutation "" and "" crossover "" ?",genetic-algorithms evolutionary-algorithms difference crossover mutation,2,0,1
2465,9077,1,9083,2018-11-20T15:34:16.527,1,90,"in genetic algorithm , there are different steps . one of those steps is selection of chromosomes for reproduction ( evolution ) . in this step there are different methods are used for selection of chromosomes , one of them is "" roll - it selection "" . what are the other methods ?",19254,3217,2018-11-20T16:29:50.823,2018-11-20T18:09:50.713,selection methods in genetic algorithms,genetic-algorithms,1,0,
2466,9078,1,,2018-11-20T15:46:37.783,2,644,"i 'm working in a company that opens restaurants in enterprises . every day at lunch , we want our clients to be able to scan their trays , sothat the food is detected automatically thanks to ai / image recognition . technically speaking , we have a number of food items that grows over time in our database , but everyday there are about 30 items available at the same time in the restaurant . about 5 items are changing each day ( for example , the main dish changes , but the bottle of water is always the same ) . this means , when the client go to the till to pay , the client will place the tray by himself , the camera will take photos of the tray and will try to identify different items separetely among the 30 items available this day . clients pay per food article , which means we do n't need to track the weight or the quantity in the main dish . i have absolutely no experience in ai / ml and do n't know how to start for my need , i 'm a web developer . which tool should i look at first ? which skill do i need to acquire ? i mean , are there easy to use high level libraries , or do i need to learn ml from scratch ? first i was thinking of amazon recognition or google vision but it seems to be made for recognizing any food item among their own database . my need seems easier since i just need to recognize several items on a tray among 30 known items . thanks a lot for your help .",20067,1671,2018-11-20T20:42:39.530,2018-12-08T06:59:20.120,which ai tool for food recognition ?,machine-learning image-recognition computer-vision software-evaluation,2,6,2
2467,9081,1,9172,2018-11-20T16:54:42.003,1,64,"what are the many ways that artificial intelligence robots protect their existence ? isaac asimov 's "" three laws of robotics "" a robot may not injure a human being or , through inaction , allow a human being to come to harm . a robot must obey orders given it by human beings except where such orders would conflict with the first law . a robot must protect its own existence as long as such protection does not conflict with the first or second law .",20068,1671,2018-11-20T20:45:30.773,2018-11-26T03:48:28.027,how does a robot protect its own existence,mythology-of-ai asimovs-laws survival,2,0,
2468,9085,1,,2018-11-20T21:10:18.993,1,59,"i am using ppo with an lstm agent . my agent is performing 10 actions for each episode , one action is corresponding to one lstm timestep and the action space is discrete . i have only one reward per episode which i can compute after the last action of the episode . for each timestep ( ~ action ) my agent has 20 choices . the following plot shows the reward ( y - axis ) versus the current episode ( x - axis ) . the plot shows a decreasing reward because i want to mimize this reward so i use : minus of the true reward . at the beginning of the process , the agent seems to learn very well and the reward is decreasing but then it 's converging to a value which is not the best . when i look at the results of my experiment it appears that the index of all actions are same ( for example the agent is always choosing the second value of my discrete action space ) . does anyone have an idea about what is happening here ?",20079,,,2018-11-20T21:10:18.993,why are all the actions converging to the same index ?,reinforcement-learning recurrent-neural-networks,0,0,
2469,9086,1,,2018-11-21T00:41:26.140,0,19,"in a paper published 6 years ago a novel domain specific language ( dsl ) for character animation was presented . high - level action commands are described in the xml format for authoring a virtual human . the problem is , that such kind of framework is to complex for the reality . the predecessor , “ confusius ” which was used for animation of a shadow play , was not very well documented and this new software xsampl3d has the same bottleneck . the xml schema for describing possible actions does n't make much sense , because it 's not possible to combine the motion primitives to a longer sequence . and the gantt chart in the middle of the paper looks a bit outdated . the idea is perhaps , to arrange the tasks on a timeline , but how can this be realized in xml ? if we are looking how often the paper was cited , we will recognize that it does n't attracted a large audience . a possible reason is that the concept of using a dsl ca n't be transferred to in - game ai problems , for example to play atari game with the openaigym . am i right ? vitzthum , arnd , et al . "" xsampl3d : an action description language for the animation of virtual characters . "" jvrb - journal of virtual reality and broadcasting 9.1 ( 2012 ) .",11571,,,2018-11-21T00:41:26.140,task and motion planning with domain specific languages,natural-language-processing game-ai,0,0,
2470,9088,1,9093,2018-11-21T04:53:24.057,-2,61,describe the basic algorithm for performing local search . give an example of a search problem for which it is an appropriate solution . when would an algorithm such as a * search be preferred ?,19204,,,2018-11-21T09:20:50.540,describe the basic algorithm for performing local search . when would an algorithm such as a * search be preferred ?,algorithm,1,1,
2471,9089,1,,2018-11-21T05:00:25.957,1,60,"when feature scaling for a multivariate linear regression housing predictor ( housing area and # bedrooms ) , the exercise suggested "" scale both types of inputs by their standard deviations and set their means to zero "" matlab method 1 : x = [ ones(m , 1 ) , x ] ; sigma = std(x ) ; mu = mean(x ) ; x(:,2 ) = ( x(:,2 ) - mu(2))./ sigma(2 ) ; x(:,3 ) = ( x(:,3 ) - mu(3))./ sigma(3 ) ; matlab method 2 : why not simply scale inputs between zero and one ? i.e. divide by the maximum : x_range = max(x ) x(:,2 ) = ( x(:,2)/x_range(2 ) ) ; x(:,3 ) = ( x(:,3)/x_range(3 ) ) ; method 2 feature plots : the exercise can be reproduced on matlab or octave : question is there a computational advantage with the first method over the second method ?",18819,18819,2018-11-21T16:33:06.207,2018-11-26T08:35:33.180,gradient descent feature scaling,gradient-descent,2,1,
2472,9096,1,9100,2018-11-21T13:33:32.310,1,156,"i 'm trying to train a dqn , so i 'm using openai gym and breakout ( breakout - v0 ) . i have altered the reward supplied by the environment : if the episode is not completed fully , the agent gets a -10 reward . could be this counterproductive for learning ?",9818,1847,2018-11-21T19:55:55.390,2018-11-21T20:44:30.470,dqn breakout adding an extra negative reward to help training ?,ai-design training game-ai dqn open-ai,1,2,
2473,9097,1,,2018-11-21T14:44:26.623,2,43,"if we take the vanilla variational auto - encoder ( vae ) , we $ p(z)$ is a gaussian distribution with zero mean and unit variance and we approximate $ p(z|x ) \approx q(z|x)$ to be a gaussian distribution as well , for each latent variable $ z$ . but what if $ z$ is a discrete variable ? what kind of distributions can be used to model discrete latent variables ? for example , what kind of distribution can be used to model $ p(z)$ and $ p(z|x ) \approx q(z|x)$ ?",18312,2444,2018-11-24T04:32:08.133,2018-11-24T04:32:08.133,what kind of distributions can be used to model discrete latent variables ?,machine-learning generative-model random-variable probability-distribution latent-variable,0,0,1
2474,9098,1,9099,2018-11-21T16:50:11.150,2,50,"i 've just started studying genetic algorithms and i 'm not able to understand why a genetic algorithm can improve if , at each learning , the ' world ' that the population encounters change . for example , in this demo ( http://math.hws.edu/eck/js/genetic-algorithm/ga.html ) , it 's pretty clear to me that the eating statistics will improve every year if bunches of grass grow exactly in the same place , but instead they always grow in different positions and i ca n't figure out how it can be useful to evaluate ( through the fitness function ) the obtained eating stats given that the next environment will be different .",20106,,,2018-11-21T18:02:22.970,how can a genetic algorithm adapt and get better in a changing environment ?,genetic-algorithms evolutionary-algorithms fitness-functions,1,0,
2475,9102,1,,2018-11-22T00:03:37.663,1,24,"say i have a batch of examples , each examples represent a state : [ 0.1 , 0.2 , 0.5 ] # 1st example [ 0.4 , 0.0 , 0.3 ] # 2nd example .......... [ 0.1 , 0.1 , 0.1 ] # 16th example i feed through the nn , and then the nn predict the following class : [ move up ] # 1st example [ move down ] # 2nd example ........ [ move left ] # 16th example and then i take the square loss ( which calculated to be 0.1 after taking average over 16 examples ) , and do backward propagation . so , can i assume that each of these examples will assign ( or contribute ) to a 0.1 loss ?",20113,,,2018-11-23T01:49:41.773,"training by one batch of examples , what does it mean",training backpropagation loss-functions,1,0,
2476,9103,1,,2018-11-22T00:26:12.780,4,110,"this question assumes a definition of ai based on machine learning , and was inspired by this fun technology review post : source : is this ai ? we drew you a flowchart to work it out ( karen hao , mit technology review ) as the definition of artificial intelligence has been a continual subject of discussion on this stack , i wanted to bring it to the community for perspectives . the formal question here is : is an algorithm that is no longer actively learning an ai ? specifically , can applied algorithms that are not actively learning be said to reason ?",1671,1671,2018-11-22T01:16:53.380,2018-11-28T22:50:36.880,is an algorithm that is no longer actively learning an ai ?,machine-learning philosophy terminology theory definitions,3,5,
2477,9104,1,,2018-11-22T01:19:33.413,3,187,"i do n't know much about ai or chess engines , but what is the fundamental difference between alphazero and stockfish or rybka ?",17601,,,2018-11-22T09:41:18.810,how is alphazero different from stockfish or rybka ?,chess,1,0,
2478,9105,1,,2018-11-22T04:53:35.007,6,387,"in genetic algorithms , a function called "" fitness "" ( or "" evaluation "" ) function is used to determine the "" fitness "" of the chromosomes . creating a good fitness function is one of the challenging tasks in genetic algorithms . how would you create a good fitness function ?",19254,2444,2018-11-22T11:05:45.720,2019-05-16T02:32:51.023,how to create a good fitness function ?,genetic-algorithms evolutionary-algorithms fitness-functions,2,3,
2479,9106,1,9140,2018-11-22T06:11:55.800,0,410,"in evolutionary computation and in particular in the context of genetic algorithms , there is a stochastic operation called "" fitness function "" . the better a state , the greater the value of the fitness function for that state . what would be a good fitness function for the 8-queens problem ?",19652,1671,2018-11-26T02:14:58.200,2018-11-26T02:14:58.200,how to calculate fitness function of 8-queens problem ?,game-ai genetic-algorithms fitness-functions,1,3,
2480,9108,1,9111,2018-11-22T08:36:32.940,0,66,"i 'm new to ml / reinforcement learning . i 'm looking at source from https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py where reward is calculated with reward = cmp(score(self.player ) , score(self.dealer ) ) i 'm just curious why its ok to calculate reward based on hidden state ? a player only sees the dealers first card . self.dealer[0 ]",20118,,,2018-11-22T09:55:07.417,reinforcement learning rewards with internal state,reinforcement-learning,1,1,
2481,9121,1,,2018-11-23T13:25:40.337,1,21,"suppose , for simplicity sake , to be in a discrete time domain with the action set being the same for all states $ s \in \mathcal{s}$ . thus , in a finite markov decision process , the sets , , and have a finite number of elements . we could then say the following $ $ p(s',r | s , a ) = p\{s_t = s',r_t = r | s_{t-1}=s , a_t = a\ } ~~~ \forall s',s \in \mathcal{s } , r \in \mathcal{r } \subset \mathbb{r } , a \in \mathcal{a}$$ where the function $ p$ defines the dynamics of the finite mdp and $ p$ defines the probability . how could i extend this to a general mdp ? that is , an mdp where the sets , , and have n't a finite number of elements ? to be more precise , in my case , , and . my thought is that the equation above is still true , however , the probability is zero for each tuple $ s',r , s , a$ . is it sufficient to say that for finite mdp we have $ $ while in non - finite mdp ( supposing that the sets and are continuous ) we have $ $ or is it more complex than this ?",16199,16199,2018-11-23T13:43:48.613,2018-11-23T13:43:48.613,how to generalize finite mdp to general mdp ?,markov-decision-process finite-markov-decision-process,0,1,
2482,9123,1,9124,2018-11-23T14:24:55.930,3,75,the alpha zero paper says that the the first set of features are repeated for each position in a t = 8-step history . so what happens before the first 8 moves ? do they just repeat the starting position ?,19604,,,2018-11-23T16:00:43.560,alpha zero before move 8,deep-learning deep-network chess alphazero,1,0,
2483,9125,1,9127,2018-11-23T16:09:42.080,2,326,"in csp ( constraint satisfaction problem ) state is a "" black box""- any data structure that supports a successor function , heuristic function , and goal test . what is the successor function ?",19254,19254,2018-11-23T16:30:38.737,2018-11-23T16:30:38.737,what is successor function ?,definitions problem-solving,1,0,
2484,9126,1,,2018-11-23T16:26:03.107,1,233,backtracking search is the basic uninformed algorithm for csps ( constraint satisfaction problems ) . what are the directions along which backtracking efficiency can be improved ?,19254,,,2019-05-28T00:45:45.213,how to improve efficiency of backtracking ?,algorithm problem-solving,1,2,
2485,9128,1,,2018-11-23T17:17:43.920,2,242,"i am trying to build a q learning - based bot for board games , specifically monopoly . i am fairly new to q - learning and currently , i have only implemented some bots that can play simple games like tic - tac - toe , and frozen lake . while implementing monopoly game , the problem is related to the total number of states and i see there are some research papers which talk about representing these states using markovian decision process . my question is there any library or tutorial that i refer to build such a state representation ? while implementing frozen lake , i came across openai gym library which gives a default implementation of the environment . if i can get some pointers to something similar which to it for board games that would be really helpful .",20156,,,2018-11-24T07:01:43.250,how to build ai bots for board games like monopoly ?,reinforcement-learning q-learning,1,0,2
2486,9132,1,,2018-11-23T23:36:44.133,1,11,"suppose if i am building a linear regression model with one fully connected layer and a sigmoid with minimizing mean squared error as objective . why would the error surface be convex ? does finding the optimal parameters for this network mean we can not do better than this ? under what assumptions , this solution would be optimal ? if we relax the linearity assumption and add some non - linearity to the network can we do better than this ? why so ?",20163,,,2018-11-23T23:36:44.133,what does it essentially mean if the neural network has convex error surface ?,neural-networks optimization,0,0,
2487,9133,1,9135,2018-11-24T00:24:07.990,1,115,"if human intelligence and creativity are spawned from struggle and adversity would an adverse - free world of artificial intelligence abduct human intelligence ? ( google deepdream ) simplicity is the ultimate sophistication - leonardo da vinci neat neural networks and unsupervised learning recently i have created artificial intelligence that plays the classic game space shooter . by doing so it really made me think about the future of unsupervised learning and its consequences . when i was younger i was always told that humans were special and that we can do what machines can not . the main human aspect involved would be creativity . i never truly denied nor accepted this idea until recently . after creating the artificial intelligence i got a strong feeling that i only imagine parents get when their child takes their first step or the feeling geppetto had when pinnochio came to life before he was a "" real boy "" . i would not call it gratification , i would describe it more as a shock , like how a child might be shocked that the seed they planted at the age of 5 turned into a full grown tree by 18 . although creating the neural network was exciting afterward i thought deeply about how it may - depending on how you look at it - exorcize or abduct human creativity and intelligence . applied computational neural networks in the past few decades in which we take a back - seat from problem - solving / creativity and solely rely on artificial evolution ( like that of planting a tree , were we watch its growth ) have been rapidly increasing . this , of course , raises many questions about the impacts of artificial intelligence on humanity . by creating artificial general intelligence or applied artificial intelligence are we migrating our intelligence into a machine intelligence ? are we trading intelligence for pleasure and simplicity ? by this i mean with all our problems taken care of ( the ultimate form of simplicity ) , is there even a point for creativity or innovation ? we often hear about the singularity in which robots take over the planet ; however , if we strictly focus on the applications of artificial intelligence and just make life simpler - like how the invention of the spear made acquiring food easier - the real singularity to me would be when we begin to lose creativity . the flynn effect as a bell curve ( the flynn effect ) the flynn effect makes it seem as though our pure intelligence is on the rise . but with the increase in applications of neural networks and general unsupervised learning will the graph of the flynn effect resemble a bell curve rather than o(n ) or o(log n ) ? the flynn effect seems to be a response to globalization , a severe increase in the distribution of knowledge ( i.e. the internet ) , and the general way we see the world . but if we outsource our intelligence to an artificial network or artificial intelligence in general , would it not take a hit ? could this explain why we have not - to my knowledge - been visited by other "" intelligent "" lifeforms . if humans have only been around for a couple hundred thousand years , and as of late have increased technological advances like wildfire , then if my hypothesis is correct ( that we will lose intelligence ) intelligence must have a short fuse relative to the age of the universe . this would mean meeting other intelligent lifeforms would be like trying to hop off a car onto a bus 10 kilometers ahead on a highway . a dormant highway . my point is summarized by the question : does intelligence leave a species once their domain is mastered ? da vinci and anthropocentrism ( vitruvian man ) the human foot is a masterpiece of engineering and a work of art - leonardo da vinci as a species , it is easy to see ourselves as superior to the others that roam our planet , but if we take da vinci 's quote about simplicity at the top of this post as a truth then are we really better than others . for example , would a squirrel need to evolve intelligence if it reached peak evolution , where it could propagate with little adversity because it mastered its domain ? would an artificial intelligence help us in the same way master our domain , and as a product lose intelligence ? what is it good for at the singularity ? in engineering , we seem to have adopted the mindset of da vinci with regards to looking inwards to solve our problems ( not metaphysically , literally look inwards ) . by this i mean replicate evolution and biology ( i.e. neural networks ) . to me , it seems that we are giving our humanity away or transferring it to machines . in short : are we giving away our intelligence by creating artificial intelligence ?",20159,20159,2018-11-24T02:53:47.057,2018-11-28T23:46:39.010,will human cognitive evolution drown in response to artificial intelligence ?,philosophy human-like artificial-consciousness singularity,1,1,1
2488,9134,1,,2018-11-24T03:37:30.443,1,69,ai is used to develop bots if a person uses proper ontology then can we add emotions in agents and bots ?,19784,1671,2018-11-26T01:13:07.910,2018-11-26T01:47:19.733,do bots and intelligent agents have personalities and emotions ?,intelligent-agent chat-bots anthropomorphism,1,0,
2489,9136,1,9151,2018-11-24T05:08:34.357,1,56,which search algorithm will use a limited amount of memory in online search mention its name ?,18950,18950,2018-11-24T16:22:45.273,2018-11-24T21:25:32.623,which search algorithm will use a limited amount of memory in online search ?,algorithm online-resources,1,1,
2490,9137,1,,2018-11-24T06:07:42.353,1,44,"recently i worked on a paper by hao wang , collaborative deep learning for recommender systems ; which uses a two way tightly coupled method , collaborative filtering for item correlation and stacked denoising autoencoders for the optimization of the problem . i want to know the limitations of using stacked autoencoders and hierarchical bayesian methods to recommender systems .",10118,1671,2019-04-04T20:40:18.600,2019-05-04T23:02:30.620,what are some limitations of using collaborative deep learning for recommender systems ?,deep-learning autoencoders recommender-system,1,0,
2491,9141,1,10628,2018-11-24T13:44:39.893,7,2309,"i am a new learner in nlp . i am interested in the sentence generating task . as far as i am concerned , one state - of - the - art method is the charrnn , which uses rnn to generate a sequence of words . however , bert has come out several weeks ago and is very powerful . therefore , i am wondering whether this task can also be done with the help of bert ? i am a new learner in this field , and thank you for any advice !",20170,2444,2019-02-17T11:24:05.117,2019-03-24T21:15:53.543,can bert be used for sentence generating tasks ?,neural-networks deep-learning natural-language-processing,3,1,2
2492,9146,1,9157,2018-11-24T17:05:31.833,1,130,"a paper from machinelearningmastery.com on human activity recognition states that 1d convolutional neural networks work the best on classification of human activities using data from accelometer . but , according to me , human activities like swinging the arm are sequential actions and they require lstms . so , they one should be more efficient , cnns or lstms . or , in other words , is spatial learning required or sequence learning ?",18985,,,2018-11-25T11:50:15.503,most efficient neural network for human activity recognition,machine-learning convolutional-neural-networks lstm efficiency,1,4,
2493,9150,1,9155,2018-11-24T21:09:49.787,0,46,"i think i missunderstood something when it comes to the sse (= sum of squared errors ) as a loss function . so : the formula for the sse is : but , if ti - oi is negative , doessn't the power of 2 eliminate any negative result ? how can then exist any negative gradient at the output ? what if the output is too high and so the gradient should be negative , meaning that the weights ( on average ) have to be decreased ? how can it be differentiated between ' output too high ' and ' output too low ' ? sorry if my question is absurd , but i ca n't wrap my head around this . thanks in advance",17769,,,2018-11-25T09:29:41.167,sse involving power of 2 eliminates negative gradient ?,loss-functions,1,0,
2494,9152,1,,2018-11-24T22:37:23.793,3,260,"i 've been thinking if machine learning can be used to play the game scrabble . my knowledge is limited in the ml field , thus i 've seeking some pointers :) i want to know how could i possibly build a model that picks a move from all the given valid moves of the current game state , and then plays the move and wait for the delayed reward . the actions here are n't static actions , they are basically selecting move to maximize the final score . is there any way to encode the valid moves and then use a model to pick those moves ? i 've also considered the genetic approach , but i think if i can represent my move with a set of features ( score , consonantvowels ration , rack leave score , # blank tiles after the move , ... etc ) , training a neural network like this could take a long time . another training related question , is it feasible to run the training on a gpu given that i will be waiting for a response ( the new game state ) from the opponent ( e.g. quackle ) after every action ? thank you :)",20180,20180,2018-11-26T21:31:14.243,2018-12-21T02:43:26.837,scrabble game using machine learning,machine-learning reinforcement-learning genetic-algorithms,1,2,1
2495,9153,1,,2018-11-25T06:56:11.300,1,34,how does dempster - shafer theory work in representing ignorance in ai field ?,19657,1671,2018-11-25T22:14:30.003,2018-11-25T22:14:30.003,how dempster - shafer theory work in ai ?,ai-basics theory learning-theory,0,0,
2496,9156,1,,2018-11-25T10:51:05.420,2,34,"one of the solutions to scale blockchain is to use off - chain channels . you can find its definition here : https://en.bitcoin.it/wiki/off-chain_transactions . however , one of the problems of off - chain channels is finding a suitable decentralized routing mechanism . since in bitcoin there is no routing table and transactions are only broadcast and also , in general , we need to avoid centralized approaches for routing , is it practical to use swarm intelligence algorithms , such as ant colony optimization ones , for off - chain channels ? i refer you to a proposed ant routing algorithm in the paper ant routing algorithm for the lightning network for an instance of employing aco algorithms for routing in _ lightning network . however , the paper has not been evaluated to demonstrate its performance .",19910,2444,2019-05-27T22:17:19.283,2019-05-27T22:17:19.283,is there an efficiency swarm intelligence algorithm for off - chain channels routing in blockchain ?,applications swarm-intelligence ant-colony blockchain,0,0,1
2497,9158,1,,2018-11-25T12:06:12.473,0,54,"how do we determine the cost of the parent path to its child in a * ( "" a star "" ) search ?",19817,3217,2018-11-26T18:39:36.913,2018-11-26T18:39:36.913,is the parent cost in a * added in every extended child ?,algorithm ai-basics search math a-star,1,2,
2498,9163,1,,2018-11-25T22:06:31.570,1,28,"for a school project i have been given a dataset containing images of plants and weeds . the goal is to detect when there is a weed in the pictures . the training and validation sets have already been created by our teachers , however they probably did n't have enough images for both so they "" photoshopped "" some weeds in some of the training pictures . here are examples of images with the weed label in the training set : in some cases , the "" photoshopped "" weed is hard to detect , and no shape resembling a weed is clearly visible like in this picture ( weed at the very bottom , near the middle ) : and here is an example of an image with the weed label in the validation set : how would i go about preprocessing the training set so that a cnn trained on it would perform well on the validation set ? i was thinking of applying a low - pass filter to the rough edges of the photoshopped images so that the network does n't act as an edge detector , but it does n't seem very robust . should i manually select the best images from the training set ? thank you !",20198,20198,2018-11-25T22:31:09.303,2018-11-25T22:31:09.303,how to preprocess a modified dataset so that a fitted cnn makes correct predictions on an un - modified version of the dataset ?,convolutional-neural-networks image-recognition datasets,0,0,
2499,9165,1,9170,2018-11-26T00:42:57.097,10,770,"from deepmind 's research paper on arxiv.org : in this paper , we apply a similar but fully generic algorithm , which we call alphazero , to the games of chess and shogi as well as go , without any additional domain knowledge except the rules of the game , demonstrating that a general - purpose reinforcement learning algorithm can achieve , tabula rasa , superhuman performance across many challenging domains . does this mean alphazero is an example of agi ( artificial general intelligence ) ?",17601,1581,2018-11-28T21:24:10.427,2018-11-29T21:30:54.587,is alphazero an example of an agi ?,game-ai definitions agi alphago,2,0,1
2500,9169,1,9175,2018-11-26T01:39:10.813,3,58,"when someone wants to compare 2 inputs , the most widespread idea is to use a siamese architecture . siamese architecture is a very high level idea , and can be customized based on the problem we are required to solve . is there any other architecture type to compare 2 inputs ? background i want to use a neural network for comparing 2 documents ( semantic textual similarity ) . siamese network is one approach , i was wondering if there is more .",18852,,,2018-11-27T00:20:46.850,neural network architecture for comparison,neural-networks natural-language-processing models architecture,1,0,
2501,9173,1,9283,2018-11-26T04:16:01.023,-1,241,"i 'm trying to build a dqn to replicate the deepmind results . i 'm doing with a simple dqn for the moment , but it is n't learning properly : after +5000 episodes , it could n't get more than 9 - 10 points . each episode has a limit of 5000 steps but it could n't reach more than 500 - 700 . i think the problem is in the replay function , which is : def replay(self , replay_batch_size , replay_batcher ) : j = 0 k = 0 replay_action = [ ] replay_state = [ ] replay_next_state = [ ] replay_reward= [ ] replay_superbatch = [ ] if len(memory ) & lt ; replay_batch_size : replay_batch = random.sample(memory , len(memory ) ) replay_batch = np.asarray(replay_batch ) replay_state_batch , replay_next_state_batch , reward_batch , replay_action_batch = replay_batcher(replay_batch ) else : replay_batch = random.sample(memory , replay_batch_size ) replay_batch = np.asarray(replay_batch ) replay_state_batch , replay_next_state_batch , reward_batch , replay_action_batch = replay_batcher(replay_batch ) for j in range ( ( len(replay_batch)-len(replay_batch)%4 ) ) : if k & lt;= 4 : k = k + 1 replay_state.append(replay_state_batch[j ] ) replay_next_state.append(replay_next_state_batch[j ] ) replay_reward.append(reward_batch[j ] ) replay_action.append(replay_action_batch[j ] ) if k & gt;=4 : k = 0 replay_state = np.asarray(replay_state ) replay_state.shape = shape replay_next_state = np.asarray(replay_next_state ) replay_next_state.shape = shape replay_superbatch.append((replay_state , replay_next_state , replay_reward , replay_action ) ) replay_state = [ ] replay_next_state = [ ] replay_reward = [ ] replay_action = [ ] states , target_future , targets_future , fit_batch = [ ] , [ ] , [ ] , [ ] for state_replay , next_state_replay , reward_replay , action_replay in replay_superbatch : target = reward_replay if not done : target = ( reward_replay + self.gamma * np.amax(self.model.predict(next_state_replay)[0 ] ) ) target_future = self.model.predict(state_replay ) target_future[0][action_replay ] = target states.append(state_replay[0 ] ) targets_future.append(target_future[0 ] ) fit_batch.append((states , targets_future ) ) history = self.model.fit(np.asarray(states ) , np.array(targets_future ) , epochs=1 , verbose=0 ) loss = history.history['loss'][0 ] if self.exploration_rate & gt ; self.exploration_rate_min : self.exploration_rate -= ( self.exploration_rate_decay/1000000 ) return loss what i 'm doing is to get 4 experiences ( states ) , concatenate and introduce them in the cnn in shape ( 1 , 210 , 160 , 4 ) . am i doing something wrong ? if i implement the ddqn ( double deep q net ) , should i obtain similar results as in the deepmind breakout video ? also , i 'm using the breakout - v0 enviroment from openai gym . edit : am i doing this properly ? i implemented an identical cnn ; then i update the target each 100 steps and copy the weights from model cnn to target_model cnn . should it improve the learning ? anyway i 'm getting low loss . for state_replay , next_state_replay , reward_replay , action_replay in replay_superbatch : target = reward_replay if not done : target = ( reward_replay + self.gamma * np.amax(self.model.predict(next_state_replay)[0 ] ) ) if steps % 100 = = 0 : target_future = self.target_model.predict(state_replay ) target_future[0][action_replay ] = target states.append(state_replay[0 ] ) targets_future.append(target_future[0 ] ) fit_batch.append((states , targets_future ) ) agent.update_net ( ) history = self.model.fit(np.asarray(states ) , np.array(targets_future ) , epochs=1 , verbose=0 ) loss = history.history['loss'][0 ] edit 2 : so as far i understand , this code should work am i right ? if not done : target = ( reward_replay + self.gamma * np.amax(self.target_model.predict(next_state_replay)[0 ] ) ) target.shape = ( 1,4 ) target[0][action_replay ] = target target_future = target states.append(state_replay[0 ] ) targets_future.append(target_future[0 ] ) fit_batch.append((states , targets_future ) ) if step_counter % 1000 = = 0 : target_future = self.target_model.predict(state_replay ) target_future[0][action_replay ] = target states.append(state_replay[0 ] ) targets_future.append(target_future[0 ] ) fit_batch.append((states , targets_future ) ) agent.update_net ( ) history = self.model.fit(np.asarray(states ) , np.array(targets_future ) , epochs=1 , verbose=0 )",9818,9818,2018-12-01T02:34:51.007,2018-12-01T02:34:51.007,dqn it 's not working properly,deep-learning reinforcement-learning ai-design q-learning dqn,1,2,
2502,9176,1,,2018-11-26T12:24:28.953,2,33,"are all ant routing algorithms the same ? if no , what is the common properties of all of them ? in other words , how we can detect a routing algorithm is an ant routing algorithm ?",19910,2444,2019-05-27T22:06:58.500,2019-05-27T22:06:58.500,are all ant routing algorithms the same ?,algorithm swarm-intelligence ant-colony,0,0,1
2503,9179,1,,2018-11-26T13:12:13.263,3,51,"i am learning to create a dialogue system . the various parts of such a system are intent classifier , slot filling , dialogue state tracking ( dst ) , dialogue policy optimization and nlg . while reading this paper on dst , i found out that a discriminative sequence model of dst can identify goal constraints , fill slots and maintain state of the conversation . does this mean that now i do nt need to create an intent classifier and slot filling models separately as the tasks are already being done by this dst ? or i am misunderstanding both the things and they are separate ?",19244,19244,2018-11-26T14:19:21.107,2018-11-27T20:31:33.160,does an advanced dialogue state tracking eliminate the need of intent classifier and slot filling models in dialogue systems/ chatbots ?,neural-networks machine-learning natural-language-processing recurrent-neural-networks chat-bots,1,4,
2504,9180,1,,2018-11-26T14:02:25.583,1,27,"i am working on an image data - set . as you may have guessed it is imbalanced data . i have ' class a , 19,000 images ' and ' class b , 2,876 images ' . so i did an undersampling by removing randomly from the majority class till it becomes equal to the minority class . on doing this i am loosing lot of information from those 19000 images which i could get . so i do an oversampling of minority class , by simply copying the 2,876 images again and again . is this undersampling method correct , will it effect my accuracy ? i trained an inceptionv4 model using this oversampled data and it is not at all stable and i am getting poor accuracy . what should be my strategy ?",19842,4709,2018-11-26T18:38:55.283,2018-11-29T16:57:37.127,can i do oversampling by copying the same image multiple times ? will it effect my neural network accuracy ?,convolutional-neural-networks ai-design,1,0,
2505,9182,1,9183,2018-11-26T14:59:20.600,2,910,how do i show that uniform - cost search is a special case of a * ? how do i prove this ?,19448,2444,2018-11-26T15:30:19.660,2018-11-26T15:30:19.660,how do i show that uniform - cost search is a special case of a * ?,search proofs,1,0,
2506,9184,1,9185,2018-11-26T15:24:23.780,2,48,"i understand both terms , linear regression and maximum likelihood , but , when it comes to the math , i am totally lost . so i am reading this article the principle of maximum likelihood ( by suriyadeepan ramamoorthy ) . it is really well written , but , as mentioned in the previous sentence , i do n't get the math . the joint probability distribution of $ y,\theta , \sigma$ is given by ( assuming $ y$ is normally distributed ) : this equivalent to maximizing the log likelihood : the maxima can be then equating through the derivative of l(θ ) to zero : i get everything until this point , but do n't understand how this function is equivalent to the previous one :",15391,2444,2018-11-26T18:26:48.373,2018-11-26T18:26:48.373,understanding the math behind using maximum likelihood for linear regression,machine-learning linear-regression maximum-likelihood,1,3,
2507,9186,1,9213,2018-11-26T16:52:29.400,0,330,"as , we use fifo(first in first out ) queue in breadth search algorithm but in depth first search we use lifo.why we use lifo instead of fifo in depth first search ?",20217,20217,2018-11-26T17:12:00.640,2018-11-27T14:13:45.747,why we use lifo(last in first out ) queue in depth first search ?,algorithm python search learning-algorithms,1,1,
2508,9189,1,,2018-11-26T19:00:01.253,4,68,"i 'm doing an epq ( extended - project qualification ) on artificial intelligence bias , and would like to gather some primary data for analysis . do driver - less cars lead into controversies that are to do with ethics ? for example : if the ai had to choose to minimize damage and could not avoid casualties .",20057,1847,2018-11-26T19:46:27.360,2018-11-27T00:19:09.213,does research into driver - less cars cause ethical problems ?,machine-learning self-driving ethics,1,4,1
2509,9190,1,9205,2018-11-26T19:41:12.317,4,39,"i 'm wondering if i can visualize the backprop process as follows ( please excuse me if i have written something terrible wrong ) . if the loss function $ l$ on a neural network represents the function has the form $ $ l = f(g(h(\dots u(v(\dots))))$$ then we can visualize the derivative of $ l$ wrt the $ i$ th function $ v$ as $ $ am i able to view all neural networks as having a loss function of the form of $ l$ given above ? that is , am i correct in saying that any neural network is just a function composition and that i can write the partial derivative wrt any parameter as written above ( i know i took the partial with respect to the function $ v$ ) . thanks",20222,,,2018-11-27T08:47:18.337,am i able to visualize the differentiation in backprop as follows ?,neural-networks backpropagation,1,0,1
2510,9191,1,,2018-11-26T20:56:12.613,1,14,"let 's say we have 2 blocks of text , where there are some similarities as in 2 interpretations of the same story . is there any solution that would recognize these consistencies and ultimately be able to recreate a story with all the details of each ?",20223,,,2018-11-26T20:56:12.613,intelligent text combination,natural-language-processing,0,0,
2511,9194,1,,2018-11-27T01:18:20.170,-1,15,i have been reading introduction to statistical learning and i was going through multiple linear regression . this is the topic that i m reading as i was reading further i encountered an equation that i m not able to understand . below is that equation it further says that please try to explain the above mentioned problem as simply as possible,7101,,,2018-11-27T01:27:18.937,is there a relationship between the response and predictors ?,machine-learning linear-regression,1,0,
2512,9197,1,9218,2018-11-27T04:07:47.937,2,130,"advantage actor - critic algorithm may use the following expression to get 1-step estimate of the advantage : $ a(s_t , a_t ) = r(s_t , a_t ) + \gamma v(s_{t+1 } ) ( 1 - done_{t+1 } ) - v(s_t ) $ where $ done_{t+1}=1 $ if $ s_{t+1}$ is a terminal state ( end of the episode ) and $ 0 $ otherwise . suppose our learning environment has a goal , collecting the goal gives reward $ r=1 $ and terminates the episode . agent also receives $ r=-0.1 $ for every step , encouraging it to collect the goal faster . we 're learning with and we terminate the episode after $ t$ timesteps if the goal was n't collected . for the state before collecting a goal we have the following advantage , which seems very reasonable : $ a(s_t , a_t ) = 1 - v(s_t)$ . for the timestep $ t-1 $ , regardless of the state , we have : $ a(s_{t-1},a_{t-1 } ) = r(s_{t-1 } , a_{t-1 } ) - v(s_{t-1 } ) \approx -0.1 -\frac{-0.1}{1-\gamma } = -0.1 + 10 = 9.9 $ ( this is true under the assumption that we 're not yet able to collect the goal reliably often , therefore the value function converges to something close to ) . usually , $ t$ is not a part of the state , so the value function has no way to anticipate the sudden change in reward - to - go . so , all of a sudden , we got a ( relatively ) big advantage for the arbitrary action that we took at the timestep $ t-1 $ . following the policy gradient rule , we will significantly increase the probability of an arbitrary action that we took at the end of the episode , even if we did n't achieve anything . this can quickly destroy the learning process . how do people deal with this problem in practice ? my ideas : differentiate between actual episode terminations and ones caused by the time limit , e.g. for them we will not replace next step value estimate with $ 0 $ . somehow add $ t$ to the state such that the value function can learn to anticipate the termination of the episode . as i noticed , the a2c implementation in openai baselines does not seem to bother with any of that : a2c implementation - calculation of discounted rewards discount_with_dones function",20229,20229,2018-11-28T00:15:50.793,2018-11-28T02:58:52.077,how to deal with episode termination in advantage actor - critic algorithm ?,reinforcement-learning,2,2,
2513,9198,1,,2018-11-27T04:35:02.757,0,44,i just started using ibm cloud private for data this week and i was n't sure if i can use other public clouds to connect with my icp for data account . so i spoke with an ibm representative and i wanted to share their responses ...,17067,,,2019-04-26T05:00:40.083,does ibm cloud private for data run on public clouds like aws or azure ?,structured-data,1,1,
2514,9200,1,,2018-11-27T06:20:54.403,2,80,"apparently , the hill climbing algorithm just produces a local maximum , and not necessarily a global optimum . it 's stuck on a local maximum . why does hill climbing algorithm only produce a local maximum ?",19656,2444,2019-03-02T11:04:55.543,2019-03-02T11:04:55.543,why does hill climbing algorithm only produce a local maximum ?,search optimization hill-climbing,1,0,
2515,9209,1,9211,2018-11-27T09:41:14.717,3,183,"is the research field "" artificial intelligence "" a science or is it engineering ? or neither or both ?",20140,3217,2018-11-28T22:32:10.490,2018-11-28T22:32:10.490,"is ai a science , or is it engineering ? or neither or both ?",ai-field,2,3,
2516,9212,1,,2018-11-27T13:40:58.347,0,23,"i am working on an autoregression problem where i use sequential lstm . my target is well defined , but i think i am facing a problem with the features . as the features were non - stationary , then i decided to apply the log - returns to each of them . in other words , if $ f_t$ is a feature at a certain time $ t$ , then i apply $ $ why non - stationary data is hard to analyse ? the property makes it easier to analyse , but produce a lot of zeros when $ f_t = f_{t-1}$ . as there is lots of zeros , then the predictions tend to be closer to 0 . how can i reduce that effect mathematically ?",18765,18765,2018-11-27T19:22:25.920,2018-11-27T23:49:55.167,reduce the effect of excessive zeros,deep-learning lstm,1,0,
2517,9217,1,9243,2018-11-27T16:52:12.273,1,66,"in the trust region policy optimization paper , in lemma 2 of appendix a , i did not quite understand deriving inequality ( 31 ) from ( 30 ) , which is : $ $ $ $ |\bar{a}(s)| \le \alpha . 2 \max_{s , a } |a_{\pi}(s , a)|$$ would you mind let me know how the inequality is derived ?",16912,2444,2019-05-02T16:01:28.527,2019-05-02T16:01:28.527,"understanding lemma 2 of the "" trust region policy optimization "" paper",reinforcement-learning deep-rl proofs trpo,1,0,1
2518,9221,1,,2018-11-27T20:00:46.283,1,26,"i 'm trying to gain some insight into acoustic voice data composed of 19 features . i want to understand what features contribute most for classification . added : most features are related with the fundamental frequency stability . in particular i 'm using voice shimmer and jitter and some related calculations . i 'm trying to use mrmr ( max relevance min redundancy ) , but i would like to compare with some other options . added : i have tried to use featureminer ( http://featureselection.asu.edu/index.php ) which provides some interesting algorithms implementations . however many of them use deprecated python functions and require some effort to work properly . are there any popular tools for these purposes ?",20248,20248,2018-11-27T22:39:29.470,2019-04-26T23:01:45.343,feature selection of an acoustic voice dataset for classification,feature-selection,1,0,
2519,9226,1,,2018-11-27T22:41:25.287,1,26,"i am new for machine learning and i am tried to understand basic steps to get final modal of logistic regression . i know logistic regression is supervisory learning technique . therefore we want to give training data to training the modal . according to my understanding , i can take steps to get the final modal as follows . step 1 - make an algorithm . for logistic regression is y = sigmoid(w x + b ) function . get zero or some value for w and b. step 2 - give sample known training data and find w and b values . x(1 ) , x(2) ... x(m ) inputs and get y(1 ) , y(2) ... y(m ) outputs . gradient descent use for find w and b that minimizes the cost function . step 3 - then apply w and b values which i found.y^ = sigmoid(w x + b ) . and then again apply sample known training data to get the predicting of y^. step 4 - get the final modal to test unknown data . are these steps right ? please give me basic fundamental steps to understand supervisory learning technique . i would like to know un - supervisory learning technique steps also .",20251,,,2018-12-07T18:09:11.143,steps for final logistic regression modal,neural-networks machine-learning unsupervised-learning,1,1,
2520,9228,1,,2018-11-27T23:56:07.910,0,75,"specifically , i am looking for a dataset that is concerned with pattern recognition . i could find no such dataset on kaggle , and google has been of little help .",19932,,,2018-11-28T15:16:19.630,where can i find a dataset of iq questions and answers ?,datasets,2,0,
2521,9230,1,,2018-11-28T05:03:20.983,1,30,"i 'm trying to replicate the results of the deepmind 's paper with breakout included in openai gym . i wonder how much frames should i keep until i reach the fixed exploration rate . actually it reaches the minimum value at about 1,5 m frames ( about 5000 episodes ) . it is much ? if it is , does it impact negatively on training ? my minimum rate is fixed at 0.01 .",9818,,,2018-11-28T05:03:20.983,exploration rate decay and training in q learning,ai-design ai-basics training q-learning dqn,0,0,
2522,9231,1,,2018-11-28T09:04:56.163,1,132,"i 'm working on stock price prediction and automatic or semi - automatic control of trading . the price trends of these stocks exhibit recurring patterns that may be exploited . my dataset is currently small , only in the thousands of points . there are no images or very high dimensional inputs at all . the system must select from among the usual trading actions . buy $ n$ shares at the current bid price hold at the current position sell $ n$ shares at what the current market will bear i 'm not sure if reinforcement learning is the best choice , deep learning is the best choice , something else , or some combination of ai components . it does n't seem to me to be a classification problem , with hard to discern features . it seems to be an action - space problem , where the current state is a main input . because of the recurring patterns , the history that demonstrates the observable patterns is definitely pertinent . i 've tried some code examples , most of which employ some form of artificial nets , but i 've been wondering if i even need deep learning for this , having seen the question , when is deep - learning overkill ? on this site . since i have very limited training data , i 'm not sure what ai design makes most sense to develop and test first .",20257,4302,2018-11-29T19:27:05.387,2018-11-29T23:32:44.487,"for forecasting and trading control , given limited data , what ai approaches are well matched ?",deep-learning reinforcement-learning forecasting,2,0,
2523,9234,1,,2018-11-28T09:50:09.747,4,56,"if my algorithm detects the type of object , how should i know if that object is moving or not ? suppose a person carrying an umbrella . how to know that the umbrella is moving ? i am working on a project where i want to know whether that particular object belongs to the person entering inside the store . i was thinking about the bounding boxes(bb ) approach where if the person 's bb overlaps with the object 's bb . but the problem arises when there are multiple objects with a person . i would appreciate your help . thanks .",20259,20259,2018-11-28T10:03:46.437,2019-01-26T18:29:27.137,how to know whether the object is moving after it is being detected ?,python object-recognition,2,3,1
2524,9236,1,,2018-11-28T14:04:01.180,1,68,when we get the offspring and fitness so why we prefer to apply mutation in genetic algorithm ?,19603,,,2018-12-08T19:22:37.693,why we apply mutation after getting offspring in genetic algorithm ?,algorithm genetic-algorithms,2,3,
2525,9245,1,,2018-11-29T02:25:18.550,1,86,"i want to build a semi autonomous robot / machine that will clean up trash in cities . for this to be possible it needs to recognize ' trash ' . as trash can be all sorts of things ( think ciggaret buts , plastic , bottles or anything that humans leave behind ) i was curious if it would be possible for a ai to learn this broad concept with current technologies ? and if so what would i need to build it ? i was thinking tensorflow and a dataset with different kinds of trash . thank you for taking the time to help !",20276,1671,2018-11-29T21:16:02.653,2019-01-02T01:02:20.480,possible to train some model to recognize trash ?,machine-learning ai-design computer-vision robotics,1,2,
2526,9248,1,,2018-11-29T08:18:19.443,0,68,"we want to figure out the connection between people , based on their speech . assume , that a conversation is a poetry with lines belongs to characters . there are a lot of poetries and the lines are mixed . now we want to define a conversation to which each line belongs to . we assume , that people in a conversation use similar words ( their dictionaries should be similar ) . it means that there is a correlation between words of person a and words belongs to person b , and we could detect a connection between the peoples which had a conversation . what are the next steps for content understanding after nlp ? can some of you advise us about the field of study and tools / libraries , which are dealing with content processing ? maybe , some of you know good articles or online resources , which can help us to dive into this field .",20267,1671,2018-11-29T22:22:11.477,2019-05-08T00:02:15.430,content analysis tools,natural-language-processing software-evaluation,2,1,
2527,9253,1,,2018-11-29T20:33:25.790,2,44,"most implementations i 'm seeing for playing games like atari ( usually similar to deepmind 's work using dqn ) have 4 graphical frames of input fed into 3 convolutional layers which are then fed into a single fully connected layer . the explanation of no pooling layer is due to positioning of features / objects being very critical to most games . my concern with this is that it may be weighing visual features based on position without regard for feature->feature proximity . by this , i mean to question if learning to avoid bullets in the bottom left of the screen is knowledge also used in the bottom right of the screen in a game like space invaders . so , question 1 : is my concern with only using 3 conv layers into a fc layer legitimate regarding spatially localized learning ? question 2 : if my concern is legitimate , how might the network be modified to still treat feature position as significant , but to also take note of feature to feature proximity ? ( i 'm still quite the novice if that is n't extremely obvious , so if my questions are n't completely ridiculous on their own , please try to keep responses relatively high level if you would . )",20277,,,2018-11-29T20:33:25.790,how a game playing agent could identify potential objects and proximity ?,neural-networks convolutional-neural-networks image-recognition object-recognition,0,0,1
2528,9270,1,,2018-11-30T06:03:21.530,0,51,"i am having a video feed with multiple faces in it . i need to detect each face and the gender as well and assign the gender against each person . i am not sure how to uniquely identify a face as face1 and face2 etc . i do not need to know their names , just need to track a person in the entire video . i thought of tracking methods but people are constantly moving and changing so a box location can be occupied by another person after some frames . i am interested in a way where i can assign an i d to a face but i am not sure how to do it . i can use facial recognition based embedding on each face and track that . but that seems to be an overkill for the job . is there any other method available or facial recognition / embedding is the only method to uniquely identify people in a video ?",11387,,,2018-11-30T06:46:44.853,can i detect unique people in a video ?,deep-learning computer-vision,2,0,
2529,9275,1,,2018-11-30T07:19:37.077,0,34,"can someone suggest an ai approach to moving blocks , one at a time , assuming control of an robotic arm , to get from the initial state on the left to the final state on right , preferably using goal stack planning . actions pickup ( ) & mdash ; to pick up a block from table only putdown & mdash ; to putdown a block on table only unstack & mdash ; unstack a block from another block stack & mdash ; stack a block on another clear block only property functions on(x , y ) above(x , y ) table(x ) clear(x )",20301,4302,2018-12-01T19:25:04.627,2018-12-03T21:02:28.590,block of worlds with position aware goal stack planning,ai-design path-planning,1,2,
2530,9276,1,,2018-11-30T07:38:52.187,3,129,"i have to build a ki for a made - up game similar to chess . as i did research for a proper solution , i came upon the minmax algorithm , but i 'm not sure it will work with the given game dynamics . the challenge is that we have far more permutations per turn than in chess because of these game rules . six pieces on the board , with different ranges . in average , there are 8 possible moves for a piece per turn . the player can choose as many pieces to move as he likes . for example none , all of them , or some number in between ( whereas in chess you can only move one . ) actual questions : is it feasible to implement minmax for the described game ? can alpha - beta - pruning and a refined evaluation function help ( despite of the large number of possible moves ) ? if no , is there a proper alternative ?",20303,4302,2018-12-08T13:24:31.087,2019-05-11T14:02:45.543,minmax and enormous branches,game-ai logic,2,1,2
2531,9278,1,,2018-11-30T10:26:11.017,1,70,which one gives better optimization results ? genetic algorithm or particle swarm optimization ? can i use them for online tuning problems ? thanks in advance !,20300,,,2018-11-30T14:08:29.693,genetic algorithm vs particle swarm optimization,neural-networks machine-learning genetic-algorithms optimization control-problem,1,0,
2532,9296,1,,2018-12-01T07:08:09.450,3,130,"biology is used in ai terminology . what are the reasons ? what does biology have to do with ai ? for instance , why is the genetic algorithm used in ai ? does it fully belong to biology ?",19886,2444,2019-03-22T14:56:27.680,2019-03-22T19:02:07.853,what is the role of biology in ai ?,philosophy terminology biology,2,1,1
2533,9298,1,9300,2018-12-01T08:53:47.123,0,41,"https://stackoverflow.com/questions/36162180/gradient-descent-vs-adagrad-vs-momentum-in-tensorflow here , the nice gifs explain how different algorithms approach towards the root . unfortunately , the environment in the gif is way too simple and real cases have much more complex environments . also , in reinforcement learning , the solutions should change each moment in a difficult enough environment since things are dynamic . my question is which optimizer is best for reinforcement learning in such dynamically changing environment ? adadelta should not move beyond local minima so do we have to use sgd or adadelta with an exploration heuristic ? please let me know in detail your thoughts .",19027,,,2018-12-01T10:00:38.467,neural network optimizers in reinforcement learning non - well behaved environments,reinforcement-learning optimization,1,0,
2534,9304,1,,2018-12-01T15:28:25.773,2,47,ai ca nt handle all the problem.in robot intelligence they ca nt understand the human brain process they ca nt fell happiness or sadness.what are our misconception about robots intelligence system ?,19886,,,2018-12-01T15:28:25.773,what are misconception about robots intelligence ?,robotics,0,2,
2535,9305,1,9336,2018-12-01T18:33:00.083,2,415,what problems of ai are not machine learning ? what is difference between ai and machine learning ? which problems handle both ai and machine learning ?,19886,4709,2018-12-04T21:29:09.653,2019-01-26T05:23:12.980,what problems in ai are not machine learning ?,machine-learning,2,0,1
2536,9306,1,9308,2018-12-01T21:12:04.480,1,54,"this might be a stupid question , but bear with me i 'm only a beginner at this . recently i started to look at policy gradient methods and policies are represented as functions with features for larger problems with many states . many articles and pseudocodes of algorithms mention sampling an action from the policy but it is unclear to me how . actions are something we do in the environment like going left , right , etc ... and functions take some feature values and parameters , make calculations and ' spit ' some number . so how do we actually map that number to certain action , how do we know what action to take ?",20339,,,2018-12-02T11:17:53.907,how do we actually sample action from policy in policy gradient methods ?,reinforcement-learning,1,0,
2537,9307,1,,2018-12-02T07:20:45.563,1,61,"which is the most straight forward approach for planning algorithm?can we use random algorithm like bfs , dfs , rbfs etc ?",19784,,,2018-12-02T07:20:45.563,which is the most straight forward approach for planning algorithm ?,algorithm,0,1,
2538,9311,1,9318,2018-12-02T20:50:27.887,2,70,can the same input for a plain neural network be used for cnns ? or does the input matrix need to be structured in a different way for cnns compared to regular nns ?,20358,,,2018-12-03T13:36:29.160,can the same input for a plain neural network be used for a convolutional neural network ?,neural-networks convolutional-neural-networks,2,0,1
2539,9312,1,,2018-12-02T21:09:26.017,10,111,"modular / multiple neural networks ( mnns ) revolve around training smaller , independent networks that can feed into each other or another higher network . in principle , the hierarchical organization could allow us to make sense of more complex problem spaces and reach a higher functionality , but it seems difficult to find examples of concrete research done in the past regarding this . i 've found a few sources : https://en.wikipedia.org/wiki/modular_neural_network https://www.teco.edu/~albrecht/neuro/html/node32.html https://vtechworks.lib.vt.edu/bitstream/handle/10919/27998/etd.pdf?sequence=1&amp;isallowed=y a few concrete questions i have : has there been any recent research into the use of mnns ? are there any tasks where mnns have shown better performance than large single nets ? could mnns be used for multimodal classification , i.e. train each net on a fundamentally different type of data , ( text vs image ) and feed forward to a higher level intermediary that operates on all the outputs ? from a software engineering perspective , are n't these more fault tolerant and easily isolatable on a distributed system ? has there been any work into dynamically adapting the topologies of subnetworks using a process like neural architecture search ? generally , are mnns practical in any way ? apologies if these questions seem naive , i 've just come into ml and more broadly cs from a biology / neuroscience background and am captivated by the potential interplay . i really appreciate you taking the time and lending your insight !",20357,,,2019-05-11T12:01:45.330,"are modular neural networks more effective than large , monolithic networks at any tasks ?",neural-networks topology architecture neurons biology,1,1,1
2540,9313,1,9314,2018-12-02T22:25:16.043,3,115,"so , in situations where some output - data - chunks could change without changing the input data from time to time , it could be challenging to automate the testing process . my question is , are there any ways by using ai in general , in which those output data analyzed(after the training ) and as a result of the actual and mutual data ( that could be tested ) return ?",20360,,,2018-12-03T01:26:38.227,are there any machine learning algorithms for comparing files,machine-learning deep-learning ai-basics automation,1,0,
2541,9317,1,,2018-12-03T09:22:30.637,0,8,"motion capture ( mocap ) is about recording human motions in 3d space . in most cases , a marker based tracking is used , but some newer development goes into markerless capture . the result of the mocap step is the recorded trajectory which is stored in a file . it contains of 3d position information which are connected to a timeframe : time 0 , ( 0,0,0 ) time 1 , ( 4,3,0 ) time 2 , ( 5,2,0 ) time 3 , ( 1,4,3 ) this file has to be parsed on a semantic level . according to the literature , a parser works with a domain specific language ( dsl ) which is about biped walking , human motions , grasping tasks and so on . the advantage of a dsl for a mocap parser is , that it is equal to a heuristic . if the trajectories are mapped to textual commands it 's possible to reproduce a task on a robot . my question is : which domain specific languages for human motion capture are available ?",11571,,,2018-12-03T09:22:30.637,which domain specific language are available for mocap ?,natural-language-processing computer-vision robotics,0,0,
2542,9319,1,,2018-12-03T16:28:25.627,2,145,"we have convolutional neural networks and recurrent neural networks for analysing respectively images and sequential data . how do i determine which neural network architecture is more appropriate to approximate a certain function ? for example , suppose i want to approximate the function $ f(x , y ) = \sin(2\pi x)\sin(2\pi y)$ with domain , that is , $ x$ and $ y$ can be between $ 0 $ and $ 1 $ ( inclusive ) . for example , which kind of activation functions would be better suited for approximating this specific function ?",18975,2444,2019-05-10T14:34:32.903,2019-05-10T14:34:32.903,which neural network should i use to approximate a specific function ?,neural-networks machine-learning math activation-function function-approximation,2,1,
2543,9320,1,9350,2018-12-03T16:29:42.023,0,115,"i just wanted to gather some perspective on why this is a great opportunity to be able to study machine learning today ? with all the online resources ( online courses like andrew ng 's , availability of datasets such as kaggle , etc ) , learning machine learning has become possible . i understood that you can have high paid jobs ; but you also need a lot of work dedication to be good at it , which makes your salary not so attractive ! ( in comparison to the number of hours you spend to keep up with this fast moving field ) why it is so desirable to take this opportunity and start learning machine learning today ? ( community , ability to start a business , etc . )",20376,16355,2018-12-04T21:29:21.453,2018-12-05T19:08:14.087,why studying machine learning is an opportunity in today 's world ?,machine-learning social profession,2,0,
2544,9322,1,,2018-12-03T16:48:15.917,1,106,"why do we need both encoder and decoder in sequence to sequence prediction ? we could just have a single rnn that , given input $ x$ , outputs some value $ y(t)$ and hidden state $ h(t)$ . next , given $ h(t)$ and $ y(t)$ , the next output $ y(t+1)$ and hidden state $ h(t+1)$ should be produced , and so on . the architecture shall consists of only one network instead of two separate ones .",20378,2444,2019-05-03T10:42:48.400,2019-05-03T10:42:48.400,why do we need both encoder and decoder in sequence to sequence prediction ?,machine-learning ai-design sequence-modelling,1,6,
2545,9325,1,9328,2018-12-03T18:50:01.203,2,42,"i am currently looking at different documents to understand back - propagation , mainly at this document . now , at page 3 , there is the symbol involved : while i understand the main part of the equation , i do n't understand the factor . searching for the meaning of the in math , it means ( for example ) a error value to be minimized , but why should i multiply with the error ( it is denoted as e anyways ) . should n't the be the learning rate in this equation ? i think that would be what makes sense , because we want to calculate by how much we want to adjust the weight , and since we calculate the gradient , i think the only thing that 's missing is the multiplication with the learning rate . the thing is , is n't the learning rate usually denoted with the ?",17769,2444,2019-05-02T16:12:14.037,2019-05-02T16:12:14.037,what is the use of the term in this back - propagation equation ?,backpropagation notation,1,0,
2546,9333,1,9366,2018-12-04T19:08:00.153,1,105,"we have a series of data which want to label part of each series . as we do not have any training data , we try using active learning as a solution . but , the problem is our classifier is something like rnn which needs a lot of data to be trained . hence , we have a problem in converging fast to just label proportional small parts of unlabeled data . the question is "" is there any article about this problem ( active learning and some complex classifiers like rnn ) or not ? "" . or is there any alternative to approach this problem or not ? ( as data is a series of actions )",4446,,,2018-12-06T15:21:53.940,active learning and rnn,machine-learning recurrent-neural-networks active-learning,1,0,
2547,9337,1,,2018-12-05T00:33:01.603,-1,35,"when will it be possible to give a computer program a bunch of assumptions and ask it if a certain statement is true or false , giving a proof or a counterexample respectively ?",17601,,,2018-12-05T09:10:02.717,when will we have computer programs that can compose mathematical proofs ?,math,1,1,
2548,9345,1,,2018-12-05T12:01:58.070,1,17,i 'm working on a problem and need to use karaboga 's code of the abc algorithm but i have some questions ... does this formula for calculating a parameter have to be changed : { /v_{ij}=x_{ij}+\phi_{ij}(x_{kj}-x_{ij } ) * / } a standard one or this what karaboga see is better for algorithm . the second is the same question for this formula of calculating fitness : { ffitness(ind)=1./(fobjv(ind)+1 ) } link to abc algorithm coded using c programming language,20434,4709,2018-12-06T21:18:29.257,2018-12-06T21:18:29.257,change parameter in karaboga 's code of abc algorithm,neural-networks algorithm training,0,0,
2549,9351,1,,2018-12-05T19:58:07.433,0,132,"my task involves a large grid - world type of environment ( grid size may be $ 30\times30 $ , $ 50\times50 $ , $ 100\times100 $ , at the largest $ 200\times200 $ ) . each element in this grid either contains a 0 or a 1 , which are randomly initialized in each episode . my goal is to train an agent , which starts in a random position on the grid , and navigate to every cell with the value 1 , and set it to 0 . ( note that in general , the grid is mostly 0s , with sparse 1s ) . i am trying to train a dqn model with 5 actions to accomplish this task : move up move right move down move left clear ( sets current element to 0 ) the "" state "" that i give the model is the current grid ( $ n\times n$ tensor ) . i provide the agent 's current location through the concatenation of a flattened one - hot ( $ 1\times(n^2)$ ) tensor to the output of my convolutional feature vector ( before the fc layers ) . however , i find that the epsilon - greedy exploration policy does not lead to sufficient exploration . also , early in the training ( when the model is essentially choosing random actions anyway ) , the pseudo - random action combinations end up "" canceling out "" , and my agent does not move far enough away from the starting location to discover that there is a cell with value 1 in a different quadrant of the grid , for example . i am getting a converging policy on a $ 5\times5 $ grid w/ a non - convolutional mlp model , so i think that my implementation is sound . how i might encourage exploration that will not always "" cancel out "" to only explore a very local region to my starting location ? is this approach a good way to accomplish this task ( assuming i want to use rl ) ? i would think that attempting to work with a "" continuous "" action space ( model outputs 2 values : vertical and horizontal indices of grid cells that contain 1s ) would be more difficult to achieve convergence . is it wise to always try to use discrete action spaces ?",19789,19789,2018-12-06T19:11:28.643,2018-12-06T19:11:28.643,dqn exploration strategy for large grid - world environment,deep-learning reinforcement-learning dqn,1,4,
2550,9354,1,,2018-12-06T00:27:48.983,3,75,"i am wondering how the output of randomly initialized mlps and convnets behave with respect to their inputs . can anyone point to some analysis or explanation of this ? i am curious about this because in the random network distillation work from openai , they use the output of randomly initialized network to generate intrinsic reward for exploration . it seems that this assumes that similar states will produce similar outputs of the random network . is this generally the case ? do small changes in input yield small changes in output , or is it more chaotic ? do they have other interesting properties ?",10985,10985,2018-12-07T23:25:01.860,2019-01-01T18:42:39.130,how do randomly initialized neural networks behave ?,neural-networks deep-learning convolutional-neural-networks,2,0,2
2551,9358,1,,2018-12-06T06:44:42.633,1,17,"i am preparing to perform research comparing the performance of two different systems that probabilistically generate the next word of an input sentence . for example , given the word ' the ' , a system might output ' car ' , or any other word . given the input ' the round yellow ' , a system might output ' sun ' , or it might output something that does n't make sense . my question is , how can i quantitatively evaluate the performance of the two different systems performing this task ? of course if i tested each system manually i could qualitatively determine how often each system responded in a way that makes sense , and compare how often each system responds correctly , but i 'd really like a meaningful quantitative method of evaluation that i could preferably automate . precision and recall do n't seem like they would work here , seeing as for each given input there are many potentially acceptable outputs . any suggestions ?",3989,,,2018-12-06T10:03:55.080,how can i evaluate the performance of a system that generates text ?,natural-language-processing software-evaluation data-science,1,0,
2552,9368,1,,2018-12-06T16:23:38.990,1,37,"i use a modified training script for modeling images with tensorflow / keras / mobilenet_v2 . after a few errors that i could solve i now get the following error : file "" /home / machine_learning_verstening / scripts / object_detection / builders / model_builder.py "" , line 234 , in _ build_ssd_model freeze_batchnorm = ssd_config.freeze_batchnorm , attributeerror : ' ssd ' object has no attribute ' freeze_batchnorm ' within this script i see the following pieces of code : freeze_batchnorm = ssd_config.freeze_batchnorm freeze_batchnorm = ssd_config.freeze_batchnorm freeze_batchnorm = ssd_config.freeze_batchnorm on google i can not find the solution , i guess it is very specific for using the ssd in tensorflow . anyone has experience with this error ?",19858,7800,2018-12-06T21:18:39.647,2018-12-12T14:12:53.557,how to solve the attributeerror : ' ssd ' object has no attribute ' freeze_batchnorm ',machine-learning convolutional-neural-networks python tensorflow keras,1,0,
2553,9369,1,9374,2018-12-06T16:57:36.687,1,83,the field of artificial intelligence is so vast . so many methodologies of handling continuous data and just now i have read about hybrid bayesian network . i just want to know that what a hybrid bayesian network contains ?,19660,9647,2018-12-07T17:31:49.737,2018-12-07T17:31:49.737,what does a hybrid bayesian network contain ?,data-science,1,1,
2554,9370,1,9387,2018-12-06T18:05:33.997,1,23,"sorry if this is basic or covered elsewhere , i am just starting here and i was n't able to find an answer , but i might have not been searching for the right thing . so : i am training a neural network to predict current draw in a system . there are a number of obvious numerical inputs , like temperature , counting rate , voltage , etc . the most predictive thing , however , is what operation the system is doing . so like , if it 's doing a ' calibration ' then the current profile is much different than if it 's in ' standby ' . i know that i can just use a different network for each operation , but in this case i have a couple hundred different macros defined and i do n't want to have 200 + neural networks retrain all the time . i also know that i can have a digital value as an input , but my understanding is that it has to be either 0/1 . also , the relationship to operation is not at all correlated - so operation 100 is not necessarily more current draw than 99 or less than 101 . so , is there a way to have an operation i d or something factor in , but not have it be in the linear combination mathematically ? so , basically , tell the system to do a different training based on i d or something ? i 'll be using python and scikit - learn . thanks !",20482,,,2018-12-07T19:41:46.983,using an ' operation i d ' as a neural network input,neural-networks python,1,0,
2555,9371,1,9388,2018-12-06T18:18:41.433,0,120,"i am trying to generate 90 and 270 degrees rotated versions of my sample images on the fly during training . i found an example and modifying it . but i am confused about what should be the order ? for instance in one batch i have 32 images and my image generator should return total 64 images . let 's say upper case letters are 90 degree and lower case letters are 270 degree rotated images . should the order be aabbcc or abcabc ? i apply the same to the validation set . here is the related code fragment : edit : code fragment added . def _ get_batches_of_transformed_samples(self , index_array ) : # create list to hold the images batch_x = [ ] # create list to hold the labels batch_y = [ ] # rotation angles target_angles = [ 0 , 90 , 180 , 270 ] angle_categories = list(range(0 , len(target_angles ) ) ) self.classes = target_angles self.class_indices = angle_categories # generate rotated images and corresponding labels for i , j in enumerate(index_array ) : is_color = int(self.color_mode = = ' rgb ' ) image = cv2.imread(self.filenames[j ] , is_color ) if is_color : image = cv2.cvtcolor(image , cv2.color_bgr2rgb ) for rotation_angle , cat_angle in zip(target_angles , angle_categories ) : rotated_im = rotate(image , rotation_angle , self.target_size[:2 ] ) if self.preprocess_func : rotated_im = self.preprocess_func(rotated_im ) # add dimension to account for the channels if the image is greyscale if rotated_im.ndim = = 2 : rotated_im = np.expand_dims(rotated_im , axis=2 ) batch_x.append(rotated_im ) batch_y.append(cat_angle ) # convert lists to numpy arrays batch_x = np.asarray(batch_x ) batch_y = np.asarray(batch_y ) batch_y = to_categorical(batch_y , len(target_angles ) ) return batch_x , batch_y i actually rotate them as 0 , 90 , 180 and 270 degrees . as seen in the code , for each batch i return all the rotated versions of all the images in the batch . but is this correct or should i return first 0 degree rotated versions , second 90 degree rotated versions so on ? edit2 : i checked my previous work which i use the built in keras imagedatagenerator . generator.classes returns [ zeros(100,1 ) ; ones(100,1)]. in that study i only have two classes . i understand that keras indexes the images as [ class1 , class2 , ... ] . i think i have to do the same .",18283,18283,2018-12-08T11:53:08.567,2018-12-09T13:21:19.303,label arrangement for custom keras image generator,python keras,1,0,
2556,9372,1,,2018-12-06T18:34:22.423,2,234,"can machine learning be applied to decipher the script of lost ancient languages ? ( languages that many years ago were being used , but currently are not used in the human societies and have been forgotten e.g. avestan language . ) and if yes , is there already any successful experiment to decipher the script of unknown ancient languages machine learning ?",19910,19910,2018-12-06T19:10:32.680,2018-12-07T09:41:31.867,can machine learning be applied to decipher the script of lost ancient languages ?,machine-learning natural-language-processing natural-language language-processing,2,0,
2557,9375,1,,2018-12-06T23:37:33.787,1,94,"question to nn practicioners . i 'd like to encode azul board game state as an input to nn , let 's focus on 2-player variant for a while . there are 5 round "" factories "" on the table ( 7 on picture , ignore it ) . each one can keep 4 tiles of 5 colors . there is also center of the board which can keep up to 15 tiles . what are the advantages and disadvantages of different encodings ? here are my ideas : every factory has five integer counters , one for each tile color . sample encoding of single factory : [ 3,1,0,0,0 ] every factory has 20 binary flags , four for each color . single flag encodes presence of tile of given color . sample encoding of single factory : [ 1,1,1,0 , 1,0,0,0 , 0,0,0,0 , 0,0,0,0 , 0,0,0,0 ] every factory has 20 binary flags , four for each color , but only one flag can be set for given color and position of raised flag encodes number of tiles of given color . sample encoding of single factory : [ 0,0,1,0 , 1,0,0,0 , 0,0,0,0 , 0,0,0,0 , 0,0,0,0 ] every factory has 4 enum fields , with 6 possible values each ( 5 colors + empty ) . sample encoding of single factory : [ red , red , red , blue ] or [ empty , empty , empty , empty ] ( note : encoding schema would also cover center of the board , up to 15 tiles , as said earlier . of course player 's board would also be encoded , but i do n't want to ask too broad question ) i 'd like to train nn to play azul , which means it needs to properly process number of tiles taken in given round ( up to 15 in theory , 2 - 4 in practice ) because it would also need to indicate where to put all those tiles to player board positions . based on your experience , which encoding is most promising ? or maybe there is some better method i did n't think of ? or it is not possible to tell or it does n't matter ?",20125,,,2018-12-06T23:37:33.787,how to encode azul game state as nn input,neural-networks deep-learning game-ai combinatorial-games,0,0,
2558,9377,1,9750,2018-12-07T07:01:46.120,3,42,when we perform tik tok toe game using adversarial search i know how make tree my question is : is there any way to find the depth of tree which level is the last level .,19886,,,2018-12-30T03:03:55.090,how we find the length(depth ) of game tik tok toe in adversarial search ?,game-ai search,1,0,
2559,9383,1,,2018-12-07T18:33:55.367,0,30,"i need a phone or device i can talk to as if i had no hands or sight . i need to be able to tell my phone to read my text , "" tell me who called me in the last hour ( or whatever ) "" then it tells me who the number is registered to using the internet . most important i want the ai to understand me the first time i say it . there are more facets to the ai i would need to be able to do without ever touching the phone . is there anything out there like that i can carry with me ?",11579,1671,2018-12-07T20:31:29.573,2018-12-08T02:48:06.733,i need a completely hand and eye free software,software-evaluation audio-processing accessibility,1,0,
2560,9392,1,9407,2018-12-07T23:55:16.357,0,138,"i am working on character recognition using convolutional neural networks . i have 9 layer model and 19990 training data and 4470 test data . but when i am using keras with tensorflow backend . when i try to train the model , it runs extremely slow , like 100 - 200 samples per minute . i tried adding batch normalization layer after flattening , using regularization , adding dropout layers , using fit_generator to load data from disk batch wise so that ram remain free(that did the worse performance ) using different batch sizes , but nothing is working . so , i tried reducing network size to 4 layers and added more channels to initial layers to increase parallel computing but now i started getting memory allocation errors . it says allocation of some address exceeds 10 % and than my entire system freezes . i have to restart my laptop every time . i tried going back to the earlier version with 9 layers but that is giving me same error as well now , even though it worked earlier(not really worked , but atleast started training ) . so , what is the solution for this problem ? is it the problem of hardware being less capable or something else ? i have 8 gb ram and 2 gb gpu , but i do nt use gpu for training . i have intel i5 7gen processor .",20512,,,2018-12-08T12:06:41.093,keras giving memory allocation error and running extremely slow,machine-learning deep-learning convolutional-neural-networks keras,2,0,
2561,9396,1,,2018-12-08T05:24:56.380,3,171,"in section 7.1 ( about the n - step bootstrapping ) of the book reinforcement learning : an introduction ( 2nd edition ) , by andrew barto and richard s. sutton , the authors write about what they call the "" n - step return error reduction property "" : but they do n't prove it . i was thinking it should not be too hard but how can we show this ? i was thinking of using the definition of n - step return ( eq . 7.1 on previous page ) : $ $ g_{t : t+n } = r_{t+1 } + \gamma*r_{t+2 } + ... + \gamma^{n-1}*r_{t+n } + \gamma^{n}*v_{t+n-1}(s_{t+n})$$ because then this has the $ v_{t+n-1}$ in it already . but in the definition above of the n - step return it uses $ v_{t+n-1}(s_{t+n})$ but on the right side of the inequality ( 7.3 ) that we want to prove it is just little s $ v_{t+n-1}(s)$ ? so kind of confused here which state s it is using ? and then i guess after this probably pull out a term or something , how should we go from here ? this is the newest sutton barto book ( book page 144 , equation 7.3 ) : https://drive.google.com/file/d/1oppsz5az_kva1uwodoivenibfieohjkg/view",20515,2444,2019-02-16T02:08:32.307,2019-03-18T09:27:54.243,how do we prove the n - step return error reduction property ?,reinforcement-learning q-learning math proofs rl-an-introduction,1,0,1
2562,9397,1,9402,2018-12-08T06:25:30.990,2,64,"speech is a major primary mechanism of communication between humans . with respect to artificial intelligence , which signal is used to identify the sequence of words in speech ?",19660,4302,2018-12-08T08:50:13.867,2018-12-08T08:50:13.867,"in speech recognition , what kind of signal is used ?",natural-language-processing,2,0,
2563,9404,1,,2018-12-08T09:45:53.027,0,28,"i was wondering whether a lfsr could be approximated by a nn ( output or current state ) . we know that a lfsr is called linear in some sort of mathematical sense , but is that true ? considering it follows galois field mathematics . so can a neural network approximate a lfsr ? answers with mathematical proof or actual experience is preferred .",9947,9947,2018-12-08T09:51:50.547,2018-12-08T10:16:39.720,can a lfsr be approximated by a neural network ?,neural-networks deep-learning lfsr,1,0,
2564,9405,1,9408,2018-12-08T09:48:43.597,3,203,"according to the chaos report of the standish group most it projects fail . a famous example is the os/2 operating system , but large scale database projects from the government have also a high probability of wasting time and money . it 's important to focus on such software projects , especially if they went wrong because this helps to not repeat the same error twice . unfortunately , the examples in the chaos report are from classical computing and not from artificial intelligence and robotics . in ai history , the so called ai winter was a direct result of failed expectations and not working software , for example the “ general problem solver ” was n't able to solve practical problems . a failed robotics project from the past was the automation of an assembly line at the volkswagen car manufacturer , called “ halle 54 ” . general motors had made in the 1980s a similar failed robotics project which was “ gm hamtramck ” . are more projects from that category available , and why did they fail ? was it a software problem , or was the communication in the team not very well ?",11571,11571,2018-12-08T10:13:08.190,2018-12-28T14:15:53.730,why did artificial intelligence projects fail ?,history,3,0,
2565,9410,1,,2018-12-08T12:07:08.283,4,72,"around 2000 , existed popular fights videogames . there , if you made the same kick movement for several running times , the cpu normally defended and caught your strategy . this is the case in tekken 3 ie . how did the work ? is that considered intelligence ? did videogames in that time worked based on graphs ?",6345,1671,2018-12-09T23:30:13.287,2018-12-09T23:30:13.287,old video games intelligence,game-ai history,0,5,2
2566,9412,1,,2018-12-08T16:41:12.173,2,177,"background info in python , i 've implemented a rudimentary engine to play "" cheat "" , supporting both bots and a human or only bots . when only bots are playing , the game is simulated . when placing cards , input is represented by an array of integers corresponding to the indices of the cards . when bots play , they are presented with all valid combinations of cards to place ( currently , the choice made is random ) . for example , if the bot has two cards , their options are : [ [ 0 ] , [ 1 ] , [ 0 , 1 ] ] after a player places cards , the other players get a chance to call cheat ( true to accuse , false not to ) . when a player depletes their cards , they are appended to the winners list . the goal of the game is to have the lowest index possible in the winners list . summary of game data and end goal in summary , here is the data which i believe would be useful for the bots to play : the current number of cards that have been placed the current type ( e.g. ace ) to play the type of and suit of each card of the bot 's hand the number of cards that were just placed by a player the possible inputs to play during a bot 's turn the options for calling cheat ( true , false ) with the goal of ending up with the lowest index in the winners list . help wanted i 'm very new to machine learning , so i apologize for a such a high level question , but how might i go about using a python module to implement a system for bots to learn to play intelligently as they play ? are there any modules which you think would be ideal for this situation ? thank you !",20514,1671,2018-12-19T00:37:23.693,2018-12-19T00:37:23.693,"implementing ai / ml for the card game "" cheat """,machine-learning game-ai python,0,1,1
2567,9417,1,9538,2018-12-08T21:09:21.717,4,257,"consider a perceptron where $ w_0=1 $ and $ w_1=1 $ : now , say we use an activation function $ f(x)=1,~for ~ x=1 $ $ ~~~~~~~~~~~~~0 , otherwise$ the output is then summarised as : $ x_0~~~~~x_1~~~~~w_0*x_0 + w_1*x1~~~~~f(.)$ $ 0~~~~~~~0~~~~~~~~~~~~~~0~~~~~~~~~~~~~~~~~~~~~~~~~~~~0 $ $ 0~~~~~~~1~~~~~~~~~~~~~~1~~~~~~~~~~~~~~~~~~~~~~~~~~~~1 $ $ 1~~~~~~~0~~~~~~~~~~~~~~1~~~~~~~~~~~~~~~~~~~~~~~~~~~~1 $ $ 1~~~~~~~1~~~~~~~~~~~~~~2~~~~~~~~~~~~~~~~~~~~~~~~~~~~0 $ someone tell rosenblatt i solved his problem ... ... or have i ? is there something wrong with the way i 've defined the activation function ?",20526,,,2018-12-16T11:34:00.113,why ca n't the xor linear inseparability problem be solved with one perceptron - like this ?,neural-networks perceptron,6,0,2
2568,9418,1,,2018-12-08T22:02:53.463,0,66,"i 'm trying to train a siamese network to check if two images are similar . my implementation is based on this . i find the euclidian distance of the feature vectors(the final flattened layer of my cnn ) of my two images and train the model using the contrastive loss function . my question is , how do i get a binary output from the siamese network for testing ( 1 if it two images are similar , 0 otherwise ) . is it just by thresholding the euclidian distance to check how similar the images are ? if so , how do i go about selecting the threshold ? if i wanted to measure the training and validation accuracies , the threshold would have to be increased as the network learns better . is there a way to learn this threshold for a given dataset ? i would appreciate any leads , thank you .",20527,,,2018-12-12T19:46:26.120,how to get a binary output from a siamese neural network,machine-learning deep-learning convolutional-neural-networks,1,0,
2569,9424,1,9443,2018-12-09T08:29:55.637,3,95,what is the difference between additive and discounted rewards ?,19657,,,2018-12-26T17:16:09.033,what is the main difference between additive rewards and discounted rewards ?,difference reward-clipping,2,0,
2570,9425,1,,2018-12-09T10:09:27.203,1,53,"consider an extremely complicated feed - forward neural network training example but with no need of computational efficiency or limiting of processing time . what is the maximum number of hidden neurons h that a hidden layer should possess to detect all unique features/ correlations between input data from the previous layer which has n nodes ? in other words if we wanted to create a neural network with a large number of neurons in a hidden layer , what is the maximum neuron count possible that helps the network train ( give n neurons are in the previous layer ) ?",18640,,,2019-05-08T13:03:00.553,maximum number of neurons in a layer given number of neurons in previous layer,neural-networks neurons feedforward hyper-parameters,1,0,1
2571,9429,1,,2018-12-09T15:13:44.007,7,169,"in some tweets about neurips 2018 , this video from nvidia appeared . at around 0.37 , she says : ... if you think about the current computations in our deep learning systems , they are all based on linear algebra . can we come up with better paradigms to do multi - dimensional processing . can we do truly tensor algebraic techniques in our tensor cores ... i was wondering what she is talking about . i 'm not an expert so i 'd like to understand better this specific point .",20540,4302,2018-12-29T12:01:52.960,2018-12-29T12:01:52.960,paradigm shift in machine learning,machine-learning deep-learning ai-design linear-algebra,1,0,3
2572,9434,1,,2018-12-10T07:16:26.167,-3,123,"what is the hidden message inside human dna seequences ? more specificly , the relationship between the braiding of the double helix to a person 's emotions ?",20553,1671,2018-12-12T22:15:01.823,2018-12-12T22:15:01.823,dna is a language or code,self-awareness,3,5,
2573,9436,1,9437,2018-12-10T13:30:08.223,3,134,"to be honest , i had no idea where to put this question , but it 's sure that it 's related to ai . i want to build an application which uses camera , and by the movement it can calculate the -camera 's position compared to the objects -the objects creator and edge points by the movement . what it means that if the camera is in a static position , it 's just a picture . a set of coloured pixels . if we move the camera , we calculate the time , the gyroscope 's values , but most importantly , we can have a comparison of two images taken by the same objects . this way : -we can detect the edges -from the edges , we can detect which is closer than the others today 's phone camera 's are accurate enough to create ~60 crystal clear images per second , and it should be enough resource to accurately create high res models from just moving the camera according to some instructions ( that 's why i 'm surprised why it is n't existing in just a phone app ) . here comes the problem . i think the idea is worth for the try , but i 'm just a javascript developer . the browser can have access to the camera , with tensorflow i can use machine learning to detect edges , but if i want to be honest , i have no idea where to start , and how to continue step by step . can you please provide me some guidelines how it would be ideal to create the idea ?",20563,,,2018-12-11T17:30:41.940,how to use ai to depth map video ?,machine-learning deep-learning mapping-space,2,0,
2574,9439,1,,2018-12-10T15:19:12.553,1,67,"which do you think would be more helpful to humanity , ai or automation , and why ? how can we differentiate between them ?",19699,,,2018-12-12T05:20:55.693,"which do you think would be more helpful to humanity , ai or automation , and why ?",automation,3,0,
2575,9442,1,9453,2018-12-10T17:52:37.883,0,59,"karel the robot is an education software comparable to turtle graphics to teach programming for beginners . it 's a virtual stackbased interpreter to run a domain specific language for moving a robot in a maze . in it 's vanilla version , the user authors the script manually . that means , he writes down a computer program like “ 1 . moveforward 2 . if reachedobstacle==true then stop 3 . moveleft ” . this program gets started in the virtual machine . in contrast , genetic programming has the aim to produce computercode without human intervention . so called permutations are tested if they are fulfill the constraints and after a while the sourcecode is generated . in most publications , the concept is explained on a machine level . that means , assembly instructions are generated with the aim to replace normal computercode . in “ karel the robot ” a high - level language for controlling a robot is presented which has a stack but has a higher abstraction . the advantage is , that the state space is smaller . my question is : is it possible to generate “ karel the robot ” programs with genetic programming ?",11571,,,2019-01-01T12:01:34.190,"genetic programming with "" karel the robot "" ?",game-ai robots genetic-programming,2,2,1
2576,9445,1,,2018-12-10T20:18:57.977,1,36,"i am searching for an interpretation of lstms and recurrent neural networks within cognitive neuroscience . is there a mechanism in the human brain , that works analog to lstms ? how does long - term and short - term memory work in the brain on a neuron level ? i am looking forward to hints , explanations and links to in depth literature . thank you .",18713,,,2018-12-11T11:27:04.103,neurological interpretation of lstms,lstm artificial-neuron neurons brain long-short-term-memory,1,0,1
2577,9446,1,,2018-12-10T22:05:55.040,2,12,"i want like to be able to draw a shape outline e.g. ( pen and paper ) triangle , square , circle .. then label the vertices and sides and have ml identify the shape and the symbol associates with each vertex / side . for example a triangle and i label it with the adjacent , opposite and hypotenuse . or draw parallel and perpendicular lines and label angles and such . i would prefer it make one myself rather than use a pre built one . however if you know of a simple pre built one then i will ould love to pick it apart . can you please share some guidance on how to solve the above problem , namely shape identification / vertex and side labelling . any language",20579,,,2018-12-10T22:05:55.040,geometry shape identification and vertex / side label association,image-recognition,0,0,
2578,9448,1,,2018-12-11T06:01:05.643,2,292,"in aima , 3rd edition on page 125 , simulated annealing is described as : hill - climbing algorithm that never makes “ downhill ” moves toward states with lower value ( or higher cost ) is guaranteed to be incomplete , because it can get stuck on a local maximum . in contrast , a purely random walk — that is , moving to a successor chosen uniformly at random from the set of successors — is complete but extremely inefficient . therefore , it seems reasonable to try to combine hill climbing with a random walk in some way that yields both efficiency and completeness . simulated annealing is such an algorithm . in metallurgy , annealing is the process used to temper or harden metals and glass by heating them to a high temperature and then gradually cooling them , thus allowing the material to reach a lowenergy crystalline state . to explain simulated annealing , we switch our point of view from hill climbing to gradient descent ( i.e. , minimizing cost ) and imagine the task of getting a ping - pong ball into the deepest crevice in a bumpy surface . if we just let the ball roll , it will come to rest at a local minimum . if we shake the surface , we can bounce the ball out of the local minimum . the trick is to shake just hard enough to bounce the ball out of local minima but not hard enough to dislodge it from the global minimum . the simulated - annealing solution is to start by shaking hard ( i.e. , at a high temperature ) and then gradually reduce the intensity of the shaking ( i.e. , lower the temperature ) i know its about its example , but i just want more examples where stimulated annealing used in daily life",19442,2444,2019-03-02T11:07:49.820,2019-03-02T11:07:49.820,what are examples of daily life applications that use simulated annealing ?,search applications simulated-annealing,2,1,
2579,9450,1,,2018-12-11T07:37:19.740,2,172,"some early ai research , inspired by claude shannon 's maze learning mouse , theseus , sought to discover resolutions to conflict . in the case of theseus , the goal was to resolve the conflict between the simulated hunger and the walls of the maze separating theseus from the cheese . researchers of early ai theorem proving software ( mostly written in lisp ) sought ways out of mathematical mazes . finding the cheese , for those theorem provers , was to find a logical proof in the maze of mathematical corridors . the walls were illegal mathematical operations . in both cases , there is no mandatory opponent , only an individual working toward an achievement . although others may have the same objective & mdash ; although some may take the objective as a race and other mice or theorem provers as opponents , that is an arbitrary conception . the only real obstacle is the difficulty imposed by the naturally occurring features of the problem . framing intelligence as an adaptive response to opposition when morgenstern and von neumann 's game theory was applied , it was decided that the games would be games of opposition rather than games collaboration , possibly a consequence of the source of funding for much of the research . the software was designed such that the only other intelligence encountered in game play was an opponent and the goal was to annihilate it . dialog created by ted chiang and eric heisserer in denis villeneuve 's arrival , 2016 , starring amy adams , jeremy renner , and forest whitaker , exposes the folly of approaching new minds with the assumption of adversity . & nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;louise following suit . suits . suits , honor , flowers . colonel , those are all tile sets in mah - jongg . god , are they ... are the chinese using a game to converse with their heptapods ? & nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weber maybe . why ? & nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;louise well , let 's say that i taught them chess instead of english . every conversation would be a game . every idea expressed through opposition , victory , defeat . you see the problem ? if all i ever gave you was a hammer ... & nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weber everything 's a nail . in the third act , the viewer discovers that the objective of game play was more like shannon 's maze and that projected adversarialism was the only wall in the maze . consciously directing technological advancement this leads to some questions underlying the main question . do we want ai developed in a laboratory setting to be developmentally equivalent to a human child growing up in a war zone ? is elimination of the enemy a proven successful strategy in human geopolitical conflict ? what is the trend of blow - back from annihilation demonstrated throughout history ? would n't an ai system that seeks to discover win - win scenarios where players work together to overcome shared obstacles be better ? should ai research return to its claude shannon roots , where the conflict is between obstacles of the physical universe and objectives shared by living things ? do we want to imbue into intelligent robots and disembodied intelligence systems an obsession with winning or a more balanced set of objectives that includes collaborating ? how can ai be developed to win over things like poverty , crime , disease , ignorance , addiction , and economic instability ? as automated decision making develops , is it time to think about good ai citizenship ? with the power and complexity of ai systems increasing , when we researchers and engineers create loss , error , value , and reward functions , should we develop the discipline of always considering whether we are creating learning incentives that point the ai in the direction of becoming good contributors rather than narrow minded sociopaths ? must adaptation in artificial mental capacities be neither adversarial ( as in a chess or go player ) nor codependent ( as in asimov 's second law ) but rather compassionate , loving , transparent , and interested in growing authentic relationships of mutual benefit ? central question and specifics would ai systems not obsessed with winning become better citizens of the world ? what work is being done along these lines and how can best practices be developed to intentionally and responsibly steer ai development ? although these questions of the direction of technology were topics of science fiction and philosophy in the twentieth century , in the twenty first century , they are necessities of responsible research . it is wise to consider them legal and social questions that deserve the rigor of mathematical formalization and long term risk management , just as should be done with nuclear and genetic technologies . addendum response to comments at some point does n't it boil down to winning against the common enemy , shared obstacles ? whether the assignment of enemy status to members of the same species is of value to the species is questionable . evidence indicates human excellence to be primarily the result of collaboration , win - win scenarios , and symbiotic relationships . it is possible that further work on modelling civilization may someday reveal that framing all activity as a competition of some type may be a disease of the collective the primary consequence of which is the erosion of excellence .",4302,4302,2019-05-14T02:36:14.990,2019-05-14T02:36:14.990,would ai not obsessed with winning be better citizens of the world ?,training ai-community decision-theory risk-management greedy-ai,2,6,1
2580,9461,1,,2018-12-11T14:57:10.493,0,57,which is best approach to choose for game playing problem ?,19699,1671,2018-12-12T02:08:41.230,2018-12-12T07:00:12.553,which is best approach to choose for game playing problem ?,game-ai,1,6,1
2581,9463,1,9467,2018-12-11T16:43:11.477,4,56,"i want to learn a lot about the ai of ccg , such as hearthstone . and now i have known one of the main algorithms that used in this kind of games , mcts . it analyses the most promising moves , and expands the search tree based on random sampling of the search space . but there are too many random events in this game that can cause different results to one battle . for example , a card can randomly deal x damage to a hero or other follower , and x is a random number from 0 to 30 . the number of x is important for the next decision , but there will be a low accuracy by only using mcts . so what does the ai do to deal with these random events ?",20601,1641,2018-12-11T20:53:05.270,2018-12-11T20:53:05.270,how does hearthstone ai deal with random events,machine-learning game-ai monte-carlo-tree-search,1,0,
2582,9468,1,,2018-12-11T21:32:46.220,1,44,"i am trying to understand how to train a neural network to win a pong game using reinforcement learning , by following the blog post spinning up a pong ai with deep reinforcement learning . the environment is provided by gym ai . it gives the ai a reward of 1 if the opponent misses the ball , and a reward of -1 if it misses the ball . i am confused about how reward discounting works in this context . this is the function that the blog post used : def discount_rewards(r , gamma ) : "" "" "" take 1d float array of rewards and compute discounted reward "" "" "" r = np.array(r ) discounted_r = np.zeros_like(r ) running_add = 0 # we go from last reward to first one so we do n't have to do exponentiations for t in reversed(range(0 , r.size ) ) : if r[t ] ! = 0 : running_add = 0 # if the game ended ( in pong ) , reset the reward sum running_add = running_add * gamma + r[t ] # the point here is to use horner 's method to compute those rewards efficiently discounted_r[t ] = running_add discounted_r -= np.mean(discounted_r ) # normalizing the result discounted_r /= np.std(discounted_r ) # idem return discounted_r basically , the list of rewards is mostly filled with zeros , because usually nothing happens . when something happens , e.g. the reward is 1 , this is not only due to the action taken in that step . therefore , we need to smoothen the list of rewards so that some of that reward also belongs to previous actions . so far so good . however , it seems to me that if the opponent misses the ball and the reward is 1 , then this will be smeared such that it will emphasize the actions taken right before the opponent missed the ball . this seems wrong to me , the actions taken by the ai right before the opponent missed the ball are irrelevant . they do n't affect the ball in any way . i only think reward discounting makes sense when you have lost a point , then the actions just preceding the loss are surely very important and should be emphasized . however , the function takes into account both wins and losses . how should reward discounting be understood in the context of this pong game ?",20607,,,2018-12-11T21:32:46.220,reward discounting in reinforcement learning for a pong game,reinforcement-learning,0,0,
2583,9470,1,,2018-12-11T22:24:48.217,2,779,i want to implement yolo v3 . i want to know which framework will give me a faster result . what are the advantages of implementing yolo v3 on darknet framework vs keras framework ?,20025,,,2019-03-22T01:11:52.597,how to speed up yolov3 detection speed ?,deep-learning,3,0,1
2584,9474,1,,2018-12-12T01:45:17.587,1,20,a * algorithm is based on which search methods?explain me with example and is it optimal or complete if yes then how explain briefly ?,19370,,,2018-12-12T01:45:17.587,which search methods belongs to a *,algorithm search,0,5,
2585,9479,1,,2018-12-12T07:13:17.103,2,113,"what kind of problems is simulated annealing better suited for compared to genetic algorithms ? from my experience , genetic algorithms seem to perform better than simulated annealing for most problems .",19254,2444,2019-02-21T11:26:34.303,2019-03-23T12:00:50.137,when should i use simulated annealing as opposed to a genetic algorithm ?,genetic-algorithms search optimization simulated-annealing,2,2,
2586,9490,1,9500,2018-12-12T11:06:03.693,0,36,"i am classifying about 9 books from the image of their cover pages . i am using a tensorflow keras cnn model for this . but , the model predicts a book even when a picture of a book is not taken , like a wall , sofa , house etc . i want to avoid this . i want the model to first classify whether there is a book in the image and then classify the book in 9 classes . how could i achieve this ?",18985,,,2018-12-12T19:10:04.440,classification of books by their cover page,machine-learning tensorflow,1,1,
2587,9491,1,,2018-12-12T13:27:58.903,6,976,"this question is regarding reinforcement learning and different / inconsistent action space for every / some states . what do i mean by inconsistent action space ? let say you have an mdp where the number of actions varies between states ( for example like in figure 1 or figure 2 ) . we can express "" inconsistent action space "" formally as $ $ that is , for every state , there exists some other state which do not have the same action set . in the figures ( 1 , 2 ) there 's a relatively small amount of actions per state . instead imagine states $ s \in s$ with $ m_s$ number of actions , where $ 1 \leq m_s \leq n$ and $ n$ is a really large integer . environment to get a better grasp of the question , here 's an environment example . take figure 1 and let it explode into a really large directed acyclic graph with a source node , huge action space and a target node . the goal is to traverse a path , starting at any start node , such that we 'll maximize the reward which we 'll only receive at the target node . at every state , we can call a function $ m : s \rightarrow a'$ that takes a state as input and returns a valid number of actions . approches ( 1 ) a naive approach to this problem ( discussed here and here ) is to define the action set equally for every state , return a negative reward whenever the performed action $ a \notin a(s)$ and move the agent into the same state , thus letting the agent "" learn "" what actions are valid in each state . this approach has two obvious drawbacks : learning $ a$ takes time , especially when the q - values are not updated until either termination or some statement is fulfilled ( like in experience replay ) we know $ a$ , why learn it ? ( 2 ) another approach ( first answer here , also very much alike proposals from papers such as deep reinforcement learning in large discrete action spaces and discrete sequential prediction of continuous action for deep rl ) is to instead predict some scalar in continuous space and by some method map it into a valid action . the papers are discussing how to deal with large discrete action spaces and the proposed models seam to be a somewhat solution for this problem as well . ( 3 ) another approach that came across was to , assuming the number of different action set $ n$ is quite small , have functions $ f_{\theta_1}$ , $ f_{\theta_2}$ , ... , $ f_{\theta_n}$ that returns the action regarding that perticular state with $ n$ valid actions . e.i , the performed action of a state $ s$ with 3 number of actions will be predicted by . none of the approaches ( 1 , 2 or 3 ) are found in papers , just pure speculations . i 've searched a lot but can not find papers directly regarding this matter . my questions are therefore does anyone know any paper regarding this subject ? is the terminology wrong ? "" inconsistant "" , "" irregular "" , "" different "" ... ? anyone having another approach worth digging into ?",20627,20627,2018-12-13T07:57:15.950,2018-12-13T17:07:53.980,inconsistent action space in reinforcement learning,reinforcement-learning,2,0,4
2588,9494,1,9499,2018-12-12T14:33:00.403,0,30,"i 'm working on a home tool that will help create a shopping list from a list of recipes chosen for a coming week . this boils down to : extracting ingredients and their quantities from recipes . grouping similar ingredients together . summing up quantities for similar ingredients . naming groups of similar products in a shopping list . the tasks seem non - trivial for a few reasons . similar ingredients are described differently , depending on the recipe book / portal , e.g. : 5 lemons 5 lemons ( to be squeezed ) 5 fresh lemons 5 big yellow lemons recipes lists alternatives for ingredients ( e.g. , "" 3 lemons or 5 limes "" ) , leaving decision up to a user . recipes involve some information about product - preprocessing . for instance , one has to buy lemons instead of lemon juice when the recipe says : 100ml lemon juice 100ml freshly squeezed lemon juice my language has a complex inflection . for instance , there can be multiple plural forms of a noun and the form of an adjective must be agreed with a form of noun . adapting nlp algorithms designed for english language might be not straightforward and require some lemmatizing / stemming but not for single words , but whole phrases . naming products group is hard . once fresh lemons and big yellow lemons are group together and their quantities summed up , one need to decide how to name this group in a shopping list , e.g. : "" 10 lemons "" or "" 10 fresh lemons "" . is there any research paper that would cover those challenges ? especially applied in the same domain ?",20631,20631,2018-12-12T14:57:31.997,2018-12-12T18:01:17.957,grouping products and naming groups,natural-language-processing applications,1,1,
2589,9496,1,,2018-12-12T15:15:26.693,1,1011,"i wonder if it 's possible to ' clean up ' an audio recording of a lecture from a smartphone , using some type of ai system ?",20633,,,2018-12-12T15:39:56.747,audio enhancing using ai ? ( removing background noise during lecture recording ),deep-learning,1,5,2
2590,9502,1,9519,2018-12-13T00:15:17.507,2,73,"how to select an action in a state if the action does not necesarily cause the environment to change state ? given 10 states ( $ s_0 $ to $ s_9 $ ) and in each state $ i$ there are two actions defined $ ( 1,-1)$ . $ 1 $ increases a parameter of the environment and $ -1 $ decreases it . for example , if the parameter is speed and it is currently 1 rad / s , corresponding to $ { s_i}$ , an action can be either increase or decrease this and therefore transition to the next state , ideally $ s_{i-1}$ or $ s_{i+1}$ . it is unclear how to formulate a reinforcement learning problem in this case . the problem is the same magnitude of the parameter change through an action . the range of the parameter is ( 3 , 25 ) and step size is 1 . the problem is thath the responce of the environment is not the same in every satate . in some states a parameter change with magnitude 1 results in a state transition in some state the magintude proves to be to small to provoke a transition change . for example , if the envitonment is instate $ s_1 $ and action 1 is applied , how can the magintude of the paremter be adopted in a way which assures that the environment will transition to state $ s_2 $ ? how can the step in the paremeter change be made adaptive ? actually my environment is uncertain that is why i do n't know exactly whether this action will take me to next state or not . for your information , i am using q learning off policy algorithm . suppose state 9 is the goal state and my q table is $ 10 \times 2 $ .",20655,2585,2018-12-14T21:01:28.890,2018-12-14T22:03:46.020,reinforcement learning with adaptive action magnitude,reinforcement-learning,1,0,
2591,9504,1,,2018-12-13T04:33:11.933,2,124,"i 've been working on a game - playing engine for about half a year now , and it uses the well known algorithms . these include minimax with alpha - beta pruning , iterative deepening , transposition tables , etc . i 'm now looking for a way to include monte carlo tree search , which is something i 've wanted to do for a long time . i was thinking of just making a new engine from scratch , but if possible i 'd like to somehow import mc tree search into the engine i 've already built . are there any interesting strategies to import mc tree search into a standard game - playing ai ?",16917,2444,2019-05-12T21:32:34.567,2019-05-12T21:32:34.567,any interesting ways to combine monte carlo tree search with the minimax algorithm ?,game-ai monte-carlo-tree-search minimax,1,0,
2592,9506,1,,2018-12-13T09:44:01.557,0,31,"i am learning about probabilistic graphical models and i was wondering if there is an example explaining the math behind conditional random fields . looking solely on the formula i have no idea what we actually do . i found a lot of examples for hidden markov model . there is a part speech - tag task where we have to find the tags for the sentence "" flies like a flower "" . on these slides ( slide 8) ambiguity resolution : statistical method - prof . ahmed rafea a hmm is used to find the correct tags . how would i transform this model to a crf and how would i apply the math ?",15391,,,2019-05-14T02:02:00.397,is there a mathemtical example for conditional random fields ?,models generative-model sequence-modelling probability-distribution,1,0,1
2593,9507,1,,2018-12-13T12:06:58.553,2,105,"according to this website : http://yarpiz.com/67/ypea104-acor ( in the website it is mentioned that it is a project aiming to be a resource of academic and professional scientific source codes and tutorials . ) : "" originally , the ant algorithms are used to solve discrete and combinatorial optimization problems . various extensions of ant colony optimization ( aco ) are proposed to deal with optimization problems , defined in continuous domains . one of the most useful algorithms of this type , is acor , the ant colony optimization for continuous domains , proposed by socha and dorigo , in 2008 ( here ) . "" what is the difference between continuous domains and discrete combinatorial optimization ? i appreciate if you could also mention some examples for each type .",19910,2444,2019-05-27T22:25:22.687,2019-05-27T22:25:22.687,what is the difference between continuous domains and discrete combinatorial optimization ?,difference swarm-intelligence combinatorics ant-colony,2,0,
2594,9508,1,,2018-12-13T13:27:52.433,1,27,"is it possible to mention the drawbacks / advantages of swarm routing ( such as ant routing etc ) in comparison with classical routing algorithms in communication networks in a general view ? in other words , what will we gain if we replace a classical routing algorithm with a swarm routing based algorithm ? can we compare these two type of routing algorithms in a general view to mention their to count the drawbacks / advantages ? the main purpose of this question is to define applications of each of those routing approaches . which one is more decentralized ? and which one has more efficient performance ? here is my personal opinion ( i am not sure about it ) : classical internet routing in more centralized than swarm routing ( such as aco based routing ) which does not use any routing table and router to avoid moving towards centralization where those routing tables and routers can be manipulated ( as a point of failure ) . instead , classical internet routing may be faster than swarm based routing . briefly , classical may be more centralized , but faster and on the other side , swarm based is more decentralized but may be slower . am i wrong ? please note that when i say "" decentralized "" , i mean a network like "" ad hoc networks "" that do not rely on routers or access points ( as a point of failure ) to avoid moving towards centralization . in case of using routers or access point , some kind of centralization is inherent . with this definition of decentralization , it seems swarm based routing such as ant routing would be more decentralized . however , i am not sure about it and that 's my main question .",19910,19910,2018-12-14T09:43:59.793,2018-12-14T09:43:59.793,classical internet routing vs. swarm routing ( such as ant routing ) ?,optimization swarm-intelligence,0,0,
2595,9510,1,,2018-12-13T14:33:20.940,2,55,"[ english is not my native language and i find it hard to express something that i want to search for in artificial intelligence papers . ] people could be sad , happy , depressive , angry , nervous , calm , relaxed , bored , etc . i do n't know how to express all of these feelings and emotions in english terms that enable me to search papers related to them . i want to know if there is some research about how to identify these feelings with artificial intelligence . how can i search for the above in search engines like ieee xplore or scopus , sciencedirect , etc . ? udpate i want to identify a person 's emotions using facial expression , heartbeat , body temperature , sweating or nervous behaviour ( using one or all of them ) .",4920,4920,2019-01-17T12:16:40.023,2019-01-17T12:16:40.023,recognize human 's feelings using artificial intelligence,research,0,3,
2596,9512,1,,2018-12-13T16:37:18.700,1,24,"when adding dropout to a neural network , we are randomly removing a fraction of the connections ( setting those weights to zero for that specific weight update iteration ) . if the dropout probability is p , then we are effectively training with a neural network of size 1−p*(number of original units ) . using this logic , there is not limit how big i can make a network , as long as i proportionately increase dropout , i can always effectively train with the same sized network , and thereby just increasing the number of "" independent "" models working together , making a larger ensemble model . thereby improving generalization of the model . for example , if a network with 2 units already achieves good results in the training set ( but not in unseen data -i.e validation or test sets- ) , also a network with 4 units + dropout 0.5 ( ensemble of 2 models ) , and also a network with 8 units + dropout 0.75 ( ensemble of 4 models ) ... and also a network with 1000 units with a dropout of 0.998 ( ensemble of 500 models ) ! in practice it is recommended to keep dropout at 0.5 , which advises against the approach mentioned above . so there seem to be reasons for this . what speaks against blowing up a model together with an adjusted dropout parameter ?",20638,20638,2018-12-13T18:42:33.823,2018-12-13T18:42:33.823,to remove neural - network units or to increase drop - out ?,dropout regularization,1,0,1
2597,9518,1,9523,2018-12-13T21:07:05.210,3,148,"in reinforcement learning book from sutton & amp ; barto ( 2018 edition ) , specifically in section 7.5 of the book , they present an n - step off - policy algorithm that does n't require importance sampling called n - step tree backup algorithm . in other algorithms the return in the update was consisted of rewards along the way and the estimated value(s ) of the node(s ) at the bottom , but in tree backup update a return is consisted of things mentioned before plus the estimated values of the actions that were n't picked during these n steps , all weighted by the probability of taking the action from the previous step . i have few questions about things in this algorithm that are unclear to me . question : why is this algorithm consider an off - policy algorithm ? as far as i could notice , only a single target policy is mentioned and there is no talk about behaviour policy generating actions to take . question : in control we want our target policy to be deterministic , greedy policy , so how do we exactly generate actions to take in this case since behaviour policy is n't used ? if we generate actions from greedy policy we wo nt explore so we wo n't learn the optimal policy . what am i missing here ? question : if i understood something wrong and we are actually using behaviour policy , i do n't understand how would the update work in the case where our target policy is greedy . the return is consisted of estimates taken from actions that were nt picked , but because our policy is greedy the probabilities used in calculating those estimates would be 0 so the total estimate of those actions would be 0 as well . the only non 0 probability in our target policy is the one of the greedy action(which is probability of 1 ) so the entire return would fall down to n - step sarsa return . so basically my question here is how are we allowed to do this update in this case , why is this return allowed to replace the one with importance sampling ?",20339,,,2018-12-14T20:11:58.527,questions about n - step tree backup algorithm,reinforcement-learning,1,0,
2598,9520,1,,2018-12-14T15:08:08.750,4,97,"in this article here , the writer claims that a new type of neural net is required to deal with data that is both continuous , and also sparsely sampled . it was my understanding that this was the entire purpose of techniques that use neural nets , to make assumptions about a system with a non - continuous data set . so why do we need to switch to a non - layered design to deal with these data sets better ?",20696,,,2019-05-28T08:03:15.347,why do layered neural nets struggle with continous data ?,neural-networks,2,2,
2599,9521,1,9553,2018-12-14T16:30:50.230,0,50,"i am trying to use tensorflow / keras to play a text based game . the game opposes two players that play by answering questions by choosing an answer among the proposed ones . game resembles this : questions asked from player 1 , choose value { 0 , 1 , 2 } player 1 chooses answer 1 questions asked from player 2 , choose value { 0 , 1 } player 2 chooses answer 0 ( and so on ) the issue is that i do not have any data to use for training the agents and it not possible to evaluate each actions of the agent individually . my idea is to get 2 agents to play against each other and evaluate them depending on who won / lost ( the games are very short with about 20 to 30 decisions made for each player ) . the issue i have is that i do not know where to start . i normalized my input , but i do not know how to get the 2 agents to compete , as i do not have any training data as shown in the tutorials and the agents have to complete a full game in order to evaluate their performance .",20698,2585,2018-12-15T10:40:43.690,2018-12-16T17:44:59.837,how can i oppose two ai agents with keras / tensoflow ?,tensorflow keras self-play,1,0,
2600,9526,1,9528,2018-12-14T22:25:11.380,4,104,"i 've been re - reading the wikipedia article on the chinese room argument and i 'm ... actually quite unimpressed by it . it seems to me to be largely a semantic issue involving the conflation of various meanings of the word "" understand "" . of course , since people have been arguing about it for 25 years , i doubt very much that i 'm right . however ... the argument can be thought of as consisting of several layers , each of which i can explain away ( to myself , at least ) . there is the assumption that being able to understand ( interpret a sentence in ) a language is a prerequisite to speaking in it . let 's say that i do n't speak a word of chinese , but i have access to a big dictionary and a grammar table . i could work out what each sentence means , answer it , and then translate that answer back into chinese , all without speaking chinese myself . therefore , being able to interpret ( parse ) a language is not a prerequisite to speaking it . ( of course , by the theory of extended cognition i can interpret the language , but we can all agree that the books and lookup tables are simply a source of information and not an algorithm ; i 'm still the one using them . ) nevertheless , this task can be removed by a dumb natural language parser and a dictionary , converting chinese to the set of concepts and relationships encoded in it and vice versa . there is no understanding involved at this stage . there is the assumption that being able to understand ( identify and maintain a train of thought about concepts in ) a language is a prerequisite to speaking in it . we 've already optimised away the language , to a set of concepts and relationships between concepts . now all we need is another lookup table : a sort of verbose dictionary that maps concepts to other concepts and relationships between them . for example , one entry for "" computer "" might be "" performs calculations "" and another might be "" allows people to play games "" . an entry for "" person "" might be "" has opinions "" , and another might be "" has possessions "" . then an algorithm ( yes , i 'm introducing one now ! ) would complete a simple optimisation problem to find a relevant set of concepts and relationships between them , and turn "" i like playing computer games "" into "" what is your favourite game to play on a computer ? "" or , if it had some entries on "" computer games "" , "" which console do you own ? "" . the only "" understanding "" here , apart from the dumb optimisation algorithm , is the knowledge bank . this could conceivably be parsed from wikipedia , but for a good result it would probably be at least somewhat hand - crafted . following this would fall down , because this process would n't be able to talk about itself . there is the assumption that being able to understand ( "" know "" how information in affects one 's self ) a language is a prerequisite to speaking in it . a set of "" opinions "" and such associated with the concept "" self "" could be implemented into the knowledge bank . all meta - cognition could be emulated by ensuring that the knowledge bank had information about cognition in it . however , this program would still just be a mapping from arbitrary inputs to outputs ; even if the knowledge bank was mutable ( so that it could retain the current topics from sentence to sentence and learn new information ) it would still , for example , not react when a sentence is repeated to it verbatim 49 times . there is the assumption that being able to have effective meta - cognition is a prerequisite to speaking in a language . except ... there 's not . the program described would probably pass the turing test . it certainly fulfils the criteria of speaking chinese . and yet it clearly does n't think ; it 's a glorified search engine . it 'd probably be able to solve maths problems , would be ignorant of algebra unless somebody taught it to it ( in which case , with sufficient teaching , it 'd be able to carry out algebraic formulae ; haskell 's type system can do this without ever touching a numerical primitive ! ) , and would probably be turing - complete , and yet would n't think . and that 's ok . so why is the chinese room argument such a big deal ? what have i misinterpreted ? this program understands the chinese language as much as a python interpreter understands python , but there is no conscious being to "" understand "" . i do n't see the philosophical problem with that .",125,,,2019-05-05T04:06:17.533,why is the chinese room argument such a big deal ?,philosophy chinese-room-argument,2,0,2
2601,9527,1,,2018-12-14T23:27:33.827,1,53,"i recently encountered an interesting problem and was wondering how rl would solve it . the objective of the problem is to maximize the coffee quality , given by box x. the coffee quality objective function is defined by the company . to maximize the quality of the coffee , we can perform 2 actions : change stirring speed of the coffee machine change the temperature of the coffee machine now to the tricky part . the coffee bean characteristic coming into the coffee machine is random . we can measure its characteristics before sending them to the coffee machine , but we can not change them . i formulated this problem into a control problem , such that $ x$ is a function of my previous states , $ x(t - k)$ , the control input , $ u$ , and the measured disturbance , $ d$ . given a constant $ d$ , the problem is trival to solve because the disturbance is a constant part of the environment . however , during times when $ d$ changes rapidly , the policy is no longer optimal . how do i inject the information of the measured disturbance into my rl agent ?",17706,17706,2018-12-15T07:50:21.437,2018-12-28T23:17:11.170,how does reinforcement learning handle measured disturbances ?,reinforcement-learning control-problem measured-disturbance,2,0,
2602,9533,1,,2018-12-15T10:22:50.983,0,62,"i 'm currently implementing an android app for street sign recognition . my solution works quite well for the gtsrb dataset , since it provides a labeled test set of centered images . however , it does n't scale up to more realistic scenarios like for images in the gtsdb , where the signs only take up some pixels . is it still recommended to downsample the image to 224x224 ?",19177,,,2019-01-14T12:02:25.483,recognition of small objects,neural-networks convolutional-neural-networks image-recognition object-recognition,1,0,
2603,9534,1,,2018-12-15T10:44:50.110,2,76,"i already post the question on stackoverflow : https://stackoverflow.com/questions/53785922/is-the-use-of-a-non-monotonic-activation-function-is-a-correct-and-viable-solut?noredirect=1#comment94423971_53785922 i found several papers about how to build a single layer perceptron able to solve the xor problem . the papers describe a solution where the heaviside step function is replaced by a non - monotonic activation function . here is the papers : http://hsmazumdar.net/single_layer_neural_net.htm https://www.researchgate.net/publication/227309651_solving_the_xor_and_parity_nproblems_using_a_single_univers i also found this topic on stackoverflow : https://stackoverflow.com/questions/30412427/solving-xor-with-single-layer-perceptron can we really solve the xor problem with a simple perceptron ? because the use of a non - monotonic activation function is not really common on this topic . papers about this idea are scarce . generally , the main solution is to build a multilayer perceptron .",20716,20716,2018-12-16T03:14:26.637,2018-12-16T03:14:26.637,non - monotonic activation function and xor problem with perceptron,neural-networks perceptron activation-function,0,3,
2604,9541,1,,2018-12-15T21:31:53.780,-1,80,"i 'm completely new at ml , but really interested . to be honest , read many articles about it , but still do n't understand the workings of it . i just started to understand this example : https://storage.googleapis.com/tfjs-examples/mnist/dist/index.html my thinking about it is that tf has some resources , some examples of how numbers look like , and try to match them with the ones in the test . i saw that sometimes the test changes a right prediction to a wrong , but makes better and better predictions . but how ? i think that the program does n't know the right predictions ( and this way it wo n't know the wrong ones ) . in the training how it makes better predictions ? test by test , from what exceptions it will change it 's predictions ? what happens in a new test ?",20563,,,2018-12-20T19:41:49.517,how tensorflow will know if the prediction is true or false ?,neural-networks machine-learning tensorflow,2,0,
2605,9542,1,,2018-12-15T22:42:06.817,0,69,"i am a computer science student and my task is to develop a mobile app for android and/ or ios using artificial intelligence , designed to help people reduce/ combat depression.regarding the programming language , i am thinking to go with javascript because this is the one i am more comfortable with . i mention that my knowledge on artificial intelligence is little and i haven ' t done anything in ai or of this kind before . i am planning to use ai to help people suffering from depression by creating a mobile app : user - friendly app in terms of design and sequence of actions , easily customizable looks and functionality to be able to import all the relevant information : key words , text , images , videos from facebook , instagram , twitter to prevent or report the existence of depression and correctly interpret the nature , intensity and frequency of depression symptoms the app includes : a patient profile , an achievement board and a dream chart page , to - do list page , led lights which react to emotions , facial recognition technology , thinking patterns page ( the app uses the principals of cognitive- behavioral therapy ) incorporates sensors that can measure changes in mood , sleep patterns and increased isolation , measuring the device lock time , ambient lighting and audio to predict sleep time utilizes gps sensor , recent calls to monitor the wellbeing of the patients the app provides users with self - help guidelines / techniques , questionnaire designed to track severity of symptoms over time , educational resources for treatment the users are encouraged to write down daily thoughts in order to analyze and identify negative thinking patterns the app sends notifications to remind the patients to take their medication , is offering activity suggestions the app incorporates appointment scheduler tool for the counseling sessions including private messages , online meetings how do i get started ? what platform would you recommend me to develop a mental health mobile app ( for android and/or ios ) using ai in order to help people suffering from depression to combat it and why ? a platform based on cloud or better not ? can a chat bot be integrated in a mental health mobile app ? please , suggest me some related literature , ebooks to set my research objectives because i 'm having a hard time finding it . please , help me!thank you very much in advance .",20725,20735,2018-12-18T18:26:56.793,2018-12-18T18:26:56.793,can someone suggest me a platform to develop a mental health mobile app for android and/or ios using ai ?,getting-started programming-languages software-evaluation healthcare,3,2,2
2606,9545,1,,2018-12-16T08:06:37.283,1,90,"i have multiple pictures that look exactly like the one below this text . i 'm trying to train cnn to read the digits for me . problem is isolating the digits . they could be written in any shape , way , and position that person who is writing them wanted to . i thought of maybe training another cnn to recognize the position / location of the digits , but i 'm not sure how to approach the problem . but , i need to get rid of that string and underline . any clue would be a great one . btw . i would love to get the 28x28 format just like the one in mnist . thanks up front .",18676,18676,2018-12-16T22:37:25.953,2019-02-07T12:02:15.403,how to approach this handwritten digit recognition ?,convolutional-neural-networks image-recognition computer-vision handwritten-characters,2,2,
2607,9550,1,,2018-12-16T12:33:59.517,4,74,"i 've been reading up on how neat ( neuro evolution of augmenting topologies ) works and i 've got the main idea of it , but one thing that 's been bothering me is how you split the different networks into species . i 've gone through the algorithm but it does n't make a lot of sense to me and the paper i read does n't explain it very well either so if someone could give an explanation of what each component is and what it 's doing then that would be great thanks . the 2 equations are : $ f_{i}^ { ' } = \frac{f_i}{\sum_{j=1}^{n}sh(\delta(i , j))}$ btw i can understand the greek symbols so you do n't need to explain those to me the original paper",20736,20736,2018-12-22T20:32:36.630,2018-12-22T20:32:36.630,how does the neat speciation algorithm work ?,neural-networks machine-learning neat,0,0,1
2608,9552,1,9583,2018-12-16T17:04:49.833,0,388,"what 's the strategy if the resolution of an image is very low such as 28 x 28 or 100 x 100 or 150 x 150 , for transfer learning ? pre - trained models such as inception , xception , vgg-16 etc are required specific size images . in these case , how to solve this issue ? in addition , what also about very high - resolution images ? training such model and concern weights on such data set which is far more different than image - net data set , i know that we should take whole model structure but except that last softmax classifier by setting include_top = false . and manually setting this for the specific task and performing backpropagation for fine - tuning . can anyone please inform more info on this ? i read keras doc but these few silly things stuck on my head . thanks . with appreciation , in .",20043,,,2018-12-18T14:15:31.810,pre trained model for low resolution image ! how to handle ?,neural-networks convolutional-neural-networks models,2,0,
2609,9557,1,,2018-12-16T21:30:00.607,1,28,"i have heard about bidirectional rnn lstm units ( endcoders - decoders ) , but my question is - is there bidirectional neural machine translation , that uses a->b weights for the translation in the opposite direction b->a ? if not , then what are the obstacles to such system ?",8332,,,2018-12-29T02:57:04.063,is there bidirection sequence - to - sequence neural machine translation ?,neural-networks machine-learning natural-language-processing lstm,2,0,
2610,9561,1,,2018-12-17T08:48:34.260,1,41,"i was reading this paper hadamard product for low - rank bilinear pooling i understand what they are trying to say but i do n't know why do we have to convert the element - wise multiplication into a scalar(using the dot product ) why do we have to multiply the resulting vector by the one vector ? i mean , we would still use the multiplicative interaction between elements if we did not consider multiplying by that one vector",20755,,,2019-01-16T14:01:45.380,why do we have to dot product in the low - rank bilinear pooling ?,deep-learning,1,0,
2611,9563,1,9566,2018-12-17T10:52:00.850,3,210,"if i got well the global idea of dropout it allows to improve the sparsity of the information that come from one layer to another by setting some weights to zero . in another hand , pooling , let 's say max pooling , takes the maximum value in a neighborhood , reducing as well to zero , the influence of values apart from this maximum . without considering shape transformation due to pooling layer , can we say that pooling is a kind of dropout step ? does adding dropout or dropconnect layer after a pooling layer has a sense in cnn ? and does it help further more the training process and generalization property ?",11069,,,2019-02-12T16:26:41.420,is pooling a kind of dropout,convolutional-neural-networks dropout,2,0,
2612,9564,1,,2018-12-17T11:05:29.750,0,12,"in the literature , many papers are written about fuzzy control systems . usually they are based on diagrams which shows the relationship between input signal and control signal . the interesting aspect of fuzzy controllers is , that this relationship can be nonlinear and become error tolerant . unfortunately , it is hard to grasp to concept of a linguistic variable which is used in the diagrams . let us take a step back to a different kind of concept , described outside of fuzzy control , namely motion primitives . a motion primitive is a subroutine which can have a name in english , for example “ walk ” or “ standup ” . combining many motion primitives together results into a behavior tree and in a subsumption architecture ( rodney brooks ) . similar to linguistic variables in fuzzy control theory , a motion primitive has a name in natural language . that means , that the function name is grounded in terms known outside of the software and by humans . my question : are motion primitives and fuzzy control the same , because they are containing both linguistic variables like walk , stand - up , or “ speed = middle ” ?",11571,,,2018-12-17T11:05:29.750,is fuzzy control equal to grounded motion primitives ?,natural-language-processing fuzzy-logic,0,0,
2613,9569,1,9600,2018-12-17T14:20:50.983,1,106,"when reading reinforcement learning by sutton and barto , i came across the importance sampling ratio . the first equation , i believe , describes the probability a particular sequence is obtained given the current state , and the policy . the next part takes takes the ratio between the probabilities of the two trajectories : i do n't understand how this ratio could lead to this : the $ g_t$ rewards are obtained through the $ b$ policy , not the policy . i think there is something to do with bayes rule , but i could not derive it . could someone guide me through the derivation ?",20762,,,2018-12-19T10:31:25.260,importance sampling ratio probability,reinforcement-learning markov-decision-process,1,0,
2614,9571,1,9575,2018-12-17T17:57:43.167,2,37,"i came across this article today : these faces show how far ai image generation has advanced in just four years . i would never in a million years have guessed that the people on the right ( in the first image in the article ) were fakes ! will it be possible to create videos of such ai generated images ? what , then , will become of actors and actresses ?",17601,1671,2018-12-18T01:27:55.453,2018-12-18T01:27:55.453,creating videos of ai generated photographs,neural-networks machine-learning image-generation,1,0,
2615,9574,1,9581,2018-12-17T22:03:26.010,1,65,"i do n't know what to search for , so i 'll try to describe what i 'm trying to do : i want an ai that learns by showing it how to navigate on the internet . let 's say i want to save 50 images from a website . 1 ) what do you call this style of learning : i show the ai 1 to 5 saves , and it has to do the next 2 ) does it exist ? is it possible to go "" this far "" ? in particular , the ai has to learn how to deal with connection errors ( repeat a bit later for instance ) . 3 ) should it be based on the code of the page , or a screenshot , or something else ? thank you in advance , i need the english terms for further researchs , and leads for algorithms ( i want it to work also in windows , for renaming files ) ^^",19094,,,2018-12-19T00:16:56.463,ai learning by demonstration for web navigation,neural-networks deep-learning reinforcement-learning ai-design learning-algorithms,1,2,
2616,9578,1,9648,2018-12-18T07:59:36.253,1,228,"i 'm learning ai , but this confuses me . the derivative function used in backpropagation is the derivative of activation function or the derivative of loss function ? these terms are confusing : derivative of act . function , partial derivative wrt . loss function ? ? i 'm still not getting it correct .",2844,,,2018-12-22T14:45:28.253,what is the derivative function used in backpropagration ?,backpropagation activation-function loss-functions,2,0,1
2617,9580,1,,2018-12-18T11:27:55.680,1,79,"how would one go about predicting which characters ( actors ) are going to die in the next avengers movie . to elaborate a bit , given all leaked scripts ( fake or not ) , interviews of different actors and directors , contracts with different actors and directors . how would / should one go about predicting if an actor is going to die in the upcoming movie or not . note : i am not sure but i think a similar effort has already been made for game of thrones .",19722,19722,2018-12-18T11:44:30.870,2019-05-18T16:01:15.193,predicting fate of different actors in upcoming avengers 4,machine-learning ai-design game-ai strong-ai,1,5,
2618,9582,1,,2018-12-18T13:24:10.003,2,61,"i am a computer science student and my task is to develop a mobile app for android and/ or ios using artificial intelligence , designed to help people reduce/ combat depression.regarding the programming language , i am thinking to go with javascript because this is the one i am more comfortable with . i mention that my knowledge on artificial intelligence is little and i haven ' t done anything in ai or of this kind before . i am planning to use ai to help people suffering from depression by creating a mobile app : user - friendly app in terms of design and sequence of actions , easily customizable looks and functionality to be able to import all the relevant information : key words , text , images , videos from facebook , instagram , twitter to prevent or report the existence of depression and correctly interpret the nature , intensity and frequency of depression symptoms the app includes : a patient profile , an achievement board and a dream chart page , to - do list page , led lights which react to emotions , facial recognition technology , thinking patterns page ( the app uses the principals of cognitive- behavioral therapy ) incorporates sensors that can measure changes in mood , sleep patterns and increased isolation , measuring the device lock time , ambient lighting and audio to predict sleep time utilizes gps sensor , recent calls to monitor the wellbeing of the patients the app provides users with self - help guidelines / techniques , questionnaire designed to track severity of symptoms over time , educational resources for treatment the users are encouraged to write down daily thoughts in order to analyze and identify negative thinking patterns the app sends notifications to remind the patients to take their medication , is offering activity suggestions the app incorporates appointment scheduler tool for the counseling sessions including private messages , online meetings how do i get started ? what platform would you recommend me to develop a mental health mobile app ( for android and/or ios ) using ai in order to help people suffering from depression to combat it and why ? a platform based on cloud or better not ? i was reading about appmachine , appmakr , shoutem , but i still can not decide about the most appropriate platform i am going to use in order to develop my mental health mobile app . can a chat bot be integrated in a mental health mobile app ? please , suggest me some related literature , ebooks to set my research objectives because i 'm having a hard time finding it . please , help me!thank you very much in advance .",20735,,,2019-01-26T23:02:10.433,can someone suggest me a platform to develop a mental health mobile app for android and/or ios using ai ?,getting-started software-evaluation programming-languages healthcare,0,0,0
2619,9584,1,9586,2018-12-18T14:29:49.637,2,97,"i 'm using rl to train a network on the game connect4 . it learns quickly that 4 connected pieces is good . it gets a reward of 1 for this . a zero is rewarded for all other moves . it takes quite a time until the ai tries to stop the opponent from winning . is there a way this could be further reinforced ? i thought about giving a negativ reward for the move played before the winning move . thinking about this i came to the conclusion that this is a bad idea . there 'll be always a looser ( except for ties ) , therefor there always be a last move from the loosing player . this one has n't to be a bad one . mistakes could have been made much earlier . is there a way to improve this awareness of opponents ? or does it just have to train more ? i 'm not perfectly sure if the rewards will propagate back in a way that encourages this behavior with my setup .",13336,,,2018-12-18T15:44:08.923,how to set the reward for reinforcement learning in board games,reinforcement-learning,1,0,
2620,9588,1,,2018-12-18T16:34:16.317,2,55,"i 've been given an assignment to create a neural network that will suggest a croatian word for a word given in any other european language ( out of those found here ) . the words are limited to drinks you can find on a bar menu . i 've looked at many nn examples , both simple and complex , but i 'm having trouble with understanding how to normalize the input . for example , words "" beer "" , "" birra "" and "" cervexa "" should all translate to "" pivo "" . if i include those 3 in the training set , and after the network has finished training i input the word "" bier "" , the output should be "" pivo "" again . i 'm not looking for a working solution to this problem , i just need a nudge in the right direction regarding normalization .",20787,,,2019-05-27T00:02:20.290,translating a single word neural networks,neural-networks,2,0,
2621,9590,1,9653,2018-12-18T17:02:51.690,1,147,"i am learning reinforcement learning from the lectures from david silver . i finished lecture 6 and went on to try sarsa with linear function approximator for mountaincar - v0 environment from openai . a brief explanation of the mountaincar - v0 environment . the state is denoted by two features , position , and velocity . there are three actions for each state , accelerate forwards , do n't accelerate , accelerate backward . the goal of the agent is to learn how to climb a mountain . the engine of the car is not strong enough to power directly to the top . so speed has to be built up by oscillating in the cliff . i have used a linear function approximator , written by myself . i am attaching my code here for reference : - class linearfunctionapproximator : ' '' a function approximator must have the following methods:- constructor with num_states and num_actions get_q_value get_action fit ' '' def _ _ init__(self , num_states , num_actions ) : self.weights = np.zeros((num_states , num_actions ) ) self.num_states = num_states self.num_actions = num_actions def get_q_value(self , state , action ) : return np.dot ( np.transpose(self.weights ) , np.asarray(state ) ) [ action ] def get_action(self , state , eps ) : return randint(0 , self.num_actions-1 ) if uniform(0 , 1 ) & lt ; eps else np.argmax ( np.dot(np.transpose(self.weights ) , np.asarray(state ) ) ) def fit(self , transitions , eps , gamma , learning_rate ) : ' '' every transition in transitions should be of type ( state , action , reward , next_state ) ' '' gradient = np.zeros_like(self.weights ) for ( state , action , reward , next_state ) in transitions : next_action = self.get_action(next_state , eps ) g_target = reward + gamma * self.get_q_value(next_state , next_action ) g_predicted = self.get_q_value(state , action ) gradient [ : , action ] + = learning_rate * ( g_target - g_predicted ) * np.asarray(state ) gradient /= len(transitions ) self.weights + = gradient i have tested the gradient descent , and it works as expected . after every epoch , the mean squared error between current estimate of q and td - target reduces as expected . here is my code for sarsa : - def sarsa(env , function_approximator , num_episodes=1000 , eps=0.1 , gamma=0.95 , learning_rate=0.1 , logging = false ) : for episode in range(num_episodes ) : transitions = [ ] state = env.reset ( ) done = false while not done : action = function_approximator.get_action(state , eps ) next_state , reward , done , info = env.step(action ) transitions.append ( ( state , action , reward , next_state ) ) state = next_state for i in range(10 ) : function_approximator.fit(transitions[::-1 ] , eps , gamma , learning_rate ) if logging : print('episode ' , episode , ' : ' , end= ' ' ) run_episode(env , function_approximator , eps , render = false , logging = true ) basically , for every episode , i fit the linear function approximator to the current td - target . i have also tried running fit just once per episode , but that also does not yield any winning episode . fitting 10 times ensures that i am actually making some progress towards the td - target , and also not overfitting . however , after running over 5000 episodes , i do not get a single episode where the reward is greater than -200 . eventually , the algorithm choses one action , and somehow the q - value of other actions is always lesser than this action . # now , let 's see how the trained model does env = gym.make('mountaincar-v0 ' ) num_states = 2 num_actions = env.action_space.n function_approximator = linearfunctionapproximator(num_states , num_actions ) num_episodes = 2000 eps = 0 sarsa(env , function_approximator , num_episodes = num_episodes , eps = eps , logging = true ) i want to be more clear about this . say action 2 is the one which is the action which gets selected always after say 1000 episodes . action 0 and action 1 have somehow , for all states , have their q - values reduced to a level which is never reached by action 2 . so for a particular state , action 0 and action 1 may have q - values of -69 and -69.2 . the q - value of action 2 will never drop below -65 , even after running the 5000 episodes .",20424,,,2018-12-27T00:32:30.690,sarsa wo n't work for linear function approximator for mountaincar - v0 in openai environment . what are the possible causes ?,reinforcement-learning gradient-descent,2,0,
2622,9592,1,9593,2018-12-18T23:47:02.990,1,102,"once a book is published in a language , why ca n't the publishers use google translate ai or some similar software to immediately render the book in other languages ? likewise for wikipedia : i 'm not sure i understand why we need editors for each language . ca n't the english wikipedia be automatically translated into other languages ?",17601,1671,2018-12-19T01:22:54.070,2018-12-19T02:23:39.937,why ca n't we use google translate ai for everything ?,neural-networks google machine-translation,2,0,
2623,9595,1,9598,2018-12-19T02:38:37.567,1,130,"in short , imitation learning means learning from the experts . suppose i have a dataset with labels based on actions of experts . i use a simple binary classifier algorithm to assess whether it is good expert action or bad expert action . how is this binary classifier different from imitation learning ? imitation learning is associated with reinforcement learning but in this case , it looks more like a basic classification problem to me . what is the difference between imitation learning and classification done by experts ? i am getting confused because imitation learning relates to reinforcement learning while classification relates to supervised learning .",16839,,,2018-12-19T09:06:05.957,what is the difference between imitation learning and classification done by experts ?,reinforcement-learning classification,2,0,1
2624,9597,1,,2018-12-19T06:15:01.797,1,26,"in "" 4.1 learning multi - layer deconvolutional filters "" section , the last paragraph says that "" since our model is generative , we can sample from it . in fig . 3 we show samples from the two different models from each level projected down into pixel space . the samples were drawn using the relative firing frequencies of each feature from the training set . "" i do n't know how the pictures in fig.3 are generative . since that the filters has been learned and feature maps in every layer can be infered , for example , in terms of fruit samples , in "" layer 1 "" , is the the first layer 's feature map ? i feel that not true ... seems like the sample are low - level ... paper says "" ... from each level projected into pixel space "" , these words are short and confuse me . somebody could explain that for me ? thank you very much !",20797,,,2018-12-19T06:15:01.797,a question about zeiler 's paper “ deconvolutional networks ”,deep-learning unsupervised-learning,0,0,
2625,9601,1,,2018-12-19T13:26:46.660,0,39,"so , i 'm using a pretrained pnasnet5large model to do some image classification ( https://github.com/cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py ) in the file , it says that the input range is in [ 0,1 ] ( i 'm assuming pixel values of input images ) . the images i have are already in this range . the channel means and standard deviation for rgb channels are stated as [ 0.5 , 0.5 , 0.5 ] , [ 0.5 , 0.5 , 0.5 ] respectively . now when i use the torchvision.transforms.normalize ( https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.normalize ) to normalize the images using the stated means and standard deviations , the pixel values get to the range [ -1,1]. the code i wrote for normalization : transforms.normalize([0.5 , 0.5 , 0.5],[0.5 , 0.5 , 0.5 ] ) i believe i 'm missing something fundamental . should i normalize the images or should i not ? thanks !",20807,,,2019-05-18T17:03:15.670,"relationship between input range and channel means , standard deviations for cnns",convolutional-neural-networks classification,1,0,
2626,9602,1,9670,2018-12-19T14:15:52.613,3,78,"consider some image classification problem . conceptually , we then have some high dimensional space where all the images can be represented as points , and having large enough labeled data set we can build a classifier . but how do we know that our data in this space has some structure ? like this one in two dimensional case : if we have a data set with images of , say , cats and dogs , why these two classes are not just uniformly mixed with each other but have some distribution or shape in appropriate space ? why it can not be like this : thanks !",20809,,,2018-12-26T02:41:25.283,structure in data,datasets structure probability-distribution,2,3,
2627,9608,1,,2018-12-19T18:11:26.677,0,32,can anyone suggest different methods with which to approach this problem ?,20814,1671,2018-12-20T00:52:43.807,2019-01-20T04:05:19.150,analyze the pattern of merchants account transactions in bank and identify the merchants that are becoming inactive,data-science ai-methods,1,1,
2628,9610,1,,2018-12-19T20:58:02.760,1,48,"i am interested in optimizing the memory capacity of an agi . given a specific complex input an ai can create a simplified model . this is a problem that can be solved using sparse coding [ 1 ] . however , this solves only the problem of encoding and not the maintenance of online representations — in cognitive terms : the state of mind . a default cognitive model of short - term memory can be separated in three different stages : encoding → maintenance → retrieval one solution is to use specialized hardware [ 2][3 ] , but i am interested in software approaches to this problem and i would thus like to emphasize that it is the digital representation , which i am most interested in . with the exception of qubits , the smallest possible representation are binary digits . however , additional architecture is required to represent phase spaces ( i.e. floating point precision memory ) and higher - order representations maybe include arrays and dictionaries . ( optimizing these are trivial ... or simply to be postponed until needed , according to knuth ) . how can a specific connectivity pattern be stored in an optimally compact representation ? is there an implementation with concrete example code ? what is the state - of - the - art ? * * i will not specify "" real - time "" here , but the context is humanoid agi . [ 1 ] papyan , v. , romano , y. , & amp ; elad , m. ( 2017 ) . convolutional neural networks analyzed via convolutional sparse coding . journal on machine learning research , 18 ( 83 ) : 1–52 . arxiv:1607.08194 http://jmlr.org/papers/volume18/16-505/16-505.pdf [ 2 ] legallo et al . ( 2018 ) . mixed - precision in - memory computing . https://www.nature.com/articles/s41928-018-0054-8 [ 3 ] ibm . ( 2018 ) . ibm scientists demonstrate mixed - precision in - memory computing for the first time ; hybrid design for ai hardware . https://www.ibm.com/blogs/research/2018/04/ibm-scientists-demonstrate-mixed-precision-in-memory-computing-for-the-first-time-hybrid-design-for-ai-hardware/",16411,1671,2018-12-20T01:04:50.067,2018-12-28T00:18:30.347,how can a specific connectivity pattern be stored in an optimally compact representation ?,optimization sparse-input memory binary information-theory,2,2,1
2629,9613,1,9620,2018-12-20T00:07:49.153,0,40,"i 'm wondering if there exists a network for simple image classification . what i mean by this is if i have two image datasets , one of horses and one of zebras , i want to train off the horses and classify an image as either a horse or not a horse , so if i test it on an image of a horse , it says it is a horse , but if i use a zebra , it says it is not a horse . does any library / project for this exist ?",20826,,,2018-12-20T12:58:38.170,python network for simple image classification,classification,1,0,
2630,9614,1,10369,2018-12-20T01:00:04.310,3,106,"in policy gradient method , there 's a trick to reduce a variance of policy gradient . we use causality , and remove part of the sum over rewards so that only actions happened after the reward are taken into account ( see here http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf , slide 18 ) . why does it work ? i understand the intuitive explanation , but what 's the rigorous proof of it ? can you point me to some papers ?",18074,1641,2019-02-02T19:19:54.490,2019-02-02T19:19:54.490,"why does the "" reward to go "" trick in policy gradient methods work ?",reinforcement-learning theory math policy-gradients rewards,1,0,1
2631,9621,1,,2018-12-20T15:14:50.537,1,28,"i have only a limited dataset ( & lt;25 ) with large - sized images ( > 1500x2000 ) and their pixelwise labels . the aim is to find unusual patterns in this industry dataset and highlight them . to generate training images i crop 256x256 grids out of every image and do some data augmentation and use these images to train my u - net . for my prediction i split my image with numpy again into 256x256px grids and predict every grid separately and put them together to an image . but this will take some time , like > 10 minutes . but it has a quite good accuracy . how can i optimize my prediction to be faster ? is it faster to create this with a tensorflow pipeline ? when i want to predict the full image with giving shape(none , none,3 ) , i get "" concatop : dimensions of inputs should match "" after some time .",20200,,,2018-12-20T15:14:50.537,image segmentation prediction with cropping 256x256 grids is very slow,neural-networks deep-learning convolutional-neural-networks computer-vision,0,0,
2632,9624,1,9683,2018-12-20T17:30:40.910,2,201,problem : fraud detection task : classifying transactions as either fraud/ non - fraud using gan,20842,,,2019-01-25T01:01:45.347,can we implement gan ( generative adversarial neural networks ) for classication problem like fraud detecion ?,neural-networks generative-adversarial-networks,2,1,1
2633,9635,1,,2018-12-21T07:28:43.780,2,253,"in semi - supervised learning , there are hard labels and soft labels . could someone show me what 's exactly the meaning of the two things ?",20831,2193,2018-12-22T01:38:21.803,2018-12-22T01:38:21.803,"about the definition of "" soft label "" and "" hard label """,definitions,1,1,
2634,9637,1,,2018-12-21T12:05:24.880,0,359,"i fine tuned mobilenetssd for object detection using a dataset with just one class ( ~4000 images ) . all the training images include at least one bounding box related to that class ( no empty images ) . by following the example with the voc dataset , the labelmap includes two classes , the background and my custom class . however , as i mentioned , there are no annotations related to the background and i am not sure if there should be any . now my fine tuned network performs very well when objects belonging to my class are present , however there are some false detections with very high confidence when the class is not present . can this be related to the fact that i do n't have empty images in my training set ?",16671,16671,2018-12-24T09:50:47.907,2019-01-19T15:03:35.333,"when training an object detection network for one class , should i include empty images in the dataset ?",datasets object-recognition,1,2,
2635,9638,1,,2018-12-21T12:21:13.527,1,57,my dog goes bonkers every time the sound of a barking dog is heard on a television program . i never noticed this before but literally every movie or show with an outdoors setting eventually includes the sound of a barking dog . is it possible to develop a real - time filter that blocks or masks these sounds ?,20854,1671,2018-12-22T01:30:36.370,2019-05-21T21:03:35.703,can i filter barking sounds on the tele ?,theory real-time audio-processing ai-methods real-world,1,5,
2636,9639,1,,2018-12-21T13:15:41.980,1,71,this question was asked in an ai exam . how would you answer such question ?,20856,1671,2018-12-22T01:37:30.807,2018-12-30T02:46:28.590,can alpha – beta be used on symmetric zero sum games ?,game-ai game-theory alpha-beta-pruning,2,4,1
2637,9641,1,,2018-12-21T18:37:27.863,1,54,"i have a couple different segmentation tasks that i would like to perform on medical imaging data using cnn 's . i 'm currently trying to wrap my head around how well a 3d network might work , using a u - net architecture , but i have some hesitation . my specific questions are as follows : question 1 - say we have medical imaging taken at different slices ( different heights / depths ) of the patient 's body . in order for a 3d cnn to work properly on such data , do the images need to be taken at a heights that are close together , i.e. does the 3rd dimension need to be fairly continuous ? for example , if you had a 3d stack / volume of 5 images that you wanted to feed to a 3d cnn like so , img_1_depth_0cm.png img_2_depth_5cm.png img_3_depth_10cm.png img_4_depth_15cm.png img_5_depth_20cm.png and the images were taken 5 cm apart from one and other , i would imagine that a 3d convolution might not perform very well because of the 5 cm of depth in between images ? is this an incorrect assumption ? ( as a reference point , this nice repository on github was designed for training on images of the brain , but the images do appear to be fairly continuous / close together , almost like a video : https://github.com/ellisdg/3dunetcnn ) question 2 - for a 3d network to properly segment non - labelled volumes after training is finished , i know that the input data dimensions would have to match those of the training data . but i would also think that the new images must have been taken in a similar fashion ( taken at similar depths and in general a similar orientation to the training data ) in order for the nn to be able to perform its task . so unless the medical imaging processes are always performed similarly across machines and hospitals , i 'm guessing that the nn performance might vary pretty wildly when it tries to segment new data . is this correct ?",20867,20868,2018-12-22T01:38:32.470,2019-05-21T03:03:10.750,appropriateness of 3d convolutional neural network for segmentation of medical image data,neural-networks convolutional-neural-networks computer-vision,1,0,
2638,9643,1,,2018-12-21T20:07:29.427,-2,2692,"i was trying to implement capsulenet for classifying some of the native digits . all the images are rgb images and resize to 32 x 32 and dataset has 10 classification output . x_train_all.shape : ( 72045 , 32 , 32 , 1 ) y_train_all.shape : ( 72045 , 10 ) here is the pre - processing i 've done on the dataset . x = [ ] # initialize empty list for resized images for i , path in enumerate(paths_img ) : img = cv2.imread(path,cv2.imread_grayscale ) # cv2.imread_color : loads a color image . any transparency of image will be neglected . it is the default flag . if resize_dim is not none : img = cv2.resize(img , ( resize_dim , resize_dim ) , interpolation = cv2.inter_area ) # resize image to 28x28 # x.append(np.expand_dims(img,axis=2 ) ) # expand image to 28x28x1 and append to the list . x.append(img ) # expand image to 28x28x1 and append to the list # display progress if i = = len(paths_img ) - 1 : end='\n ' else : end='\r ' print('processed { } /{}'.format(i+1 , len(paths_img ) ) , end = end ) x = np.array(x ) # tranform list to numpy array if path_label is none : return x else : # concatenate all data into one dataframe df = pd.dataframe ( ) l = [ ] for file _ in path_label : df_x = pd.read_csv(file_ , index_col = none , header=0 ) l.append(df_x ) df = pd.concat(l ) df = df.set_index('filename ' ) y_label = [ df.loc[get_key(path)]['digit ' ] for path in paths_img ] # get the labels corresponding to the images y = to_categorical(y_label , 10 ) # transfrom integer value to categorical variable return x , y first , define the capsnet model which takes the following parameters.i used the capsule net ( capsnet , capslayer ) implemented code from here . # define model model = capsnet(input_shape=[32 , 32 , 1 ] , n_class=10 , num_routing=3 ) next , another function naming train has been defined for actual training . it defines as follows : def train(model , data , epoch_size_frac=1.0 ) : "" "" "" training a capsulenet : param model : the capsulenet model : param data : a tuple containing training and testing data , like ` ( ( x_train , y_train ) , ( x_test , y_test ) ) ` : param args : arguments : return : the trained model "" "" "" # unpacking the data ( x_train , y_train ) , ( x_val , y_val ) = data # callbacks log = callbacks.csvlogger('capsulenet/log.csv ' ) checkpoint = callbacks.modelcheckpoint('capsulenet/capsulenet weights each epochs / weights-{epoch:02d}.h5 ' , save_best_only = true , save_weights_only = true , verbose=1 ) lr_decay = callbacks.learningratescheduler(schedule = lambda epoch : 0.001 * np.exp(-epoch / 10 . ) ) # compile the model model.compile(optimizer='adam ' , loss=[margin_loss , ' mse ' ] , loss_weights=[1 . , 0.0005 ] , metrics={'out_caps ' : ' accuracy ' } ) # using the tensorboard callback of keras . tbcallback = keras.callbacks.tensorboard(log_dir='capsulenet/capgraph ' , histogram_freq=0 , write_graph = true , write_images = true ) # -----------------------------------begin : training with data augmentation ----------------------------------- # def train_generator(x , y , batch_size , shift_fraction=0 . ) : train_datagen = imagedatagenerator(width_shift_range = shift_fraction , height_shift_range = shift_fraction ) # shift up to 2 pixel for mnist generator = train_datagen.flow(x , y , batch_size = batch_size ) while 1 : x_batch , y_batch = generator.next ( ) yield ( [ x_batch , y_batch ] , [ y_batch , x_batch ] ) # training with data augmentation . model.fit_generator(generator=train_generator(x_train , y_train , 64 , 0.1 ) , steps_per_epoch = int(epoch_size_frac*y_train.shape[0 ] / 64 ) , epochs = 1 , validation_data = [ [ x_val , y_val ] , [ y_val , x_val ] ] , callbacks = [ log , checkpoint , lr_decay , tbcallback ] ) # -----------------------------------end : training with data augmentation ----------------------------------- # return model ok , i think this chunk of information only enough here . however , then i define a function for data augmentation and both training the model . this function will iteratively train on the shuffling fold of the data set , defined below . kfold = kfold(n_splits=10 , shuffle = true , random_state=42 ) cvscores = [ ] fold = 1 for train , val in kfold.split(x_train_all , y_train_all ) : gc.collect ( ) k.clear_session ( ) print ( ' fold : ' , fold ) x_train = x_train_all[train ] x_val = x_train_all[val ] x_train = x_train.astype('float32 ' ) x_val = x_val.astype('float32 ' ) y_train = y_train_all[train ] y_val = y_train_all[val ] # train the model with data augmentation train(model = model , data = ( ( x_train , y_train ) , ( x_val , y_val ) ) , epoch_size_frac = 0.5 ) # evaluate the model scores = model.evaluate(x_val , y_val , verbose = 0 ) print(""%s : % .2f%% "" % ( model.metrics_names[1 ] , scores[1]*100 ) ) cvscores.append(scores[1 ] * 100 ) fold = fold + 1 now , when i start training , i get following error : typeerror traceback ( most recent call last ) & lt;ipython - input-67-a8ca211d12a2&gt ; in & lt;module&gt ; ( ) 19 20 ---&gt ; 21 train(model = model , data = ( ( x_train , y_train ) , ( x_val , y_val ) ) , epoch_size_frac = 0.5 ) 22 23 typeerror : ' numpy.ndarray ' object is not callable i think i need to look over the dimension of the image . while i visualize the image i got the following : import matplotlib.pyplot as plt print(x_train_all.shape ) plt.imshow(x_train_all[1 ] ) ( 72045 , 32 , 32 , 1 ) any informative help will be highly appreciated .",20043,20043,2018-12-21T20:13:11.100,2018-12-22T05:34:55.033,typeerror : ' numpy.ndarray ' object is not callable,deep-learning classification,1,1,1
2639,9650,1,9668,2018-12-22T15:44:02.340,1,53,"according to wikipedia , a notation is a semiotics term to describe artistic disciplines . famous examples are : chess notation , siteswap notation for juggling , labanotation for dancing , basketball play diagrams and the aresti catalog ( flight maneuvers ) . in some papers about advanced robotics , also a notation was used to parse motion capture data , for example in the eu poeticon project lead by aloimonos , yiannis . the interesting fact is , that a notation is usually discussed outside of core artificial intelligence . it has nothing to do with programming computers itself nor with deeplearning , instead notations are researched by linguists . they are interesting because they reduce the state space . a notation is some kind of structure to summarize millions of potential states of a system into a handful , which is expressed in a handy grammar . it 's not direct an artificial intelligence , but it allows to build such a software more easily . in all cases , notations are grounded in natural language , which means that the communication about the topic is done between humans . for example , a basketball team is using a notation for discussing the next move they want to make and they are making signs during the game as a reference . so my question is : how important are notations for artificial intelligence ? can they be used to build robot - control - systems ?",11571,,,2018-12-23T22:01:41.047,how important are notations for artificial intelligence ?,theory definitions,1,0,1
2640,9651,1,,2018-12-22T15:46:44.227,2,48,"i 'm a relative newbie to fuzzie logic systems but i have some knowledge in mathematics . i have the following problem : i want to fuzzify certain values . some are in the range [ - , ] and some are in the range [ $ 0 $ , ] . for the first range i have chosen the sigmoid function : $ f(x ) = \frac{1}{1+e^{-x}}$ the question is , which fuzzification process i should use for the second range . since the function $ f(x ) = \ln(x)$ transforms [ $ 0 $ , ] to [ - , ] a natural choice could be : $ f(x ) = \frac{1}{1+e^{-\ln(x ) } } = \frac{x}{x+1}$ a different function could also be : $ f(x ) = 1 - 2^{-x}$ which one would be more suitable ? particularly when considering that i may want to compare values from both ranges .",20880,,,2019-05-30T09:01:45.187,choice of fuzzification function,math logic fuzzy-logic,1,1,
2641,9652,1,9655,2018-12-22T17:34:42.387,0,207,"i tried to build a neural network for working on iris dataset using only numpy after reading an article ( link : https://iamtrask.github.io/2015/07/12/basic-python-network/ ) . i tried to search the internet but everyone was using ml libraries and found no solution using just numpy . i tried to add different hidden layers to my feed forward neural network still it was n't converging . i tried to use backpropagation . i used sigmoid and also relu neither of which was successful . can someone please give me the code which will work on iris dataset and built only using feed forward neural networks and numpy as the only library or if it is not possible to built such a thing with these constraints then please let me know what goes wrong with these constraints . also tell me will it be possible to create a neural network to predict values of a matrix multiplication i.e. if we have a * b = c with matrix a as input and c as output , can we acheive substantial amount of accuracy with feed forward neural networks here ? .",20881,,,2018-12-22T20:37:31.960,feed forward neural network using numpy for iris dataset,neural-networks deep-learning backpropagation gradient-descent feedforward,1,0,
2642,9660,1,,2018-12-23T06:20:11.647,0,32,"problem : "" for a given news article , generate another title for the article if the article is to be published under a different publication . "" which algorithm will be well suited for this ? should i use naive bayesian or any nlp algorithm ?",20878,4302,2018-12-23T23:39:03.760,2019-05-23T00:02:05.217,what ai designs are suited for producing title replacements ?,machine-learning ai-design natural-language-processing,1,0,
2643,9661,1,,2018-12-23T07:09:23.033,1,86,i have a steady hex - map and turn - based wargame featuring wwii carrier battles i would like to improve the fixed policy for the ai using reinforcement learning and have a bunch of noob questions which i will try to spread them on several posts . software & mdash ; is python mandatory ? the goal is to learn through self - play and the program is written in objective - c with the aim to migrate it to unity / c#. hardware & mdash ; is it hopeless to achieve anything with a single computer ?,20886,4302,2018-12-23T23:54:16.680,2018-12-23T23:54:16.680,wargame — software and hardware requirement for reinforcement learning,reinforcement-learning game-ai,1,0,1
2644,9662,1,,2018-12-23T07:49:29.403,2,82,"i have a steady hex - map and turn - based wargame featuring wwii carrier battles on a given turn , a player may choose different and independent actions ( moving one , two naval unit , assigning a mission to an air unit , changing some battle parameters , reorganizing naval task forces etc … ) . usually , boardgames deals mainly with one action ( go , chess ) or very few ( backgammon ) . here the player may select - several actions - the actions are of different nature - each action getting some variants ( strength , payload , destination ) how to approach this problem ? found this article of interest https://project.dke.maastrichtuniversity.nl/games/files/msc/roelofs_thesis.pdf",20886,20886,2018-12-23T21:47:01.490,2019-05-25T15:01:10.320,large and multiple - actions space,reinforcement-learning game-ai,1,1,1
2645,9665,1,,2018-12-23T10:44:26.103,5,68,"i have a steady hex - map and turn - based war game featuring wwii carrier battles . i would like to improve the fixed policy for the ai using reinforcement learning . i have some beginner 's questions , which i will try to spread across several posts . the game use fog of war . information about the game is zero at the beginning and is slowly disclosed ( approximate position and composition of naval task forces ) what is the best approach to deal with that ? partial info on blue naval forces . others are not visible on map",20886,2444,2019-02-15T17:10:17.503,2019-02-15T17:10:17.503,what is the appropriate approach to playing a game with incomplete state information ?,reinforcement-learning game-ai markov-decision-process pomdp,1,1,1
2646,9667,1,10478,2018-12-23T18:13:37.787,3,44,"i 'm trying to implement the neat algorithm using c # , based off of kenneth o. stanley 's paper . on page 109 ( 12 in the pdf ) it states "" matching genes are inherited randomly , whereas disjoint genes ( those that do not match in the middle ) and excess genes ( those that do not match in the end ) are inherited from the more fit parent . "" does this mean that the child will always have the exact structure that the more fit parent has ? it seems like the only way the structure could differ from crossover was if the two parents were equally fit .",20892,,,2019-02-09T20:56:31.710,"using neat , will the child of two parent genomes always have the same structure as the more fit parent ?",neural-networks genetic-algorithms neat,1,0,1
2647,9669,1,,2018-12-24T02:48:58.917,1,142,i am starting to learn lstm by understanding how it is used for creating a char - rnn and had a fundamental question . does the number of nodes in the hidden layer need to be the same as that of the input sequence as depicted in the figure or can we have less than the number of input sequences . if less than number of input sequence can be used how do we interpret the lstm structure,20900,,,2018-12-24T07:53:14.917,number of nodes in hidden layer for lstm,recurrent-neural-networks lstm,0,0,
2648,9674,1,,2018-12-24T17:12:58.153,1,24,"for http://neuralnetworksanddeeplearning.com/chap5.html , could anyone suggest : 1 ) how to approach the derivation of expression ( 123 ) ? 2 ) what constitutes value ~ 0.45 ? 3 ) why the need of taylor series when we can observe the identity property without any maths proof ( input = = output ) ?",20570,,,2018-12-24T17:12:58.153,neuralnetworksanddeeplearning.com chapter 5 problems,neural-networks gradient-descent artificial-neuron,0,0,
2649,9677,1,,2018-12-25T10:47:57.407,-2,68,"i am trying to use a artifical neural network to produce a single output , which in my mind should be an index into a list of data ( or close to it ) . all of the results i get are 0.9999 + and very close to each other . i do n't know if my whole way of thinking here is off , or if i am just missing an approach , or if perhaps my network code is just broken . i am trying to make use of the simple neural network from microsoft here : https://social.technet.microsoft.com/wiki/contents/articles/36428.basis-of-neural-networks-in-c.aspx i have tried this with a significantly more complex data set , but i 've also tried using a very simple data set . here is the simple training data i 'm trying to use : eat poo bad eat dirt bad eat cookies okay eat fruit good study poo okay study dirt okay study cookies okay study fruit okay dispose poo good dispose dirt okay dispose cookies bad dispose fruit bad the basic idea is that the network has two input neurons and a single output neuron . i assigned a unique number to each distinct word such that i can train the network with two inputs ( verb and object ) , and expect a single output ( good , bad , or okay ) . example training : input : 1 ( for eat ) 10 ( for dirt ) output : 15 ( for bad ) input : 1 ( for eat ) 11 ( for cookies ) output : 16 ( for good ) i would expect that after training , i would see the output numbers close to 15 , 16 , etc , but all i get are numbers like 0.999997333313168 , etc . example run : input : 1 ( for eat ) 10 ( for dirt ) , output is 0.999997333313168 ( instead of ~15 expected ) what do these outputs mean , or what am i missing in how i should be thinking about making a basic classification system ( given inputs , get a meaningful output ) ? the c # code i am using , if it is helpful : using neuralnet.neuralnet ; using system ; using system.collections.generic ; using system.linq ; using system.text ; namespace neuralnet { internal class testsmallsamplenetwork { internal static void run ( ) { var data = @""eat poo bad eat dirt bad eat cookies okay eat fruit good sell poo bad sell dirt okay sell cookies okay sell fruit okay study poo okay study dirt okay study cookies okay study fruit okay dispose poo good dispose dirt okay dispose cookies bad dispose fruit bad "" ; var simples = data.split(new [ ] { "" \r\n "" } , stringsplitoptions.none ) .select ( _ = & gt ; new simple(_)).tolist ( ) ; var verbs = simples.select(_ = & gt ; _ .verb).distinct().select ( _ = & gt ; new networkvalue(_)).tolist ( ) ; var objects = simples.select(_ = & gt ; _ .object).distinct().select ( _ = & gt ; new networkvalue(_)).tolist ( ) ; var judgments = simples.select(_ = & gt ; _ .good).distinct().select ( _ = & gt ; new networkvalue(_)).tolist ( ) ; var values = verbs.concat(objects).concat(judgments).todictionary(_ = & gt ; _ .term , _ = & gt ; _ ) ; // create a network with 2 inputs , 2 neurons on a single hidden layer , and 1 neuron output . var net = new neuralnetwork(0.9 , new int [ ] { 2 , 2 , 1 } ) ; for ( int itrain = 0 ; itrain & lt ; 1000 ; itrain++ ) { for ( int isimple = 0 ; isimple & lt ; simples.count ; isimple++ ) { net.train(makeinputs(simples[isimple ] , values ) , makeoutputs(simples[isimple ] , values ) ) ; } } foreach ( var value in values.values ) { console.writeline(value ) ; } console.writeline ( ) ; // run samples and get results back from the network run(""study "" , "" poo "" , values , net ) ; run(""eat "" , "" poo "" , values , net ) ; run(""dispose "" , "" fruit "" , values , net ) ; run(""sell "" , "" dirt "" , values , net ) ; } private static void run(string verb , string obj , dictionary&lt;string , networkvalue&gt ; values , neuralnetwork net ) { var result = net.run(new list&lt;double&gt ; { values[verb].value , values[obj].value , } ) .single ( ) ; var good = "" xxx "" ; console.writeline($""{verb } { obj } { good } ( { result } ) "" ) ; } private static list&lt;double&gt ; makeinputs(simple simple , dictionary&lt;string , networkvalue&gt ; values ) { return new list&lt;double&gt ; ( ) { values[simple.verb].value , values[simple.object].value } ; } private static list&lt;double&gt ; makeoutputs(simple simple , dictionary&lt;string , networkvalue&gt ; values ) { return new list&lt;double&gt ; { values[simple.good].value } ; } public class simple { public string verb { get ; set ; } public string object { get ; set ; } public string good { get ; set ; } public simple(string line ) { var words = line.trim().split ( "" "" .tochararray ( ) , 3 , stringsplitoptions.none ) ; verb = words[0 ] ; object = words[1 ] ; good = words[2 ] ; } public override string tostring ( ) { return $ "" { verb } { object } { good } "" ; } } public class networkvalue { private static int next = 1 ; public string term { get ; set ; } public double value { get ; set ; } public networkvalue(string term ) { term = term ; value = next++ ; } public override string tostring ( ) { return $ "" { value}. { term } "" ; } } } }",20930,,,2019-01-02T05:28:26.070,how do i get a meaningful output value for a simple neural network that can map to a set of data ?,neural-networks,1,1,
2650,9681,1,,2018-12-25T20:30:47.270,1,109,"i am wondering is i can use a 2-dimensions features matrix rather than a feature vector as inout layer of a neural network for a wwii naval wargame , i have sorted out the features of interest to approximate the game state s at a given time t they are maximum of 50 naval task forces and airbases on map each one has a lot of features ( hex , weather , number of ships , type , distance to other task forces , distance to objective , cargo , intelligence , combat value , speed , damage , etc ... ) the output would be the probability of winning and the level of victory",20886,20886,2018-12-26T12:20:49.943,2018-12-26T12:45:21.743,2-dimension input layer ?,neural-networks game-ai,2,1,
2651,9682,1,,2018-12-25T20:32:24.363,1,84,suppose i want to predict the position of a sensor based on its reading . i can first predict the unit vector and predict the distance to be multiplied to this vector . and i know that distance will never be negative because all the negative parts are inside unit vector already . should i apply relu to the distance before multiplying it to the unit vector ? i 'm thinking that this can be helpful to eliminate the network from needing too much training data by restricting the output ranges the network could give . but i also think that it could make the learning slower when the relu unit dies ( value=0 ) so the gradient does n't flow properly somehow .,20819,,,2019-05-30T03:03:50.597,should i apply relu to non negative output ?,neural-networks deep-learning tensorflow keras,1,0,
2652,9686,1,9690,2018-12-26T10:55:25.273,1,27,"imagine i wish to classify images of digits from 0 - 9 . let 's say i have trained the network to recognise ' 1 ' . if i were to train the same network to recognise ' 2 ' , would n't the backpropagation process mess up the weights and biases for ' 1 ' ? or do programs like tensorflow allocate a new layer of neural network for different object classification ? thanks .",20519,,,2018-12-26T16:56:58.943,beginner - object classification data in a neural network,neural-networks classification,1,0,
2653,9689,1,,2018-12-26T12:44:54.080,1,133,"i am a novice developer in ai . any help appropriated . i have a set of images and from that i want to predict position(x , y co - ordinates ) of the ball . thanks in advance .",20953,,,2019-05-30T17:04:10.473,i need to predict ball position from set of images,machine-learning computer-vision tensorflow,2,2,0
2654,9692,1,,2018-12-26T18:37:30.567,1,37,"am working on credit card fraud detection problem using autoencoders . regarding that i have some doubts given below : the dataset for the above problem has been downloaded from kaggle which is highly imbalanced . that is , only 494 frauds are there in the dataset comprising of 2,84,807 transactions . so my doubt is why the dataset is not balanced before applying autoencoder on it . https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd i have read this blog post and my doubt is what is the threshold value for setting a boundary for anomaly detection . are autoencoders used for anomaly detection ?",20842,,,2018-12-26T18:37:30.567,autoencoders for credit card frud detection,neural-networks unsupervised-learning autoencoders,0,0,1
2655,9695,1,,2018-12-26T21:30:32.470,0,15,"in many papers about expert systems so called rules are introduced to the reader . in most papers , the rules are expressed in a certain syntax , because the terms are written in uppercase . for example a rule for an expert system could be : if condition then result . the first assumption in interpreting these expert systems rules might be , that the syntax is equal to what is known from the c programming language . in c , it is possible to write the following statement : if ( number==3 ) { printf(“number is three ” ) ; } but , in the c syntax , the statements are usually written in small cases and no dedicated “ then statement ” is there . the only language which provides the correct syntax is the forth programming language . according to the official tutorial , the syntax is rule1 rule2 and if action then result what is interesting in forth is , that the condition is written before the if statement and that a stack is used . that means , a forth if - then statement has a different kind of logic than a c style sourcecode . but does it make sense , that the papers about expert systems are using the forth syntax ? or is it only a coincidence , and in reality the expert systems in the 1980s were realized with lisp ?",11571,,,2018-12-26T21:30:32.470,"are rulebased expertsystem statements “ if , then , and ” derived from forth ?",history,0,0,
2656,9698,1,,2018-12-26T23:54:25.537,3,80,"each chromosome contains an array of genes , each gene contains a letter and a number , both letter and number can only exist once in each chromosome . parent a = { a,1}{c,2}{e,3}{g,4 } parent b = { a,2}{b,1}{c,4}{d,3 } what would be the best crossover operator to create a child that does n't break the rule described above ?",20962,20962,2018-12-27T00:01:19.410,2019-01-12T03:16:20.773,genetic algorithm : how to crossover 2d permutation ?,genetic-algorithms genetic-programming,2,0,
2657,9700,1,,2018-12-27T05:32:48.897,-3,292,"i am trying to train a rnn with text from wikipedia but i having having trouble getting the rnn to converge . i have tried increasing the batch size but it does n't seem to be helping . all data is one hot encoded before being used and i am using the adam optimizer which is implemented like this . for k in m.keys ( ) : # # for k in weights m[k ] = beta1 * m[k ] + ( 1-beta1)*grad[k ] r[k ] = beta2 * r[k ] + ( 1-beta2)*grad[k]**2 m_k = m[k ] / ( 1-beta1**n ) r_k = r[k ] / ( 1-beta2**n ) model[k ] = model[k ] - alpha * m_k / np.sqrt(r_k + 1e-8 ) beta1 is set to 0.9 , beta2 to 0.999 and alpha is set to 0.001 . when i train it for 50,000 i get very high fluctuation of the cost and it never seems to significantly decrease ( only sometimes due to the fluctuations ( and i catch the weights with the lowest cost)).my hidden_size is 400 and the batch size is 200 after sketching the cost of iterations i get a graph like this : it seems to be increasing on average only seeming to decrease to the the large fluctuations . what can i change to have better success and have it converge ? thanks for any help edit : i plotted the norm of the gradient using the slightly different cost function which @dennissoemers suggested and the gradient does not reduce significantly . sorry the title should read "" gradient over iterations """,20328,20328,2019-01-01T03:31:09.133,2019-01-01T03:31:09.133,rnn lstm not converging with adam,python recurrent-neural-networks optimization convergence,1,6,
2658,9701,1,9724,2018-12-27T09:25:28.060,3,47,"i 'm relatively new to this whole ai thing and have a question .. let 's say i have two different fully trained neural networks . the first one is trained for mathematical addition and the second one on mathematical multiplication and now i want to "" merge these to a cluster "" that knows about both operations . is there a representative name for this kind of technique ? i had read somthing about bilinear cnn models that sounds similar to what i 'm looking for , right ?",20968,,,2018-12-28T10:50:01.920,combining different trained neural networks,neural-networks machine-learning deep-learning,1,2,2
2659,9704,1,9706,2018-12-27T12:10:36.823,3,59,is an artificial intelligence a program ( or a set of programs ) or an hardware ?,15469,2444,2019-05-03T12:33:22.697,2019-05-03T12:33:22.697,is an artificial intelligence a program or an hardware ?,ai-basics definitions,1,1,
2660,9708,1,,2018-12-27T17:26:27.467,4,93,how to choose the dimensions of the encoding layer in autoencoders?please explain what should be the dimensions of the encoding and decoding layers,20842,,,2019-01-27T17:00:45.760,how to choose the dimensions of the encoding layer in autoencoders ?,neural-networks deep-learning autoencoders,1,0,2
2661,9709,1,,2018-12-27T17:30:21.010,3,212,"description i have designed this robot in urdf format and its environment in pybullet . each leg has a minimum and maximum value of movement . what reinforcement algorithm will be best to create a walking policy in a simple environment in which a positive reward will be given if it walks in the positive x - axis direction ? i am working in the following but i don´t know if it is the best way : the expected output from the policy is an array in the range of ( -1 , 1 ) for each joint . the input of the policy is the position of each joint from the past x frames in the environment(replay memory like deepq net ) , the center of mass of the body , the difference in height between the floor and the body to see if it has fallen and the movement in the x - axis . limitations left_front_joint = > lower=""-0.4 "" upper=""2.5 "" id=0 left_front_leg_joint = > lower=""-0.6 "" upper=""0.7 "" id=2 right_front_joint = > lower=""-2.5 "" upper=""0.4 "" id=3 right_front_leg_joint = > lower=""-0.6 "" upper=""0.7 "" id=5 left_back_joint = > lower=""-2.5 "" upper=""0.4 "" id=6 left_back_leg_joint = > lower=""-0.6 "" upper=""0.7 "" id=8 right_back_joint = > lower=""-0.4 "" upper=""2.5 "" id=9 right_back_leg_joint = > lower=""-0.6 "" upper=""0.7 "" id=11 the code below is just a test of the environment with a set of movements hardcoded in the robot just to test how it could walk later . the environment is set to real time , but i assume it needs to be in a frame by frame lapse during the policy training . ( p.setrealtimesimulation(1 ) # disable and p.stepsimulation ( ) # enable ) a video of it can be seen in : https://youtu.be/j9sysg-eikq the complete code can be seen here : https://github.com/rubencg195/walkingspider_openai_pybullet_ros code import pybullet as p import time import pybullet_data def moveleg ( robot = none , id=0 , position=0 , force=1.5 ): if(robot is none ) : return ; p.setjointmotorcontrol2 ( robot , i d , p.position_control , targetposition = position , force = force , # maxvelocity=5 ) pixelwidth = 1000 pixelheight = 1000 camtargetpos = [ 0,0,0 ] camdistance = 0.5 pitch = -10.0 roll=0 upaxisindex = 2 yaw = 0 physicsclient = p.connect(p.gui)#or p.direct for non - graphical version p.setadditionalsearchpath(pybullet_data.getdatapath ( ) ) # optionally p.setgravity(0,0,-10 ) viewmatrix = p.computeviewmatrixfromyawpitchroll(camtargetpos , camdistance , yaw , pitch , roll , upaxisindex ) planeid = p.loadurdf(""plane.urdf "" ) cubestartpos = [ 0,0,0.05 ] cubestartorientation = p.getquaternionfromeuler([0,0,0 ] ) # boxid = p.loadurdf(""r2d2.urdf"",cubestartpos , cubestartorientation ) boxid = p.loadurdf(""src/spider.xml"",cubestartpos , cubestartorientation ) # boxid = p.loadurdf(""spider_simple.urdf"",cubestartpos , cubestartorientation ) toggle = 1 p.setrealtimesimulation(1 ) for i in range ( 10000 ) : # p.stepsimulation ( ) moveleg ( robot = boxid , id=0 , position= toggle * -2 ) # left_front moveleg ( robot = boxid , id=2 , position= toggle * -2 ) # left_front moveleg ( robot = boxid , id=3 , position= toggle * -2 ) # right_front moveleg ( robot = boxid , id=5 , position= toggle * 2 ) # right_front moveleg ( robot = boxid , id=6 , position= toggle * 2 ) # left_back moveleg ( robot = boxid , id=8 , position= toggle * -2 ) # left_back moveleg ( robot = boxid , id=9 , position= toggle * 2 ) # right_back moveleg ( robot = boxid , id=11 , position= toggle * 2 ) # right_back # time.sleep(1./140.)g # time.sleep(0.01 ) time.sleep(1 ) toggle = toggle * -1 # viewmatrix = p.computeviewmatrixfromyawpitchroll(camtargetpos , camdistance , yaw , pitch , roll , upaxisindex ) # projectionmatrix = [ 1.0825318098068237 , 0.0 , 0.0 , 0.0 , 0.0 , 1.732050895690918 , 0.0 , 0.0 , 0.0 , 0.0 , -1.0002000331878662 , -1.0 , 0.0 , 0.0 , -0.020002000033855438 , 0.0 ] # img_arr = p.getcameraimage(pixelwidth , pixelheight , viewmatrix = viewmatrix , projectionmatrix = projectionmatrix , shadow=1,lightdirection=[1,1,1 ] ) cubepos , cubeorn = p.getbasepositionandorientation(boxid ) print(cubepos , cubeorn ) p.disconnect ( )",20979,20979,2018-12-30T03:54:02.900,2019-05-30T00:00:37.090,how to implement a continuous control of a quadruped robot with deep reinforcement learning in pybullet and openai gym ?,deep-learning reinforcement-learning tensorflow unsupervised-learning open-ai,3,0,
2662,9725,1,,2018-12-28T12:53:21.973,4,111,"i 've inherited a neural network project at the company i work for . the person who developed gave me some very basic training to get up and running . i 've maintained it for a while . the current neural network is able to classify messages for telcos : it can send them to support people in different areas , like "" activation "" , "" no signal "" , "" internet "" , etc . the network has been working flawlessly . the structure of this neural network is as follows : model = sequential ( ) model.add(dense(500 , input_shape=(len(train_x[0 ] ) , ) ) ) model.add(activation('relu ' ) ) model.add(dropout(0.6 ) ) model.add(dense(250 , input_shape=(500 , ) ) ) model.add(activation('relu ' ) ) model.add(dropout(0.5 ) ) model.add(dense(len(train_y[0 ] ) ) ) model.add(activation('softmax ' ) ) model.compile(loss='categorical_crossentropy ' , optimizer='adamax ' , metrics=['accuracy ' ] ) this uses a word2vec embedding , and has been trained with a "" clean "" file : all special characters and numbers are removed from both the training file and the input data . now i 've been assigned to make a neural network to detect if a message will be catalog as "" moderated "" ( meaning it 's an insult , spam , or just people commenting on a facebook post ) , or "" operative "" , meaning the message is actually a question for the company . what i did was start from the current model and reduce the number of categories to two . it did n't go very well : the word embedding was in spanish from argentina , and the training data was spanish from peru . i made a new embedding and accuracy increased by a fair margin ( we are looking for insults and other curse words . in spanish a curse word from a country can be a normal word for another : in spain "" coger "" means "" to take "" , and in argentina it means "" to f__k "" . "" concha "" means shell in most countries , but in argentina it means "" c__t "" . you get the idea ) . i trained the network with 300.000 messages . roughly 40 % of these were classified as "" moderated "" . i tried all sorts of combinations of cycles and epochs . the accuracy slowly increased to nearly 0.9 , and loss stays around 0.5000 . but when testing the neural network , "" operative "" messages generally seem to be correctly classified , with accuracy around 0.9 , but "" moderated "" messages are n't . they are classified around 0.6 or less . at some point i tried multiple insults in a message ( even pasting sample data as input data ) , but it did n't seem to improve . word2vec works fantastically . the words are correctly "" lumped "" together ( learned a few insults in peruvian spanish thanks to it ) . i put the neural network in production for a week , to gather statistics . basically 90 % of the messages went unclassified , and 5 % were correctly classified and 5 % wrong . since the network has two categories , this seems to mean the neural network is just giving random guesses . so , the questions are : is it possible to accomplish this task with a neural network ? is the structure of this neural network correct for this task ? are 300k messages enough to train the neural network ? do i need to clean up the data from uppercase , special characters , numbers etc ?",17272,,,2019-01-04T00:04:56.810,"neural network to detect "" spam "" ?",neural-networks feedforward,2,0,1
2663,9731,1,9733,2018-12-29T03:26:38.570,0,23,"for example , if i constructed a neural network and the computer running it where to be demolished , is the information / program of the neural network still an existent entity within or outside the remnants of the hardware ?",21012,,,2018-12-29T07:01:21.887,does software remain even when hardware is demolished ?,hardware,1,0,
2664,9735,1,9771,2018-12-29T07:52:34.703,4,290,"so i googled this but other than some papers i could nt find any reverse engineering tool that was built using machine learning i 'm not an expert in machine learning and deep learning but it seems rational to think that considering we have billions of open source codes out there , we can use them so our "" machine "" can learn how the assembly and executable of these codes look like and just study on them , mastering the art of reversing , and therefore building a tool this way that can reverse any given program with a great accuracy now is this doable or am i missing something here ? is there any tool built this way or on its way to coming out ? what are your thoughts on this ? is better reversing tool even needed or is there already a great reversing tool that can do the job with the best accuracy possible ?",12782,12782,2018-12-29T10:28:13.473,2018-12-31T13:00:34.900,can machine learning be used to develop better reverse engineering / decompilation tools ?,machine-learning deep-learning security,2,3,
2665,9738,1,,2018-12-29T10:33:55.510,1,70,"one - shot learning seems to work really well in many application domains . are there any major ( or even minor ) drawbacks of using one - shot learning ? does it have flaws that could prevent it from being used in certain image identification scenarios ? in this case , i 'm specifically referring to the siamese neural network and memory augmented neural network approaches to one - shot learning .",9432,9432,2018-12-30T01:56:33.103,2018-12-30T01:56:33.103,what are some of the drawbacks of one - shot learning ?,neural-networks machine-learning image-recognition datasets,0,2,
2666,9739,1,9740,2018-12-27T07:50:32.890,1,91,"i have some problems with understanding of the batch concept and batch size . i messed something up . first i start it consider based on convolutional neural network i heard two versions : when the batch size is set to 50 , the first network is fed with 50 images and then learned / recalculated ( it does n't make sense to me , because in this case the network learns one of 50 images ) . when the batch size is set to 50 , one of 50 neurons is recalculated in the learning process on a single image . both of these explanations seem to be wrong to me , so i assume , that i completely do n't understand this . what is batch / batch size in rnn ? could you show any example ? i can tell you how i would teach a recurrent neural network . let 's say , that i would like to teach a neural network to predict the weather the next day : i would take weather data from an expected area from the last 30,000 days . i would assume that my prediction would be based on measurements from the last 365 days . i would take data from day 1 to 365 - feed rnn with it and learn . then i would take data from day 2 to 366 = > feed + learn then day 3 to 367 = > feed + learn and so on . is this 365 measurement concept a batch size ?",21171,16229,2019-05-29T17:51:08.747,2019-05-29T17:51:08.747,what is batch / batch size in neural networks ?,python machine-learning neural-networks,1,0,
2667,9741,1,9742,2018-12-29T16:52:01.060,0,27,"artificial intelligence seems like a modelling program for interfaces of possible processes forming into another possible or greater architectures of further or differing processes . in what capacity is it intelligence and not simply a continuous flux of ever - increasing builds of emergent information that is n't necessarily feasibility in function but rather feasibility of new emergent abstractions ? more clearly , how does artificial intelligence create valid data we can deem as intelligence as oppose to simply new avenues of abstraction that are fixed to our criteria of validity ?",21012,,,2018-12-29T17:45:18.900,is ai truly doing anything that we can consider ' ' intelligent ' ' outside of subjective perception of what we perceive to function ?,philosophy theory,1,4,
2668,9743,1,9748,2018-12-29T18:27:48.933,2,59,"i 've often heard mcts grouped together with neural nets and machine learning . from what i gather , mcts uses a refined intuition ( from maching learning ) to evaluate positions . this allows it to better guess which moves are worth playing out more . but i 've almost never heard of using machine learning for minimax+alpha - beta engines . could n't machine learning be used for the engine to better guess which move is best , and then look at that move 's subtree first ? a major optimization of the minimax algorithm is move - ordering , and this seems like a good way to accomplish that .",16917,,,2018-12-30T01:59:31.923,"why do neural nets and machine learning tend to work well with mcts , but not with regular minimax game - playing ai ?",neural-networks machine-learning game-ai monte-carlo-tree-search minimax,1,0,
2669,9745,1,,2018-12-29T19:16:02.120,1,43,"i have been reading a few papers ( paper1 , paper2 ) on stereo matching using genetic algorithms . i understand how genetic algorithms work in general and how stereo matching works , but i do not understand how genetic algorithms are used in stereo matching . the first paper by han et al says that "" 1 ) individual is a disparity set , 2 ) a chromosome has a 2d structure for handling image signals efficiently , and 3 ) a fitness function is composed of certain constraints which are commonly used in stereo matching "" . does it mean that an individual is a disparity map with random numbers ? then a chromosome is a block within the individual 's disparity map . the constraint used for fitness function could be the famous epipolar line . i do nt seem to understand how this works and even why you should use genetic algorithm on an algorithm that at its simplest form uses 5 for loops , for example , like in here .",14863,,,2019-01-22T03:59:50.173,stereo matching using genetic algorithm,genetic-algorithms,2,1,
2670,9746,1,,2018-12-29T23:34:04.103,2,65,"i am trying to find a good approach to create a computer player for the game "" lines "" from gamious on android . the concept of the game is pretty straightforward : lines is an abstract ‘ zen ’ game experience where form is just as important as function . place or remove dots to initiate a colourful race that fills a drawing . the colour that dominates the race wins . the game starts with a drawing ( that can be described as a set of "" blank "" lines , with connection to other lines ) . dots of different colour are placed somewhat randomly on the lines . the player get a colour assigned . when the game start , paint start flowing from the dots and filling the ( at first blank ) lines of the drawing . you win if your colour dominates . the game gives you different tools to win ( the game starts when all of them have been used ) : [ 0 to 2 ] scissor to cut lines [ 0 to 5 ] additional dot of your own color to place on the drawing [ 0 to 4 ] enemy dots eraser [ 0 to 3 ] additional straight lines to connect different part of the drawing a quick example : the first image is the initial state of a round . "" my "" colour is the yellow ( 1 enemy = brown ) and i have 4 tools ( 2 eraser and 2 lines ) . the second image shows the game running after i used the tools to put my colour in a winning position ( yes , we can do better ) if i try to approach this as a classical optimization problem , things get messy pretty fast : highly non - linear high number of dimensions ai seems to be the right way to go , but i would like your help to get in the right direction : what would be your approach to create an ai to play this game ? to limit the scope of this question , you can consider that i already have a data structure to represent the game initial state , the use of different tools and the game "" physics "" . what i really want to do is finding how to create an ai which can learn how to efficiently use the tools . regarding my experience , i took 2 semesters of ai classes during the last year getting my engineering degree and have used non - linear optimization tools for a while : you can go technical ... but not you - need - a - specialized - ai - degree - to - understand - the - answer technical",21020,21020,2018-12-30T15:59:15.983,2019-05-29T16:04:30.607,"game ai for "" lines "" : help to get started",game-ai,1,7,
2671,9751,1,9753,2018-12-30T06:36:03.300,4,512,"i am trying to understand what channels mean in convolutional neural networks . when working with grayscale and colored images , i understand that the number of channels is set to 1 and 3 ( in the first conv layer ) , respectively , where 3 corresponds to red , green , and blue . say you have a colored image that is 200x200 pixels . the standard is such that the input matrix is a 200x200 matrix with 3 channels . the first convolutional layer would have a filter that is size $ n \times m \times 3 $ , where $ n , m&lt;200 $ ( i think they 're usually set to 3 or 5 ) . would it be possible to structure the input data differently , such that the number of channels now becomes the width or height of the image ? i.e. , the number of channels would be 200 , the input matrix would then be 200x3 or 3x200 . what would be the advantage / disadvantage of this formulation versus the standard ( # of channels = 3 ) ? obviously , this would limit your filter 's spatial size , but dramatically increase it in the depth direction . i am really posing this question because i do n't quite understand the concept of channels in cnns .",20358,,,2019-01-01T07:01:58.827,what is the concept of channels in cnns ?,machine-learning deep-learning convolutional-neural-networks,3,0,1
2672,9756,1,9762,2018-12-30T15:45:51.433,0,51,"i am trying to use a keras lstm neural network for character level language modelling . as the input , i give it the last 50 characters and it has to output the next one . it has 3 layers of 400 neurons each . for the training data , i am using ' war of the worlds ' by h.g . wells which adds up to 269639 training samples and 67410 validation samples . after 7 epochs the validation accuracy has reached 35.1 % and the validation loss has reached 2.31 . however , after being fed the first sentence of war of the worlds to start it outputs : the the the the the the the the the the the the the the the the ... i 'm not sure where i 'm going wrong ; i do n't want it to overfit and output passages straight from the training data but i also do n't want it to just output ' the ' repeatedly . i 'm really at a loss as to what i should do to improve it . any help would be greatly appreciated . thanks !",12376,,,2018-12-31T03:03:56.093,lstm language model not working,neural-networks natural-language-processing recurrent-neural-networks keras lstm,1,0,
2673,9759,1,9769,2018-12-30T22:42:15.047,0,26,"related questions which transition will be most difficult for some who grew up in a culture where driving autonomy is a mechanism of personal expression ? which transition will be most difficult for institutions designed around the existence of enforcement , justice , licensing , and litigation ? possible sequence of transitions reins ( for horses ) & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ steering wheels , accelerators , breaks , and signals & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ add cruise control and anti - skid & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ hybrid ai - manual driving & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ av vehicle lanes on highways & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ av safety stats prove manual driving dangerous & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ removal of steering wheel and pedals & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ av the norm , with diminishing manual lanes and roads & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ elimination of driver 's licenses & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ legislation to prohibit manual driving & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ swarm updated real time map maintenance & nbsp;&nbsp;&nbsp;&nbsp ; ⇓ emergence of community shared automated chauffeuring returning to the main question what socioeconomic points of resistance do autonomous auto designers and manufacturers face ? does the resistance have any real basis , considering the dangers of manual driving , or just the typical yet baseless fear of change ?",4302,4302,2018-12-31T20:15:35.763,2018-12-31T20:15:35.763,what socioeconomic points of resistance do autonomous auto designers and manufacturers face ?,social autonomous-vehicles risk-management,1,0,
2674,9765,1,,2018-12-31T04:58:09.793,1,114,"i was wondering whether there is mathematical evidence or proof of functions that are happening at the backend of deep learning . particularly in training and testing operations . secondly , deep learning is invented many years ago but still very minimal hardware systems to do something real .",21038,1671,2018-12-31T21:24:20.357,2019-01-14T09:42:35.280,if deep learning is blackbox then why companies are still investing ?,neural-networks machine-learning deep-learning data-science real-world,3,1,
2675,9766,1,,2018-12-31T07:27:15.720,3,43,"i 'm reading this really interesting article cyclegan , a master of steganography and i understand everything up until this paragraph : we may view the cyclegan training procedure as continually mounting an adversarial attack on g , by optimizing a generator f to generate adversarial maps that force g to produce a desired image . since we have demonstrated that it is possible to generate these adversarial maps using gradient descent , it is nearly certain that the training procedure is also causing f to generate these adversarial maps . as g is also being optimized , however , g may actually be seen as cooperating in this attack by learning to become increasingly susceptible to attacks . we observe that the magnitude of the difference y ∗ − y 0 necessary to generate a convincing adversarial example by equation 3 decreases as the cyclegan model trains , indicating cooperation of g to support adversarial maps how is the cyclegan training procedure an adversarial attack ? i do n't really understand the quoted explanation .",21040,,,2019-01-03T00:17:04.123,"what is an "" adversarial attack """,getting-started,2,0,1
2676,9768,1,9773,2018-12-31T11:41:28.043,2,25,"i have the following binary classification problem , my labeled dataset contains images 96x96 px . now in every image the interest area is of size 32x32 px in the center of the image , and the images are labeled based on that 32x32 px area . if whatever i am trying to detect is in the outer region of the 32x32 px area the label of that image is not affected . the problem here is that if i use the whole image when traing , my model will not learn that the interest area is only in the center of the image but on the other hand if i crop the images to be of size 32x32 i am loosing a lot of information which can help the model to train on . i found out that i am getting the best results if i crop the images to be of size 64x64 ( kind of a trade of ) . now going to the test set , for the test set it does n't make sense to use this trade of because the model is not learning anything anyways so i would rather crop the test images to 32x32 but then the test set and train set sizes are not the same . has anyone came across this problem before ? can i just pad the test images to the size of the train images ? is this a good way to go ?",7463,,,2018-12-31T15:14:29.930,smaller interest area for images than the size of the image in classification neural networks,convolutional-neural-networks image-recognition,1,0,
2677,9781,1,,2019-01-01T22:07:26.810,-1,24,"i want to create a nhl game predictor and have already trained one neural network on game data . what i would like to do is train another model on player seasonal / game data and combine the two models to archive better accuracy . is this approach feasible ? if it is , how do i go about doing it ? edit : i have currently trained a neural network to classify the probability of the home team winning a game on a dataset that looks like this : h_won / lost h_metric2 h_metric3 h_metric4 a_metric2 a_metric3 a_metric4 h_team1 h_team2 h_team3 h_team4 a_team1 a_team2 a_team3 a_team4 1 10 10 10 10 10 10 1 0 0 0 0 1 0 0 1 10 10 10 10 10 10 1 0 0 0 0 1 0 0 1 10 10 10 10 10 10 1 0 0 0 0 1 0 0 and so on . i am preparing a dataset of player - data for each game that will have the shape of this : player playerid won / lost team opponent metric1 metric2 henke 1 1 ny cap 10 10 hopefully , this new dataset will have some accuracy on if team is going to have some predictive features that are good and recognised . now , say i have these two trained nural networks and they both have an accuracy of 70 % by them self . but i want to combine them both in the hopes to achieve better predictability . how is this archived ? how will the test - dataset be structured ?",21077,21077,2019-01-01T23:22:05.460,2019-01-01T23:22:05.460,is it possible to train several neural networks on different types of data and combine them ?,neural-networks deep-learning data-science,1,0,
2678,9785,1,,2019-01-02T08:56:35.587,3,119,i 'm just curious as to the state of the field since i 'm planning to have it as my a.i grad school specialization . from what i 've observed it peaked in the 90s to the 2000s but is seen as more of a novelty within a.i / ml research today . i 'm aware however that mit press journals and ijalr are still publishing new research . just that i have rarely if not ever heard these topics discussed in a.i / ml conferences .,21083,1671,2019-01-02T21:40:05.117,2019-02-02T14:00:40.100,is artificial life a dying field ?,neural-networks reinforcement-learning getting-started swarm-intelligence artificial-life,1,1,
2679,9786,1,,2019-01-02T08:58:05.600,2,89,i 'm new to data science i 'm currently working on regression problem and i have 10 inputs / attributes . my question is what to do if there are correlations among different features of the input data ? does correlation b / w inputs affect the performance of model ?,21084,4302,2019-01-07T08:10:32.633,2019-01-28T13:39:09.623,does correlation between input affect regression model ?,ai-basics models data-science feature-selection performance,3,3,
2680,9794,1,9801,2019-01-02T20:05:21.397,1,40,i ’m a researcher and i ’m currently conducting a research project . i will conduct a study where i would like to trigger different emotions using chatbots on a smartphone ( e.g. on facebook messenger ) . are there any existing chatbots which are able to trigger different emotions intentionally ( also negative ones ) ?,21103,,,2019-01-02T23:30:47.527,chatbots triggering emotions,intelligent-agent chat-bots emotional-intelligence,1,2,1
2681,9803,1,,2019-01-03T00:17:50.593,-2,40,"hey i am training an initialized neural network with this method public void rlearn(arraylist&lt;tuple&gt ; tupels , double learningrate , double discountfactor ) { mldataset set = new basicmldataset ( ) ; mldataset input = new basicmldataset ( ) ; mldataset ideal = new basicmldataset ( ) ; for(int i = 0 ; i & gt ; tupels.size()-1 ; i++ ) { mldata datain = new basicmldata(45 ) ; mldata dataout = new basicmldata(4 ) ; int index = 0 ; for(double w : tupels.get(i).statefirst.elements ) { datain.add(index++,w ) ; } //added state //add new q values index = 0 ; for(int k = 0 ; k & lt ; tupels.get(i).qactions.elements.length;k++ ) { if(k = = tupels.get(i).actiontaken ) { //new q - value double currentqvalue = tupels.get(i).qactions.getelement(k ) ; double reward = tupels.get(i).rewardafter ; //calculate maximal q value of next state double max = double.min_value ; for(double w : tupels.get(i+1).qactions.elements ) { if(w & gt ; max ) { max = w ; } } dataout.add(index++,currentqvalue + learningrate*(reward + discountfactor*max - currentqvalue ) ) ; } else { dataout.add(index++ , tupels.get(i).qactions.getelement(k ) ) ; } } set.add(datain,dataout ) ; } system.out.println(""training data : "" + set.size ( ) ) ; if(set.size ( ) ! = 0 ) { backpropagation prop = new backpropagation(nn , set ) ; prop.setlearningrate(0.1 ) ; prop.iteration(10 ) ; system.out.println(""training done : "" + prop.geterror ( ) ) ; } } unfortunately this does not work pretty well . the error is converging to zero ( pretty fast from 10000 ) , but the neural net does not seem to have learned something ( it is big enough ) the actual goal is to create a nn which can play something similar like astroids . therefore the goal is to survive as long as possible . after every frame the nn gets +1 point , if it dies -100 ; edit : the game looks like https://youtu.be/qxgr2bgj8vy ( i coded it ) but the ai can only go up , down , left and right . furthermore , the ai can only steer the player every 1/3 second ...",19062,19062,2019-01-03T09:22:05.377,2019-01-03T09:22:05.377,q - learning algorithmus does not work,deep-learning q-learning,1,0,
2682,9805,1,,2019-01-03T06:24:54.853,1,14,there are some happenings in human swarm artificial intelligence and some schools k-12 running without teachers and the kids studying anything on their own . what would be a possible approach for human swarm ai toward classroom management ?,21111,,,2019-01-03T06:24:54.853,what would be a possible approach for swarm ai toward democratic education ?,artificial-consciousness swarm-intelligence,0,1,
2683,9808,1,,2019-01-03T14:20:32.507,4,380,"i understand the minimax algorithm , but i am unable to understand deeply the minimax algorithm with alpha - beta pruning , even after having looked up several sources ( on the web ) and having tried to read the algorithm and understand how it works . do you have a good source that explains alpha - beta pruning clearly , or can you help me to understand the alpha - beta pruning ( with a simple explanation ) ?",21125,2444,2019-02-26T09:40:09.050,2019-02-26T09:40:09.050,can someone help me to understand the alpha - beta pruning algorithm ?,search minimax alpha-beta-pruning,3,2,1
2684,9809,1,,2019-01-03T15:27:33.400,1,72,what are the steps involved to create an ai agent which can do the following can learn from text in digital format from a book gain knowledge from the digital text input can answer question from the book fed to it,10955,,,2019-02-03T11:08:11.033,where to start to develop an ai model which can learn from a book ?,ai-design knowledge-representation,2,2,2
2685,9812,1,9830,2019-01-03T16:42:59.147,1,78,"going through the dqn paper , it said the state - space is high dimensional . i am a little bit confused here . suppose my state is a high dimensional vector of n length where n is a huge number . let 's say i solve this task using q - learning and i fix my state space to 10 vectors each of n dimensions . q - learning can easily work with these settings as we need only a table of dimensions 10 x number of actions . let 's say my state space can have an infinite number of vectors each of n dimensions . in these settings q - learning would fail as we can not store q - values in a table for each of these infinite vectors . meanwhile on the other hand dqn would easily work as neural networks can generalize for any vector in the state - space . let 's also say i have a state space of infinite vectors but each vector is now of length 2 i.e. very small dimensional vectors . would it make sense to use dqn in these settings ? should this state - space be called high dimensional or low dimensional ?",21131,,,2019-01-04T16:20:14.967,what does it mean by high dimensional state in dqn ?,deep-learning reinforcement-learning dqn,2,0,
2686,9813,1,,2019-01-03T17:45:03.813,2,83,"this is a question related to neural network to detect & quot;spam&quot ; ? . i 'm wondering how it would be possible to handle the emotion conveyed in text . in informal writing , especially among a juvenile audience , it 's usual to find emotion expressed as repetition of characters . for example , "" hi "" does n't mean the same as "" hiiiiiiiiiiiiiii "" but "" hiiiiii "" , "" hiiiiiiiii "" , and "" hiiiiiiiiii "" do . a naive solution would be to preprocess the input and remove the repeating characters after a certain threshold , say , 4 . this would probably reduce most long "" hiiiii "" to 4 "" hiiii "" , giving a separate meaning ( weight in a context ? ) to "" hi "" vs "" long hi "" . the naivete of this solution appears when there are combinations . for example , haha vs hahahahaha or lol vs lololololol . again , we could write a regex to reduce lolol[ol]+ to lolol . but then we run into the issue of hahahaahhaaha where a typo broke the sequence . there is also the whole issue of emoji . emoji may seem daunting at first since they are special characters . but once understood , emoji may actually become helpful in this situation . for example , may mean a very different thing than , but may mean the same as and . the trick with emojis , to me , is that they might actually be easier to parse . simply add spaces between to convert to in the text analysis . i would guess that repetition would play a role in training , but unlike "" hi "" , and "" hiiii "" , word2vec wo n't try to categorize and as different words ( as i 've now forced to be separate words , relying in frequency to detect the emotion of the phrase ) . even more , this would help the detection of "" playful "" language such as , where the emoji might imply there is anger , but alongside and especially when repeating multiple times , it would be easier for a neural network to understand that the person is n't really angry . does any of this make sense or i 'm going in the wrong direction ?",17272,,,2019-02-03T00:01:10.200,handling emotion in informal text ( hi vs hiiiiii ! ! ! ! ) ?,neural-networks machine-learning natural-language-processing natural-language,1,1,
2687,9814,1,9866,2019-01-03T17:48:14.577,4,188,"on google scholar the term fuzzy logic results into a list of 1 million papers , which is a lot . the subproblem of “ fuzzy control ” has around 238000 . if we are assuming , that the total number of papers in academia is only 50 million , than the term fuzzy logic occupies 2 % of the published information . but how important is the topic for artificial intelligence and robotics ? as far as i know , there is a hype around deeplearning . the topic is pushed forward and many lectures are held at universities about machine learning , neural network and stochastic automaton . in contrast , the topic fuzzy control is not covered very well . it sounds a bit esoteric . we have on the one hand a huge number of published papers about the subject , while on the other hand nobody is using fuzzy control in reality . or at least , it seems so . a closer look into the papers shows , that fuzzy logic is everywhere : from simple line follower robots , over starcraft playing ai engines up to fuzzy nanoparticles which are cruising in the human body . my question is : is fuzzy logic something which can be ignored because other topics like neural networks are more important or does it make sense to focus on fuzzy control and support the idea of describing a system with linguistic variables ? a similar question was asked by why did fuzzy logic fall out of fashion ? two months ago . but it does n't contains numbers about published articles .",11571,,,2019-02-06T08:02:40.627,how important is fuzzy logic for artificial intelligence ?,fuzzy-logic,1,0,1
2688,9816,1,,2019-01-03T20:09:55.593,0,62,"i am trying to implement a deep q network to play asteroids . unfortunately , i am not sure how to calculate the q value exactly , if i am exploring . for example , the agent is exploring for 1 second ( otherwise makes no sense ; i can not let it just explore one step ) . unfortunately , it makes a mistake at 0.99s , and the reward collapses . at the moment , i am using the following formula to evaluate or update the q value : $ $ q_{new , t } = reward + \gamma q_{max , t+1}$$ but how do i know the max q value of the next step ? i could consider the best q value the network says , but this is not necessarily true . you can see the current implementation at the following url : https://github.com/suchtytv/rlearningbird/blob/master/src/main/java/rlgame/brain.java .",19062,2444,2019-02-16T00:14:25.797,2019-02-16T00:14:25.797,how do i update the q values of a deep q network when exploring ?,reinforcement-learning game-ai q-learning deep-rl,1,0,
2689,9817,1,,2019-01-03T22:48:45.190,1,15,"below is a taxonomy of neurons . some of these types occur in different locations in the brain , but there are adjacent neurons of varying types with clearly functional type diversity in many parts of the brain , so the idea of having a layer of homogeneous cells in artificial networks may be limiting . is there any reason why that is definitely the case or definitely not the case ? since there are different types of neurons in adjacent positions in the brain 's arrays , should heterogeneous layers be developed , mathematically , experimentally , and in apis designed to provide ai components for applications ? references brain - wide analysis of electrophysiological diversity yields novel categorization of mammalian neuron types , 2015 , tripathy et . al .",4302,,,2019-01-03T22:48:45.190,"since there are different types of neurons in adjacent positions in the brain 's arrays , should heterogeneous layers be developed ?",ai-design math artificial-neuron neurons neuromorphic-engineering,0,0,
2690,9819,1,,2019-01-03T23:19:08.443,1,139,"background my understanding is the input neurons seem to seem to compute a weighted sum moving from one layer to another . $ $ \sum_i a_i w_i = a'_{k } $ $ but to compute this weighted sum the sum must be discrete . is there any known method to compute the sum when the activation is a continuous function ? is the below formula of an any problems in artificial intelligence ? can anyone give a specific problem where it might be useful ? my method $ $ \lim_{k \to \infty } \lim_{n \to \infty}\ \sum_{r=1}^n d_r \left ( f(\frac{k}{n}r)\frac{k}{n } \right ) = \lim_{s \to 1 } \ ! \underbrace{\frac{1}{\zeta(s ) } \sum_{r=1}^\infty \frac{d_r}{r^s}}_{\text{removable singularity } } \int_0^\infty f(x ) \ , dx $ $ i will not go into the proof of this over but for those who are interested : https://math.stackexchange.com/questions/2888976/a-rough-proof-for-infinitesimals i will merely state what the formula means : consider we have a curve $ f(x)$ now if one wishes to perform a weighted sum in the limiting case of this function . consider the curve $ f(x)$ . then splitting it to $ k / n= h$ intervals then adding the first strip ( $ d_1 $ times ) : $ f(h ) \cdot d_1 $ . then the second strip ( $ d_2 $ times ) $ f(2h ) \cdot d_2 $ times ... and so on . hence . $ d_r$ can be thought of as the weight at $ f(rh)$ . a sample example of $ f(x)$ that should work is $ f(x ) = e^{-x}$ disclaimer i am not familiar with this field and am merely a physicist in training . i recently watched 3 blue 1 browns video of artificial intelligence https://www.youtube.com/watch?v=aircaruvnkk and realised a formula i had constructed for fun might be of relevance ( ? ) .",21136,,,2019-01-04T16:45:50.137,would this formula be relevant to the field of a.i ?,artificial-neuron activation-function,1,0,1
2691,9824,1,,2019-01-04T09:28:17.163,-1,49,"i am looking for an non - ml method for two chat bots to communicate to each other about a specific topic . i am looking for an "" explainable ai "" method , as opposed to a "" black - box "" one ( like a neural network ) .",20378,2444,2019-05-01T17:03:59.017,2019-05-01T17:03:59.017,how do i create chatbots without machine learning ?,natural-language-processing chat-bots,1,0,
2692,9828,1,9942,2019-01-04T13:39:22.473,9,105,"there are several activation functions , such as relu , sigmoid or . what happens when i mix activation functions ? i recently found that google has developed swish activation function which is ( x*sigmoid ) . by altering activation function can it increase accuracy on small neural network problem such as xor problem ?",21143,2444,2019-05-15T15:16:53.050,2019-05-15T15:16:53.050,what happens when i mix activation functions ?,neural-networks machine-learning activation-function relu sigmoid,1,0,1
2693,9829,1,9843,2019-01-04T14:11:38.477,2,125,"i am not sure if i understood the q learning algorithms correctly . therefore i would give a concrete example and ask if someone can tell me how to update the q value correctly . first i initialized a neural network with random weights . it shall henceforth evaluate the q value for all possible actions(4 ) given a state s. then the following happens . the agent is playing and is exploring . for 3 steps the q values evaluated were : ( 0,-1,-5,0 ) , ( 0,-1,0,0 ) , ( 0,-.6,0,0 ) the reward given was : 0,0,1 the action took were : ( 1.,1.,1 . ) in the random walk example ( same reward given ) , it was : ( 1.,2.,3 . ) so what are the new q - values , assuming a discount factor of 0.99 and the learning rate 0.1 ? the states for simplicity are only one number : 1,1.3,2.4 where 2.4 is the state who ends the game ... the same example holds for exploiting . is the algorithm the same here ? here you see my last implementation : public void rlearn(arraylist&lt;tuple&gt ; tupels , double learningrate , double discountfactor ) { //newq = sum of all rewards you have got through for(int i = tupels.size()-1 ; i & gt ; 0 ; i-- ) { mldata in = new basicmldata(45 ) ; mldata out = new basicmldata(5 ) ; //add state as in int index = 0 ; for(double w : tupels.get(i).statefirst.elements ) { in.add(index++ , w ) ; } //now start updating q - values double qnew = 0 ; if(i & lt;= tupels.size()-2 ) { qnew = tupels.get(i).rewardafter + discountfactor*qmax(tupels.get(i+1 ) ) ; } else { qnew = tupels.get(i).rewardafter ; } tupels.get(i).qactions.elements[tupels.get(i).actiontaken ] = qnew ; //add q values as out index = 0 ; for(double w : tupels.get(i).qactions.elements ) { out.add(index++ , w ) ; } bigset.add(in , out ) ; } } edit : this is the qmax - function : private double qmax(tuple tuple ) { double max = double.min_value ; for(double w : tuple.qactions.elements ) { if(w & gt ; max ) { max = w ; } } return max ; }",19062,19062,2019-01-05T10:58:07.197,2019-01-07T21:44:52.540,concrete example for q learning,deep-learning reinforcement-learning q-learning,2,9,1
2694,9834,1,,2019-01-04T18:46:31.110,4,68,what are the strengths of the hierarchical temporal memory model compared to competing models such as ' traditional ' neural networks as used in deep learning ? and for those strengths are there other available models that are n't as bogged down by patents ?,21155,21155,2019-01-04T18:53:57.043,2019-03-30T00:56:55.470,what are the strengths of the hierarchical temporal memory model compared to competing models ?,neural-networks htm,1,0,2
2695,9838,1,,2019-01-04T23:45:15.573,3,270,"after reading an excellent blog post deep reinforcement learning : pong from pixels and playing with the code a little , i 've tried to do something simple : use the same code to train a logical xor gate . but no matter how i 've tuned hyperparameters , the reinforced version does not converge ( gets stuck around -10 ) . what am i doing it wrong ? is n't it possible to use policy gradients , in this case , for some reason ? the setup is simple : 3 inputs ( 1 for bias=1 , x , and y ) , 3 neurons in the hidden layer and 1 output . the game is passing all 4 combinations of x , y to the rnn step - by - step , and after 4 steps giving a reward of +1 if all 4 answers were correct , and -1 if at least one was wrong . the episode is 20 games the code ( forked from original and with minimal modifications ) is here : https://gist.github.com/dimagog/de9d2b2489f377eba6aa8da141f09bc2 p.s . almost the same code trains xor gate with supervised learning in no time ( 2 sec ) .",20941,2444,2019-02-15T20:48:47.603,2019-02-15T20:48:47.603,how to train a logical xor with reinforcement learning ?,reinforcement-learning,3,2,3
2696,9842,1,,2019-01-05T04:51:08.610,1,15,"i 'm learning machine learning by looking through other people 's kernel on kaggle , specifically this mushroom classification kernel . the author first applyed pca to the transformed indicator matrix . he only used 2 principal components for visualization later . then i checked how much variance it has maintained , and found out that only 16 % variance is maintained . in [ 18 ] : pca.explained_variance_ratio_.cumsum ( ) out[18 ] : array([0.09412961 , 0.16600686 ] ) but the test result with 90 % accuracy suggests it works well . my question is if variance stands for information , then how can ml model works well when so - much information has lost ?",21166,,,2019-02-04T11:03:25.690,why pca works well while the total variance retained is small ?,machine-learning,1,0,
2697,9848,1,9851,2019-01-05T10:56:45.170,0,77,this might sound dumb but i kept scratching my head for long and could n't understand the non linearity concept . let 's say i have a 2 x 2 pixel of grayscale picture where there is one edge such that the left pixel contains a value 30 and right pixels contain a value 0 . and for edge detection i have padded the input image and then used the sobel vertical filter to find out the vertical edges and apply relu to the output . the output is a 2 x 2 matrix with all pixel values 0 . so that should mean there is no edge in the picture whereas in actual case it has one . where am i going wrong ?,21172,,,2019-01-05T12:51:03.460,understanding cnn || step - > convolution followed by relu,convolutional-neural-networks,1,0,
2698,9849,1,,2019-01-05T11:08:11.157,1,46,"consider the following loss function $ $ l(\mathbf{w } ) = [ ( r + \gamma max_{a ' } q(s ' , a ' , \mathbf{w^- } ) ) - q(s , a , \mathbf{w})]^2 $ $ where $ q(s , a , \mathbf{w^-})$ and $ q(s , a , \mathbf{w})$ are represented as neural networks , where $ w^-$ and $ w$ are the corresponding weights . but how do you calculate $ max_{a ' } q(s ' , a ' , \mathbf{w^-})$ ? do you really need to hold always an older version of the network ? if yes , why and how old should it be ?",19062,2444,2019-02-15T20:21:10.120,2019-02-15T20:21:10.120,"how do i calculate $ max_{a′}q(s′,a′,w−)$ when it is represented as a neural network ?",reinforcement-learning q-learning loss-functions deep-rl,1,0,
2699,9854,1,9865,2019-01-05T21:24:54.670,0,112,consider this game state : d5 captures c6 quiescence search returns about 8.0 as evaluation because after dxc6 and bxc6 qxd6 would be played ( then qxd6 by black ) . a normal player would not play this move but quiescence search includes it in the evaluation and it would result in this end state : which would result in a huge advantage for black . is my interpretation of quiescence search wrong ?,19783,,,2019-01-07T01:54:47.067,does quiescence search even improve the minimax algorithm ?,minimax chess,1,0,
2700,9860,1,,2019-01-06T17:12:32.703,1,57,why is the actor - critic algorithm limited to using on - policy data ? or can we use the actor - critic algorithm with off - policy data ?,21180,2444,2019-02-15T19:38:43.640,2019-02-15T19:43:04.453,why is the actor - critic algorithm limited to using on - policy data ?,reinforcement-learning actor-critic on-policy off-policy,1,0,
2701,9862,1,9872,2019-01-06T21:54:03.970,2,65,when using rectified linear unit after convolution layers we have to have twice as much filters to be able to detect features ( eg both left and right edge detector ) . why do we just throw out negative output of the unit ? as i understand relu should have two outputs - positive and negative,21203,,,2019-01-07T13:13:09.497,why do we throw out negative relu value ?,convolutional-neural-networks relu,2,0,
2702,9870,1,9891,2019-01-07T11:54:32.547,0,47,"i 'm trying to understand exactly what does a convnet do to what , and i have trouble finding the dimensions alongside the convolutions . if we take vgg 16 architecture , how do i get from 224x224x3 to 112x112x64 ? ( the 112 is understandable , it 's the last part i do n't get ) i thought the cnn was to apply filters / convolutions to layers ( for instance , 10 different filters to channel red , 10 to green ... : are they the same filters between channels ? ) , but obviously 64 is not divisible by 3 . and then , how do we get from 64 to 128 ? do we apply new filters to outputs of previous filters ? ( in this case we only have 2 filters applied to previous outputs ) or is it something different ?",19094,,,2019-01-08T12:15:37.960,dimension after multiple convolutions in convnets,neural-networks deep-learning convolutional-neural-networks,3,0,
2703,9871,1,,2019-01-07T11:59:43.977,1,13,i have a time series data where i use a sliding window to detect anomalies in those windows . a sliding window is an interval of the dataset that steps one datapoint for each iteration . datapoints are seen multiple times in this way equal to the size of the window . in short the algorithm works like this : choose window length : wl learn normal data with sliding window try to detect anomalies on test data with sliding window i want to keep the sliding window method since it is nessecary for the performance of the algorithm . however one anomaly occurs multiple times in the sliding window . when the anomaly appears in the sliding window for the first time it 's on the ' right ' side of the window . how do we measure accuracy of anomaly detection in this case ? we could say that detecting the anomaly once in the window is enough or detect it wl times . what 's best practice ?,21223,,,2019-01-07T11:59:43.977,performance measure on windowed time series data,deep-learning performance,0,0,
2704,9873,1,,2019-01-07T15:36:01.530,1,9,"during reading some papers about fuzzy systems , i 've recognized a subtopic called “ fuzzy commands ” . the idea is to provide a sentence in natural language , like “ move ball to right ” , and the fuzzy reinforcement learning algorithm will determine by it 's own how to fulfill the task . that means the fuzzy rules are adapted , they are learned . from other papers about reinforcement learning it is known , that such systems are able to adapt to new problems . that means , the same reinforcement learning algorithm can be used to solve the cart - balancing problem as well as the pong game or the inverted pendulum problem . this is usually done with policy iteration . the policy is stored in a q - table and describes state - action values or it is described by a markov chain which is probabilistic transition diagram . if this is combined with natural language what is the result ? is a fuzzy command reinforcement learning system equal to a general game playing agent ? or has such a system some kind of disadvantages , that means it fails to solve more complex problems ?",11571,,,2019-01-07T15:36:01.530,are fuzzy commands resulting into a general game playing architecture ?,game-ai fuzzy-logic,0,0,
2705,9880,1,,2019-01-07T23:25:30.187,3,29,"i am a strategy consultant , deeply interested in edge computing and distributed and decentralized systems . in performing some analysis on current edge offerings , i am curious as to how ml and ai tools ( like tensorflow and deeplearning4j ) will integrate with aws greengrass and azure edge . say i want to deploy my iiot solution on azure , can i use azure ml solutions and sit tensorflow on top of it to coordinate the logic between programs in my system ? am i incorrect in my thought that tensorflow , pytorch , onnx , deeplearning4j , and some others sit on top of native ml functions ? cheers , jack !",21239,4302,2019-01-08T08:13:56.103,2019-01-08T09:20:11.217,artificial intelligence services at the edge,ai-design tensorflow distributed-computing integration iiot,1,1,1
2706,9887,1,9888,2019-01-08T10:43:46.457,1,74,"which algorithms , between ant colony or classical routing algorithms , have a better time complexity for the shortest path problem ? in general , can we compare efficiency of these two types of algorithm for the shortest path problem in a graph ?",19910,2444,2019-05-27T22:06:06.400,2019-05-27T22:06:06.400,"which algorithms , between ant colony or classical routing algorithms , have a better time complexity for the shortest path problem ?",algorithm swarm-intelligence time-complexity ant-colony,1,0,
2707,9889,1,,2019-01-08T11:44:04.017,3,209,ai should be perceiving - i think . there are debates around the world about the intelligence of an super - intelligent ai . i 'm curious whether ai can have sentience or not .,21179,1671,2019-01-08T18:16:23.190,2019-01-18T08:43:54.213,"if ai can perceive , can it be sentient ?",philosophy terminology theory definitions sentience,3,3,3
2708,9890,1,,2019-01-08T11:57:24.937,1,79,"on recommendation of kanak on stackoverflow i am posting this question here : currently i am experimenting with various loss functions and optimizers for my binary image segmentation problem . the loss functions that i use in my unet however give different output segmentation maps . i have a highly imbalanced dataset , thus i am trying dice loss for which the customized function is given below . def dice_coef(y_true , y_pred , smooth=1 ) : "" "" "" dice = ( 2*|x & amp ; y|)/ ( |x|+ |y| ) = 2*sum(|a*b|)/(sum(a^2)+sum(b^2 ) ) ref : https://arxiv.org/pdf/1606.04797v1.pdf "" "" "" intersection = k.sum(k.abs(y_true * y_pred ) , axis=-1 ) return ( 2 . * intersection + smooth ) / ( k.sum(k.square(y_true ) , -1 ) + k.sum(k.square(y_pred ) , -1 ) + smooth ) def dice_coef_loss(y_true , y_pred ) : return 1 - dice_coef(y_true , y_pred ) binary cross entropy results in a probability output map , where each pixel has a color intensity that represents the chance of that pixel being the positive or negative class . however , when i use the dice loss function , the output is not a probability map but the pixels are classed as either 0 or 1 . my questions are : 1.how is it possible that these different loss functions have these vastly different results ? is there a way to customize the dice loss function so that the output segmentation map is a probability map similar to the one of binary crossentropy loss .",21257,,,2019-01-08T11:57:24.937,dice loss gives binary output whereas binary crossentropy produces probability output map,deep-learning convolutional-neural-networks keras loss-functions,0,0,
2709,9895,1,,2019-01-08T16:48:11.920,2,76,"i have this question in my head : does the current level of ai development allow us to spot faked or photoshoped images ? ( i.e forged i d card or personal documents ) . if it is possible , what is such a process to follow in order to build an ai that achieves this task ?",19059,19059,2019-01-24T10:25:05.293,2019-04-08T07:46:32.920,is it possible to spot photoshoped or edited photos using ai ?,deep-learning image-recognition deepfakes,1,0,
2710,9896,1,,2019-01-08T18:49:38.193,0,60,are there any projects where you can detect and measure the width of a crack ? i am using tensorflow and labeling the data sets for now .,21272,1671,2019-01-12T22:59:33.170,2019-01-12T22:59:33.170,measuring width of crack,neural-networks convolutional-neural-networks image-recognition,1,2,
2711,9897,1,,2019-01-08T22:27:57.413,4,121,"is "" emotion "" ever used in ai ? psychologists have a lot to say about emotion and it 's functional utility for survival - but i 've never seen any ai research that uses something resembling "" emotion "" inside an algorithm . ( yes , there 's some work done on trying to classify human emotions , called "" emotional intelligence "" , but that 's extrememly different from /using/ emotions within an algorithm ) for example , you could imagine that a robot might need fuel and be "" very thirsty "" - causing it to prioritize different tasks ( seeking fuel ) . emotions also sometimes do n't just focus on objectives / priorities - but categorize how much certain classifications are "" projected "" into a particular emotions . for example , maybe a robot that needs fuel might be very "" afraid "" of going towards cars because it 's been hit in the past - while it might be "" frustrated "" at a container that does n't open properly . it seems very natural that these things are helpful for survival - and they are likely "" hardcoded "" in our genes ( since some emotions - like sexual attraction - seem to be mostly unchangeable by "" nurture "" ) - so i would think they would have a lot of general utility in ai .",20685,,,2019-01-09T09:54:53.317,"is "" emotion "" ever used in ai ?",ai-design,2,4,1
2712,9903,1,9923,2019-01-09T08:59:44.857,2,77,"i 'm new to machine learning , and ai in general ( but with 20 + years for programming ) . i 'm wondering if machine learning is a good general approach to find the seed of a random number generator . suppose i have a list of 2000 numbers . is there a machine learning algorithm to correctly guess the next number ? just to be clear , as there are many random number generator algorithms . i 'm taking about rand and srand from the stdlib . thanks , eden",21284,21240,2019-01-10T18:01:52.747,2019-01-10T18:01:52.747,finding the seed of a random number,machine-learning,1,0,1
2713,9904,1,,2019-01-09T09:12:34.027,1,43,"i 'm reading the book "" reinforcement learning : an introduction "" ( by andrew barto and richard s. sutton ) . the authors provide the pseudocode of the prioritized sweeping algorithm , but i do not know what is the meaning of model(s , a ) . does it mean that model(s , a ) is the history of rewards gained when we are in state s and the action a is taken ? does r , s_new = model(s , a ) mean that we should take a random sample from rewards gained in state s and action a is taken ?",10191,2444,2019-02-15T19:22:50.727,2019-02-15T19:22:50.727,"what is the meaning of model(s , a ) in the prioritized sweeping algorithm ?",reinforcement-learning rl-an-introduction prioritized-sweeping,1,0,
2714,9905,1,10206,2019-01-09T10:37:14.890,3,111,"in my view intelligence begins once the thoughts / actions are logical rather than purely randomn based . the learning environments can be random but the logic seems to obey some elusive rules . there is also the aspect of a parenting that guides through some really bad decisions by using the collective knowledge . all of this seems to hint that intelligence needs intelligence to coexist and a sharing communication network for validation / rejection . personally i believe that we must keep the human intelligence in a parental role for long enough time until at least the ai had fully assimilated our values . the actual danger is to leave the artificial intelligence parenting another ai and loose control of it . this step is not necessary from our perspective but can we resist the temptation and try it eventually , only time will tell . above all we must remember the purpose of ai . i think the purpose should always be to help humans achieve mastery of the environment while ensuring our collective preservation . ai should not be left unsupervised as we would not give guns to kids , do we ? to resume it all ai needs an environment and supervision where to learn and grow . the environment can vary but the supervision must stay in place . are initiated thoughts / actions by the means of guidance and supervision considered random ? lastly i believe that the sensible think to do is to only develop artificial intelligence that is limited by our own beliefs and values rather than searching for something greater than us . it seems not possible to create greater than our intelligence without letting it go exploring ! exploring has greater access to random actions and can go against the intended purpose .",21285,,,2019-01-25T22:17:46.640,is learning possible without random thoughts and actions ?,ai-design control-problem random-variable,1,2,1
2715,9908,1,9915,2019-01-09T11:11:51.937,3,101,when trying to map artificial neuronal models to biological facts it was not possible to find an answer regarding the biological justification of randomly initializing the weights . perhaps this is not yet known from our current understanding of biological neurons ?,21269,,,2019-01-10T08:00:31.110,how do biological neurons weights get initialized ?,machine-learning training artificial-neuron neurons biology,2,0,5
2716,9909,1,,2019-01-09T12:25:22.140,1,205,"i 'm doing a research on a finite - horizon markov decision process with $ t=1 , \dots , 40 $ periods . in every time step $ t$ , the ( only ) agent has to chose an action $ a(t ) \in a(t)$ , while the agent is in state $ s(t ) \in s(t)$ . the chosen action $ a(t)$ in state $ s(t)$ affects the transition to the following state $ s(t+1)$ . in my case , the following holds true : $ a(t)=a$ and $ s(t)=s$ , while the size of $ a$ is $ 6 000 000 $ ( 6 million ) and the size of $ s$ is $ 10 ^ 8 $ . furthermore , the transition function is stochastic . would monte carlo tree search ( mcts ) an appropriate method for my problem ( in particular due to the large size of $ a$ and $ s$ and the stochastic transition function ? ) i have already read a lot of papers about mcts ( e.g. progressive widening and double progressive widening , which sound quite promising ) , but maybe someone can tell me about his experiences applying mcts to similar problems or about appropriate methods for this problem ( with large state / action space and a stochastic transition function ) .",21287,2444,2019-02-15T19:34:00.913,2019-02-15T19:34:00.913,is monte carlo tree search appropriate for problems with large state and action spaces ?,reinforcement-learning monte-carlo-tree-search markov-decision-process finite-markov-decision-process,2,3,0
2717,9912,1,9913,2019-01-09T17:28:06.877,3,115,"in the book "" reinforcement learning : an introduction "" ( 2018 ) sutton and barto define at page 102 the importance - sampling - ration as follows : $ $ for a target policy and a behaviour policy $ b$ . one page before however they state : "" the target policy [ ... ] may be deterministic [ ... ] "" . when is deterministic and greedy it gives 1 for the greedy action and 0 for all other possible actions . so how can the above formular give something else than zero , except for the case where policy $ b$ takes a path that would have taken as well ? because if any selected action of $ b$ is different from 's choice than the whole numerator is zero and thus the whole result .",21299,2444,2019-02-11T21:22:25.870,2019-02-11T21:22:25.870,how can the importance - sampling ratio be different than zero ?,reinforcement-learning rl-an-introduction,1,0,2
2718,9914,1,9916,2019-01-10T03:38:42.657,1,111,"in deep learning by goodfellow et al . , i came across the following line on the chapter on stochastic gradient descent ( pg . 287 ) : the main question is how to set . if it is too large , the learning curve will show violent oscillations , with the cost function often increasing significantly . i 'm slightly confused why the loss function would increase at all . my understanding of gradient descent is that given parameters and a loss function , the gradient update is performed as follows : $ $ the loss function is guaranteed to monotonically decrease because the parameters are updated in the negative direction of the gradient . i would assume the same holds for sgd , but clearly it does n't . with a high learning rate , how would the loss function increase in its value ? is my interpretation incorrect or does sgd have different theoretical guarantees than vanilla gradient descent ?",19403,,,2019-01-10T05:47:59.047,cost function increasing with sgd,neural-networks machine-learning deep-learning gradient-descent,1,0,
2719,9918,1,9928,2019-01-10T08:29:05.757,0,61,"i have non - smooth loss function - e.g. loss(x)=min(x , 0.5 ) . can gradient descent be used for training neural networks with such functions . can gradient descent be used for fairly general , mathematically not - nice functions ? pytorch or tensorflow can calculate numerically gradients from almost any function , but it is acceptable practice to use general , not - nice loss functions ?",8332,,,2019-01-10T17:37:36.110,can gradient descent training be used for nonsmooth loss functions ?,neural-networks training gradient-descent policy-gradients,1,0,
2720,9919,1,10789,2019-01-10T10:01:16.663,2,183,"in the paper markov games as a framework for multi - agent reinforcement learning ( which introduces the minimax q learning algorithm ) , at the bottom left of page 3 , my understanding is that the author suggests , for a simultaneous 1v1 zero - sum game , to do bellman iterations with $ $ v(s)=\min_{o}\sum_{a}\pi_{a}q(s , a , o)$$ with the probability of playing action $ a$ for the maximizing player in his best mixed strategy to play in state $ s$ . if my understanding is correct , why does the opponent in this equation play a pure strategy ( ) rather than his best mixed strategy in state $ s$ . this would instead give $ $ v(s)=\sum_{o}\sum_{a}\pi_{a}\pi_{o}q(s , a , o)$$ with the opponent 's best mixed strategy in state $ s$ . which of these two formulations is correct and why ? are they somehow equivalent ? the context of this question is that i am trying to use minimax q learning with a neural network outputting the matrix $ q(s , a , o)$ for a simultaneous zero - sum game . i have tried both methods and so far have seen seemingly equally bad results , quite possibly due to bugs or other errors in my method .",21311,2444,2019-02-21T14:43:36.137,2019-02-21T14:43:36.137,using the opponent 's mixed strategy in estimating the state value in minimax q learning,reinforcement-learning q-learning minimax,1,0,
2721,9921,1,9926,2019-01-10T12:45:33.193,2,57,"the spectrum of human sensory inputs seems to fall within certain ranges suggesting normalization is built - in into biological nns ? it also adapts to circumstantial conditions , e.g. people living in a city with certain factory smell eventually do n't perceive the smell anymore , at least consciously ( within working memory ) / it adapts to a new baseline ?",21269,,,2019-01-11T07:59:29.153,is input normalization built - in into mammals sensory neurons ?,machine-learning artificial-neuron neurons biology sense,1,0,1
2722,9924,1,,2019-01-10T13:44:58.710,1,81,"as according to the definition of ai something that can learn overtime , can imitate human behaviors , comes under artificial intelligence . if expert system(eg . mycin ) that only involves if then else statements qualifies to be an ai then every program we write in our daily life that involves some condition based question answering should be an ai . right ? if not then what should be an exact and universal definition for ai . how can a software qualify to be called ai . can someone please explain it to me ? thanks in advance ! !",21316,21316,2019-01-10T13:50:39.220,2019-01-10T13:50:39.220,"if expert systems are a bunch of if , then , else statements then how are they termed as ai ?",ai-basics expert-system,0,2,
2723,9925,1,,2019-01-10T14:42:59.520,5,125,"disclaimer : i 'm not a student in computer science and most of my knowledge about ml / nn comes from youtube , so please bear with me ! let 's say we have a classification neural network , that takes some input data $ w , x , y , z$ , and has some number of output neurons . i like to think about a classifier that decides how expensive a house would be , so its output neurons are bins of the approximate price of the house . determining house prices is something humans have done for a while , so let 's say we know a priori that data $ x , y , z$ are important to the price of the house ( square footage , number of bedrooms , number of bathrooms , for example ) , and datum $ w$ has no strong effect on the price of the house ( color of the front door , for example ) . as an experimentalist , i might determine this by finding sets of houses with the same $ x , y , z$ and varying $ w$ , and show that the house prices do not differ significantly . now , let 's say our neural network has been trained for a little while on some random houses . later on in the data set . it will encounter sets of houses whose $ x , y , z$ and price are all the same , but whose $ w$ are different . i would naively expect that at the end of the training session , the weights from $ w$ to the first layer of neurons would go to zero , effectively decoupling the input datum $ w$ from the output neuron . i have two questions : is it certain , or even likely , that $ w$ will become decoupled from the layer of output neurons ? where , mathematically , would this happen ? what in the backpropagation step would govern this effect happening , and how quickly would it happen ? for a classical neural network , the network has no "" memory , "" so it might be very difficult for the network to realize that $ w$ is a worthless input parameter . any information is much appreciated , and if there are any papers that might give me insight into this topic , i 'd be happy to read them .",21319,,,2019-04-17T09:36:10.390,can neural networks learn to ignore an input datum ?,neural-networks classification,2,0,
2724,9927,1,,2019-01-10T15:48:57.280,1,915,i 'm looking to build a pc for deep learning will tensoflow work on amd gpu with the same speed as on nvidia ones as amd does n't have tensorcore or cuda cores but it will have 16 gb of hbm vram his much do the tensor cores impact training,21321,1671,2019-01-12T23:00:00.687,2019-02-12T10:46:53.213,which gpu will be better for deep learning the upcoming radeon 7 or rtx 2080ti,machine-learning hardware-evaluation,2,2,
2725,9930,1,9931,2019-01-11T06:35:26.027,1,30,"this inquiry appeared in the comments to one of the answers of this question , but is actulaly not related to emotional intelligence , so it is reproduced here as a separate question i thought ( maybe incorrectly ) fuzzy logic was mostly used in sort of pre - programmed control systems , as opposed to ai or learning systems that regress on some datasets .",4302,,,2019-01-11T06:51:34.293,can fuzzy logic control self - tune as other learning systems do ?,reinforcement-learning ai-design fuzzy-logic real-time meta-rules,1,0,
2726,9933,1,,2019-01-11T09:20:37.030,3,71,"i understand why deep generative models like dbn ( deep belief nets ) or dbm ( deep boltzmann machines ) are able to capture underlying structures in data and use it for various tasks ( classification , regression , multimodal representations etc ... ) . but for the classification tasks like in learning deep generative models , i was wondering why the network is fine - tuned on labeled - data like a feed - forward network and why only the last hidden layer is used for classification ? during the fine - tuning and since we are updating the weights for a classification task ( not the same goal as the generative task ) , could the network lose some of its ability to regenerate proper data ? ( and thus to be used for different classification tasks ? ) instead of using only the last layer , could it be possible to use a partition of the hidden units of different layers to perform the classifications task and without modifying the weights ? for example , by taking a subset of hidden units of the last two layers ( sub - set of abstract representations ) and using a simple classifier like an svm ? thank you in advance !",21335,21335,2019-01-16T15:59:42.293,2019-01-16T15:59:42.293,why is the last layer of a dbn or dbm used for classification task ?,machine-learning deep-learning classification generative-model,1,0,
2727,9934,1,9997,2019-01-11T15:00:12.263,3,1029,"i have checked out many methods and paper like yolo , ssd , etc with very promising result in detecting a rectangular box around object , but could not find any paper , which shows an learning a rotated bounding box . is it difficult to learn the rotated bounding box for an object ? for example for this object src , its bounding box should be of the same shape(the rotated rectangle shown in 2nd right image ) , but prediction result for the yolo will be ist right . can somebody refer some paper , by which we can learn such box , or it is an expensive task to learn ? thanks",16313,21337,2019-01-12T03:49:30.153,2019-01-16T09:10:06.290,learning rotated bounding box for object detection,neural-networks convolutional-neural-networks object-recognition,2,1,
2728,9935,1,9961,2019-01-11T16:48:59.080,5,87,"i 'm using an object detection neural network and i employ data augmentation to increase a little my small dataset . more specifically i do rotation , translation , mirroring and rescaling . i notice that rotating an image ( and thus it 's bounding box ) changes its shape . this implies an erroneous box for elongated boxes , for instance on the augmented image ( right image below ) the box is not tightly packed around the left player as it was on the original image . the problem is that this kind of data augmentation seems ( in theory ) to hamper the network to gain precision on bounding boxes location as it loosens the frame . are there some studies dealing with the effect of data augmentation on the precision of detection networks ? are there systems that prevent this kind of thing ? thank you in advance ! ( obviously , it seems advisable to use small rotation angles )",19859,21337,2019-01-12T10:25:13.497,2019-01-13T11:56:12.210,how data augmentation like rotation affects the quality of detection ?,convolutional-neural-networks object-recognition,2,0,1
2729,9936,1,,2019-01-11T18:21:57.300,1,29,"let 's consider a classic feedforward neural network $ f$ with input dimension $ d$ , output dimension $ k$ , $ l$ layers $ l_i$ with $ m$ neurons each . relu activation . this means that , given a point $ x \in r^d$ its image $ f(x ) \in r^k$ . let 's now assume i add some gaussian noise in every hidden layer $ l_i(x)$ at the same time , where the norm of this noise is 5 % the norm of its layer computed on the point $ x$ . let 's call this new neural network $ f_*$ i know that , empirically , neural networks are resistant to this kind of noise , especially on the first layers . how can i show this theoretically ? the question i 'm trying to answer is the following : after having injected this noise in every layer $ l_i(x)$ , how far the output $ f_{*}(x)$ will be from the output of the original neural network $ f(x)$ ?",21338,21338,2019-01-12T13:23:07.730,2019-01-12T13:23:07.730,are neural network layers resistent to noise ?,neural-networks deep-learning,0,0,
2730,9937,1,,2019-01-11T22:31:31.807,2,46,"if i 'm performing a text classification task using a model built in keras , and for example , am attempting to predict the appropriate tag given a stack overflow question : "" how to subtract 1 from an int how do i subtract 1 from an int ? "" and the true / gold tag for this question is : "" objective - c "" but my model is predicting : "" c # "" if i were to retrain my model but this time add the above question and tag in both the training and testing data , would the model be guaranteed to predict the correct tag for this question in the test data ? i suppose the tl;dr is : are neural networks deterministic if they encounter identical data during training and testing ? i 'm aware it 's not a good idea to use the same data in both training and testing , but i 'm interested from a hypothetical perspective , and for gaining more insight into how neural networks actually learn . my intuition for this question is "" no "" , but i 'd really be interested in being pointed to some relevant literature that expands / explains that intuition .",21347,,,2019-01-12T10:13:39.297,will a neural network always predict the correct label if it sees the exact same input during training and testing ?,neural-networks deep-learning classification keras theory,2,0,
2731,9939,1,,2019-01-12T00:29:18.730,1,35,"the markov property is the dependence of a system 's future state probability distribution solely on the present state , excluding any dependence on past system history . the presence of the markov property saves computing resource requirements in terms of memory and processing in ai implementations , since no indexing , retrieval , or calculations involving past states is required . however , the markov property is often an unrealistic and too strong assumption . precisely , what limitations does the markov property place on real - time learning ?",4302,2444,2019-02-13T02:35:38.183,2019-02-13T02:35:38.183,what limitations does the markov property place on real time learning ?,reinforcement-learning markov-decision-process markov-property,0,0,
2732,9940,1,,2019-01-12T00:54:38.873,1,19,"as the question states , i am wondering how , if at all , a varying length of a trajectory ( series of state , action pairs ) will impact training / performance of policy gradient algorithms such as ppo , trpo and vpg . let 's say an agent runs in an environment where the length of each episode may not always be the same ( this is true especially in games such as poker ) . the cumulative reward for longer trajectories will inevitably be larger than the rewards for smaller trajectories . to me , this seems to cause an imbalance favoring the actions of a policy executed for a longer period of time ( non terminal state ) even if that policy may be sub - optimal compared to the policy applied for a shorter trajectory . are my assumptions correct ? how does trajectory size end up impacting the updates on a policy ?",21349,,,2019-01-12T00:54:38.873,impact of varying length trajectories on policy gradient optimization,reinforcement-learning policy-gradients,0,0,
2733,9941,1,,2019-01-12T01:04:04.253,0,184,"bdi agents are usually implemented using logical and symbolic methods , e.g. agentspeak , some robot control is being done by cognitive architectures , like opencog . but - if neural networks allows to do the generalization , the reasoning and the language processing - why neural networks are not used for goal - directed ( e.g. utilitarian ) autonomous agents , e.g. controlling service robots of providing intelligent services more than chatbots can do , e.g. doing automatic programming based on the dialog with the end users ? i asked a bit similar question about continuous learning bdi agents in particular neural network as ( bdi ) agent - running in continuous mode ( that do inference in parallel with learning ) ? and received good answer , but i wonder , why google is not givien recent results for search "" neural networks autonomous agents "" . i have bad experience with google , that is why i am asking here . e.g. google gives very few good answer for general query "" grammar extraction from neural network "" , but google gives excellent answers for the very specific question "" learning context free and context sensitive grammars with neural networks "" . i feel that there is similar problem with neural networks and autonomous agents - something should be going on , but it uses different , specific keywords , it takes some distinct angle and it is impossible to get good search results because of google failure to make some semantic search . this findings http://www.cs.cmu.edu/~dchaplot/papers/arnold_aaai17.pdf and https://github.com/glample/arnold most likely show that reinforcement learning game playing agents are the current state of the art and there are not much efforts to go beyond them . there are more or less autonomous agents in logical - symbolic worlds that can induce and reason about goal hierarchy ( deriving goals from the very general goals , e.g. maximizing utility in some scheme of preferences , earning profit from provided services ) and that can act following deduced goals , but apparently - neural networks are not ready for this . but maybe still there are some good research trends ?",8332,8332,2019-01-12T01:30:20.393,2019-01-12T01:30:20.393,why neural networks are not used for the implementation of autonomous ( e.g. bdi ) agents ?,neural-networks intelligent-agent,0,0,
2734,9944,1,,2019-01-12T06:27:04.450,2,82,how does darts compare to enas ? which one is better or what advantages does they each have ? links : darts : differentiable architecture search efficient neural architecture search via parameter sharing,21351,1671,2019-01-12T23:29:55.420,2019-01-12T23:29:55.420,how does darts compare to enas ?,neural-networks deep-learning convolutional-neural-networks search,0,2,
2735,9953,1,,2019-01-12T14:24:07.297,2,18,let 's say for : 1 . image tasks 2 . deep rl in high dimensional state space,21158,,,2019-01-12T14:24:07.297,"at what point are aws gpu instances worth it compared to cpu , * price wise * ?",deep-learning reinforcement-learning,0,0,
2736,9954,1,,2019-01-12T14:55:36.597,3,76,"some examples of low - variance machine learning algorithms include linear regression , linear discriminant analysis and logistic regression . examples of high - variance machine learning algorithms include decision trees , k - nearest neighbors and support vector machines . source : what makes a machine learning algorithm a low variance one or a high variance one ?",15368,15368,2019-01-19T01:38:44.730,2019-01-19T01:38:44.730,"why knn , decision trees , etc have a high variance ?",machine-learning linear-regression statistical-ai decision-tree,1,0,1
2737,9958,1,,2019-01-12T22:28:22.917,1,12,"for a neural turing machine , there is an attention distribution over the memory cells . a read operation consists of multiplying the memory cell 's value by its respective probability , and adding these results for all memory cells . suppose we only did the above operation for memory cells with a probability greater than 0.5 , or suppose we concatenated the results instead of adding them . can this be implemented/ trained with stochastic gradient descent ? or would it not be differentiable ? thanks !",21375,,,2019-01-12T22:28:22.917,is discrete reading in neural turing machines differentiable ?,neural-networks deep-learning,0,0,
2738,9962,1,9964,2019-01-13T13:41:27.793,0,24,why is the e - function used to decide whether to accept a worse solution or not ? to be more specific : why was $ e$ chosen as basis ? the propability to accept a worse solution is described with : $ p = e^{-\frac{e(y)-e(x)}{kt}}$ $ e(y)$ is the energy from the old solution $ e(x)$ is the energy from new solution $ t$ is a constant temprature decreasing with a constant factor k in every iteration .,19413,21157,2019-01-14T09:20:03.577,2019-01-14T09:20:03.577,simulated annealing : why is e - function used as propability function to decide to accept a worse solution,algorithm,1,0,
2739,9965,1,,2019-01-13T18:10:36.873,1,26,"i am currently working with classical roboticists who insist on inverse kinematics , and what i ( perhaps mistakenly ) call the old way of thinking about robots accomplishing tasks . much of the relatively recent research focuses on robots using brain models such as multiple timescales ( artificial intelligence models ) that segment sequences and reproduce them , having learned them . the problem i face is this bunch of roboticists insist that a robot already knows the sequence , and training it to be reproduced is redundant , since a robot can already reproduce the sequence anyway . how accurate would you rate this assessment of using ai in robotics ? are there any advantages of using ai to learn sequences for robot control ?",21397,,,2019-01-13T18:10:36.873,using artificial intelligence for robot movement instead of regular inverse kinematics,deep-learning robotics,0,0,1
2740,9966,1,,2019-01-13T18:38:00.237,1,93,"i would appreciate your help with this ( naive ) question of mine . given the set of points located on a circle , $ x_{i } , y_{i}$ as the input data , can a deep / machine learning algorithm infer that radius of the circle is constant ? in other words , given the data $ x_{i } , y_{i}$ is there way that algorithm discovers the constraint : $ x_{i}^2 + y_{i}^2 = \text{constant}$ ? i would also appreciate any related reference on the subject .",21399,,,2019-02-19T17:02:50.543,extracting algebraic constraints from the input data,machine-learning deep-learning training,3,2,
2741,9968,1,,2019-01-13T20:34:47.680,1,24,"i have data that are a result of rules that are exceptionless . i want to my program to ' look ' at my data and figure out those rules . however , the data might contain what might look like an exception ( rule within a rule ) but that is too , true for all occasions e.g. all men of the dataset with x common characteristics go out for a beer on thursday after work . that is true for all men with those characteristics . however , they will cancel their plans if their wife is sick . that last condition might initially look as an exception to the rule ( go out for beer on thursdays ) , but it is not as long as it is true for all men with those x characteristics . so the question is : which approach / method would be suitable for this ?",19393,,,2019-01-13T20:34:47.680,how can i model regularity ?,regularization,0,3,
2742,9969,1,,2019-01-14T01:32:20.787,2,111,"i have a machine learning project in which i have to separate spam and ham emails from a given dataset which has many txt files . some of them are spam emails and the other ham emails . i have to implement an id3 algorithm for this case . how do i begin with this implementation ? should i use a hashmap to have the words i read from the txt files and the times i read every single word ? something like hashmap&lt;string , int&gt ; or should i use two arrays maybe ?",21403,2444,2019-06-01T10:45:53.847,2019-06-01T10:45:53.847,separating spam and ham emails using the id3 algorithm,machine-learning getting-started java,3,0,
2743,9973,1,9995,2019-01-14T05:38:33.113,8,105,"i was wondering if machine learning algorithms ( cnns ? ) can be used / trained to differentiate between small differences in details between images ( such as slight differences in shades of red or other colours , or the presence of small objects between otherwise very similar images ? ) ? and then classify images based on these differences ? if this is a difficult endeavour with our current machine learning algorithms , how can it be solved ? would using more data ( more images ) help ? i would also appreciate it if people could please provide references to research that has focused on this , if possible . i 've only just begun learning machine learning , and this is something that i 've been wondering from my research . thank you .",16521,16521,2019-01-14T06:25:00.917,2019-01-27T12:02:09.643,can machine learning algorithms ( cnns ? ) be used / trained to differentiate between small differences in details between images ?,machine-learning convolutional-neural-networks image-recognition classification,2,0,
2744,9975,1,10197,2019-01-14T09:33:09.793,2,73,"i have come across the question simple model vs complex model . how to decide which one have to use ? and one more question connect to this . how to decide which is a simple model and which is a complex model , with the help of which parameters we can differentiate this i read about one of this but how we differentiate . is it really based on a number of inputs or number of layer of required to train that model ? this question is very generic i think but i except answer or some hint or direction . thanks in advance ! !",7681,7681,2019-01-14T09:55:33.577,2019-01-26T08:06:30.010,how to decide which model have to use ? simple vs complex,neural-networks machine-learning deep-learning artificial-neuron,4,0,
2745,9978,1,,2019-01-14T09:47:29.040,2,31,"i am reading about cann , however , i do not seem to grasp what it is . maybe someone who has worked with it can explain it ? i found out about it while reading about ratslam . i understand that it helps to keep long / short term memory .",14863,,,2019-01-14T09:47:29.040,continuous - attractor neural network explanation,neural-networks terminology,0,0,
2746,9980,1,,2019-01-14T10:12:14.613,1,17,are there any working ai system designs or theory to support a system where an artificial network is trained to to adjust fuzzy probabilities and modify the parameters of a genetic algorithm that mutates the fuzzy rules ?,4302,,,2019-01-14T10:12:14.613,genetic algorithm generation of fuzzy rules with artificial network adjustment of fuzzy probabilities,ai-design training topology fuzzy-logic,0,0,
2747,9982,1,9986,2019-01-14T11:59:12.760,3,190,is there software available which can extract metaphors from texts ? eg : his words cut deeper than a knife . or a simpler form like : eg : life is a journey that must be travelled no matter how bad the roads and accommodations .,21415,1671,2019-01-14T19:00:26.677,2019-01-15T16:55:06.800,how to recognise metaphors in nlp ?,machine-learning natural-language-processing computational-linguistics,2,0,1
2748,9983,1,,2019-01-14T12:43:31.843,2,101,"i was thinking of something of the sort : 1 ) build a program ( call this one fake user ) that generates lots and lots and lots of data based on the usage of another program ( call this one target ) using stimuli and response . for example , if the target is minesweeper , the fake user would play the game a carl sagan number of times , as well as try to click all buttons on all sorts of different situations , etc ... 2 ) run a machine learning program ( call this one the copier ) designed to evolve a code that works as similar as possible to the target . 3 ) kablam , you have a "" sufficiently nice "" open source copy of the target . is this possible ? is something else possible to achieve the same result ? namely : to obtain a "" suffienciently nice "" open source copy of the original target program ?",20976,,,2019-02-22T09:01:40.337,"is it possible to use ai ( machine learning , deep learning ... ) to reverse engineer software ?",machine-learning,2,1,1
2749,9987,1,,2019-01-14T16:03:44.710,0,69,"i want to start a project for my artificial intelligence class about speaker recognition . basically , i want to train my ai to detect if it 's me who 's speaking or somebody else . i would like some suggestions or libraries to work with .",21421,16229,2019-01-21T20:58:07.653,2019-02-20T21:02:15.450,training an ai to recognize my voice ( or any voice ),neural-networks machine-learning deep-learning training,1,0,
2750,9990,1,,2019-01-14T18:11:35.290,2,202,"i was trying to understand the loss function of gans , while i found a little mis - match between different papers . this is the screen - shot from the original paper of goodfellow at https://arxiv.org/pdf/1406.2661.pdf : , and equation ( 1 ) in this version of pix2pix paper at https://arxiv.org/pdf/1611.07004.pdf putting aside the fact that pix2pix is using conditional gan , which introduces a second term $ y$ , the 2 formulas are quite resemble , except that in the pix2pix paper , they try to get minimax of $ { \cal{l}}_{cgan}(g , d)$ , which is defined to be $ e_{x , y } [ ... ] + e_{x , z}[ ... ]$ , whereas in the original paper , they define . i am not coming from a good math background , so i am quite confused . i 'm not sure where the mistake is , but assuming that $ e$ is expectation ( correct me if i 'm wrong ) , the version in pix2pix makes more sense to me , although i think it 's quite less likely that goodfellow could make this mistake in his amazing paper . maybe there 's no mistake at all and it 's me who do not understand them correctly .",3098,3098,2019-01-14T18:18:09.683,2019-02-24T15:01:55.373,confusing on gan loss function,neural-networks deep-learning generative-model,3,1,
2751,9991,1,10002,2019-01-14T21:40:33.787,1,78,"according to the press information from dwave , quantum annealing is up and running and offers a lot of opportunities for society . “ basically , a quantum computer is a non - deterministic turing machine which is exciting and cool ” – this is at least the statement of ai influencer siraj raval who has researched the topic in detail . he has made a video about the subject and is trying to communicte the subject to larger audience . dwave itself is also interested in promoting their product . they have a lot of information out there and some companies are using these machines , for example google . but what i did n't understand is , why exactly do society needs a quantum computer ? i mean , image recognition and robotics control can be done with classical deeplearning on mainstream hardware , why do we need superposition and qubits . or asking more directly , which challenges are out there , that only quantum computers are able to master it ?",11571,1671,2019-01-15T20:09:10.227,2019-01-15T20:09:10.227,why do we need quantum computers ?,social quantum-computing,3,2,
2752,9994,1,,2019-01-15T01:28:58.640,0,31,"looking primarily for research papers , but any info or insights on this subject are welcome . strength does not have to exceed that of the best human player ( such as kasparov ) , just skilled human players . thoughts on the structure of the problems constituting the games , which might lend themselves , at least partially , to a symbolic approach , is also welcome . also thoughts on potential flaws or limitations , despite successes .",1671,,,2019-01-15T01:28:58.640,"examples of strong symbolic ai for unsolved , non - trivial combinatorial games ?",game-ai combinatorial-games symbolic-ai,0,0,
2753,9996,1,,2019-01-15T04:16:38.597,0,44,"i am trying to generate a model that uses several physico - chemical properties of a molecule ( incl . number of atoms , number of rings , volume , etc . ) to predict a numeric value y. i would like to use pls regression , and i understand that standardization is very important here . i am programming in python , using scikit - learn . the type and range for the features varies . some are int64 while other are float . some features generally have small ( positive or negative ) values , while other have very large value . i have tried using various scalers ( e.g. standard scaler , normalize , minmax scaler , etc . ) . yet , the r2 / q2 are still low . i have a few questions : ( 1 ) is it possible that by scaling , some of the very important features lose their significance , and thus contribute less to explaining the variance of the response variable ? ( 2 ) if yes , if i identify some important features ( by expert knowledge ) , is it ok to scale other features but those ? or scale the important features only ? ( 3 ) some of the features , although not always correlated , have values that are in a similar range ( e.g. 100 - 400 ) , compared to others ( e.g. -1 to 10 ) . is it possible to scale only a specific group of features that are within the same range ? thank you for your help . yannick",21431,,,2019-01-25T12:09:28.590,scaling and standardizing,machine-learning python data-science,2,0,
2754,10003,1,,2019-01-15T10:08:17.593,2,77,is there any way to control the extraction of features?how to recognize what features are been learnt during training i.e relevant information is been learnt or not ?,21441,1671,2019-01-15T20:15:23.443,2019-02-05T09:56:53.143,cnn - feature extraction,deep-learning convolutional-neural-networks ai-basics concepts feature-extraction,2,0,
2755,10007,1,,2019-01-15T16:04:22.477,1,30,"let 's say i want to model purchase data ( i.e. purchase records of many households across time ) . for simplicity , let 's assume each household only picks one alternative at the time . a simple starting point is a multinomial logit model . then , more flexible network architectures could be used . people have applied nn to this , but kept the number of alternatives ( k ) constant . in reality , the number of available options changes over time . also , it would be interesting to predict how choices change when the number of alternatives is changed . in bullet points there are n households t_n purchases for each household there are k_t alternatives at time t dependent variable y = k indicates that alternative k was bought each alternative is characterized by features , so x_kt is a vector of those features ( including brand name , price , ... ) . the number of features is constant across time . any guidance or ideas ?",21451,,,2019-01-15T16:04:22.477,nn : predicting choices when number of alternatives changes,neural-networks,0,0,
2756,10009,1,,2019-01-15T19:28:57.570,1,36,"the image is one of many similar exam questions can anyone pelase help me understand it fully ? ' internal node ' : this is simply every node except a ? move choices : his only options are b , c and d for this move ? focusing on b : e=8 f=4 and g are all opponent responses , therefore they will pick the minimum value . now my confusion , are m n and p your known responses in the case the opponent picks g , so you should pick m=0 ( the highest value ) , so then g gets passed 0 which the opponent should choose so b has a h - value of 0 ? are the correct value then b=0 , c=1 and d=2 so pick d as next move ?",21459,,,2019-01-15T19:28:57.570,ca n't grasp minimax diagram ( no alpha beta pruning ),minimax,0,1,
2757,10010,1,10089,2019-01-15T19:58:41.363,1,151,"recurrent neural networks ( rnn ) with attention mechanism is generally used for machine translation and natural language processing . in python , implementation of rnn with attention mechanism is abundant in machine translation ( for eg . https://talbaumel.github.io/blog/attention/ , however what i would like to do is to use rnn with attention mechanism on a temporal data file ( not any textual / sentence based data ) . i have a csv file with of dimensions 21000 x 1936 , which i have converted to a dataframe using pandas . the first column is of datetime format and last column consists of target classes like "" class1 "" , "" class2 "" , "" class3 "" etc . which i would like to identify . so in total , there are 21000 rows ( instances of data in 10 minutes time - steps ) and 1935 features . the last ( 1936th column ) is the label column . it is predominant from existing literature that an attention mechanism works quite well when coupled into the rnn . i am unable to locate any such implementation of rnn with attention mechanism , which can also provide a visualisation as well . any help in this regard would be highly appreciated . cheers !",21460,21460,2019-01-15T20:10:25.127,2019-04-25T17:46:41.900,how to use rnn with attention mechanism on non textual data ?,neural-networks python recurrent-neural-networks attention,2,0,
2758,10013,1,10088,2019-01-16T02:44:26.203,3,113,"what is "" bad local minima "" ? the following papers all mention this expression . eliminating all bad local minima from loss landscapes without even adding an extra unit limination of all bad local minima in deep learning adding one neuron can eliminate all bad local minima",18443,2444,2019-02-20T16:20:04.513,2019-02-20T16:20:04.513,what is a bad local minimum in machine learning ?,machine-learning deep-learning terminology papers calculus,2,0,1
2759,10018,1,,2019-01-16T09:05:13.580,2,71,"what exactly is meant by "" humanitarian ai "" ? what research areas does this cover ? ai in healthcare ? algorithmic fairness ? applications of ai for economic development ? can anyone provide links to relevant papers ?",21475,1671,2019-01-16T21:42:37.417,2019-02-16T10:01:20.227,"what is meant by the research topic "" humanitarian ai "" ?",terminology ethics applications social,1,1,
2760,10019,1,,2019-01-16T09:20:20.377,2,49,i have this problem where i need to get information out of pdf document sent from a scanner . the program needs to be learnable in some way to recognize what different figures mean . most of this should happen without human interference so it could just give a result after scanning the file . do anyone know if it 's possible to do with a machine learning program or any alternative way ?,21476,,,2019-01-16T10:36:39.920,"could it be possible to detect text , symbols , and components directly in a scanned pdf file with a program like tensorflow or another program ?",problem-solving computer-programming,1,1,
2761,10021,1,,2019-01-16T11:28:17.310,1,35,"for example , i have the following csv : training.csv i want to know how i can determine which column will be the best feature for getting the output prediction before i go for machine training . please do share your responses",9126,,,2019-01-16T14:29:13.760,how to analyze data before going for machine learning training ?,neural-networks machine-learning problem-solving feature-selection,2,0,
2762,10025,1,10029,2019-01-16T13:36:55.310,2,46,"i have a deep feedforward neural network $ f : w \times \mathbb{r}^d \rightarrow \mathbb{r}^k$ ( where $ w$ is the space of the weights ) with $ l$ hidden layers , $ m$ neurones per layer and relu activation . the output layer has a softmax activation function . i can consider two different loss functions : $ l_1 = \frac{1}{2 } \sum_i || f(w , x_i ) - y||^2 $ $ \ \ \ $ and where the first one is the classic quadratic loss and the second one is cross entropy loss . i 'd like to study the norm of the derivative of the loss function and see how the two are related , which means : 1 ) let 's assume i know that $ || \frac{\partial l_2(w , x_i)}{\partial w}|| & gt ; r$ , where $ r$ is a small constant . what can i assume about $ || \frac{\partial l_1(w , x_i)}{\partial w}||$ ? 2 ) are there any result which tell you that , under some hypothesis ( even strict ones ) such as a specific random initialisation , $ || \frac{\partial l_1(w , x_i)}{\partial w}||$ does n't go to zero during training ? thank you",21338,21338,2019-01-16T14:13:31.057,2019-01-17T04:00:36.763,comparing and studying loss functions,neural-networks feedforward loss-functions,1,0,
2763,10027,1,10030,2019-01-16T14:33:31.617,5,158,"there are many people trying to show how computer models are still very different from humans , but i fail to see in what way are people different from neural models in anything but complexity ? the way we learn is similar , the way we process information is similar , the ways we predict outcomes and generate outputs are similar . give a model enough processing power , enough training samples and enough time and you can train a human . how are we different ?",20399,1671,2019-01-16T21:33:04.877,2019-01-22T09:22:11.437,what is the difference between people and neural - network models ?,neural-networks learning-algorithms human-like,2,1,2
2764,10032,1,10033,2019-01-16T17:03:45.383,1,149,"i tried to build an q - learning agent which you can play tic tac toe against after training . unfortunately the agent performs pretty poorly . he tries to win but does not try to make me ' not winning ' which ends up in me beating up the agent no matter how many loops i gave him for training . i added a reward of 1 for winning the episode and it gets a reward of -0.1 when he tries to put his label on an non empty square ( after the attempt we have s = s ' ) . i also start with an epsilon=1 which decreases in every loop to add some more randomness at the beginning because i witnessed that some ( important in my opinion ) states did not get updated . since i spend some hours of debugging without noticeable progress i 'd like to know what you think . best rewgards ps : do n't care about some print statements and count variables . those where for debugging . code here or on github import numpy as np import collections import time gamma = 0.9 alpha = 0.2 class environment : def _ _ init__(self ) : self.board = np.zeros((3 , 3 ) ) self.x = -1 # player with an x self.o = 1 # player with an o self.winner = none self.ended = false self.actions = { 0 : ( 0 , 0 ) , 1 : ( 0 , 1 ) , 2 : ( 0 , 2 ) , 3 : ( 1 , 0 ) , 4 : ( 1 , 1 ) , 5 : ( 1 , 2 ) , 6 : ( 2 , 0 ) , 7 : ( 2 , 1 ) , 8 : ( 2 , 2 ) } def reset_env(self ) : self.board = np.zeros((3 , 3 ) ) self.winner = none self.ended = false def reward(self , sym ) : if not self.game_over ( ) : return 0 if self.winner = = sym : return 10 else : return 0 def get_state(self , ) : k = 0 h = 0 for i in range(3 ) : for j in range(3 ) : if self.board[i , j ] = = 0 : v = 0 elif self.board[i , j ] = = self.x : v = 1 elif self.board[i , j ] = = self.o : v = 2 h + = ( 3**k ) * v k + = 1 return h def random_action(self ) : return np.random.choice(self.actions.keys ( ) ) def make_move(self , player , action ) : i , j = self.actions[action ] if self.board[i , j ] = = 0 : self.board[i , j ] = player def game_over(self , force_recalculate = false ) : # returns true if game over ( a player has won or it 's a draw ) # otherwise returns false # also sets ' winner ' instance variable and ' ended ' instance variable if not force_recalculate and self.ended : return self.ended # check rows for i in range(3 ) : for player in ( self.x , self.o ) : if self.board[i].sum ( ) = = player*3 : self.winner = player self.ended = true return true # check columns for j in range(3 ) : for player in ( self.x , self.o ) : if self.board [ : , j].sum ( ) = = player*3 : self.winner = player self.ended = true return true # check diagonals for player in ( self.x , self.o ) : # top - left -&gt ; bottom - right diagonal if self.board.trace ( ) = = player*3 : self.winner = player self.ended = true return true # top - right -&gt ; bottom - left diagonal if np.fliplr(self.board).trace ( ) = = player*3 : self.winner = player self.ended = true return true # check if draw if np.all((self.board = = 0 ) = = false ) : # winner stays none self.winner = none self.ended = true return true # game is not over self.winner = none return false def draw_board(self ) : for i in range(3 ) : print(""------------- "" ) for j in range(3 ) : print ( "" "" , end= "" "" ) if self.board[i , j ] = = self.x : print(""x "" , end= "" "" ) elif self.board[i , j ] = = self.o : print(""o "" , end= "" "" ) else : print ( "" "" , end= "" "" ) print ( "" "" ) print(""------------- "" ) class agent : def _ _ init__(self , environment , sym ) : self.q_table = collections.defaultdict(float ) self.env = environment self.epsylon = 1.0 self.sym = sym self.ai = true def best_value_and_action(self , state ) : best_val , best_act = none , none for action in self.env.actions.keys ( ) : action_value = self.q_table[(state , action ) ] if best_val is none or best_val & lt ; action_value : best_val = action_value best_act = action return best_val , best_act def value_update(self , s , a , r , next_s ) : best_v , _ = self.best_value_and_action(next_s ) new_val = r + gamma * best_v old_val = self.q_table[(s , a ) ] self.q_table[(s , a ) ] = old_val * ( 1-alpha ) + new_val * alpha def play_step(self , state , random = true ) : if random = = false : epsylon = 0 cap = np.random.rand ( ) if cap & gt ; self.epsylon : _ , action = self.best_value_and_action(state ) else : action = np.random.choice(list(self.env.actions.keys ( ) ) ) self.epsylon * = 0.99998 self.env.make_move(self.sym , action ) new_state = self.env.get_state ( ) if new_state = = state and not self.env.ended : reward = -5 else : reward = self.env.reward(self.sym ) self.value_update(state , action , reward , new_state ) class human : def _ _ init__(self , env , sym ) : self.sym = sym self.env = env self.ai = false def play_step(self ) : while true : move = int(input('enter position like : \n0|1|2\n------\n3|4|5\n------\n6|7|8 ' ) ) if move in list(self.env.actions.keys ( ) ) : break self.env.make_move(self.sym , move ) def main ( ) : env = environment ( ) p1 = agent(env , env.x ) p2 = agent(env , env.o ) draw = 1 for t in range(1000005 ) : current_player = none episode_length = 0 while not env.game_over ( ) : # alternate between players # p1 always starts first if current_player = = p1 : current_player = p2 else : current_player = p1 # current player makes a move current_player.play_step(env.get_state ( ) ) env.reset_env ( ) if t % 1000 = = 0 : print(t ) print(p1.q_table[(0 , 0 ) ] ) print(p1.q_table[(0 , 1 ) ] ) print(p1.q_table[(0 , 2 ) ] ) print(p1.q_table[(0 , 3 ) ] ) print(p1.q_table[(0 , 4 ) ] ) print(p1.q_table[(0 , 5 ) ] ) print(p1.q_table[(0 , 6 ) ] ) print(p1.q_table[(0 , 7 ) ] ) print(p1.q_table[(0 , 8) ] ) print(p1.epsylon ) env.reset_env ( ) # p1.sym = env.x while true : while true : first_move = input(""do you want to make the first move ? y / n : "") if first_move.lower ( ) = = ' y ' : first_player = human(env , env.x ) second_player = p2 break else : first_player = p1 second_player = human(env , env.o ) break current_player = none while not env.game_over ( ) : # alternate between players # p1 always starts first if current_player = = first_player : current_player = second_player else : current_player = first_player # draw the board before the user who wants to see it makes a move if current_player.ai = = true : current_player.play_step(env.get_state ( ) , random = false ) if current_player.ai = = false : current_player.play_step ( ) env.draw_board ( ) env.draw_board ( ) play_again = input('play again ? y / n : ' ) env.reset_env ( ) # if play_again.lower ! = ' y ' : # break if _ _ name _ _ = = "" _ _ main _ _ "" : main ( )",21487,1671,2019-01-16T21:27:29.193,2019-01-16T21:27:29.193,q - learning tic tac toe - bad player,reinforcement-learning game-ai q-learning combinatorial-games,1,0,
2765,10034,1,,2019-01-16T18:14:43.297,1,36,"i 'm a programmer with a background in mathematics , but i have no experience whatsoever with artificial intelligence / neural networks . i 'd like to study it as a hobby , and my goal for now is to solve the following simple poker game , by letting the program play against itself : we have two players , each with a certain number of chips . at the start of the game , they are obligated to put a certain amount of chips in the pot . then they each get a random real number between 0 and 10 . they know their own number , but not the one of their opponent . then we have one round of betting . the first player puts additional chips in the pot ( some number between 0 and their stack size ) . the second player can either fold ( put no additonal chips in the pot , 1st player gets the entire pot ) , call ( put the same number of chips in the pot , player with highest number gets the pot ) or raise ( put even more chips in the pot , action back on 1st player ) . there is no limit to the amount of times a player can raise , as long as he still has chips behind to raise . i have several questions : - is this indeed a problem that can be solved with neural networks ? - what do you recommend me to study in order to solve this problem ? - is it feasible to solve this game when allowing for continuous bet / raise sizes ? or should i limit it to a few options as a percentage of the pot ? - do you expect it to be possible to get close to an equilibrium with one nightly run on an ' average ' laptop ?",21488,1671,2019-01-16T21:27:54.057,2019-01-16T21:27:54.057,what to study for this simple poker game ?,neural-networks game-ai game-theory imperfect-information,0,1,
2766,10036,1,10046,2019-01-17T01:04:36.763,0,26,"i have source data that can be represented as a 2d image of many similar curves . they may oftentimes cross over one another , so regions of interest will overlap . my goal is to implement a neural network solution to identify each instance or the curves and the pixels that are associated with each instance . ( each image is simple in its representation of the data . a pixel in the image is either a point on one of these curves or it is empty . so the image is represented by one or zero at each pixel . for training purposes , i have labels for every pixel , and i have about 150,000 images . the information in the images can be noisy in that there may be omissions of points and point locations are quantized due to measurement limitations and preprocessing for the image preparation . ) i started looking into what semantic segmentation can do for me , but since all of the instances are of the same class , distinguished mainly by their location in the image , i do n't think semantic segmentation is the type of processing i would want to perform . ( am i wrong ? ) i am very interested in seeing how a neural network will work on this problem to separate each instance . my question is this : what is the terminology that describes the process i 'm looking for ? ( how can i effectively research for this problem ? ) is this an extension of semantic segmentation or is it referred to some other way ?",8439,,,2019-01-17T15:43:08.763,pixel - level detection of each object of the same class in an image,neural-networks convolutional-neural-networks image-recognition,1,0,
2767,10037,1,,2019-01-17T05:29:56.447,1,27,"at slide 17 of the david silver 's series , the soft - max policy is defined as follows $ $ \pi_\theta(s , a ) \propto e^{\phi(s , a)^t \theta } $ $ that is , the probability of an action $ a$ ( in state $ s$ ) is proportional to the "" exponentiated weight "" . the score function is then defined as follows $ $ \nabla_\theta \log \pi_\theta ( s , a ) = \phi(s , a ) - \mathbb{e}_{\pi_{\theta}}[\phi(s , \cdot ) ] $ $ where does the expectation term come from ?",16313,2444,2019-02-15T18:59:03.090,2019-02-15T18:59:03.090,where does the expectation term in the derivative of the soft - max policy come from ?,reinforcement-learning math policy expectation,0,3,
2768,10040,1,,2019-01-17T08:37:30.307,0,51,"i need to evaluate the monetary potential of ai at my company for the year 2025 to justify a long term investment in a dedicated ai center of competence . the departments in which ai would be used later on ( purchasing , r&amp;d , manufacturing , … ) are generally very interested and some of them have already built some prototype use - cases . however , they are unwilling to predict and quantify any future potential . the investments in ai can only be justified if we can state the monetary benefit . are there any creative approaches out there to estimate the monetary potential of ai ? any help is greatly appreciated !",21502,,,2019-02-26T16:01:20.287,looking for approach for monetary evaluation of ai potential in 2025,research,2,2,1
2769,10043,1,,2019-01-17T10:13:05.493,4,143,"i am new to this community , i just have a question : is doing a m.s . in the combination of natural language processing and machine learning worth it ? what are the opportunities that can be opened by having such a master 's ?",8844,,,2019-02-04T08:14:58.783,asking about the combination of nlp and ml,machine-learning natural-language-processing,4,0,
2770,10044,1,,2019-01-17T12:23:23.670,2,43,"how do i check which algorithm solves my problem best ? given a optimaization problem , i apply different well known optimization algorithms ( genetic algorithm , simulated annealing , ant colony etc . ) to solve my problem . however , how do i know if my implementation ( e.g. cost function ) is working for every case ? how can i compare the algorithms or their goodness in the context of my problem ?",19413,,,2019-01-24T20:36:54.703,method to check goodness of combinatorial optimization algorithm implementation,optimization,1,0,
2771,10048,1,10069,2019-01-17T19:06:36.177,1,57,"i am new to this community , i have simple question . can inputs nodes or output nodes be connected to each other in neat algorithm ? note : inputs to -- > inputs or output to -- > outputs",21517,21517,2019-01-20T18:25:47.203,2019-01-20T18:25:47.203,neat algorithm question,ai-design ai-basics neat,1,0,1
2772,10049,1,10061,2019-01-17T19:27:47.763,3,81,"i 've seen monte - carlo reward $ g_{t}$ used in reinforce and td(0 ) reward $ r_t + \gamma q(s ' , a')$ used in vanilla actor - critic . i 've never seen someone use lambda reward $ g^{\lambda}_{t}$ in these situations , nor in any other algos . is there a specific reason for this ? could there be performance improvements if we used $ g^{\lambda}_{t}$ ?",21518,,,2019-01-18T14:29:37.537,why are lambda returns so rarely used in policy gradients ?,reinforcement-learning,1,0,2
2773,10050,1,,2019-01-17T19:45:15.953,5,86,"suppose there is an evaluation policy called and there are two behavior policies and . i know that it is possible to estimate the return of policy through behavior policies via importance sampling , which is unbiased . but i do not know about the variance of return estimated through two behavior policies and . does anybody know about the variance or any bound on the variance of estimated return ? let $ g_{0}^{b1}=\sum_{t=1}^{t}\gamma^{t-1}r_{t}^{b1}$ represent the total return for an episode through behavior policy and $ g_{0}^{b2}=\sum_{t=1}^{t}\gamma^{t-1}r_{t}^{b2}$ represent the total return for an episode through behavior policy . it is possible to estimate the return of policy as follows : $ $ g_{0}^{(e , b1)}=\prod_{t=1}^{t}\frac{\pi_{e}(a_{t}|s_{t})}{\pi_{b1}(a_{t}|s_{t})}*g_{0}^{b1}$$ $ $ g_{0}^{(e , b2)}=\prod_{t=1}^{t}\frac{\pi_{e}(a_{t}|s_{t})}{\pi_{b2}(a_{t}|s_{t})}*g_{0}^{b2}$$ i want to compare the variance of $ g_{0}^{(e , b1)}$ and $ g_{0}^{(e , b2)}$ . is there any formulation to compute the variance $ g_{0}^{(e , b1)}$ and $ g_{0}^{(e , b2)}$ ?",10191,2444,2019-03-06T12:13:06.420,2019-04-05T14:01:05.180,how do i compute the variance of the return of an evaluation policy using two behaviour policies ?,reinforcement-learning policy off-policy,1,1,
2774,10051,1,,2019-01-17T22:59:06.970,2,108,"attention has been used widely in recurrent networks to weight feature representations learned by the model . this is not a trivial task since recurrent networks have a hidden state that captures sequence information . the hidden state can be fed into a small mlp that produces a context vector summarizing the salient features of the hidden state . in the context of nlp , convolutional networks are not as straightforward . they have the notion of channels that are different feature representations of the input , but are channels the equivalent to hidden states ? particularly , this raises two questions for me : why use attention in convolutional networks at all ? convolutions have shown to be adept feature detectors––for example , it is known that higher layers learn small features such as edges while lower layers learn more abstract representations . would attention be used to sort through and weight these features ? in practice , how would attention be applied to convolutional networks ? the output of these networks is usually ( batch , channels , input_size ) ( at least in pytorch ) , so how would the attention operations in recurrent networks be applied to the output of convolutional networks ? references convolutional sequence to sequence learning , jonas gehring , michael auli , david grangier , denis yarats , yann n. dauphin , 2017",19403,4302,2019-01-20T20:56:02.317,2019-02-28T00:01:55.353,additive attention in convolutional networks,machine-learning deep-learning convolutional-neural-networks recurrent-neural-networks,1,0,1
2775,10053,1,,2019-01-18T01:52:49.553,0,35,"for the purposes of this question i am asking about training the generator , assume that training the discriminator is another topic . my understanding of generative adversarial networks is that you feed random input data to the generator and it generates images . out of those images , the ones which the discriminator thinks are real are used to train the generator . for example , i have the random inputs $ i_1 $ , $ i_2 $ , $ i_3 $ , $ i_4 $ ... from which the generator produces $ o_1 $ , $ o_2 $ , $ o_3 $ , $ o_4 $ . say for example , the discriminator thinks that $ o_1 $ and $ o_2 $ are real but $ o_3 $ and $ o_4 $ are fake , i then throw away input output pairs 3 and 4 , but keep 1 and 2 , and run back propagation on the generator to tell it that $ i_1 $ should produce $ o_1 $ , and $ i_2 $ should produce $ o_2 $ since these were "" correct "" according to the discriminator . the contradiction seems to come from the fact that the generator already generates those outputs from those inputs , so nothing will be gained by running backprop on those input output pairs . where is the flaw in my logic here ? i seem to have something wrong in my reasoning , or a misunderstanding of how the generator is trained .",21524,16565,2019-02-01T13:34:07.343,2019-03-03T14:02:35.593,training the generator in a gan pair with back propagation,backpropagation generative-adversarial-networks,1,0,
2776,10055,1,10062,2019-01-18T04:57:59.140,1,106,"everyone in the field of ai is aware that some of the objectives of ai could pose risks . a few of the risks exceed just car accidents as automated vehicles are beta tested or the replacement of cubicle jobs by processes running in data centers . various stories , such as frankenstein , 2001 : a space odyssey , terminator , and transcendence have made readers and movie watchers aware of some of the sequences of technological events that could lead to risks of significant magnitude and permanence . from a risk management point of view , should ethics classes be taught universally ? should legislative steps be taken to require ethical information to accompany all technical presentations of ai approaches , designs , and implementations ? should universities require ethics classes for all high powered technologies such as ai ? are there classes in ai ethics taught in high schools and universities yet ? if so , where ? if not , why not ? one possible syllabus in response to the query in the comment , here is one possible ai ethics course syllabus . university deans could decide whether it should be an academic requirement for 2nd year students enrolled in their school . brief history of ethics in economics , law , science , and geopolitics triumphs in ethics applied to technology in the past ways current society is benefiting from past ethical integrity past negative effects resulting from ethical negligence team project : a corporate ai policy per board of directors request distinguishing plausible futures from artifacts of sci fi creativity ways of evaluating outcomes relationship between creators of a system and the system created developing assessments of cost , loss , value , and benefit functions essay assignment : does humanity have a manifest destiny ? dealing with predictive uncertainty in technological ethics legislative and judicial considerations considering career options in an ethical context final exam and team project due",4302,4302,2019-01-18T08:11:04.830,2019-04-30T12:05:06.733,what ai ethics classes are available and should they be taught universally ?,ethics risk-management asimovs-laws legislation,3,4,
2777,10060,1,,2019-01-18T11:09:29.960,1,30,"i have a neural network $ f(w , x ) : \mathbb{r}^d \rightarrow \mathbb{r}^k$ with $ l$ layers , $ m$ neurones per layer , relu activation , softmax on the last layer and $ n$ datapoint . my loss function is the classic $ l(w ) = \frac{1}{2}\sum_{i \in [ n ] } || f(w , x_i ) - y_i||^2 $ . this means that every weight matrix $ w \in \mathbb{r}^{m \times m}$ . these matrices are started with random initialisation $ w_{i , j } = \mathcal{n}(0 , \frac{2}{m})$ . first of all , what 's the frobenius norm of this matrix ? if it was symmetric , wigner 's theory would suggest us that $ $ i 'm not sure it is right , but it should . what i need is an answer to the following questions : once i start training , how do the weights changes affect the norm of the weight matrices ? what 's the expected norm change after every iteration ? what 's the expected norm of a well trained network ? ( i have never specified if i need the $ ||*||_2 $ norm or the frobenius one because i actually need both , so whatever answer with one of them is valid : d ) thank you !",21338,21338,2019-01-18T16:43:06.763,2019-01-18T16:43:06.763,how does the norm of a weight matrix changes during training ?,neural-networks deep-learning,0,0,
2778,10064,1,,2019-01-18T15:00:39.957,0,141,"i am working to build a reinforcement agent with dqn . the agent would be able to place buy and sell orders for a day trading purpose . i am facing a little problem with that project . the question is "" how to tell the agent to maximize the profit and avoid the transaction where the profit is less than 100 $ "" . i want to maximize the profit inside a trading day and avoid to place the pair ( limit buy order , limit sell order ) if the profit on that transaction is less than 100$. the idea here is to avoid the little noisy movements . instead , i prefer long beautiful profitable movements . be aware that i thought using the "" profit & amp ; loss "" as the reward . "" i want the minimal profit per transaction to be 100 $ "" = = > it seems this is not something that is enforceable . i can train the agent to maximize profit per transaction , but how that profit is can not be ensured . at the beginning , i wanted to tell the agent , if the profit of a transaction is 50 dollars , i will remove 100 dollars , then it becomes a penalty of 50 dollars for the agent . i thought it was a great way to tell the agent to not place a limit buy order if you are not sure it will give us a minimal profit of 100$. it seems that all i would be doing there is simply shifting the value of the reward . the agent only cares about maximizing the sum of rewards and not taking care of individual transactions . how to tell the agent to maximize the profit and avoid the transaction where the profit is less than 100 $ ? with that strategy , what guarantee that the agent will never make a buy / sell decision that results in less than 100 dollars profit ? does the sum of reward - # transaction * 100 can be a solution ?",21539,21539,2019-01-19T21:29:29.493,2019-02-27T12:02:55.730,"for some reasons , a reward becomes a penalty if",reinforcement-learning dqn rewards,3,2,1
2779,10070,1,10076,2019-01-18T21:43:11.930,0,52,"i have seen a few articles about neural nets . mostly they went along these lines : we tried these architectures , these meta parameters , we trained it for x hours on y cpus and it gave us these results that are 0.1 % better than state of the art . what i am interested is whether there exists ( at least as a work in progress ) a framework , that gives some explanation why is some architecture better than other , what makes one activation function more suitable for image recognition than other , etc . do you have some tips where to start ? i would prefer something more systematic than google search ( a book , list of key articles is ideal ) .",11359,,,2019-01-19T09:09:05.703,neural networks theory,ai-basics,1,0,
2780,10071,1,,2019-01-19T00:25:30.303,2,52,"i have an electromagnetic sensor and electromagnetic field emitter . the sensor will read power from the emitter . i want to predict the position of the sensor using the reading . let me simplify the problem , suppose the sensor and the emitter are in 1 dimension world where there are only position x ( not x , y , z ) and the emitter emits power as a function of distance squared . from the painted image below , you will see that the emitter is drawn as a circle and the sensor is drawn as a cross . e.g. if the sensor is 5 meter away from the emitter , the reading you get on the sensor will be 5 ^ 2 = 25 . so the correct position will be either 0 or 10 , because the emitter is at position 5 . so , with one emitter , i can not know the exact position of the sensor . i only know that there are 50 % chance it 's at 0 , and 50 % chance it 's at 10 . so if i have two emitters like the following image : i will get two readings . and i can know exactly where the sensor is . if the reading is 25 and 16 , i know the sensor is at 10 . so from this fact , i want to use 2 emitters to locate the sensor . now that i 've explained you the situation , my problems are like this : the emitter has a more complicated function of the distance . it 's not just distance squared . and it also have noise . so i 'm trying to model it using machine learning . some of the areas , the emitter do n't work so well . e.g. if you are between 3 to 4 meters away , the emitter will always give you a fixed reading of 9 instead of going from 9 to 16 . when i train the machine learning model with 2 inputs , the prediction is very accurate . e.g. if the input is 25,36 and the output will be position 0 . but it means that after training , i can not move the emitters at all . if i move one of the emitters to be further apart , the prediction will be broken immediately because the reading will be something like 25,49 when the right emitter moves to the right 1 meter . and the prediction can be anything because the model has not seen this input pair before . and i can not afford to train the model on all possible distance of the 2 emitters . the emitters can be slightly not identical . the difference will be on the scale . e.g. one of the emitters can be giving 10 % bigger reading . but you can ignore this problem for now . my question is how do i make the model work when the emitters are allowed to move ? give me some ideas . some of my ideas : i think that i have to figure out the position of both emitters relative to each other dynamically . but after knowing the position of both emitters , how do i tell that to the model ? i have tried training each emitter separately instead of pairing them as input . but that means there are many positions that cause conflict like when you get reading=25 , the model will predict the average of 0 and 10 because both are valid position of reading=25 . you might suggest training to predict distance instead of position , that 's possible if there is no problem number 2 . but because there is problem number 2 , the prediction between 3 to 4 meters away will be wrong . the model will get input as 9 , and the output will be the average distance 3.5 meters or somewhere between 3 to 4 meters . use the model to predict position probability density function instead of predicting the position . e.g. when the reading is 9 , the model should predict a uniform density function from 3 to 4 meters . and then you can combine the 2 density functions from the 2 readings somehow . but i think it 's not going to be that accurate compared to modeling 2 emitters together because the density function can be quite complicated . we can not assume normal distribution or even uniform distribution . use some kind of optimizer to predict the position separately for each emitter based on the assumption that both predictions must be the same . if the predictions are not the same , the optimizer must try to move the predictions so that they are exactly at the same point . maybe reinforcement learning where the actions are "" move left "" , "" move right "" , etc . i told you my ideas so that it might evoke some ideas in you . because this is already my best but it 's not solving the issue elegantly yet . so ideally , i would want the end - to - end model that are fed 2 readings , and give me position even when the emitters are moved . how would i go about that ? ps . the emitters are only allowed to move before usage . during usage or prediction , the model can assume that the emitter will not be moved anymore . this allows you to have time to run emitters position calibration algorithm before usage . maybe this will be a helpful thing for you to know .",20819,20819,2019-01-19T01:17:06.940,2019-01-21T06:52:45.587,how do i combine two electromagnetic readings to predict the position of a sensor ?,machine-learning deep-learning probabilistic bayes,2,1,
2781,10077,1,,2019-01-19T11:44:16.090,2,65,"i have a solid understanding what the numbers are . if i want , i can see numbers in everything . could an ai have the same ability for any incoming information to tag them by numbers same as i have ?",21556,,,2019-01-21T05:41:58.930,could ai understand what the number one is and detect it in our real world same as ai can recognize cars and other things ?,object-recognition pattern-recognition,2,3,
2782,10082,1,,2019-01-20T00:44:44.533,2,291,"i am working to build an deep reinforcement learning agent which can place orders ( i.e. limit buy and limit sell orders ) . the actions are { "" buy "" : 0 , "" do nothing "" : 1 , "" sell "" : 2 } . suppose that all the features are well suited for this task . i wanted to use just the standard "" profit & amp ; loss "" as a reward , but i hardly thought to get something similar to the above image . the standard p&amp;l will simply place the pair ( limit buy order , limit sell order ) on every up movement . i do n't want that because very often it wo n't cover the commission and it is not a good indicator to trade manually . i would be interested that the agent can maximize the profit and give me a minimum profit of $ 100 on every pair ( limit buy order , limit sell order ) . on the picture , i would be interested in something similar to the above picture . is there a reward function that could allow me to get such a result ? if so , what is it ? update is the following utility function can work with the purpose of that question ? $ $ u(x ) = \max(\$100 , x ) $ $ that seems correct , but i do n't know how the agent will be penalized if it covers a wrong transaction , i.e. the pair ( limit buy order , limit sell order ) creates a loss of money .",21539,21539,2019-01-20T13:47:38.757,2019-02-28T05:24:05.853,suitable reward function for trading buy and sell orders,reinforcement-learning deep-network rewards,1,9,3
2783,10084,1,,2019-01-20T09:40:47.963,3,114,are current ai models entirely empiricist ? can they be rationalist?if so how ?,21578,1671,2019-01-21T21:14:08.653,2019-01-21T21:14:08.653,can an ai model be based on the principles of rationalism ?,ai-basics philosophy definitions rationality,3,0,1
2784,10090,1,,2019-01-20T16:00:12.600,2,268,"how would i go about designing a ( relatively ) simple ai that discovers and invents random more complex concepts on its own ? for example , say i had a robot car . it does n't know it 's a car . it has several inputs and outputs , such as a light sensor and the drive motors . if it stays in the dark , it 's score drops ( bad ) , and if it moves into the light , it 's score rises ( good ) . it 'd have to discover that it 's motor outputs cause the light input to change ( because it 's moving closer or farther away from a light source ) , and that brighter light means higher score . of course it 'd be easier to design an ai that does specifically that , but i want it 's behaviour discovery system to be more generic , if that makes any sense . like later on , it could discover a way to fight or cooperate with other robots to increase it 's score ( maybe other robots destroy light sources when they drive over them , and they can be disabled by driving into them ) , but it 'd have to discover this without initially knowing that another robot could possibly exist , how to identify one , what they do , and how to interact with one . also , i want it to be creative instead of following a ' do whatever is best to increase your score ' rule . like maybe one day it could decide that cooperating with other robots is another way to increase it 's score ( it finds out what love is ) , but if it 's unable to do that , it becomes depressed and stops trying to increase it 's score and just sits there and dies . or it could invent any other complete random and possibly useless behaviour . how hard would it be to make something like this , that essentially builds itself up from a very basic system , provided i give it a lots of different kinds of inputs and outputs that it can discover how to use and apply to its own evolving behaviour ?",21586,,,2019-03-29T07:32:20.547,self - evolving ai,ai-design evolutionary-algorithms,3,0,
2785,10095,1,,2019-01-20T19:09:14.530,1,48,i want to develop an ai based object ( mainly toy ) picker that can clean my kid 's room and put toys in toy basket . can somebody help me how to acheive this ? i want to make a custom solution so that it would be a learning for me . thanks !,21590,21590,2019-02-01T07:33:34.203,2019-02-01T07:33:34.203,how do i make ai based object picker ?,machine-learning image-recognition tensorflow robotics,1,4,
2786,10098,1,,2019-01-20T20:18:02.613,2,209,"people often value their pets , livestock , plants , yachts , cars , and plants . dogs are considered family members by many families . cats that might contribute little more than a purr or a look are also . horses and people develop a connection and the horse permits riding . people have favorite trees and keep flower gardens and lawns . water vessels , stuffed animals , and favorite livestock are given names . the requirement in society for an organism or inanimate object to be lovable can be quite low . the need for creating artificial pets is high for regions or housing situations were pets ca n't be . what makes a dog loving and lovable ? what are the actual behavioral characteristics that make it work for so many people , some of whom consider their dog a better friend than their spouse , parents , or work associates ? warmth to the touch ( being a mammal ) fuzziness smoothness in contour aesthetic design ( based mostly on millennia of breeding ) attentiveness ( based mostly on millennia of breeding ) appreciative response to provision of the most basic things like trivially prepared food , a dish of water , or access to the yard to expel waste learned responses to positive and negative verbal expressions and body language appreciation of simple gestures , such as petting barking when someone is in the yard unexpectedly performance of the the most basic tasks , such as a golden retriever on a hunt or merely fetching a stick or a toy perhaps attacking an attacker for some of the more fiercely bred breeds what of these is outside the realm of existing ai technology ? perhaps only the last two items , and that limitation may be overcome within the next ten or twenty years . although it would be nice to have a robot that cleaned the kitchen , painted the house , drove the car , or mowed the lawn , perhaps love , even if simulated , may be less superficial and more valuable than what makes some pets , plants , water vessels , and stuffed animals so dear . how can some artificial loving pets be designed ? is there a good architecture or algorithm for lovable artificial pets ?",4302,4302,2019-01-29T12:05:16.973,2019-01-29T12:05:16.973,design for a lovable and loving ai ?,ai-design emotional-intelligence artificial-life gesture-recognition,3,1,2
2787,10101,1,,2019-01-21T02:57:05.313,1,19,"i 'm relatively new to neural networks , and i 've been trying to program my own hopfield network . i got it to the point where it can reliably reproduce a single pattern from a completely scrambled starting state , for up to 400 neurons ( potentially more , my computer takes a while for anything bigger than that ) . when i try it with more than 1 stored pattern however , it appears to settle on a spurious state that looks like some combination ( sum ? ) of them rather than any of them in particular . my question is whether the method i 'm using to generate my patterns , the size of the network , or any conditions in general can cause this to happen . at the moment , i 'm using randomly - generated patterns . to test the network , i set it to be identical to one of the patterns , introduce some random noise by flipping a certain number of nodes , and then seeing if it can recreate the pattern . could anything about that process be causing the network to settle on that combination of states ? thanks in advance .",21599,,,2019-01-21T02:57:05.313,under what conditions will a hopfield network tend to converge to spurious states ?,ai-basics recurrent-neural-networks,0,0,
2788,10102,1,,2019-01-21T03:38:23.373,3,17,"does it help to "" pre - classify "" natural language inputs using labeled input fields ? e.g. , "" who , "" "" what , "" "" where , "" "" when , "" "" why , "" "" how , "" and "" how much ? "" or is a single , monolithic , free - form , long - text input field equally effective and efficient for model training purposes ? scenario 1 : without input labels we are three research fellows , alice , bob and charlie at the university of copenhagen . we want to understand the development of the human visual system . this knowledge will help in the prevention and treatment of certain vision problems in children . further , the rules that guide development in the visual system can be applied to other systems within the brain . our work , therefore , has wide application to other developmental disorders affecting the nervous system . we will conduct this research in 2019 under a budget of $ 15,000 . scenario 2 : with input lables who : we are three research fellows , alice , bob and charlie . what : we want to understand the development of the human visual system . where : at the university of copenhagen . when : during the calendar year of 2019 . why : this knowledge will help in the prevention and treatment of certain vision problems in children . how : further , the rules that guide development in the visual system can be applied to other systems within the brain . how much : the research will cost $ 15,000 . use case : i am building an ai / ml recommendation system . users subscribe to the system to get recommendations of research projects they might like to participate in or fund . there will be many projects from all over the globe . far too many for a human to sort through and filter . so ai will sort and filter automatically . will pre - classifying input fields using labels help the training algorithm be more efficient or effective ?",21598,21598,2019-01-21T19:20:10.700,2019-01-27T11:45:36.483,natural language recommendation system : to pre - classify inputs or not ?,natural-language-processing classification natural-language,1,0,
2789,10106,1,,2019-01-21T07:30:07.137,1,24,"consider a smielapp 1 , pronounced smile - app . it 's a proposed app for android , ios , linux , and other phone , tablet , laptop , and desktop environments . 2 the system requirements are a microphone , a speaker , and a user - facing camera , devices to interact with them in the operating system , and application access to those devices . the simplified data flows are as follows : camera smile detection nl generator text to speech speaker microphone wave to spectrum ( fft ) nl generator learning updates from server detector , generator , or speech the user facing camera acquires one frame per second . smile detection is a cnn system designed and trained via a labelled data set to locate and determine the presence of a smile or scowl and its degree of pronouncement , leading to a number that is zero when the mouth is at rest and expressionless , more negative with greater scowl , and more positive with greater smile up through laughter . that 's the "" smile detection "" above . so far , this seems quite achievable with current technology , especially if interaction between client and server allows parameter updates from learning that occurs on the back end . the nl generator on the server and accessible from the client produces strings containing natural language using a gan topology and drawing from an encoding of a 100,000 most common word vocabulary . the first data set for gan training of word sequence generation is originally a data set of funny and encouraging statements , but new word sequences can be generated by the gan . an additional two subsystems of the nl generator need to be a word ordering component that improves the order of words using linguistic heuristics or another deep network , and there must be a phrase mutator perhaps as simple as a regular expression engine so that the text to speech converters get the punctuation and doubling of letters they need to use the intonations needed for natural sounding speech . the audio input , processed with an fft and spectral transform and normalized to forms similar to the auditory features of language ( pitch , tone , consonant , and volume envelopes ) are mapped to the selection of funny or encouraging statements through a dqn designed and configured with a value function tied to the smile detector . its actions direct the selection . the text is converted to speech using tacotron 2 and presented to the user through the speaker . if the user smiles , the selection is reinforced . if the user scowls , the selection is dissuaded . all associations of text and user affect response is sent to the server to continue to further refine smile detection , nl generation 's gan input and word ordering heuristics , and speaker selection for text to speech conversion . that 's smielapp . is there a flaw in this ai design ? is there a flaw in the concept for the app ? is anyone aware of something that takes this non - traditional , non - chat - bot approach to natural language interaction ? are one of the components above relying on abilities that are beyond the current state of ai technology ? would an encouraging and possibly humorous app something that would be of benefit users ? 3 footnotes [ 1 ] smielapp is not a registered trademark and is just an example name for this functionality . [ 2 ] since content here is creative commons share alike , this means that the app can be developed for profit provided the attribution returns to this site and this post but no claim to authorship of design can be protected as someone else 's intellectual property , which would obviously be inappropriate , unless it has already been independently invented and protected as intellectual property . i 'm not aware of any similar app in existence . ( if there are none , that means any reader can develop and monetize it . ) [ 3 ] this idea came from the idea of actor - critic and the fact that critics are more appreciated by casual movie watchers than actors , who would in most cases want either respectful direction from a qualified director or encouragement from the fans and from movie popularity upon release . no one picks friends because they are good critics , even though critique may be part of what is said . what friends say is encouraging and perhaps funny , even when it has a corrective element in the intent of the words .",4302,4302,2019-01-21T09:35:26.683,2019-01-21T09:35:26.683,smielapp design feasible now ?,ai-design natural-language-processing applications emotional-intelligence generative-model,0,0,
2790,10110,1,,2019-01-21T10:39:51.977,2,33,"let 's say there 's a ball with features position , velocity , acceleration . these three are all concatenated as inputs to my neural network . however , i have prior knowledge that position is way more predictive than the other features . how do i weight the position feature much more strongly than the others ? would just applying a large scalar coefficient to it as preprocessing work ? seems unprincipled ...",21158,,,2019-01-21T11:11:58.957,how to weight important features,neural-networks,1,0,
2791,10112,1,10118,2019-01-21T12:12:07.447,-3,72,"i was trying to write a simple cnn in keras during a course , and i wrote one that does not learn at all , but i do n't understand why . do n't bother about the coding , first i load two images of a dog and one of a cat : img1 = np.array(image.open('dog1.jpg').convert('l ' ) ) img2 = np.array(image.open('dog2.jpg').convert('l ' ) ) img3 = np.array(image.open('dog3.jpg').convert('l ' ) ) img4 = np.array(image.open('cat1.png').convert('l').resize((225,225 ) ) ) x = np.array([img1.reshape(1,225,225),img2.reshape(1,225,225),img4.reshape(1,225,225 ) ] ) y = np.array([np.array([1,0]),np.array([1,0]),np.array([0,1 ] ) ] ) then i define the cnn model : model = sequential ( ) model.add(conv2d(96 , ( 11 , 11 ) , input_shape=(1 , 225 , 225 ) , activation='relu ' ) ) model.add(maxpooling2d(pool_size=(3 , 3),strides=2 ) ) model.add(flatten ( ) ) model.add(dense(2,activation='relu ' ) ) and then i train the cnn model : model.compile(loss='categorical_crossentropy ' , optimizer='adam ' , metrics=['accuracy ' ] ) model.fit(x,y,epochs=5 ) model.predict(img3.reshape(1,1,225,225 ) ) i always get the following for the training ( even for epochs=100 ) : epoch 1/5 3/3 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 2s 778ms / step - loss : 5.3727 - acc : 0.6667 epoch 2/5 3/3 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 2s 645ms / step - loss : 5.3727 - acc : 0.6667 epoch 3/5 3/3 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 2s 649ms / step - loss : 5.3727 - acc : 0.6667 epoch 4/5 3/3 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 2s 640ms / step - loss : 5.3727 - acc : 0.6667 epoch 5/5 3/3 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 2s 777ms / step - loss : 5.3727 - acc : 0.6667 and for the predict , sometimes i have values ( always before the training ) but often i have ( always after the training ) : array([[nan , nan ] ] , dtype = float32 ) so my questions are : why does my cnn not learn at all ? ? and why do i end up with "" nan "" after learning but have none before ? since it does nt learn apparently , the predict should n't change ?",19094,,,2019-01-21T17:41:02.817,keras simple cnn not learning,neural-networks machine-learning convolutional-neural-networks keras,2,3,
2792,10114,1,10116,2019-01-21T13:54:08.467,0,62,"if "" image captioning "" is utilized to make a commercial product , what application fields will need this technique ? and what is the level of required performance for this technique to be usable ?",21613,21130,2019-01-31T20:54:26.113,2019-01-31T20:54:26.113,"what 's the commercial usage of "" image captioning "" ?",deep-learning natural-language-processing image-recognition,1,0,
2793,10119,1,10120,2019-01-21T18:59:11.423,0,64,"this corresponds to exercise 1.1 of rlbook , and a discussion followed from here . considering two reward schemes- win = +1 , draw = 0 , loss = -1 win = +1 , draw or loss = 0 can we say something about the optimal q - values ?",21509,1671,2019-01-21T21:15:13.173,2019-01-25T11:33:02.470,what will q - values look like in self - play tic - tac - toe ?,deep-learning reinforcement-learning q-learning combinatorial-games self-play,1,1,
2794,10127,1,,2019-01-22T06:17:48.517,-2,46,i am sharan . i am currently pursuing a mechatronics degree . i am pretty much interested in artificial intelligence . so please anyone tell me where i have to start and where i have to update . which type of software is used in this field ?,21631,,,2019-01-22T07:14:24.343,artificial intelligence beginner,programming-languages,1,1,
2795,10128,1,,2019-01-22T06:45:14.867,2,14,"i 've got a challenge that feels like it should be solvable using some kind of clustering algo , but i ca n't get my head around how i can change the perspective such that it is solvable for such an algo . maybe some of you would like to share some pointers in the right direction . here it is . ( also , i 've manually written an old - fashioned decision tree kind of thing , but it 's horrible . ) the story take in mind a , say , cookie factories . the factories work at the disposal of stores , and can work harder if stores predicts peek demand . here 's what the factory offers to the store ( data example ) , and in hind sight , if the offer was picked to actually run : cost | picked | h0 | h1 | h2 | h3 | h4 | h5 | h6 | ... | h23 -----|--------|-----|-----|-----|-----|-----|-----|-----|-----|----- 0.10 | false | 0 | 10 | 20 | 20 | 8 | 0 | 0 | ... | 0 0.11 | true | 0 | 0 | 10 | 20 | 20 | 8 | 0 | ... | 0 0.09 | false | 0 | 0 | 0 | 10 | 18 | 8 | 0 | ... | 0 0.11 | false | 0 | 10 | 20 | 20 | 8 | 0 | 0 | ... | 0 0.12 | true | 0 | 0 | 10 | 20 | 20 | 8 | 0 | ... | 0 0.10 | false | 0 | 0 | 0 | 10 | 18 | 8 | 0 | ... | 0 0.11 | true | 0 | 6 | 8 | 8 | 8 | 7 | 0 | ... | 0 0.13 | false | 0 | 0 | 6 | 8 | 8 | 8 | 7 | ... | 0 0.14 | false | 0 | 0 | 6 | 7 | 6 | 0 | 0 | ... | 0 0.09 | false | 8 | 8 | 8 | 8 | 8 | 8 | 8 | ... | 8 the challenge we want to find out what offers come from the same factory . e.g. given the sample above , records 1 , 2 , and 3 would come from the same factory , as do 4 , 5 , and 6 ; and 7 , 8 , 9 , and 10 . the rules no two picked offers can be in the same group . groups can not contain more then a certain known number of offers , say 4 . duplicates based on volume belong to different groups . not every group has to have a picked offer .",21628,,,2019-01-22T06:45:14.867,property based clustering,classification python,0,0,0
2796,10130,1,,2019-01-22T08:20:17.787,0,69,"actually , i want to make an ai model which tells the seller about the maintaining stock of food items as a parameter of time and eventually learns by itself with the customer buying data ( problem - food items gets spoiled very fast . if we can tell the seller what product and how much he has to keep in stock with respect to time using ai we can decrease the cost of maintenance of food ) what are the steps and model i can use for this problem ?",21634,9062,2019-01-23T07:47:55.477,2019-02-22T08:00:26.040,how to predict the stock of food items using ai,recurrent-neural-networks learning-algorithms linear-regression,1,0,
2797,10133,1,10630,2019-01-22T11:01:17.253,0,255,"they only reference in the paper that the position embeddings are learned , which is different from what was done in elmo . elmo paper - https://arxiv.org/pdf/1802.05365.pdf bert paper - https://arxiv.org/pdf/1810.04805.pdf",17451,17451,2019-02-18T13:36:04.243,2019-02-18T13:36:04.243,bert - what are the segment and position embeddings used in here ?,neural-networks machine-learning deep-learning natural-language-processing recurrent-neural-networks,1,5,
2798,10134,1,,2019-01-22T12:02:32.973,2,26,"when dealing with continuous action spaces , a common choice when designing a policy in policy gradient methods is to learn mean and variance of actions for a specific state and then simply sample from the normal distribution defined by the learned mean and variance to get an action . my first question is , is explicit exploration strategy even needed is such cases , because the dose of randomness in actions would come from the sampling itself , on the other hand there could probably be cases where we would be stuck in a local optimum just by sampling . my second question is , in case that explicit exploration is needed , how would one approach this problem of exploration for this specific setup .",20339,,,2019-01-22T12:02:32.973,how to include exploration in gaussian policy,reinforcement-learning policy-gradients,0,0,
2799,10135,1,,2019-01-22T12:41:02.997,1,34,"problem : maintaining the products is a big task for a retailer . if we can estimate using ai to predict which products will sell the most , we can maintain sufficient stocks of the product without stocking excess inventory . this should be tracked with respect to time because i want to estimate the purchasing levels with respect to seasons . we are building a ai model which can self learn with respect to purchasing data and provide an estimate to of which products should be purchased and in what volumes . what approach should we take ? as now we are currently thinking of using lstm networks . please give a proper approach and algorithm .",21634,1671,2019-01-25T00:49:24.317,2019-01-25T00:49:24.317,how to estimate which item is bought most using ai,ai-basics datasets keras lstm,0,4,
2800,10136,1,10146,2019-01-22T15:32:52.187,1,57,i 'm having a hard time understanding how does the size of the hidden state affects gru . for example in a concrete example lets say i want to lean a gru to count . i 'm gon na feed it fx 3 timestamps the last 3 numbers and expect it to predict the fourth . how do i know which hidden size to chose ? can i see the hidden state size as the network capabilities to encode all the past information in a fix size vector ?,20430,,,2019-01-23T05:42:36.170,hidden state of the gru,recurrent-neural-networks hidden-layers,1,1,
2801,10139,1,,2019-01-22T17:11:40.307,3,103,"are human brain processes , like intuition , creativity , imagination and the ability to create art , algorithmic processes ( that is , produced by an algorithm ) ?",21644,2444,2019-04-19T14:02:47.973,2019-04-20T19:44:00.093,"are human brain processes , like intuition or imagination , algorithmic ?",philosophy theory brain,1,3,1
2802,10143,1,,2019-01-22T22:16:08.970,2,41,"in model - based reinforcement learning algorithms , the model of the environment is constructed to efficiently use samples , models such as dyna , and prioritize sweeping . moreover , eligibility trace helps the model learns ( action ) value functions faster . can i know if it is possible to combine learning , planning , and eligibility traces in a model to increase its convergence rate ? if yes , how it is possible to use eligibility traces in the planning part , like prioritize sweeping ?",10191,2444,2019-02-16T19:05:04.947,2019-02-16T19:05:04.947,eligibility trace in model - based reinforcement learning,reinforcement-learning model-based prioritized-sweeping eligibility-traces dyna,0,0,
2803,10144,1,10153,2019-01-22T22:51:01.783,0,77,my question is whether meta - learning and zero - shot learning are synonymous ? i have seen articles where they seem to imply that they are at least very similar concepts .,18312,,,2019-01-23T16:58:40.290,meta - learning vs zero - shot learning,deep-learning ai-basics,1,0,
2804,10145,1,10156,2019-01-23T04:49:26.413,1,53,"in an attempt at designing a neural network more closely modeled by the human brain , i wrote code before doing the reading . the neuron i have modeled operates on the following method . parameters : potential , threshold , activation . [ activation ] = 0.0 receive inputs , added to [ potential]. if ( [ potential ] > = [ threshold ] ) [ activation ] = [ potential ] [ potential ] = 0.0 else [ potential ] * = 0.5 in short , the neuron receives inputs , and decides if it "" fires "" if the threshold is met . if not , the input sum , or potential , decreases . inputs are applied by adding their values to the input potentials of the input neurons , and connections multiply neuron activation values by weights before applying them to their destination potentials . the only difference between this an a spiking network is the activation model . i am , however , beginning to learn that spiking neural networks ( snns ) , the actual biologically - inspired model , operate quite differently . forgive me if my understanding is terribly flawed . i seem to have the understanding that signals in these networks are sharp sinusoidal wave - forms with between 100 and 300 "" spikes "" in a subdivision of "" time , "" given for 1 "" second . "" these signals are sampled for the "" 1 second "" by the neuron , and processed by a differential equation that determines the activation state of the neuron . synapses seem to function in the same manner - > multiplying the signal by a weight , but increasing or decreasing the period of the graph . however , i wish to know what form of neuron activation model i created . i have been unable to find papers that describe a method like this . edit . the "" learnable "" parameters of this model are [ threshold ] of the neuron and [ weight ] of the connections / synapses .",12941,12941,2019-01-23T11:26:20.937,2019-01-24T00:11:56.600,how does one characterize a neural network with threshold - based activation functions ?,neural-networks human-inspired spiking-networks,1,1,
2805,10148,1,10169,2019-01-23T11:21:58.557,3,127,"i have some documents containing some text ( machine writing text ) that i intend to apply ocr on them in order to extract the text . the problem is that these documents contain a lot of noise but in different ways ( some documents have noise in the middle , others in the top ... ) ; which means that i ca n't apply simple thresholding in order to remove the noise ( i.e applying simple threshold does not only remove the noise , but it removes some parts of the text ) . for these reasons , i thought about using ai to do de - noise the documents . does anyone know if it is possible to do that with ai or any alternative way ?",19059,,,2019-01-24T18:04:20.467,is it possible to use ai to denoise noised or ' dirty ' documents ?,machine-learning ai-community ocr,2,0,2
2806,10149,1,10152,2019-01-23T15:33:40.350,1,48,"deepmind 's paper "" mastering the game of go without human knowledge "" states in its "" methods "" section on its "" neural network architecture "" that the output layer of alphago zero 's policy head is "" a fully connected linear layer that outputs a vector of size 19 ^ 2 + 1=362 , corresponding to the logit probabilities for all intersections and the pass move "" ( emphasis mine ) . i am self - trained regarding neural networks , and i have never heard of a "" logit probability "" before this paper . i have not been able by searching and reading to figure out what it means . in fact , the wikipedia page on logit seems to make the term a contradiction . a logit can be converted into a probability using the equation $ p=\frac{e^l}{e^l+1}$ , and a probability can be converted into a logit using the equation $ l=\ln{\frac{p}{1-p}}$ , so the two can not be the same . the neural network configuration for leela zero , which is supposed to have a nearly identical architecture to that described in the paper , seems to indicate that the fully connected layer described in the above quote needs to be followed with a softmax layer to generate probabilities ( though i am absolutely new to caffe and might not be interpreting the definitions of "" p_ip1 "" and "" loss_move "" correctly ) . the alphago zero cheat sheet , which is otherwise very helpful , simply echoes the phrase "" logit probability "" as though this is a well - known concept . i have seen several websites that refer to "" logits "" on their own ( such as this one ) , but this is not enough to satisfy me that "" logit probability "" must mean "" a probability generated by passing a logit vector through the softmax function "" . what is a logit probability ? what sources can i read to help me understand this concept better ?",21674,,,2019-01-23T16:54:27.737,"what is a "" logit probability "" ?",neural-networks activation-function,1,0,
2807,10150,1,,2019-01-23T15:54:01.897,0,59,"there are many methods and algorithms dealing with planning problems . if i understand correctly , according to wikipedia , there are classical planning problems , with : a unique known initial state , duration - less actions , deterministic actions , which can be taken only one at a time , and a single agent . classical planning problems can be solved using classical planning algorithms . the strips framework for problem description and solution , using backward chaining ) of the graphplan algorithm can be mentioned here . if actions are non - deterministic , according to wikipedia , we have a markow decision process ( mdp ) , with : duration - less actions , nondeterministic actions with probabilities , full observability , or partial observability for pomdp maximization of a reward function , and a single agent . mdps are mostly solved by reinforcement learning . obviously , classical planning problems can also be formulated as mdps ( with state transition probabilities of 1 , i.e. deterministic actions ) , and there are many examples ( e.g. some openai gyms ) , where these are successfully solved by rl methods . two questions : are there some characteristics of a classical planning problem , which makes mdp formulation and reinforcement learning a better suiting solution method ? better suiting in the sense that it finds a solution faster or it finds the ( near)optimal solution faster . how do graph search methods like a * perform with classical planning problems ? does strips with backward chaining or graphplan always outperform a * ? outperform in the sense of finding the optimal solution faster .",2585,2585,2019-01-24T08:35:57.220,2019-02-23T09:01:40.310,how to choose method for solving planning problems ?,reinforcement-learning graph-theory planning,1,0,
2808,10155,1,10161,2019-01-23T21:24:24.127,2,41,"it is said , that the essence of https://www.springer.com/us/book/9780817639495 "" neural networks and analog computation . beyond the turing limit "" is that the continuous / physical / real - valued weights for neural networks can induce super - turing capabilities . current digital processors can not implement real - valued neural networks , they can only approximate them . there are very little efforts to build analog classical computers . but it is quite possible that quantum computers will be analogue . so - is there research trend that investigates true real - valued neural networks on analog quantum computers ? google is of no use for my efforts , because it does not understand the true meaning of "" true real - valued neural network "" , it gives just real - value vs complex valued neural networks articles , which are not relevant to my question .",8332,,,2019-01-24T13:32:33.963,can analog quantum computer implement real - valued neural networks and hence do hypercomputation ?,neural-networks quantum-computing hypercomputation,1,0,
2809,10157,1,,2019-01-24T02:07:08.700,1,41,"sorry if my question is at the wrong place , i 'm new in this community . so , i have dataset with total of 1 million images ( augmented ) that separated in 28 classes . i followed this tutorial https://www.tensorflow.org/hub/tutorials/image_retraining to do transfer learning using inception v3 in tensorflow to create my own model . but i have no strong background in ml or dl so i 'm not sure how to tune the parameter correctly for training step . this is the training source code that i 'm using : https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py using the default setting i was able to get 80~84 % accuracy with 16.000 steps . i 've tried to change the training , validation , test ratio , training_steps , batch . but , still the accuracy is below 90 % . so , i m seeking for advice which parameter that should i change to achieve good accuracy . parameter : training_steps learning_rate testing_percentage validation_percentage eval_step_interval train_batch_size test_batch_size validation_batch_size thank you",20612,,,2019-01-24T02:07:08.700,tensorflow : inception v3 transfer learning parameter tuning,convolutional-neural-networks python computer-vision tensorflow,0,3,
2810,10158,1,10160,2019-01-24T04:22:43.037,6,145,"imagine that i have an artificial neural network with a single hidden layer and that i am using relu as my activating function . if by change i initialize my bias and my weights in such a form that : $ $ x * w + b & lt ; 0 $ $ for every input x in x then the partial derivate of the loss function with respect to w will always be 0 ! in a setup like the above where the derivate is 0 is it true that an nn won´t learn anything ? if true ( the nn won´t learn anything ) can i also assume that once the gradient reaches the value 0 for a given weight , that weight won´t ever be updated ?",21688,,,2019-01-28T17:13:29.753,how can a neural network learn when the derivative of the activation function is 0 ?,neural-networks deep-learning gradient-descent,3,0,1
2811,10162,1,,2019-01-24T08:26:49.993,2,26,"unfortunately there is no speech - recognition or speech - to - text tag yet so i go with the voice - recognition . my question concerns various datasets for automated speech recognition and how training and test split should be generated . even more specific , i 'd like to know how important it is to separate speakers from the training , test and dev split . in very imbalanced datasets one might have very few speaker which contribute a lot to the corpus and thus the model may be biased towards those speakers and have generalized less . is this concern legitimate ?",20162,,,2019-01-24T08:26:49.993,"training , test , dev split in speech recognition",deep-learning voice-recognition,0,0,
2812,10163,1,,2019-01-24T11:47:38.330,2,130,"i am curious if it is possible to do so . for example , if i supply [ 0,1,2,3,4,5 ] the model should return natural number sequence [ 1,3,5,7,9,11 ] should return natural number with step of 2 [ 1,1,2,3,5 ] should return fibonacci number [ 1,4,9,16,25 ] should return square natural number and so on .",21696,,,2019-01-25T00:18:40.880,predict the number pattern of given sequence,machine-learning,2,1,1
2813,10164,1,,2019-01-24T13:28:51.610,4,39,"i am trying to write an ai to a game , where there is no real adversary . this means , that only the ai player has choices in which move to perform , his opponent may or may not react to the move the ai player made , but when he reacts , he will always do the one and only single move that he is able to do . the goal of this ai would be , to find a solution to the situation , which results in the least amount of monster activations . to explain this a bit further , i will describe the game in a few words : there is a 3x3 board , on which there are some monsters . these monsters has a prewritten ai , and activate based on prewritten rules , ie , they do not have to make any decision at all . this is done , by an enrage mechanic , meaning , that when a monster hits it 's enrage limit , it activates , and performs his single move action . the ai should control the other side of this board , the hero players . each hero player has a different number of possible moves , each move dealing an amount of damage to the monsters , and increasing it 's enrage value , thus getting him closer to his enrage limit . what i want to achieve , is to write an ai , that will perform this fight in the least amount of monster activations as possible . for now , i 've written a minimax algorithm for this , without the min player . i 've done this , by calculating the negative effect of the monsters move , in the maximizing and only players move . the ai works in the following way : he draws the game tree for a set amount of depth of moves , calculates the bottom move with a heuristic function , selects the highest value from the given depth , and returns the value of this function up one level , then repeat . when he reaches the top of the tree , he performs the move , with the highest quantification value . this works , somewhat , but i have a big problem : as there is no randomness in the game , i was expecting that the greater the depth that he can search forward , the better moves he will find , but this is not always the case , sometimes a greater depth , returns a worse solution then a smaller depth my questions are as follows : what could cause the above error ? my quantification function ? the weights that i use in the function ? or something else ? is minimax the correct algorithm to use , for a game where there is no real adversarry , or is there any algorithm that will perform better for a game like this ?",21700,,,2019-01-24T14:56:34.227,what kind of decision rule algorithm is usable in this situation ?,minimax heuristics decision-tree,1,0,1
2814,10172,1,,2019-01-25T00:51:25.580,3,46,"typical feed forward neural networks require a fixed sized input and output . so when you have variable sized input , it seems to be common practice to pad the input with zero vectors . why does it not seem to be common practice to have a "" is_padding "" attribute ? that way the network can easily distinguish between padding and actual data ? especially considering input is commonly centered around 0 by subtracting the mean and using unit variance .",20338,,,2019-02-01T23:46:39.540,"using a "" is_padding "" attribute in your padding instead of simply zero vectors",neural-networks feedforward,0,1,
2815,10175,1,,2019-01-25T03:53:16.343,1,52,"i do understand that there are plenty of mobile apps available for body measurement ( e.g. mtailor ) or creating 3d model ( 3dlook ) . what i would like to find out is how we can use deep learning to achieve the accurate body measurement/3d model with just smart phone camera ? for example , mtailor can predict one 's body measurement quite accurately given the cameara angle / camera distance from the human and human height . can we do the same using deep learning with some labeled images to achieve the same accurate body measurement prediction ? thanks regards , han",21710,,,2019-01-25T03:53:16.343,"deep learning on how to find out the body measurement ( e.g. shoulder length , waist , hips , legs length etc ) from mobile camera captured images ?",deep-learning image-recognition,0,2,
2816,10177,1,,2019-01-25T06:39:39.863,2,229,"i 'm quite new to the field of computer vision and was wondering what are the purposes of having the boundary boxes in object detection . obviously , it shows where the detected object is , and using a classifier can only classify one object per image but my question is that 1 ) if i do n't need to know ' where ' an object is ( or objects are ) and just interested in the existence of them and how many there are , is it possible to just get rid of the boundary boxes ? 2 ) if not , how does bounding boxes help detect objects ? from what i have figured is that a network ( if using neural network architectures ) predicts the coordinates of the bounding boxes if there is something in the feature map . does n't this mean that the detector already knows where the object is ( at least briefly ) ? so continuing from question 1 , if i 'm not interested in the exact location , would training for bounding boxes be irrelevant ? 3 ) finally , in architectures like yolo , it seems that they predict the probability of each class on each grid ( e.g. 7 x 7 for yolo v1 ) . what would be the purpose of bounding boxes in this architecture other than that it shows exactly where the object is ? obviously , the class has already been predicted so i 'm guessing that it does n't help classify better .",21712,,,2019-01-28T11:16:27.830,what 's the role of bounding boxes in object detection ?,machine-learning convolutional-neural-networks object-recognition,2,0,1
2817,10180,1,10251,2019-01-25T10:34:07.857,3,118,"i 'm studying artificial intelligence . a modern approach , stuart russell , peter norvig , specifically about search and planning arguments . i do n't understand the difference between the two terms . i was more confused when i saw that some search problems can be determined in planning way . my professor explained to me in a confusing way that the real difference on the search is that it uses an heuristic function , but my book says that planning use a heuristic too , for relaxing problem ( in cap . 10.2.3 ) . i read this page that says in a certain way what i 'm saying . is planning and search the problem ? if not , what are the differences and how are these problems related ?",21719,2444,2019-01-28T12:56:17.467,2019-01-28T12:56:17.467,what is the difference between search and planning ?,terminology search definitions planning,2,2,
2818,10184,1,10187,2019-01-25T11:51:07.573,2,42,"let 's propose , that i can define the state of a board in a board game , with 234 neurons . in theory , could i be able to train a neural network , with 468 inputs ( two game boards ) , and 1 output , to tell me which board state is ' better ' ? the output should give me ~-1 if the second board is better than the first , ~0 if they are equal , and ~1 if the first board is better than the second . if yes , what could be the number of ideal neurons on the hidden layers ? what could be the ideal number of hidden layers ?",21700,,,2019-01-25T13:09:38.863,could a neural network be capable to diferentiate between two boards of a game ?,neural-networks game-ai,1,1,
2819,10189,1,10190,2019-01-25T14:03:41.367,1,42,"currently , we can build the artificial intelligence ( ai ) approaches that respectively explain their actions within the use of goal trees [ 1]. by moving up and down across the tree , it keeps tracking the last and next movements . therefore , giving the ability to the machine for "" explain "" the actions . explainability regarding human levels , requires some cognitive effort , such as self - awareness , memory retrieval , a theory of mind and so on [ 2]. humans are adept at selecting several causes from an infinite number of causes to be the explanation . however , this selection is influenced by certain cognitive biases . the idea of explanation selection is not new in explainable artificial intelligence ( xai ) [ 3 , 4]. but , as far as we are aware , there are currently no studies that look at the cognitive biases of humans as a way to select explanations from a set of causes . despite a clear definition and description of the xai field , several questions remain present . the issues are summarized in just one sentence and listed as follows . that said , our question is : how can we create and build xai ? references [ 1 ] hadoux , emmanuel , and anthony hunter . "" strategic sequences of arguments for persuasion using decision trees . "" aaai . 2017 . [ 2 ] miller , t. , 2018 . explanation in artificial intelligence : insights from the social sciences . artificial intelligence . [ 3 ] gunning , d. , 2017 . explainable artificial intelligence ( xai ) . defense advanced research projects agency ( darpa ) , nd web . [ 4 ] samek , w. , wiegand , t. and müller , k.r . , 2017 . explainable artificial intelligence : understanding , visualizing and interpreting deep learning models . arxiv preprint arxiv:1708.08296 .",21729,,,2019-01-25T14:11:13.300,how can we create explainable artificial intelligence ?,cognitive-science,1,0,1
2820,10191,1,,2019-01-25T14:37:37.883,2,57,"i am a member of a robotics team that is measuring the amount of reflected ir light to determine the lightness / darkness of a given material . we eventually hope to be able to use this to follow a line using a pre - set algorithm , but the first step is determining whether the material is one of the binary options : light or dark . given a large population of values between 0 and 1023 , probably in two distinct groupings , how can i best go about classifying a given point as light or dark ?",20818,2444,2019-02-25T15:46:14.077,2019-03-27T16:20:38.333,how do i classify measurements into only two classes ?,neural-networks machine-learning classification,1,0,
2821,10194,1,,2018-12-15T17:06:55.313,1,98,"pytorch 's example for the reinforce algorithm for reinforcement learning has the following code : import argparse import gym import numpy as np from itertools import count import torch import torch.nn as nn import torch.nn.functional as f import torch.optim as optim from torch.distributions import categorical parser = argparse.argumentparser(description='pytorch reinforce example ' ) parser.add_argument('--gamma ' , type = float , default=0.99 , metavar='g ' , help='discount factor ( default : 0.99 ) ' ) parser.add_argument('--seed ' , type = int , default=543 , metavar='n ' , help='random seed ( default : 543 ) ' ) parser.add_argument('--render ' , action='store_true ' , help='render the environment ' ) parser.add_argument('--log-interval ' , type = int , default=10 , metavar='n ' , help='interval between training status logs ( default : 10 ) ' ) args = parser.parse_args ( ) env = gym.make('cartpole-v0 ' ) env.seed(args.seed ) torch.manual_seed(args.seed ) class policy(nn.module ) : def _ _ init__(self ) : super(policy , self).__init _ _ ( ) self.affine1 = nn.linear(4 , 128 ) self.affine2 = nn.linear(128 , 2 ) self.saved_log_probs = [ ] self.rewards = [ ] def forward(self , x ) : x = f.relu(self.affine1(x ) ) action_scores = self.affine2(x ) return f.softmax(action_scores , dim=1 ) policy = policy ( ) optimizer = optim.adam(policy.parameters ( ) , lr=1e-2 ) eps = np.finfo(np.float32).eps.item ( ) def select_action(state ) : state = torch.from_numpy(state).float().unsqueeze(0 ) probs = policy(state ) m = categorical(probs ) action = m.sample ( ) policy.saved_log_probs.append(m.log_prob(action ) ) return action.item ( ) def finish_episode ( ) : r = 0 policy_loss = [ ] rewards = [ ] for r in policy.rewards[::-1 ] : r = r + args.gamma * r rewards.insert(0 , r ) rewards = torch.tensor(rewards ) rewards = ( rewards - rewards.mean ( ) ) / ( rewards.std ( ) + eps ) for log_prob , reward in zip(policy.saved_log_probs , rewards ) : policy_loss.append(-log_prob * reward ) optimizer.zero_grad ( ) policy_loss = torch.cat(policy_loss).sum ( ) policy_loss.backward ( ) optimizer.step ( ) del policy.rewards [ : ] del policy.saved_log_probs [ : ] def main ( ) : running_reward = 10 for i_episode in count(1 ) : state = env.reset ( ) for t in range(10000 ) : # do n't infinite loop while learning action = select_action(state ) state , reward , done , _ = env.step(action ) if args.render : env.render ( ) policy.rewards.append(reward ) if done : break running_reward = running_reward * 0.99 + t * 0.01 finish_episode ( ) if i_episode % args.log_interval = = 0 : print('episode { } \tlast length : { : 5d}\taverage length : { : .2f}'.format ( i_episode , t , running_reward ) ) if running_reward & gt ; env.spec.reward_threshold : print(""solved ! running reward is now { } and "" "" the last episode runs to { } time steps!"".format(running_reward , t ) ) break if _ _ name _ _ = = ' _ _ main _ _ ' : main ( ) i am interested in the function finish_episode ( ) : the line rewards = ( rewards - rewards.mean ( ) ) / ( rewards.std ( ) + eps ) makes no sense to me . i thought this might be baseline reduction , but i ca n't see why divide by the standard deviation . if it is n't baseline reduction , then why normalize the rewards , and where should the baseline reduction go ? please explain that line",21645,,,2019-01-25T16:01:27.230,why is the reward signal normalized in openai 's reinforce ?,machine-learning reinforcement-learning,1,0,
2822,10196,1,10204,2019-01-24T13:56:08.333,1,368,"in open ai 's actor - critic and in open ai 's reinforce , the rewards are being normalized like so rewards = ( rewards - rewards.mean ( ) ) / ( rewards.std ( ) + eps ) on every episode individually this is probably baseline reduction , but i 'm not entirely sure why they divide by std ( ) . assuming this is baseline reduction , please explain why is this done per episode ? what if one episode yields rewards in the ( absolute , not normalized ) range of [ 0,1 ] , and the next episode yields rewards in the range of [ 100,200 ] ? this method seems to ignore the absolute difference between the episodes ' rewards so again , the question : why is the baseline reduction done per episode individually ? why do they divide by std ( ) ? edit : not a duplicate : this question mainly asks about the individual calculation of reward in every episode . i referenced the offered duplicate myself , as i am the author of it as well , and i found that the answers may be related . however , they are very much not the same quesion .",21645,,,2019-01-26T12:55:08.460,why does is make sense to normalize rewards per episode in reinforcement learning ?,machine-learning reinforcement-learning,2,6,
2823,10198,1,,2019-01-25T16:39:28.503,0,45,"following pytorch 's actor critic , i understand that the critic is a function mapping from the state space to the reward space , meaning , the critic approximates the state - value funcion . however , according to this paper ( you do n't need to read it , just a glance at the nice picture at page 2 is enough ) , the critic is a function mapping from the action space to the reward , meaning it approximates the action value funcion i am confused . when people say "" actor critic "" - what do they mean by "" critic "" ? is the term "" critic "" ambiguous in rl ?",21645,21645,2019-02-07T18:54:32.583,2019-02-07T18:54:32.583,what does the critic network evaluate in actor critic ?,machine-learning reinforcement-learning open-ai,1,0,
2824,10201,1,10887,2019-01-25T17:45:23.587,0,25,"if one examines ssd : single shot multibox detector code from github repository , it can be seen that , for a testing phase ( evaluating network on test data set ) , there is a parameter test batch size . it is not mentioned in the paper . i am not familiar with using batches during network evaluation . can someone explain what is the reason behind using it and what are advantages and disadvantages ?",21737,,,2019-02-26T07:37:24.623,using batches in testing,neural-networks convolutional-neural-networks object-recognition,2,1,
2825,10203,1,,2019-01-25T19:40:24.863,2,158,"i am in the process of implementing the dqn model from scratch in pytorch with the target environment of atari pong . after a while of tweaking hyper - parameters , i can not seem to get the model to achieve the performance that is reported in most publications ( ~ +21 reward ; meaning that the agent wins almost every volley ) . my most recent results are shown in the following figure . note that the x axis is episodes ( full games to 21 ) , but the total training iterations is ~6.7 million . the specifics of my setup are as follows : model class dqn(nn.module ) : def _ _ init__(self , in_channels , outputs ) : super(dqn , self).__init _ _ ( ) self.conv1 = nn.conv2d(in_channels=in_channels , out_channels=32 , kernel_size=8 , stride=4 ) self.conv2 = nn.conv2d(in_channels=32 , out_channels=64 , kernel_size=4 , stride=2 ) self.conv3 = nn.conv2d(in_channels=64 , out_channels=64 , kernel_size=3 , stride=1 ) self.fc1 = nn.linear(in_features=64*7*7 , out_features=512 ) self.fc2 = nn.linear(in_features=512 , out_features = outputs ) def forward(self , x ) : x = f.relu(self.conv1(x ) ) x = f.relu(self.conv2(x ) ) x = f.relu(self.conv3(x ) ) x = x.view(-1 , 64 * 7 * 7 ) x = f.relu(self.fc1(x ) ) x = self.fc2(x ) return x # return q values of each action hyperparameters batch size : 32 replay memory size : 100000 initial epsilon : 1.0 epsilon anneals linearly to 0.02 over 100000 steps random warmstart episodes : ~50000 update target model every : 1000 steps optimizer = optim.rmsprop(policy_net.parameters ( ) , lr=0.0025 , alpha=0.9 , eps=1e-02 , momentum=0.0 ) additional info openai gym pong - v0 environment feeding model stacks of 4 last observed frames , scaled and cropped to 84x84 such that only the "" playing area "" is visible . treat losing a volley ( end - of - life ) as a terminal state in the replay buffer . using smooth_l1_loss , which acts as huber loss clipping gradients between -1 and 1 before optimizing i offset the beginning of each episode with 4 - 30 no - op steps as the papers suggest has anyone had a similar experience of getting stuck around 6 - 9 average reward per episode like this ? any suggestions for changes to hyperparameters or algorithmic nuances would be greatly appreciated !",19789,,,2019-02-10T04:04:36.467,dqn stuck at suboptimal policy in atari pong task,deep-learning reinforcement-learning python,1,7,1
2826,10207,1,10211,2019-01-25T23:27:31.453,1,41,is this approach nonsense since reason produces algorithms as a method of solving problems . the algorithm is a sequence of events that perform a task till completion of the problem . reason is intelligence and algorithms do not substitute reason or create it .,9094,9094,2019-01-25T23:33:41.050,2019-01-26T13:21:50.763,artificial intelligence as a product of algorithms,algorithm learning-algorithms,1,10,
2827,10208,1,,2019-01-26T03:14:09.527,1,40,"i am trying to make a personal ml project where my objective is using a photo from an invoice , for instance , a walmart invoice , classify it as being a walmart invoice and extract the total amount spent . i would then save this information in a relational database and infer some statistics about my spendings . the goal would be to classify invoices not only from walmart but from the most frequent shops where i spend money and then extract the total amount spent . i already do this process manually , i insert my spendings in a relational database . i have a bunch of photos from different invoices that i have recorded over the past year for this purpose ( training a model ) . what algorithms would you guys recommend ? from my point of view , i think that i need some natural language processing to extract the total amount spent and maybe a convolutional neural network to classify the invoice as being from a specific store ? thanks !",21688,,,2019-01-27T02:20:32.890,approach to classify a photo and extract text from it,machine-learning deep-learning computer-vision,2,0,
2828,10210,1,,2019-01-26T12:28:51.903,3,55,"i am using open ai 's code to do a rl task on an environment that i built myself . i tried some network architectures , and they all converge , faster or slower on cartpole . on my environment , the reward seems not to converge , and keeps flickering forever . i suspect the neural network is too small , but i want to confirm my belief before going the route of researching the architecture . how can i confirm that the architecture is the problem and not anything else in a neural network reinforcement learning task ?",21645,,,2019-03-03T06:24:59.297,how to identify too small network in reinforcement learning ?,neural-networks machine-learning deep-learning reinforcement-learning,2,0,
2829,10213,1,10215,2019-01-26T14:11:41.233,1,27,"the paper states that every potential function wo n't alter the optimal policy . i lack of understanding why is that . the definition : $ $ r ' = r + f$$ with $ $ f = \gamma\phi(s ' ) - \phi(s)$$ where may be $ 0.9 $ if i have the following setup : on the left is my $ r$ . on the right my potential function the top left is the start state , the top right is the goal state the reward for the red route is : $ ( 0 + 0.9 * 100 - 0 ) + ( 1 + 0.9 * 0 - 100 ) = -9 $ . and the reward for the blue route is : $ ( -1 + 0 ) + ( 1 + 0 ) = 0 $ . so , for me , it seems like the blue route is better than the optimal red route and thus the optimal policy changed . do i have erroneous thoughts here ?",21685,2444,2019-02-16T01:52:51.757,2019-02-16T01:52:51.757,potential based reshaping : any potential function ?,reinforcement-learning,1,1,
2830,10214,1,10229,2019-01-26T14:24:55.517,0,42,"i ’m looking for advice regarding my ml project . using a special wristband , i am able to collect a bunch of physiological data from human subjects . i want to develop an application to recognize when these physiological signals change in a meaningful way and only then ask the user how he / she is feeling . this data will later be used for machine learning testing . the problem is , that i am struggling to find appropriate ways to classify current data input as meaningful and ask for information only when relevant user input is to be gathered , not more and not less . for me , this seems to be a novelty detection problem , combined with a binary classification problem . i have to recognize what values coming from the data stream are to be considered normal , and therefore not bother the user with unnecessary input requests . i would also use novelty detection to recognize the data coming out of the normal zone and ask the user about it . this new data is then not considered novelty anymore , and binary classification will tell if the user is to be asked about his emotions when getting the same data in the future . so , these are my questions : - what do you think about my reasoning of the problem ? do you have other perspectives on how to handle these problems ? i have been told this could also be considered an anomaly detection problem , for example . - what algorithms would you use to separate normal from more meaningful physiological data ? support vector machines perhaps ? maybe some decision theory ? - do you know any books or papers on similar matters ? even if i have found some after hours and hours of research , you may be able to point me to something different than those i have . it is worth noting that data collection is supposed to be done when no other factors are messing with signal readings , such as sport . any help would be much appreciated . best regards , augusto",21767,,,2019-01-27T05:55:35.017,help with novelty recognition and binary classification for emotion recognition,machine-learning data-science decision-theory,1,0,1
2831,10218,1,,2019-01-26T16:50:11.450,1,22,"i 'm running kera 's lstm ( not cudnnlstm ) but i notice my gpu is under load . i need recurrent dropout , so i can only stick with lstm . is the ' normal ' lstm assisted by gpu ? if so , how are lstm and cudnnlstm different ? i presume cudnnlstm uses the cudnn api ( and lstm does n't ? similarly , is the normal lstm supposed to be faster on gpu or cpu ?",13068,13068,2019-01-26T16:57:27.230,2019-01-26T16:57:27.230,kera 's ( normal ) lstm uses the gpu ?,tensorflow keras lstm,0,0,
2832,10219,1,,2019-01-26T17:10:56.033,0,8,"inputs : time series of spectra representing human speech semantic network ( as a directed graph ) associations outputs : modified version of the semantic network input edge types in the graph ( semantic , not neural , network connections ) : adjacency analogy of generalization example of composed of ideally , each edge would have the following two numeric attributes : median recency probability or strength characteristics of the permissible network topology : multiple edges types connecting the same two vertices cycles this goes beyond parsing to the modification of semantic models in ways that simulate listening to language .",4302,,,2019-01-26T17:10:56.033,any research that proposes ways to improve a semantic network from phonetic sequences ?,ai-design natural-language-processing pattern-recognition probabilistic cognitive-science,0,0,
2833,10228,1,,2019-01-27T05:54:39.257,3,131,"in an mlp with relu activation functions after each hidden layer ( except the final ) , let 's say the final layer should output positive and negative values . with relu intermediary activations , this is still possible because the final layer , despite taking in positive inputs only , can combine them to be negative . however , would using leaky relu allow faster convergence ? because you can pass in negative values as input to the final layer instead of waiting till the final layer to make things negative",21158,,,2019-04-02T07:48:20.057,does leaky relu help learning if the final output needs negative values ?,neural-networks,3,0,
2834,10233,1,,2019-01-27T10:02:42.797,-2,113,"i 'm confused about these two , is there any difference between them ? and how can i learn more about them ?",21784,,,2019-01-28T16:59:38.160,what is the difference between deep learning and machine learning ?,machine-learning deep-learning,1,0,
2835,10238,1,,2019-01-27T14:12:07.713,4,94,"platonic solids are regular , convex , and equilateral polyhedrons with congruent faces and a congruent number of edges meeting at the vertices . we can see them in toys ; the six faces of the cube introduces most children to platonic solids as building blocks , without ever telling them that their blocks are cubes or platonic solids . we can see them as plastic calendars ; the dodecahedron has twelve faces , one for each month . we do n't see them in nuclear detonation designs , at least not most of us ; the twenty faces of the icosahedron can correspond to the wave fronts of the twenty simultaneous detonations that must converge to a point to force the purified u , pu , or h across the threshold of the density - temperature surface to self - sustain its reaction . the tetrahedron and octahedron , although interesting , are not seen much , since they do not work as cleanly in cad / cam scenarios . applying this geometric abstraction to robotics , one of the most nimble and efficient walkers and workers in 3-d space are spiders 2 , and the freedom of motion in their legs are not fully in the platonic paradigm , but could be as follows . sun loving platonic robots imagine an octahedral robot with photovoltaic ( pv ) cells centered on each of its eight faces and a leg at each of its six vertices . the six legs have hip joints with robotic control over bend and rotation . ( bend is the angle between the direction the vertex points and the direction of the base of the leg . ) there is a knee part of the way down each leg , the angle of which can be controlled . at the end of each leg is a wheel with a break and rubber tire . there is no reason two power the wheel because the robot can run with the breaks on and then roll with them off and the wheels positioned in the direction of momentum . alternatively , the entire robot can roll like a ball in a spherical wave fashion . its platonic symmetry facilitates simulation of a ball that has a moving bump positioned to use gravitational force to produce the desired mobility accelerations . the robot 's only interest is maximally sustaining its electrical charge . all the systems and the ai that drives them are designed to learn one thing : find light and stay in it to sustain charge . if it discharges to much too often , its li battery will eventually not hold a charge , it becomes decrepit , and dies , until someone replaces its battery at which time it is reborn with all the knowledge of its previous life . this is a really awesome robot in many ways . mathematical symmetry walking , rolling , coasting , and climbing possibilities energy thrift potential extension of the design to produce utility the pv power levels comprise eight - pixel omnidirectional vision simplicity of purpose : to maximally sustain its electrical charge potential for locking legs with others of its kind this last one allows the robots to discover that collaborative behavior can be used to stack and make larger machines to open doors and get outside when the sun is out . that 's important because these really awesome robots likes the sun , for obvious reasons . in fact , increasing photovoltaic energy and the prospect of it is the only personal value of each robot and potentially the only social one . possible extensions of the design to produce utility might include providing a power connection so that if the battery is charged and the risk of discharge is low , the robot can offload some of its energy to charge other devices . with a sufficient robot population , they can significantly feed a power grid . this is also a great ai research platform . the symmetry reduces the cost of manufacture since many parts are in common between the eight faces and many others are in common between the six legs . the symmetry also reduces the complexity of control system hardware and software and therefore the ai system complexity . the single mindedness , the self sufficiency , the potential for social collaboration and specialization , and other features lend themselves to a plethora of ai research objectives that have both commercial and scientific value . ai design we have eight pixels that also provide power , so their light detection and their charge potential are coincident . we have six legs , each of which have four freedoms of motion . hip bend hip rotation knee bend break pressure radial and linear control is through sequences of cubic splines of the form $ $ \lambda = \lambda_{\emptyset } + v_i t + a_i t^2 + b_i t^3 \\ s_i \leq t_i & lt ; u_i \ ; \text{,}$$ where $ i$ is the spline index in the stream of splines and $ s$ and $ u$ are the limits of time for each . the coefficients must be calculated so that the concatenation of any two adjacent splines in the sequence produce both continuity and smoothness at the intersection . two hyper - parameters , maximum absolute velocity and maximum absolute acceleration , for each of the four freedoms of motion can not be exceeded . the motor control system handles the execution of the sequences from a queue , so the ai merely has to work to keep the queue from running to empty . in addition to the eight light intensity inputs measured at the photovoltaic cells , there are the battery condition indicators , which consists of voltage and inferred internal resistance . there is also indication of the time before the queue empties reported back from each motor control interface , along with a positioning fault indication for each . this means there are 6 x 4 x 4 = 96 numerical outputs and 8 + 2 + 2 x 96 = 202 numerical inputs , but there is so much symmetry , there may be many valid ways to exploit symmetry as alphazero did with go , such that learning and continuous control is resource thrifty . what 's the ai design that goes in between to produce the walking , energy acquisition planning , and coasting to conserve energy , and what would the reward function look like ? 1 footnotes [ 1 ] if these questions were answered , future enhancements to the design may lead to collaborative behavior , climbing , and secondary goals like helping others of its kind to avoid discharge or providing extra energy to other systems or robot species . [ 2 ] a picture of a web showing the nimble mid - air construction capabilities of spiders .",4302,4302,2019-03-05T00:44:34.457,2019-03-05T00:44:34.457,sun loving social ai spider robot ?,deep-learning ai-design robotics collaboration planning,1,3,1
2836,10239,1,10245,2019-01-27T14:31:51.263,0,43,"in my input tensor , i would like to use both integer values as well as booleans . for example , if there is a spelling difference between 2 texts , i want to set the value to true , and otherwise false . in the same tensor , i would like to assign a value to , for example , the maximum number of consecutive messages , which will be an integer . am i allowed to use 0 's and 1 's for the booleans together with integers , or will it have any negative impact on the working of the network ? the ann wo nt see any difference between the binary and nonbinary values , but is it a problem ?",21788,1641,2019-02-26T20:56:38.833,2019-02-26T20:56:38.833,nonbinary and binary values in input tensor,neural-networks,1,2,
2837,10244,1,10246,2019-01-27T18:26:37.263,0,34,"say i want to train a nn that generates outputs of some sort ( say , even numbers ) . note that the network does not classify outputs , but , rather generates the output . i want let it run forward and generate some number , then either give it a positive reward of 1 for an even number , and a reward of -1 for an odd number , to make i output only even numbers over time . what would be an architecture for such a nn ? i am getting caught in the part where here is actually no input , and i ca n't really start with a hidden layer , can i ? i am quite confused and would appreciate guidance",21645,,,2019-01-27T19:24:00.567,how to ( theorically ) build a neural network with input of size 0 ?,neural-networks machine-learning,1,0,
2838,10247,1,,2019-01-27T20:02:55.583,1,52,"i am trying to implement neat algorithm in python from scratch . however , i am stuck . when a new innovation number is created it has two nodes which represents the connection . also this innovation number has a weight . however , i know that innovation numbers are global variables , in other words when a innovation number is created , ex . innovation id:1 - node:1 to node:4 - weight : 0.5 it will have a i d which will be used by other connections to represent the connection between node:1 to node:4 . when this innovation is used by another neural network , will it also use the weight of the innovation 1 , which is 0.5 in this example ?",21517,2444,2019-02-28T22:49:45.980,2019-03-30T23:00:42.300,are innovation weights shared in the neat algorithm ?,neural-networks neat,1,0,
2839,10248,1,,2019-01-27T20:21:00.117,2,29,"i 'm interested in creating a convolutional neural network or lstm to locate text in an image . i do n't want to ocr the text yet , just find the text regions . yes , i know tesseract and other systems can do this , but i want to learn how it works by building my own . all of the tutorials and articles i 've seen so far have the cnn output to a classification - "" image contains a cat "" , "" image contains a dog "" . okay that 's nice , but it does n't say anything about where it was found . can anyone point me to some information that describes the output layer of a nn that can give location information ? like , x - y co - ordinates of text boxes ?",21796,,,2019-01-27T20:21:00.117,how does a neural network output text box location data ?,convolutional-neural-networks lstm ocr,0,0,
2840,10259,1,10260,2019-01-28T04:52:15.480,5,54,"i needed to make a system for recognizing people based on hundreds of text by finding similarities in their written text grammatically or similarities between words they choose for writing , i do n't want it so accurate but wanted to know if it is possible . for example , finding one person with two account or more on a forum or something in that case ( texts already gathered ) . i 'm just wondering if it 's possible and what field should i research in for .",21811,20322,2019-05-17T20:27:12.287,2019-05-17T20:27:12.287,is it possible to find same persons based on text analyzing ?,neural-networks ai-basics,1,0,3
2841,10264,1,10269,2019-01-28T09:27:59.997,1,55,"i have the following program for my neural network : n_steps = 9 n_inputs = 36 n_neurons = 50 n_outputs = 1 n_layers = 2 learning_rate = 0.0001 batch_size = 100 n_epochs = 1000#200 train_set_size = 1000 test_set_size = 1000 tf.reset_default_graph ( ) x = tf.placeholder(tf.float32 , [ none , n_steps , n_inputs],name=""input "" ) y = tf.placeholder(tf.float32 , [ none , n_outputs],name=""output "" ) layers = [ tf.contrib.rnn.lstmcell(num_units=n_neurons,activation=tf.nn.relu6 , use_peepholes = true , name=""layer""+str(layer ) ) for layer in range(n_layers ) ] layers.append(tf.contrib.rnn.lstmcell(num_units=n_neurons,activation=tf.nn.relu6 , use_peepholes = true , name=""layer""+str(layer ) ) ) multi_layer_cell = tf.contrib.rnn.multirnncell(layers ) rnn_outputs , states = tf.nn.dynamic_rnn(multi_layer_cell , x , dtype = tf.float32 ) stacked_rnn_outputs = tf.reshape(rnn_outputs , [ -1 , n_neurons ] ) stacked_outputs = tf.layers.dense(stacked_rnn_outputs , n_outputs ) outputs = tf.reshape(stacked_outputs , [ -1 , n_steps , n_outputs ] ) outputs = outputs[:,n_steps-1 , : ] i want to know whether my network is fully connected or not ? when i try to see the variables , i see : multi_layer_cell.weights the output is : [ & lt;tf.variable ' rnn / multi_rnn_cell / cell_0 / layer0 / kernel:0 ' shape=(86 , 200 ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_0 / layer0 / bias:0 ' shape=(200 , ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_0 / layer0 / w_f_diag:0 ' shape=(50 , ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_0 / layer0 / w_i_diag:0 ' shape=(50 , ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_0 / layer0 / w_o_diag:0 ' shape=(50 , ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_1 / layer1 / kernel:0 ' shape=(100 , 200 ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_1 / layer1 / bias:0 ' shape=(200 , ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_1 / layer1 / w_f_diag:0 ' shape=(50 , ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_1 / layer1 / w_i_diag:0 ' shape=(50 , ) dtype = float32_ref&gt ; , & lt;tf.variable ' rnn / multi_rnn_cell / cell_1 / layer1 / w_o_diag:0 ' shape=(50 , ) dtype = float32_ref&gt ; ] i did n't understood whether each layer is getting the complete inputs or not . i want to know whether the following figure is correct for the above code : if this is not then what is the figure for the network ? please let me know .",9126,9126,2019-01-28T09:47:23.183,2019-01-28T18:23:25.827,is my neural network program fully connected ?,neural-networks machine-learning ai-design lstm,1,2,
2842,10265,1,10297,2019-01-28T10:16:21.447,1,57,"is there a connection between the approximator network sizes in a rl task and the speed of convergence to an ( near ) optimal policy or value function ? when thinking about this , i came across the following thoughts : if the network would be too small , the problem wo n't get enough representation and would never be solved , and the network would converge to its final state quickly . if the network would be infinitely big ( assuming no vanishing gradients and the likes ) , the network would converge to some ( desirable ) over - fitting , and the network would converge to its final state very slowly , if at all . this probably means there is some golden middle ground . which leads me to the interesting question : 4 . assuming training time is insignificant relative to running the environment ( like in real life environments ) , then if a network of size m converges to an optimal policy in average after n episodes , would changing m make a predictable change on n ? is there any research , or known answer to this ? how to know that there is no more need to increase the network size ? how to know if the current network is too large ? note : please regard question 4 as the main question here .",21645,2444,2019-02-20T16:42:28.743,2019-02-20T16:42:28.743,is there a relation between the size of the neural networks and speed of convergence in deep reinforcement learning ?,neural-networks machine-learning reinforcement-learning deep-rl,1,3,
2843,10267,1,,2019-01-28T14:48:27.950,0,13,"i have 2 tabular datasets , one is clean and one is drifted . they are records of sensor measurements . i move the sensor around in the room and collected thousands of measurements . i have a sensor that is supposed to track a signal from just one main source . but there are many sources that interfere with the main source in the dirty room . in clean data , i have only one source when recording the measurements . in dirty data , i have many interference sources when recording the measurements . e.g. if the clean data has only 2 features , one of the row is 5,10 . when it 's affected by other sources , their value can be 7 and 8 or something like that . but it 's not just a white noise that disappears later . it 's a permanent drift that will never be gone unless i eliminate the interfering sources from the room . that means if i measure the value it will always report 7 or 8 in the same dirty room every single time . i want to separate out only the main source 's measurement . so given 7 and 8 , i want the output to be 5,10 . but i do n't have the input / output pair to train a machine learning model . so this should be an unsupervised learning problem . my idea is to train an unsupervised model to know what a clean data looks like . then when given dirty data , it can convert the dirty data to a corresponding clean data like the example i gave . so the model essentially learns to ignore all the other sources and report me the main source only . the number of sources is not known ( maybe the clean area i thought was clean is actually containing other sources but i am fine if the model can convert the dirty to the almost clean data i provided ) please give me a link or topics about this or explain your idea on how to make this unsupervised model . ps . there are a lot more than 2 features for the sensor , that 's why i think it 's possible to know the clean data from the dirty one . and i also have time series data as i move the sensor around collecting measurements , let me know if you know how to make use of that to clean the data .",20819,20819,2019-01-28T14:53:53.567,2019-01-28T14:53:53.567,how to remove unwanted signals from the sensor measurement ?,machine-learning deep-learning tensorflow unsupervised-learning,0,0,
2844,10272,1,,2019-01-28T20:27:34.760,7,89,can someone explain what is the process of learning ? what does it mean to learn something ?,21832,2444,2019-05-27T16:30:07.097,2019-05-27T16:30:07.097,what does learning mean ?,machine-learning philosophy terminology definitions theory,4,0,3
2845,10281,1,10283,2019-01-29T01:57:42.183,2,32,"i 'm doing transfer learning using inception on tensorflow . the code that i used for training is https://raw.githubusercontent.com/tensorflow/hub/master/examples/image_retraining/retrain.py if you take a look at the argument parser section at the bottom of the code , you will find these parameters : testing_percentage validation_percentage test_batch_size validation_batch_size so far , i understand that testing and validation percentage is the amount of images that we want to train at 1 time . but i do n't really understand the use of test batch size and validation batch size . what is the difference between percentage and batch size ?",20612,,,2019-01-29T08:14:28.800,"testing , validation percentage & test , validation batch size difference ?",convolutional-neural-networks python tensorflow,1,0,
2846,10282,1,10286,2019-01-29T04:38:40.830,3,113,"the address https://discuss.openai.com/ at this moment does not work . to be more precise , "" discuss.openai.com "" does not resolve to a ip address , even using proxies . google cached pages shows that at least until 2018 - 12 - 23 the was able to be accessed by google crawlers . i tried to find some public official response to this , but i 'm not able . my question is : what is the status of discuss.openai.com in 2019 ? one extra important question is , if this is permanent , what will happen to all content from people who trusted openai to host a online forum ? ( for example , at least exist a place to have access archived content ? note : possible related question ( but for another subdomain ) is why did the openai&#39;s gym website close ? edit 1 : thanks to @neil slater , i looked on my e - mail history , and found this . not sure if is the full text of the thread of if is just this . but the text for who is using screen readers or automatic translation is : date : 2018 - 07 - 06 category : general discussion topic : forum closing in a week message : we will be closing these forums in ~1 week , as we no longer have time to maintain them . please save any content that you wish to have access to .",11692,1671,2019-01-30T17:33:15.203,2019-01-30T17:43:22.657,status of discussion forum discuss.openai.com in 2019,ai-community,3,2,
2847,10287,1,,2019-01-29T09:54:32.667,1,41,"language is how we communicate to one other in the lab , at home , and in educational settings . word selection matters . sometimes we do n't select well . if we are writers , we may edit a word later or the editor may , but once picked and published , the something we have given a particular name may stick , and we ca n't change it easily . an example is the gan ( generative adversarial networks ) . the generative artificial network ideas predated ian j. goodfellow et . al . 's 2014 paper introducing gans . some of those ideas are listed in the paper 's bibliography . in the paper , they state , generative adversarial networks has been sometimes confused with the related concept of adversarial examples . why , after admitting that it is a cause for confusion , did they pick the term adversarial for the second adjective in the name of their design ? they also write , in the proposed adversarial nets framework , the generative model is pitted against an adversary : a discriminative model ... what does it mean to be pitted against an adversary ? the straightforward answer is that we place two entities $ a$ and $ b$ such that the pursuit of the objectives of $ a$ by $ a$ hurts or destroys $ b$ and vice versa . even if not a game of annihilation or at least a zero sum game , there is clearly no win - win scenario , where $ a$ and $ b$ both get much of what they want . mutual benefit and the bargaining or negotiation process , in game theory , is called a nash equilibrium 1 . it has been refined 2 since nash , but the basic idea of positive sum games as being the basis of bargaining in purchases and treaty negotiations remains a primary political and economic concept . when trade occurs or agreements are reached , such is done so that both parties benefit , even though there is a push - pull process in reaching the equilibrium . in the gan design , each network 's objective is represented by its loss function , and nothing else . the networks can not die , kill , dismember , hate , or even get angry at one another . there is no adrenaline or testosterone . they are not proud of themselves or their country . is n't the term blatantly anthropomorphic ? the mathematics reveals , as so often it does , the truth . the paper further explains , if $ g$ and $ d$ have enough capacity , and at each step of algorithm 1 , the discriminator is allowed to reach its optimum given $ g$ , and p_g is updated so as to improve the criterion $ $ \mathbb{e}_{x ∼ p_{data } } \big [ \log d_g^∗ ( x ) \big ] + \mathbb{e}_{x ∼ p_g } \big [ \log ( 1 − d_g^∗ ( x ) ) \big ] \ ; \text { , } $ $ then p_g converges to p_{data}. note that , in this convergence , neither $ g$ nor $ d$ are hurt or destroyed as in chess , where pieces are lost and concession is symbolized by tipping one 's own king , representing that the king has fallen , meaning killed . both networks are in nash equilibrium . some might call the interaction between $ g$ and $ d$ negotiation and the result harmony . in straight mathematical and unbiased terms , gans are successful because of convergence collaboration . if each network concedes a portion of its loss optimality so that an equilibrium can be reached , then we have a nash equilibrium , which is not zero sum . it is win - win . if both converge fully , then the win - win is even stronger . the collaboration mushrooms to a win - win - win - win scenario once the ai engineer and the project stakeholder are added in , since the consequence of the gan equilibrium is the generated output the stakeholder wants and for which the ai engineer receives monetary and professional appreciation . what causes these militant terms and negatively charged , anthropomorphic ways of looking at benign processes ? for the case of the 2014 paper , is the answer related to the fact that , since the cyber conflicts between russia and estonia , the term adversarial networks was common in papers arising from or vying for military funding ? 3 could that word association been subliminally introduced from ian goodfellow 's subconscious ? this is not to fault him or his colleagues but to get to the bottom of the conflict between the mathematics which proves non - adversarialism in a paper that names the algorithm arising from the math as adversarial . in the case of chess , the huns attacked the gupta empire ( now northern india ) in the fifth century and chaturanga , the forerunner of chess , was invented in that same region in the sixth century . that would be immaterial if chess was n't the thing most associated with ai until gans started generating images . stepping back from anthropomorphism and mathematics , from a sociology perspective , enemies are just regular people that resent others with greater abilities , social grace , food supply , and other goods . some believe that , from god 's perspective , enemies are people sent so we can learn how to love and show compassion unconditionally . if we apply this particular belief to populations , the purpose of adversarialism is solely and precisely to lay it down , along with its weapons . life for most on planet earth starts with some difficulty and increases gradually , with perhaps a decade or two of reprieve , but then increases more quickly until it ends . one interesting and multidimensional question for each individual is how we act and speak to one another under those universal conditions . and that intelligence is ancient and far from artificial . which brings us back to the primary question . is ai research culture predisposed to adversarialism while even the mathematics is more friendly ? the selection of words is how we communicate , and the choices we make perpetuate things that are good for ai and the world and things that may not be so good . footnotes [ 1 ] even nash 's paper , non - cooperative games , annals of mathematics , 1951 , describes an equilibrium with mutual advantage but uses the term non - cooperative in the paper title instead of game advantage equilibria . [ 2 ] k binmore , a rubinstein , and a wolinsky , in their the nash bargaining solution in economic modelling , rand journal of economics , 1986 , "" the players are induced to reach an agreement by their impatience for the [ mutually beneficial ] outcomes . "" [ 3 ] with regard to the cyber conflicts between russia and estonia , the bbc news stated , "" but since [ the 2007 cyber estinian incident , ] cyber warfare has been used all over the world , including in russia 's war with georgia in 2008 , and in ukraine . "" cyber has become a really serious tool in disrupting society for military purposes , "" says tanel sepp , ... a cyber security official at estonia 's ministry of defense . """,4302,4302,2019-01-29T10:21:57.277,2019-01-29T19:39:59.707,is ai research culture predisposed to adversarialism while even the mathematics is more friendly ?,game-ai philosophy ai-community risk-management,1,0,
2848,10288,1,,2019-01-29T10:26:12.727,0,11,"as an experiment , i want to teach an ann to play the game of nim . the normal game is between two players and played with three heaps of any number of objects . the two players alternate taking any number of objects from any single one of the heaps . the goal is to be the last to take an object . the game is easily solvable and i already wrote a small bot that can play nim perfectly to provide data sets for supervised learning . now i am struggling with the design question , how should i output the solution to a specific board state . the answer always consists of two components : how many stones to take ( a more or less arbitrary integer value ) which heap to take the stones from ( the index of the heap ) what are available design choices in this regard and is there a state - of - the - art design for this type of problem ?",9161,,,2019-01-29T10:26:12.727,how to design an ann to give an answer that includes two different components ?,ai-design,0,0,
2849,10289,1,12354,2019-01-29T11:08:22.457,5,181,by reading the abstract of neural networks and statistical models paper it would seem that anns are statistical models . in contrast machine learning is not just glorified statistics . i am looking for a more concise / summarized answer with focus on anns .,21269,9947,2019-01-29T11:24:22.153,2019-05-16T15:21:45.917,are neural networks statistical models ?,neural-networks machine-learning deep-learning statistical-ai,3,0,6
2850,10294,1,,2019-01-29T14:23:01.237,1,26,"i am doing neural machine translation task from language s to language t via interlingua l. so - there is the structure : s -&gt ; encoding of s ( crisp ) -&gt ; s - l encoder -&gt ; s - l decoder -&gt ; encoding of l ( non - crisp , coming from decoder ) -&gt ; l -&gt ; encoding of l ( crisp ) -&gt ; l - t encoder -&gt ; l - t decoder -&gt ; encoding of t ( non - crisp , coming from decoder ) -&gt ; t all of this can be implemented in pytorch more or less adapting the usual encoder - decoder nmt . so , the layer of interlingua l acts as a somehow symbolic / discrete layer inside the whole s - l - t neural network . my question is - how such system can be trained in end - to - end ( s - t ) manner ? the gradient propagates from the t to the l and at the l one should do some kind of symbolic gradient ? i. e. one should be able do compute the difference l1-l2 ? i am somehow confused by such setting . my question is - is there similar networks which contain the symbolic representation as the intermediate layer and how one can train such system . i have heard about policy gradients but are they relevant to my setting ? essentially - if i denote some neural network by symbols x(wi)y , then the training of this network means , that i change wi and x stays intact . i.e. the last member of backpropagation equation has the form d ... /dw1 . but if i combine ( chain ! ) 2 neural networks x(wi)y - y(wj)z , then the the last backpropagation term for the y(wj)z has the form ( d ... /dw1+d ... /dy ) and hence both the w1 and y should be changed / updated by the gradient descent too . so , does n't some ambiguity arise here ? is such chaining of neural networks possible ? is is possible to train end - to - end chains of neural networks ? i am also thinking about use of evolutionary training .",8332,8332,2019-01-29T14:44:13.997,2019-01-29T15:41:08.930,neural network with logical hidden layer - how to train it ? is it policy gradient problem ? chaining nns ?,neural-networks training gradient-descent policy-gradients machine-translation,1,0,
2851,10300,1,10330,2019-01-29T22:09:05.903,1,67,"i 'm trying to train a neural network on evaluating chess positions if rather white ( 0.0 ) or black would win ( 1.0 ) currently the input consists of 4 bits per chess field ( piece i d 0 - 12 , equals 64 * 4 ) . factors like castling are being ignored for now . also , all training sets are random positions from popular games where it 's white 's turn and the desired output is the outcome of the game ( 0.0 , 0.5 , 1.0 ) . are my input values the right choice ? how many hidden layers / neurons for each layer should be used and what 's the best learning rate ? what type of nn 's and which activation function would you recommend for this project ?",19783,,,2019-01-31T15:23:00.843,choosing the right neural network settings,neural-networks machine-learning convolutional-neural-networks,2,0,
2852,10301,1,,2019-01-29T23:29:56.810,1,24,"i would like to use chatbots for a research project . the chatbots should trigger different types of emotions during a 10 minute period of chatting . i 'm thinking about using mitsuku , zo or cleverbot ( i prefer using facebook messenger or skype ) . one option could be to let the participants do random smalltalk chatting . on the other hand , one could let the participants talk about specific topics where the bot is known to react differently ( e.g. offensive , funny etc . ) does somebody have a good idea how different emotions can be triggered using such a chatbot ( or at least one specific emotion ) ? additionally , an interesting alternative are game - based chatbots ( like lifeline ) . does somebody know a good chatbot , ideally for facebook messenger or skype , which is game - based ( e.g. a playable , branching story ) ? the interaction should be text - based ( i.e. writing with the chatbot ) and not only pressing buttons . i 'm thankful also for recommendations of other chatbots which could be useful .",21103,,,2019-01-29T23:29:56.810,how to use chatbots to trigger emotions ?,intelligent-agent chat-bots emotional-intelligence,0,0,
2853,10302,1,,2019-01-30T01:11:23.697,1,27,"i am new in the field of genetic algorithms , and i want to learn to use them in practice . how the genetic algorithm work and why it is applied ?",21865,9515,2019-01-30T17:28:08.980,2019-01-30T17:28:08.980,what is the genetic algorithm for ?,algorithm genetic-algorithms,0,1,
2854,10303,1,,2019-01-30T08:20:37.823,3,147,"from what i understand , monte carlo tree search algorithm is a solution algorithm for model free reinforcement learning ( rl ) . model free rl means agent does nt know the transition and reward model . thus for it to know which next state it will observe and next reward it will get is for the agent to actually perform an action . my question is : if that is the case , then how come the agent knows which state it will observe during the rollout , since rollout is just a simulation , and the agent never actually perform that action ? ( it never really interact with the environment : e.g it never really move the piece in a go game during rollout or look ahead , thus can not observed anything ) . it can only assume observing anything when not actually interacting with environment ( during simulation ) if it knows the transition model as i understand it . the same arguments goes for the rewards during rollout/ simulation . in this case , does nt rollout in monte carlo tree search algorithm suggests that the agent knows the transition model and reward model and thus a solution algorithm for model based reinforcement learning and not model free reinforcement learning ? * * it makes sense in alphago , since the agent is trained to estimate what it would observed . but mcts ( without policy and value netwrok ) method assumes that agent knows what it would observed even though no additional training is included .",21872,21872,2019-01-30T09:47:12.167,2019-02-08T21:37:12.063,rollout algorithm like monte carlo search suggest model based reinforcement learning ?,reinforcement-learning models monte-carlo-tree-search,2,4,
2855,10306,1,10481,2019-01-30T11:39:25.787,3,153,"status : for a few weeks now , i have been working on a double dqn agent for the pongdeterministic - v4 environment , which you can find here . a single training run lasts for about 7 - 8 million timesteps ( about 7000 episodes ) and takes me about 2 days , on google collab ( k80 tesla gpu and 13 gb ram ) . at first , i thought this was normal because i saw a lot of posts talking about how dqns take a long time to train for atari games . revelation : but then after cloning the openai baselines repo , i tried running python -m baselines.run --alg = deepq --env = pongnoframeskip - v4 and this took about 500 episodes and an hour or 2 to converge to a nice score of +18 , without breaking a sweat . now i 'm convinced that i 'm doing something terribly wrong but i do n't know what exactly . investigation : after going through the dqn baseline code by openai , i was able to note a few differences : i use the pongdeterministic - v4 environment but they use the pongnoframeskip - v4 environment i thought a larger replay buffer size was important , so i struggled ( with the memory optimization ) to ensure it was set to 70000 but they set it to a mere 10000 , and still got amazing results . i am using a normal double dqn , but they seem to be using a dueling double dqn . results / conclusion i have my doubts about such a huge increase in performance with just these few changes . so i know there is probably something wrong with my existing implementation . can someone point me in the right direction ? any sort of help will be appreciated . thanks !",21513,,,2019-02-10T03:56:15.743,"each training run for ddqn agent takes 2 days , and still ends up with -13 avg score , but openai baseline dqn needs only an hour to converge to +18 ?",deep-learning reinforcement-learning dqn open-ai deepmind,2,0,2
2856,10308,1,10326,2019-01-30T13:29:47.810,2,31,"in lms(least mean square ) since , we use a quadratic error function , and quadratic functions are generally parabola in ( some convex like shape ) . i wonder whether that is the reason why we use least square error metric ? if that is not the case(its not always convex or reason why we use lms ) , what is the reason then ? why this metric changes for deep learning / neural networks but works for regression problems ? [ edit ] : will this always be a convex function or is there any possibility that it will not be convex ?",18956,18956,2019-01-31T17:18:02.067,2019-01-31T17:18:02.067,"will lms always be convex function ? if yes , then why do we change it for neural networks ?",neural-networks deep-learning gradient-descent linear-regression,1,0,
2857,10313,1,,2019-01-30T17:56:21.073,0,16,"gradient training changes indiscriminately all the weights and nodes of the neural network . but one can imagine the situations when the training should be shaped , e.g. : one can put constraints on some of the weights . e.g. human brain contains regions whose inner connections are more dense that external connections with different regions . one can try to mimic this region - shaped structure in neural networks as well and hence one can require that inter - regional weights ( in opposit to intra - regional weights ) are close to zero ( except , possibly , for some channels among regions ) ; one can put constraints on some of the weights in such manner that some layer of neurons have specific structure . e.g. consider the popular encoder - decoder architecture of neural machine translation e.g. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html we can see that that the whole output of the encoder is expressed as a single layer of neurons which is forwarded to the input of the decoder . so - one can require that the set of all the possible outputs of the encoder ( e.g. the possible values of this single layer of the neurons ) forms some kind of structure , e.g. some grammar of some interlingua . this example is for illustration only , i have in mind more complex neural network which has one layer of neurons which indeed should output the encoded words of some interlingua . so , one is required to guid all the weights of the encoder in such manner that this single layer has only allowable values . so - my question is - are there methods that guide the gradient descent training with additional information about the weights or about the nodes ( i.e. about the whole subsets of weights that have some impact on specific layer of nodes ) ? e.g. about methods that impress the region structure on the neural network or that constrains the values of some nodes to be in specific range only ? of course , it is quite easy to include such constraints in evolutionary neural networks - one can simply reject the neural networks with weights that violates the constraints . but is it possible to do this in gradient - like training ?",8332,,,2019-01-30T17:56:21.073,how to shape the weights or nodes during gradient training of neural network ? training with constraints ?,neural-networks training gradient-descent evolutionary-algorithms,0,0,
2858,10315,1,,2019-01-30T18:22:25.850,1,31,"i am creating a vae for time series data using cnns . the data has 4800 timesteps and 4 features . it is standardized and normalized . the network i am using is implemented in keras as follows . i have used a mse reconstruction error : # network parameters ( _ , seq_len , feat_init ) = x_train.shape input_shape = ( seq_len , feat_init ) intermediate_dim = 512 batch_size = 128 latent_dim = 10 epochs = 10 img_chns = 3 filters = 32 num_conv = ( 2 , 2 ) epsilon_std = 1 inputs = input(shape = input_shape ) conv1 = conv1d(16 , 3 , 2 , padding='same ' , activation = ' relu ' , data_format = ' channels_last')(inputs ) conv2 = conv1d(32 , 2 , 2 , padding='same ' , activation = ' relu ' , data_format = ' channels_last')(conv1 ) conv3 = conv1d(64 , 2 , 2 , padding='same ' , activation = ' relu ' , data_format = ' channels_last')(conv2 ) flat = flatten()(conv3 ) hidden = dense(intermediate_dim , activation='relu')(flat ) z_mean = dense(latent_dim , name = ' z_mean')(hidden ) z_log_var = dense(latent_dim , name = ' z_log_var')(hidden ) def sampling(args ) : z_mean , z_log_var = args epsilon = k.random_normal(shape=(k.shape(z_mean)[0 ] , latent_dim ) , mean=0 . , stddev = epsilon_std ) return z_mean + k.exp(z_log_var ) * epsilon z = lambda(sampling , output_shape=(latent_dim,))([z_mean , z_log_var ] ) decoder_hid = dense(intermediate_dim , activation='relu')(z ) decoder_upsample = dense(38400 , activation='relu')(decoder_hid ) decoder_reshape = reshape((600,64))(decoder_upsample ) deconv1 = conv1d(filters=32 , kernel_size=2 , strides=1 , activation=""relu "" , padding='same ' , name='conv - decode1')(decoder_reshape ) upsample1 = upsampling1d(size=2 , name='upsampling1')(deconv1 ) deconv2 = conv1d(filters=16 , kernel_size=2 , strides=1 , activation=""relu "" , padding='same ' , name='conv - decode2')(upsample1 ) upsample2 = upsampling1d(size=2 , name='upsampling2')(deconv2 ) deconv3 = conv1d(filters=8 , kernel_size=2 , strides=1 , activation=""relu "" , padding='same ' , name='conv - decode3')(upsample2 ) upsample3 = upsampling1d(size=2 , name='upsampling3')(deconv3 ) x_decoded_mean_squash = conv1d(filters=4 , kernel_size=4 , strides=1 , activation=""relu "" , padding='same ' , name='conv - decode4')(upsample3 ) class customvariationallayer(layer ) : def _ _ init__(self , * * kwargs ) : self.is_placeholder = true super(customvariationallayer , self).__init__(**kwargs ) def vae_loss(self , x , x_decoded_mean_squash ) : x = k.flatten(x ) x_decoded_mean_squash = k.flatten(x_decoded_mean_squash ) xent_loss = mse(x , x_decoded_mean_squash ) kl_loss = - 0.5 * k.mean(1 + z_log_var - k.square(z_mean ) - k.exp(z_log_var ) , axis=-1 ) return k.mean(xent_loss + kl_loss ) def call(self , inputs ) : x = inputs[0 ] x_decoded_mean_squash = inputs[1 ] loss = self.vae_loss(x , x_decoded_mean_squash ) self.add_loss(loss , inputs = inputs ) return x outputs = customvariationallayer()([inputs , x_decoded_mean_squash ] ) # entire model vae = model(inputs , outputs ) vae.compile(optimizer='adadelta ' , loss = none ) vae.summary ( ) i wanted to ask whether it is possible for the network to nearly perfectly reconstruct the test timeseries when passed through the entire vae network , but still output junk when using a random normal input . for further details , here is one of the inputs and outputs when passing a test signal through the network . here is a reconstruction generated purely from a random sample . how can this be ? even if there was a posterior collapse , the vae should still be able to generate a good output sample with a random input . to further test this i decided to split the network into two parts ( encoder and decoder ) , and then pass the test image through it . the encoder and decoder networks were made by simply splitting the trained vae network as follows : idx = 9 input_shape = vae.layers[idx].get_input_shape_at(0 ) layer_input = input(shape=(input_shape[1 ] , ) ) x = layer_input for layer in vae.layers[idx:-1 ] : x = layer(x ) decoder = model(layer_input , x ) decoder.summary ( ) idx = 0 input_shape = vae.layers[idx].get_input_shape_at(0 ) layer_input = input(shape = input_shape ) x = layer_input for layer in vae.layers[idx + 1:7 ] : x = layer(x ) encoder = model(layer_input , x ) encoder.summary ( ) interestingly , i also got junk output here . i 'm not sure how it is possible . if the model itself is getting a near perfect reconstruction , surely just passing an image through the encoder , extracting the latent mean , and then passing that latent mean through the decoder should also create a near perfect image ? is there something i am missing here ?",21883,,,2019-01-30T18:22:25.850,how can vae have near perfect reconstruction but still output junk when using random noise input,convolutional-neural-networks keras latent-variable,0,0,
2859,10318,1,,2019-01-31T01:33:56.620,2,55,"can i treat a stochastic policy ( over a finite action space of size $ n$ ) as a deterministic policy ( in the set of probability distribution in ) ? it seems to me that nothing is broken by making this mental translation , except that the "" induced environment "" now has to take a stochastic action and spit out the next state , which is not hard using on the original environment . is this legit ? if yes , how does this "" deterministify then ddpg "" approach compare to , for example , a2c ?",21892,2444,2019-04-04T16:37:46.727,2019-04-04T16:37:46.727,can i use deterministic policy gradient methods for stochastic policy learning ?,reinforcement-learning policy-gradients ddpg,0,1,
2860,10322,1,10328,2019-01-31T10:35:34.240,2,196,"i 'm doing bachaleor thesis on traffic sign detection using single shot detector called yolo . these single shot detectors can perform detection of objects in image and so they have specific way of training , ie . training on full images . that s quite problem for me , because the biggest real dataset with full traffic sign images is belgian one with 9000 images in 210 classes , which is unfortunately not enough to train good detector . to overcome this problem , i 've created datasetgenerator , which does quite good job in generating synthetic datasets , you can see in the results directory . recently i came across gan 's which can ( besides others ) generate or extend existing dataset and i would like to use these networks to compare with my dataset generator . i 've tried this introduction to gans succesfully . the problem is it 's unsupervised learning and so there are no annotations . it means it 's able to extend my dataset of traffic signs , but the generated dataset wo n't be annotated at all , which is problem . so my question is : is there any way how to use gan 's to extend my dataset of full traffic sign images with annotations of traffic sign class and position ? actually the class is not important , because i can do it separately for each class , but what matters is the position of traffic sign in generated image .",18760,,,2019-01-31T13:03:24.213,using gan 's to generate dataset for cnn training,convolutional-neural-networks datasets generative-adversarial-networks,2,0,
2861,10323,1,10324,2019-01-31T11:00:04.067,0,74,"classical logic is an important part of university curriculum . it is about the distinction between zero and one and is useful to get a deeper understanding how a computer is working . if a student becomes familiar with boolean tables they can calculate for example that 1 0 is equal to 0 . in certain programming languages , the byte datatype consists of 1 byte which contains 8 bit and each of them can have the values 0 or 1 . such information can be explained very well to students . if someone has n't understood the concept , they can ask and the teacher will explain it again . the principle in higher education is , that assured knowledge of the world is taught . the students can profit from it , and are able to build systems in the real world . they can use their knowledge about 0 and 1 to write software . the underlying mathematics which is logic , boolean algebra and analysis helps the student to pass exams and be prepared for future challenges at work . fuzzy logic contradicts the principles of higher education . it is published in predatory journals which are not working with quality standards and have no peer - review . it is located outside the scientific community . is it possible to integrate fuzzy knowledge into the normal curriculum ? will the students profit from uncertainty ?",11571,2193,2019-01-31T11:48:47.267,2019-03-02T12:01:10.293,should fuzzy logic be taught at university ?,fuzzy-logic academia,1,2,
2862,10327,1,,2019-01-31T12:52:26.223,1,94,"i recently came across a paper on deep ranking . i was wondering whether this could be used to classify book covers as book titles . ( for example , if i had a picture for the cover of the second hp book , the classifier would return harry potter and the chamber of secrets . ) for example , say i have a dataset of book covers along with the book titles in text . could that data set be used for this deep ranking algorithm , or is there a much better way to approach my problem ? i 'm quite new to this whole thing , and this one of my first projects in this field . what i 'm trying to create is a mobile app where people can take a picture of a book cover , have an algorithm / neural net classify the title of the book , and then have some other algorithm connect that to the book 's goodreads page . thanks for the help !",21905,16565,2019-02-01T13:34:17.180,2019-02-02T23:08:19.650,deep ranking / best way to classify book covers ?,image-recognition,2,0,
2863,10329,1,10334,2019-01-31T13:23:24.273,2,95,"i 'm training an agent using rl and the sarsa function to update a q function , but i 'm confused how you handle the final state . in this case when the game ends and there is no s ' . for example , the agent performed an action based on the state s , and because of that the agent won or lost and there is no s ' to transition to . so how do you update the q function with the very last reward in that scenario because the state has n't actually changed . in that case s ' would equal s even though an action was performed and the agent received a reward ( they ultimately won or lost , so quite important update to make ! ) . do i add an extra inputs to the state agent_won and game_finished and that 's the difference between s and s ' for the final q update ? edit : to make clear this is in reference to a multi - agent / player system . so the final action the agent takes could have a cost / reward associated with it , but the subsequent actions other agents then take could further determine a greater gain or loss for this agent and whether it wins or loses . so the final state and chosen action , in effect , could generate different rewards without the agent taking further actions .",20352,20352,2019-01-31T15:30:12.753,2019-01-31T17:49:33.080,how to define the final / terminal state for q learning ?,machine-learning reinforcement-learning q-learning,1,10,
2864,10336,1,,2019-01-31T22:27:12.157,0,51,"open ai 's ( working ) actor critic code calculates the losses like so : actor_loss = -log_prob * discounted_reward policy_loss = f.smooth_l1_loss(value , torch.tensor([discounted_reward ] ) ) both are different from the regular formulas which are : actor_loss ( parameterized by ): $ log[\pi_\theta(s_t , a_t)]q_w(s_t , a_t)$ critic loss ( parameterized by $ w$ ): $ r(s_t , a_t ) + \gamma q_w(s_{t+1},a_{t+1 } ) - q_w(s_{t},a_{t})$ where $ r(s_t , a_t)$ is the immediate reward following taking the action . meaning , for the actor , the immediate critic evaluation of the transaction was replaced with the discounted reward , and for the critic , the discounted evaluation of the value from the next state $ r(s_t , a_t ) + \gamma q_w(s_{t+1},a_{t+1})$ was replaced by the discounted reward and an l1 loss is then calculated , effectively discarding the sign of the ( equation ) loss . questions : why are the changed equation elements ? why is the sign discarded for the critic loss ?",21645,21645,2019-02-01T18:15:27.543,2019-02-01T18:15:27.543,inconsistent formulas for loss calculation in openai 's actor critic ?,machine-learning reinforcement-learning loss-functions,1,0,
2865,10339,1,,2019-02-01T05:36:14.553,0,67,"i am working on a project where the neural network weights must be quantized on 8 or 16 bits for an embedded platform , thus i will lose some precision . since our platform does not have floating point arithmetic we need to quantize the weights . by quantizing i mean taking the max absolute value of the weights and divide it by the maximum signed number representable on 8 or 16 bits . this operation will give us a quantization factor $ ( qf)$ . the final quantized weights will be integer $ ( value * qf)$ . if my weights are very sparse and have a very bad distribution , i lose more precision . for example , to the left here is the distribution of weights for one layer , and to the right is the distribution of weights after i added to the loss function the kurtosis and skew measures of the weights , and it improved a bit the shape of the distribution while keeping the same accuracy , even a bit higher . does anybody have any other suggestions ? has anyone tackled this problem before ?",21918,9947,2019-04-19T13:47:45.813,2019-04-19T13:47:45.813,tips for keeping the distribution of weights normal,neural-networks hardware quantification,2,7,
2866,10340,1,,2019-02-01T06:44:28.800,1,45,"is the role played by activation function significant only during the training of neural network or they play their role during testing ( after training we supply data for prediction ) the network . i understand that a linear line can not separate data scattered in complex manner but then why we do n't used simple polynomials . why specifically sigmoid , or tanh or relu what exactly they are doing ? what activation functions do when we are supplying data during training and and when we supply test data once we have trained the network and we input test data for prediction ?",21642,21642,2019-02-01T08:23:37.180,2019-03-03T09:01:05.190,what role the activation function plays in the forward pass and how it is different from backpropagation,neural-networks,1,1,
2867,10344,1,,2019-02-01T10:20:44.013,0,94,"i 've watched this video of the recent contest of alphastar vs pro players of starcraft2 , and during the discussion david silver of deepmind said that they train alphastar on tpus . my question is , how is it possible to utilise a gpu or tpu for reinforcement learning when the agent would need to interact with an environment , in this case is the starcraft game engine ? at the moment with my training of a rl agent i need to run it on my cpu , but obviously i 'd love to utilise the gpu to speed it up . does anyone know how they did it ? here 's the part where they talk about it , if anyone is interested : https://www.youtube.com/watch?v=cutmhmvh1qs&amp;t=7030s",20352,,,2019-02-01T16:10:52.227,how does deepmind perform reinforcement learning on a tpu ?,machine-learning reinforcement-learning deepmind,1,3,
2868,10345,1,,2019-02-01T10:24:15.850,1,61,"suppose we have a data set consists of columns transactionid , cardno , transactiondate then how can we calculate the customer purchase interval ( means if customer a purchased on jan 1st and after 10 days he again purchased , and then he again purchased after 15 days . ) and how to predict the next visit of customer a by analyzing the purchasing intervals of customer a. any help will be appreciated .",16770,,,2019-03-05T20:55:25.323,predict customer visit,python forecasting,3,0,
2869,10349,1,,2019-02-01T14:00:40.307,0,9,"i have a sensor that reads electromagnetic field strength from each position . and the field is stable and unique for each position . so the reading is simply a function of the position like this : reading = emf(x , y , z ) the reading consists of 3 numbers ( not position ) . i want to find the inverse function of emf function . which means i want to find function pos that is defined like this : x , y , z = pos(reading ) i do n't have access to both emf and pos function . i think that i want to gradually estimate the pos function using a neural network . so i have input reading and acceleration ax , ay , az of the sensor through space from an imu . the acceleration is not so accurate . i want to use these 2 inputs to help me figure out the position of the sensor over time . you can assume that the starting position is at 0,0,0 on the first reading . in short , input is reading and ax , ay , az on each timestep , output will be adjustment on the weights of pos function or output will be position directly . i 've been reading about slam ( simultaneous localization and mapping ) algorithm and i think that it might help in my case because my problem is probabilistic . if i know accurately the acceleration , i would not need any probability , but the acceleration is not accurate . so i want to know how do i model this problem in term of slam ? i do n't have a camera to do vision - based slam though . why i think this is tractable ? if the first reading is 1,1,1 and the position is at origin 0,0,0 , and i move the sensor , the position can drift because the sensor has never seen other reading before , but after i go back to origin , the reading will be 1,1,1 again so the sensor should report the origin 0,0,0 as output . during the movement of the sensor , the algorithm should filter the acceleration so that all the previous positions makes sense .",20819,,,2019-02-01T14:00:40.307,how to use slam on other sensor other than camera ?,machine-learning deep-learning probabilistic bayes mapping-space,0,0,
2870,10352,1,10362,2019-02-01T17:49:20.307,1,49,"in experience replay , the update rule follows the loss : $ $ l_i(\theta_i ) = \mathbb{e}_{(s_t , a_t , r_t , s_{t+1 } ) \sim u(d ) } \left [ \left(r_t + \gamma \max_{a_{t+1 } } q(s_{t+1 } , a_{t+1 } ; \theta_i^- ) - q(s_t , a_t ; \theta_i)\right)^2 \right ] $ $ i ca n't get my head around the order of calculation of the terms in that equation : an experience element is $ ( s_t , a_t , r_t , s_{t+1 } ) $ where $ s_t$ is the state at time $ t$ $ a_t$ is the action taken from $ s_t$ at time $ t$ $ r_t$ is the reward received by taking that action from $ s_t$ at time $ t$ $ s_{t+1}$ is the next state in the on policy case , as i understand it , q of the equation above is the same q , which is the only approximator . as i understand the algorithm , at time $ t$ we save an experience $ ( s_t , a_t , r_t , s_{t+1 } ) $ . then , later , at time $ t+x$ we attempt to learn from that experience . however , at the time of saving the experience , $ q(s_t , a_t)$ was something different than at the time of attempting to learn from that experience , because the parameters of $ q$ have since changed . this could actually be written as $ q_t(s_t , a_t ) \neq q_{t+x}(s_t , a_t)$ because the q value is different , i do n't see how the reward signal at time $ t$ is of any relevance for $ q_{t+x}(s_t , a_t)$ at $ t+x$ , the time of learning . also , it is likely that following a policy which is derived from $ q_t$ would lead to $ a_{t}$ , whereas following a policy which is derived from $ q_{t+x}$ would not . i do n't see in the experience replay algorithm that the q value $ q_t(s_t , a_t)$ is saved , so i must assume that is is not . why does calculating the q value again at a later time make sense for the same saved reward and action ?",21645,,,2019-02-02T09:45:29.497,when are q values calculated in experince replay ?,machine-learning reinforcement-learning q-learning,2,0,
2871,10360,1,,2019-02-02T08:06:41.317,2,113,"i am reading bayeschess : a computer chess program based on bayesian networks ( fernandez , salmeron ; 2008 ) it is a chess playing engine using bayesian networks . the following is mentioned about heuristic function in setion 3 . here the heuristic is defined in terms of 838 parameters . there are 5 parameters indicating the value of each piece ( pawn , queen , rook , knight , and bishop -the king is not evaluated , as it must always be on the board ) , 1 parameter for controlling whether the king is under check , 64 parameters for evaluating the location of each piece on each square on the board ( i.e. , a total of 786 parameters , corresponding to 64 squares × 6 pieces each colour × 2 colours ) and finally 64 more parameters that are used to evaluate the position of the king on the board during the endgame . the above sentence contains about the parameters used by the heuristic function . but i did n't find the actual definition . what is the actual definition for the heuristic function ?",21964,19413,2019-03-11T19:23:33.893,2019-03-11T19:46:52.113,heuristic function for bayeschess,heuristics chess bayes,3,0,1
2872,10364,1,,2019-02-02T14:14:36.803,1,72,"up to now , i have been using ( my version of ) open ai 's code , with the suggested cartpole . i have been using monte carlo methods , which , for cartpole , seemed to work fine . trying to move to temporal difference , cartpole seems to fail to learn ( with simple td method ) ( or i stopped it too soon , but still unacceptable ) . i assume that is the case because in cartpole , for every timestamp , we get a reward of 1 , which has very little immediate information about weather or not the action was good or not . which gym environment is the simplest that would probably work with td learning ? by simplest i mean that there is no need for a large nn to solve it . no conv nets , no rnns . just a few small layers of a fully connected nn , just like in cartpole . something i can train on my home cpu , just to see it starting to converge .",21645,,,2019-02-02T17:12:40.057,which openai gym environment should i use to test a temporal difference rl agent ?,machine-learning reinforcement-learning open-ai,1,0,
2873,10368,1,10379,2019-02-02T17:30:53.470,2,174,"td lambda is a way to interpolate between td(0 ) - bootstraping over a single step , and , td(max ) , bootstraping over the entire episode length , or , monte carlo . reading the link above , i see that an eligibility trace is kept for each state in order to calculate its "" contribution to the future "" . but , if we use an approximator , and not a table for state - values , then can we still use eligibility traces ? if so , how would the loss be calculated ? ( and thus the gradients ) specifically , i would like to use actor critic ( or advantage actor critic )",21645,21645,2019-02-03T21:07:05.170,2019-02-03T23:23:23.010,can td - lambda be used with deep reinforcement learning ?,neural-networks machine-learning reinforcement-learning,1,0,1
2874,10371,1,10375,2019-02-02T23:24:15.137,2,181,"in my country expert system class is mandatory class if you want to take ai specialization in most universities . in class i learned how to make a rule base system , forward chaining , backward chaining , prolog , etc . i enjoyed the class until i read a few comments in stackoverflow that expert system is no longer included in the ai field , some top university have stopped teaching that again . i found the old screenshot one of the comments : is that true ? why ? and from industrial view , with the rise of machine learning , is the expert system still in use today ?",16565,16565,2019-02-05T23:22:28.420,2019-02-05T23:22:28.420,is the expert system still in use today ?,ai-basics philosophy expert-system,2,0,
2875,10372,1,10373,2019-01-31T23:35:08.680,2,63,"experience replay is buffer ( or a "" memory "" ) of transactions $ e_t = ( s_t , a_t , r_t , s_{t+1})$ . the equations for calculating the loss in actor critic are an actor loss ( parameterized by ) $ $ log[\pi_\theta(s_t , a_t)]q_w(s_t , a_t)$$ and a critic loss ( parameterized by $ w$ ) $ $ r(s_t , a_t ) + \gamma q_w(s_{t+1},a_{t+1 } ) - q_w(s_{t},a_{t}).$$ as i see it , there are two more elements that need to be saved for later use : the expected q value at the time $ t$ : $ q_w(s_{t},a_{t})$ the log probability for action $ a_t$ : $ log[\pi_\theta(s_t , a_t)]$ if they are not saved , how will we be able to later calculate the loss for learning ? i did n't see anywhere stating to save those , and i must be missing something . do these elements need to be saved or not ?",21645,21645,2019-02-06T14:41:45.123,2019-02-06T14:41:45.123,what information should be cached in experience replay for actor - critic ?,machine-learning reinforcement-learning policy-gradients experience-replay,1,0,
2876,10374,1,,2019-02-03T05:37:55.170,1,62,"i 've come up with an idea on how we could use a combination of deep learning and body sensors to create a walking talking living humanoid . here goes : first , we will recruit 1 billion people and have them wear a special full face mask and suit . this suit will contain touch sensors along the skin , cameras , smell sensors , taste sensors on the mask , basically every data and information that a human receives will be collected electronically , whether it is what they see , what they smell what they feel and so on . these suits will also have potentiometers and other sensors to measure the movement made by the person . every hand movement , leg movement , muscle movement will be recorded and saved in a database as well . after 50 years or so , all collected input and output data from every single person who participated in this experiment will be saved in a computer . we then create a neural network and then train it on the input and output data from the database . next , we create a robot that has motorized muscles and hand / leg joints that are the same specs as to our previous suits and also the touch , smell , sight and other sensors integrated inside of this robot . once everything is ready , we will load the trained neural network onto the robot and switch it on . during inference , the neural network will take data from sensors all over the robot 's body and translate it into movement in legs , hands , and muscle . could these techniques in conjunction with the data collection i describe , produce and agi android ? essentially , how feasible is current technology to produce a robot that will behave , speak , live , move like a normal human being ?",21980,1671,2019-02-05T22:21:01.580,2019-02-05T22:21:01.580,an idea i had to create the first humanoid using deep learning,neural-networks ai-design theory data-science robotics,0,4,
2877,10378,1,,2019-02-03T20:05:06.860,3,34,"as in sigmoid function when input x is very large or very small the curve is flat that means low gradient descent but when it is in between the slope is more so , my question is how this thing helps us in neural network .",21642,,,2019-02-04T01:05:55.140,how sigmoid funtion helps us in reducing error in neural networks ?,neural-networks,1,0,
2878,10380,1,10417,2019-02-04T00:33:49.347,0,52,"i was n't sure how to title this question so pardon me please . you may have seen at least one video of those "" insane a.i created simulation of { x } doing { y & amp ; z } like the following ones : a.i learns how to play mario a.i swaps faces of { insert celebrity } in this video after 16hrs . etc ... i want to know what i have to learn to be able to create for example a program that takes xyz - k images of a person as training data and changes it with another person 's face in a video . or create a program that on a basic level creates a simulation of 2 objects orbiting /attracting each other /colliding like this : what field / topic is that ? i suspect deep learning but i 'm not sure . i 'm currently learning machine learning with python . i 'm struggling because linear regression & amp ; finances /stock value prediction is really not interesting compared to teaching objects in games to do archive something or create a program that tries to read characters from images .",21988,1671,2019-02-06T02:48:09.620,2019-02-06T05:59:17.517,which field to study to learn & create a.i generated simulations ?,neural-networks machine-learning deep-learning data-science ai-field,1,0,
2879,10383,1,,2019-02-04T04:18:38.173,0,30,"i am trying to implement contractive auto - encoders in pytorch but i do n't know what i 'm doing is right or not . the architecture of the auto - encoder is given below : class ae(nn.module ) : def _ _ init__(self ) : super(ae , self).__init _ _ ( ) self.encoder = nn.sequential(nn.linear(784 , 256),nn.linear(256 , 128 ) , nn.linear(128 , 64 ) ) self.decoder = nn.sequential(nn.linear(64 , 128 ) , nn.linear(128 , 256 ) , nn.linear(256 , 784 ) ) self.sigmoid = nn.sigmoid ( ) def forward(self , input ) : h1 = self.encoder(input ) h2 = self.decoder(h1 ) sigmoid = self.sigmoid(h2 ) return h1,sigmoid i am trying to implement the contractive loss function the code for which is given below : mse_loss = nn.mseloss ( ) lam= 1e-3 def contractive_loss(w , x , recons_x , h , lam ) : mse = mse_loss(recons_x , x ) dh = h*(1-h ) w_sum = torch.sum(w**2 , dim=1 ) w_sum = w_sum.unsqueeze(1 ) contractive_loss_value = torch.sum(torch.mm(dh**2 , w_sum),0 ) return mse + contractive_loss_value.mul(lam ) the training module is given below : def train(model = ae_model , epoch=0 , train_loader= train_loader ) : model.train ( ) train_loss= 0 total= 0 for i , ( data , label ) in enumerate(train_loader ) : data= data.to(device).view(-1 , 28 * 28 ) label = label.to(device).view(-1 , 28 * 28 ) optimizer.zero_grad ( ) hidden_representation , recons_x = model(data ) w = model.state_dict()['decoder.2.weight ' ] loss = contractive_loss(w , label , recons_x , hidden_representation , lam ) loss.backward ( ) train_loss + = loss.item ( ) optimizer.step ( ) total + = label.size(1 ) any help will be appreciated .",17372,,,2019-02-04T04:18:38.173,contractive auto - encoders,deep-learning deep-network autoencoders pytorch,0,0,
2880,10386,1,,2019-02-04T12:19:18.563,0,82,what is machine learning and deep learning ? i hope there are similar questions out there . but i could n't able visualize . can someone explain me the difference between machine learning and deep learning ?,21896,21896,2019-02-04T12:58:41.153,2019-02-04T14:34:26.070,difference between machine learning and deep learning ?,machine-learning deep-learning,1,0,
2881,10387,1,,2019-02-04T13:59:19.120,0,73,i have a convolutional encoder ( a cnn ) consisting of denseblocks and a total of 50 layers ( cf . fc - densenet103 ) . the receptive field of the encoder ( after last layer ) is 660 according to tensorflow function compute_receptive_field_from_graph_def ( .. ) ) whereas the input image is 64x64 pixels . obviously the receptive field is way too big . how can the receptive field be reduced to say 46 but the capacity of the encoder be more or less kept at the same level ? by capacity i simply mean the number of parameters of the model . the capacity requirement is justified due to the complex dataset to be processed . using less layers or smaller kernels reduces the receptive field size but also the capacity . should i then just increase the number of filters in the remaining layers in order to keep the capacity ?,20191,20191,2019-02-04T20:03:27.053,2019-03-07T12:01:55.340,reduce receptive field size of cnn while keeping its capacity ?,neural-networks machine-learning convolutional-neural-networks,1,2,
2882,10391,1,,2019-02-04T18:18:53.410,1,47,"i have got multi - class object detector . one model accuracy evaluation of detection consists of : map , fp , fn , tp for each class divided to two graphs and looks like this ( i 've used this repo for evaluation ) . now , i 've got many of these evaluations ( multiple times these two graphs for different models ) and i would like to easily compare all these trained models ( results ) and put them to one graph . i 've searched through the whole internet , but was n't able to find suitable method of placing all the values to one graph . also , the values of these three classes can be put together ( eg . result map for this evaluation would be ( 75 + 68 + 66 ) / 3 = ~70 % ) , so i would have just single value of each map , fn , fp , tp for one whole model evaluation . what comes to my mind is the following graph ( or maybe some kind of plot ) : note : it may not make sense to place map together with tp , etc . into one graph , but i would like to have all these values together to easily compare all the model evaluations . also i am not really looking for a script , i can do the graph manually from values , but script would be more helpful . what really matters is , how to create meaningful graph with all the data :) . if the post is more suitable for different kind of site , please , let me know .",18760,9947,2019-02-04T18:32:12.960,2019-02-04T18:32:12.960,how to create meaningful multiple object detection evaluation comparison graph ?,neural-networks graphs,0,2,
2883,10393,1,,2019-02-04T22:27:50.903,1,13,is it possible to sample from a distribution inside a neural network forward function ? assume that there is a nn and a sample is needed to be derived from it at every forward - pass to randomly set a layer - specific hyper - parameter . is this operation differentiable,10569,,,2019-02-04T22:27:50.903,sample from a distribution inside a nn layer,neural-networks deep-learning convolutional-neural-networks,0,0,
2884,10394,1,,2019-02-04T22:50:24.823,1,19,"are there chatbots for facebook messenger or skype available which are game - based , i.e. with which it is possible to play a short funny game ? it should be possible to play the game for at least 10 minutes in sequence and it should be writing based and not based on clicking at boxes . that means the agent should be pretty clever , like microsoft zo bot , but instead of conducting random smalltalk a game should be played . second , are there bots for facebook messenger or skype available which are nasty and unfriendly , i.e. which are offending ? thank you a lot in advance . i 'm thankful for any help .",21103,1671,2019-02-06T02:18:25.013,2019-02-06T02:18:25.013,game - based or nasty chatbot for facebook messenger or skype,game-ai intelligent-agent chat-bots,0,1,
2885,10401,1,,2019-02-05T09:58:21.980,1,62,i want to implement ai in client side . any javascript libraries that has been already used . what are the libraries can be used and some samples ?,20286,16565,2019-02-05T19:32:51.000,2019-02-05T23:12:18.117,how to impliment ai using js,implementation javascript,1,1,
2886,10403,1,11297,2019-02-05T11:34:29.887,6,112,"the softsign ( a.k.a . elliotsig ) activation function is really simple : $ $ f(x ) = \frac{x}{1+|x| } $ $ it is bounded $ [ -1,1]$ , has a first derivative , it is monotonic , and it is computationally extremely simple ( easy for , e.g. , a gpu ) . why it is not widely used in neural networks ? is it because it is not infinitely derivable ?",15017,2444,2019-03-18T20:42:53.317,2019-03-18T20:42:53.317,why is n't the elliotsig activation function widely used ?,machine-learning activation-function performance,1,3,1
2887,10404,1,,2019-02-05T11:47:26.437,1,80,"i want to customize the ' pendulum - v0 ' environment such that the action ( the torque ) from previous time step as well as from the current timestep serve as the inputs in the env.step ( ) function . my problem statement is that i want to generate torque from the controller which has a white gaussian noise of magnitude 1 and then filter it with the torque generated in the previous timestep as follows : tor _ = tor_c + a*wgn ; tor(t ) = lambda*tor _ + ( 1-lambda)*tor(t-1 ) ; https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py#l37 we can see that the ' u ' is an array of some numbers as the input which is afterward clipped from -max_torque to max_torque and then only the first element is taken as the torque value to calculate the states and the reward function for the given time - step . my question is what does the value of the other elements signify ? are they torque values from the previous time steps or is it that the length of the ' u ' array is just 1 and its value is restricted between -max_torque to max_torque ? in conclusion , i just wanna access the action ( the torque value ) from the previous time - step . is it possible ? if yes , how ?",22023,,,2019-02-05T11:47:26.437,input for the env.step ( ) in the ' pendulum - v0 ' environment,reinforcement-learning,0,8,0
2888,10406,1,,2019-02-05T14:23:05.253,2,54,"in a neural network , each neuron represents some part of the input . for example , in the case of a mnist digit , consider the stem of the number 9 . each neuron in the nn represents some part of this digit . what determines which neuron will represent which part of the digit ? is it possible that if we pass in the same input multiple times , each neuron can represent different parts of the digit ? how is this related to the back - propagation algorithm and chain rule ? is it the case that , before training the neural network , each neuron does n't really represent anything of the input , and , as training proceeds , neurons start to represent some part of the input ?",19583,2444,2019-05-08T13:23:53.187,2019-05-08T15:37:40.790,which neuron represents which part of the input ?,neural-networks machine-learning convolutional-neural-networks backpropagation feature,2,0,
2889,10412,1,,2019-02-05T20:18:41.510,2,31,"i am trying to build a film review classifier where i determine if a given review is positive or negative ( w/ python ) . i 'm trying to avoid any other ml libraries so that i can better understand the processes . here is my approach and the problems that i am facing : i mine thousands of film reviews as training sets and classify them as positive or negative . i parse through my training set and for each class , i build an array of unique words . for each document , i build a vector of tf - idf values where the vector size is my number of unique words . i use a gaussian classifier to determine : $ $ p(c_i|w)=p(c_i)p(w|c)=p(c_i)*\dfrac{1}{\sqrt{2\pi}\sigma_i}e^{-(1/2)(w-\mu_i)^t\sigma_i^{-1}(w-\mu_i)}$$ where $ w$ is the my document in a vector , $ c_i$ is a particular class , is the mean vector and is my covariance matrix . this approach seems to makes sense . my problem is that my algorithm is much too slow . as an example , i have sampled over 1,500 documents and i have determined over 40,000 unique words . this mean that each of my document vectors has 40,000 entries and if i were to build a covariance matrix , it would have dimensions 40,000 by 40,000 . even i were able to generate the entirety of , but then i would have to compute the matrix product in the exponent , which will take an extraordinarily long time just to classify one document . i have experimented with a multinomial approach , which is working well . i very curious on how to make this work more efficiently . i realise the matrix multiplication runtime ca n't be improved , and i was hoping for insight on how others are able to do this . some things i have tried : filtered any stop words ( but this still leaves me with tens of thousands of words ) estimated by summing over a couple of documents .",22017,,,2019-02-05T20:18:41.510,my naive ( ha ! ) gaussian naive bayes classifier is too slow,python bayes,0,0,
2890,10413,1,,2019-02-05T20:29:30.240,3,172,could advanced ai decide to kill all humans ?,22035,1671,2019-02-05T21:48:40.390,2019-04-12T13:03:05.660,could artificial general intelligence harm humanity ?,control-problem superintelligence mythology-of-ai neo-luddism value-alignment,5,2,
2891,10414,1,,2019-02-05T21:22:43.773,1,30,"i just finished the three - part series of probabilistic graphical models courses from stanford over on coursera . i got in to them because i realized there is a certain class of problem for which the standard supervised learning approaches do n't apply , for which graph search algorithms do n't work , problems that do n't look like rl control problems , that do n't even exactly look like the kind of clustering i came to call "" unsupervised learning "" . in my ai courses in the institute , we talked briefly about bayes nets , but it was almost as if professors considered that preamble to hotter topics like neural nets . meanwhile i heard about "" expectation maximization "" and "" inference "" and "" maximum likelihood estimation "" all the time , like i was supposed to know what they were talking about . it frustrated me not to be able to remember statistics well enough to feel these things , so i decided to fill the hole by delving deeper in to pgms . throughout , koller gives examples of how to apply pgms to things like image segmentation and speech recognition , examples that seem completely dated now because we have cnns and lstms , even deep nets that encode notions of uncertainty about their beliefs . i gather pgms are good when : you know the structure of the problem and can encode domain knowledge that way . you need a generative model . you want to learn more than just one x - > y mapping , when you instead need a more general - purpose model that can be queried from several sides to answer different kinds of questions . you want to feed the model inputs that look more like probability distributions than like samples . what else are they good for ? have they not been outstripped by more advanced methods for lots of problems now ? in which domains or for which specific kinds of problem are they still king ? how are they complementary to modern advanced methods ? should i dedicate time to reading any of koller & amp ; friedman 's massive tome on this subject ? how dated is this set of moocs ?",18196,18196,2019-02-05T21:27:55.593,2019-02-05T21:27:55.593,how do pgms factor in to modern ml ?,machine-learning generative-model,0,0,
2892,10418,1,,2019-02-06T07:33:01.187,2,27,"i work with neural networks for real - time image processing on embedded softwares and i tested different architectures ( googlenet , mobilenet , resnet , custom networks ... ) and different hardware solutions ( boards , processors , ai accelerators ... ) . i noticed that the performance of the system , in terms of inference time , does not depend only on the processor but also on other factors . for example , i have two boards from different manifacturers , b1 ( with a cheap processor ) and b2 ( with a better processor ) , and two neural networks , n1 ( very light with regular convolutions and fully connected layers ) and n2 ( very large , with inception modules and many layers ) . the inference time for n1 is better on b1 , while for n2 it is better on n2 . moreover , it happens that , as the software is executed , the inference time changes over time . so my question is : in an embedded system , what are the aspects that impact on the inference time , and how ? i am interested not only in the hardware features but also in the neural network architecture ( convolutional filter size , types of layers and so on ) .",16671,,,2019-02-07T14:44:51.260,what are the aspects that most impact on the inference time for neural networks in embedded systems ?,neural-networks hardware performance embedded-design,1,0,
2893,10419,1,,2019-02-06T10:43:37.067,1,26,"i trained some gaussian process model with the python library gpflow on a dataset consisting of $ ( x , y)$ , inputs and outputs , in a regression setting . this model gives me pretty good predictions in the sense that the relative error is small almost everywhere . i want to use the uncertainty as well , which is given in a gpflow setting in the form of a standard deviation ( std ) associated with every prediction . here 's my problem : i normalised both inputs and outputs before training ( separately ) using sklearn 's standardscaler ( effectively making the data normally distributed with $ 0 $ mean and unit std ) . so the std given by the model pertains to the scaled data . how do i "" rescale "" the uncertainty estimates of the gp to the actual data ? using the inverse_transform function of the output scaler makes little sense . this issue might be easier solvable if i scaled with a minmaxscaler ( squishing all data points into the unit interval ) by dividing by the length of the range of the original output set ( at least i think it works that way ) . but how about the case of the standardscaler ? any insights will be appreciated !",16901,,,2019-02-06T10:43:37.067,gpflow : gaussian process uncertainty quantification,machine-learning python,0,1,
2894,10420,1,,2019-02-06T13:48:43.677,3,39,"how we should train a cnn model when training dataset contains only limited number of cases , and the trained model is supposed to predict class ( label ) for several other cases , which has not seen before ? supposing there was hidden independent features describing the label repeated in previously seen cases of dataset . for example let 's consider we want to train a model to movement time series signals so it can predict some sort of activities ( labels ) , and we have long record of movement signals ( e.g. hours ) for limited number of persons ( e.g. 30 ) during various type of activities ( e.g. 5 ) , we may say these signals carry three type of hidden features : noise - features : common features between every persons / activities case - features : features mostly correlated with persons class - features : features mostly correlated with activities we want to train the model such it learn mostly class - features and eliminate 1st and 2nd types of features . in conventional types of supervised - learning cnn learns all features how dataset represents them . in my test the model learned those 30 person activities very well , but on new persons it only predict randomly ( i.e. 20 % success ) . over - fitted ? it seems there are three straight workaround to this : extracting class - features and using a shallow classifier . increasing dataset wideness by recording signal on other persons : it can get so expensive or impossible in some situations . signal augmentation : by augmenting signals such it does not change class - features , and making augmented case - features . it seems to me harder than 1st workaround . is there any other workaround on this type of problem ? for example specific type of training to use , to learn the model how different cases similarly follow class - features during class changes , eliminating case - features which varies case by case . sorry for very long question !",22055,22055,2019-02-06T21:41:02.330,2019-02-06T21:41:02.330,how to train cnn such it eliminate dependent features and focuses on independent ones ?,convolutional-neural-networks training datasets overfitting,0,7,
2895,10421,1,10426,2019-02-06T15:04:21.750,2,93,"in actor critic , the equations for calculating the loss in actor critic are an actor loss ( parameterized by ) $ $ log[\pi_\theta(s_t , a_t)]q_w(s_t , a_t)$$ and a critic loss ( parameterized by $ w$ ) $ $ r(s_t , a_t ) + \gamma q_w(s_{t+1 } , a_{t+1 } ) - q_w(s_{t } , a_t).$$ this is bootstrapping in experience replay : $ $ l_i(\theta_i ) = \mathbb{e}_{(s , a , r , s ' ) \sim u(d ) } \left [ \left(r + \gamma \max_{a ' } q(s ' , a ' ; \theta_i^- ) - q(s , a ; \theta_i)\right)^2 \right ] $ $ it is clear that bootstrapping is comparable to the critic loss , except that the $ max$ operation is lacking from the critic . as i see it , ( correct me if i 'm wrong ) : $ q(s_t , a_t ) = v(s_{t+1 } ) + r_t$ where $ a_t$ is the actual action that had been taken . the critic , as i understand , estimates $ v(s)$ my question : what exactly is the critic calculating ? what in actor critic outputs $ q(s_{t+1},a_{t+1})$ ? it seems to me like the critic calculates the average next state $ s_{t+1}$ value , over all possible actions , with their corresponding probabilities , yielding $ q(s_t , a_t ) = r_t + \sum_{a_{t+1 } \in a}p(a_{t+1}|s_t)v(s_{t+1})$ which would mean that in order to get $ q(s_{t+1 } , a_{t+1})$ for the above formula , i would need to calculate $ q(s_{t+1 } , a_{t+1 } ) = r_{t+1 } + \sum_{a_{t+2 } \in a}p(a_{t+2}|s_{t+1})v(s_{t+2})$ where $ v(s_{t+2})$ is the critic output on $ s_{t+2}$ , a state we get to by taking action $ a_{t+1}$ from state $ s_{t+1}$ but i am not sure that is indeed the meaning of the critic output and still it is unclear to me how to get $ q(s_{t+1 } , a_{t+1})$ from actor critic . if indeed that is what 's being calculated , then why is it mathematically true that an improvement is being made ? or why does it make sense ( even if not mathematically always true ) ? practical use : i want to use actor critic with experience replay in an environment with a large action space ( could be continuous ) . therefore , i can not use the $ max$ term . i need to understand the correct equation for the critic loss , and why it works .",21645,21645,2019-02-06T16:53:22.487,2019-02-06T18:07:28.307,meaning of actor output in actor critic reinforcement learning,machine-learning reinforcement-learning q-learning policy-gradients actor-critic,1,3,1
2896,10422,1,10427,2019-02-06T15:06:38.913,2,62,"imagine that the agent receives a positive reward upon reaching a state . once the state has been reached the positive reward associated with it vanishes and appears somewhere else in the state space , say at state ′. the reward associated to ′ also vanishes when the agent visits that state once and re - appears at state . this goes periodically forever . will discounted q - learning converge to the optimal policy in this setup ? is yes , is there any proof out there , i could n't find anything .",22060,2444,2019-02-15T17:40:55.420,2019-02-15T17:40:55.420,will q - learning converge to the optimal state - action function when the reward periodically changes ?,reinforcement-learning q-learning,1,4,
2897,10425,1,,2019-02-06T17:48:39.643,3,16,"i am reviewing my neural network lectures and i have a doubt : my book 's ( haykin ) batch pta describes a cost function which is defined over the set of the misclassified inputs . i have always been taught to use mse & lt ; x as a stopping condition for the training process . is the batch case different ? should i use as stopping condition size(misclassified ) & lt ; y ( and as a consequence when the weight change is very little ) ? moreover , the book uses the same symbol for both the training set and the misclassified input set . does this mean that my training set changes each epoch ? than you for any answer !",21676,21676,2019-02-06T21:00:08.693,2019-02-06T21:00:08.693,batch pta stopping condition,neural-networks theory perceptron learning-theory,0,2,
2898,10428,1,,2019-02-06T18:33:39.253,1,15,"most robotics challenges like robocup soccer , micromouse and the amazon picking challenge are oriented on fully autonomous systems . a motion controller gets started , the operator takes away his hands from the keyboard and then the system is solving the task by it 's own . such a technology might be state - of - the - art but my interests go more into the direction of low - tech robotics . that 's a control paradigm which is working with a mouse and a keyboard no artificial intelligence is in the loop . this is called a teleoperated robot and the human takes his hands never away from the keyboard . realizing a plain teleoperation system is not that hard . because the user input has to be transmitted only to the servo motor . to make things more complicated the system should provide annotations . that means , the human operator is doing a task and in the status bar a natural language description of the detected events is shown . for example “ gripper pushes object ” , “ object in gripper ” , “ gripper releases object ” . the only problem is , that i do n't how to annotate a teleoperation robot system . is some kind of best practice method available for creating a semantic event recognition ? are neural networks are great choice ?",11571,,,2019-02-06T18:33:39.253,how to annotate a teleoperated robot ?,natural-language-processing image-recognition,0,0,
2899,10429,1,,2019-02-06T23:29:15.037,1,29,"i 'm seeing a lot of examples involving games , or robot problems . what about how to make something else conform to this framework ? how do you transform say a csv file of psychological data to determine the best life actions you can get from self report questionnaire ?",22070,1671,2019-02-07T00:44:52.950,2019-02-07T00:44:52.950,using neuroevolution for something other than games,neural-networks diagnostics,0,1,
2900,10430,1,10433,2019-02-06T23:51:42.447,1,66,"a lot of research has been done to create the optimal ( or "" smartest "" ) rl agent , using methods such as a2c . an agent can now beat humans at playing go , chess , poker , atari games , dota , etc . but i think these kind of agents will never be a friend of humans , because humans wo n't play with a agent that always beats them . how could we create an agent that does n't outperform humans , but it has the human level skill , so that when it plays agains a human , the human is still motivated to beat it ?",16565,2444,2019-02-15T17:37:00.600,2019-02-15T17:37:00.600,how do we create a good agent that does not outperform humans ?,reinforcement-learning philosophy intelligent-agent,2,0,
2901,10431,1,10604,2019-02-07T06:32:54.797,6,131,"i have two machine learning models ( i use lstm ) that have a different result on the validation set ( ~100 samples data ) : model a : accuracy : ~91 % , loss : ~0.01 model b : accuracy : ~83 % , loss : ~0.003 the size and the speed of both model are almost the same . so , which model should i choose ?",16565,16565,2019-03-20T04:01:11.343,2019-03-20T04:01:11.343,choose model with the smallest loss or highest accuracy ?,neural-networks machine-learning,4,2,1
2902,10436,1,10463,2019-02-07T11:34:08.653,2,46,"i have been researching about determining some key points on an image , in this case i 'm gon na use cloth ( top side of human body ) pictures . i want to detect some corner points on those . example : i have two solutions on my mind . one cnn with transpose layers resulting in heatmap where i can get points . the second is to get 24 number as output from the model meaning 12(x , y ) point . i do n't know which one will be better . in face point detection , they use the second method . in human pose estimation , they use method one . so what do you suggest me to use ? or do you have any new ideas ? thanks",16864,16864,2019-02-07T11:40:19.067,2019-02-08T17:41:12.000,key point extraction the best method ?,deep-learning feature-extraction,1,2,
2903,10441,1,,2019-02-07T15:22:17.117,1,44,"in the book "" reinforcement learning : an introduction "" , by sutton and barto , they provided the "" q - learning prioritized sweeping "" algorithm , in which the model saves the next state and the immediate reward , for each state and action , that is , $ model(s_{t},a_{t } ) \leftarrow s_{t+1 } , r_{t+1}$ . if we want to use "" sarsa prioritized sweeping "" , should we save "" next state , immediate reward , and next action "" , that is , $ model(s_{t},a_{t } ) \leftarrow s_{t+1 } , r_{t+1 } , a_{t+1}$ ?",10191,2444,2019-02-07T16:41:44.647,2019-02-07T17:16:01.583,what should be saved in sarsa prioritized sweeping ?,reinforcement-learning,1,0,
2904,10442,1,,2019-02-07T15:38:58.217,3,55,"in this video , the lecturer states that $ r(s)$ , $ r(s , a)$ and $ r(s , a , s')$ are equivalent representations of the reward function . intuitively , this is the case , according to the same lecturer , because $ s$ can be made to represent the state and the action . furthermore , apparently , the markov decision process would change depending on whether we use one representation or the other . i am looking for a formal proof that shows that these representations are equivalent . moreover , how exactly would the markov decision process change if we use one representation over the other ? finally , when should we use one representation over the other and why are there three representations ? i suppose it is because one representation may be more convenient than another in certain cases : which cases ? how do you decide which representation to use ?",2444,2444,2019-02-07T23:01:14.280,2019-02-07T23:01:14.280,"how are the reward functions $ r(s)$ , $ r(s , a)$ and $ r(s , a , s')$ equivalent ?",reinforcement-learning definitions markov-decision-process proofs,2,0,
2905,10446,1,10455,2019-02-07T22:33:01.607,2,74,"i 'm building a customer assistant chatbot in python , so a text classification task , and i have available more or less 7 hundred sentences of average length 15 words ( unbalanced class ) . what do you think , knowing that i have to do an oversampling , is this dataset large enough ?",20780,9947,2019-02-08T10:16:36.513,2019-02-08T10:16:36.513,text classification task chatbot,classification python keras,1,0,
2906,10447,1,10458,2019-02-07T22:33:27.293,2,217,"i have a dataset which i have loaded as a data frame in python . it consists of 21392 rows ( the data instances , each row is one sample ) and 1972 columns ( the features ) . the last column i.e. column 1972 has string type labels ( 14 different categories of target labels ) . i would like to use a cnn to classify the data in this case and predict the target labels using the available features . this is a somewhat unconventional approach though it seems possible . however , i am very confused on how the methodology should be as i could not find any sample code/ pseudo code guiding on using cnn for classifying non - image data , either in tensorflow or keras . any help in this regard will be highly appreciated . cheers !",21460,,,2019-02-08T22:08:33.220,how to use cnn for making predictions on non - image data ?,convolutional-neural-networks tensorflow,2,0,
2907,10448,1,,2019-02-07T23:57:08.963,0,42,"i 've seen recently a video with dr . joanne pransky on youtube about a woman who is working as a robot psychiatrist . her daily job is to speak with robots about their mental problems . that means , if a robot owner beliefs , that his robot needs advice from a doctor he can visit the psychiatrist and has to pay the bill after the visit . if i understood the woman in the video correct , she is speaking with robots about programming issues . some of the newer models have emotion oriented subroutines , similar to the famous sophia robot from hanson robotics . but even a normal ev3 mindstorms brick can have advanced problems . for example if it was programmed with behavior based algorithm which are forcing the robot to stay away from the wall . this can result into troubles with low level control loops . at the first look the video was funny . because what ai researchers have promised is the opposite . according to most researchers the social role of robots is that they are working as a doctor . and the researchers also have promised that robots will work in the factory for humans . what was presented in the video was the opposite . that means , robots have a demand for medical help and they have a demand for pizza which are produced in the factory by human workers . is this the future ? are robots useless machines as default which are producing costs like a house dog ?",11571,11571,2019-02-08T12:26:21.860,2019-02-08T12:26:21.860,do robots needs help from psychiatrist ?,philosophy ethics,0,2,
2908,10450,1,10453,2019-02-08T01:47:33.333,3,57,"in my understanding , q - learning gives you a deterministic policy . however , can we use some technique to build a meaningful stochastic policy from the learned q values ? i think that simply using a softmax wo n't work .",22105,22105,2019-02-08T14:45:11.230,2019-02-08T14:45:11.230,can q - learning be used to derive a stochastic policy ?,reinforcement-learning q-learning stochastic-policy deterministic-policy,1,1,
2909,10454,1,,2019-02-08T08:32:50.123,0,15,"let assume that we have dataset of variables ( random events ) , i apriori would like to set dependency conditions between some of them and perform structure learning to figure out the rest of the net . how it can be done practically ( e.g. some libs like bnlearn etc . ) or at least in theory ? i was trying to google it but have n't found anything related",22113,,,2019-03-24T13:01:10.143,how to perform structure learning for bayes net given already partially constructed net ?,machine-learning bayes,1,0,
2910,10456,1,,2019-02-08T10:52:11.697,1,40,"i have chromosomes with floating point representation with values between 0 and 1 . for example- let p1=[0.1,0.2,0.3 and p2=[0.5,0.6,0.7 ] be two parents . both comply with the set of constraints . however , the children produced by 1 point crossover , we get c1=[0.1 , 0.6 , 0.7 ] and c2=[0.5 , 0.2 , 0.3 ] out of which 1 or both may not comply with the given constraints . a similar scenario can also occur with small perturbation of values due to mutation strategy . correct me if i am wrong in the belief that such kind of scenarios might arise irrespective of the strategy employed for crossover and mutation . what are the options to handle such kind of cases ? edit- in my case , the major constraint is- $ $ p1[1]*p1[2 ] - k*p1[0 ] \geq 0 $ $ for any chromosome $ p1 $ . for the example above we can take $ k=0.3 $ which renders c2 infeasible .",22115,22115,2019-02-09T08:01:17.160,2019-02-09T08:01:17.160,how to handle infeasiblity caused due to crossover and mutation in genetic algorithm for optimization,genetic-algorithms,1,0,
2911,10461,1,,2019-02-08T16:45:52.937,1,98,"i 've used a table to represent the q function , while an agent is being trained to catch the cheese without touching the walls . the first and last row ( and column ) of the matrix are associated with the walls . i placed in last cell a cheese that agent must catch while being training . so far , i 've done it with dynamic states and , when necessary , i resized matrix with new states . i 've used four actions ( up , left , right and down ) . i would like now to use an ann to represent my q function . how do i do that ? what should be the input and output of such neural network ?",17405,2444,2019-02-10T20:27:56.790,2019-02-10T20:27:56.790,how do i convert table - based to neural network - based q - learning ?,neural-networks reinforcement-learning q-learning,1,0,
2912,10462,1,,2019-02-08T17:39:21.540,1,78,"lstm is supposed to be the right tool to capture path - dependency in time - series data . i decided to run a simple experiment ( simulation ) to assess the extent to which lstm is better able to understand path - dependency . the setting is very simple . i just simulate a bunch ( n=100 ) paths coming from 4 different data generating processes . two of these processes represent a real increase and a real decrease , while the other two fake trends that eventually revert to zero . the following plot shows the simulated paths for each category : the candidate machine learning algorithm will be given the first 8 values of the path ( t in [ 1,8 ] ) and will be trained to predict the subsequent movement over the last 2 steps . in other words : the feature vector is x = ( p1 , p2 , p3 , p4 , p5 , p6 , p7 , p8 ) the target is y = p10 - p8 i compared lstm with a simple random forest model with 20 estimators . here are the definitions and the training of the two models , using keras and scikit - learn : # lstm model = sequential ( ) model.add(lstm((1 ) , batch_input_shape=(none , h , 1 ) , return_sequences = true ) ) model.add(lstm((1 ) , return_sequences = false ) ) model.compile(loss='mean_squared_error ' , optimizer='adam ' , metrics=['accuracy ' ] ) history = model.fit(train_x_ls , train_y_ls , epochs=100 , validation_data=(vali_x_ls , vali_y_ls ) , verbose=0 ) # random forest rf = randomforestregressor(random_state=0 , n_estimators=20 ) rf.fit(train_x_rf , train_y_rf ) ; the results are the summarized by the following scatter plots : as you can see , the random forest model is clearly outperforming the lstm . the latter seems to be not able to distinguish between the real and the fake trends . do you have any idea to explain why this is happening ? how would you modify the lstm model to make it better at this problem ? some remarks : the data points are divided by 100 to make sure gradients do not explode i tried to increase the sample size , but i noticed no differences i tried to increase the number of epochs over which the lstm is trained , but i noticed no differences ( the loss becomes stagnant after a bunch of epochs ) you can find the code i used to run the experiment here",22129,22129,2019-02-08T22:22:00.840,2019-02-08T22:22:00.840,experiment shows that lstm does worse than random forest ... why ?,machine-learning deep-learning lstm,0,1,1
2913,10465,1,,2019-02-08T19:37:12.240,1,22,"i have a data analysis problem that i can reduce to one similar to analyzing the trajectories in the images below . these images show the tracks of subatomic particles interacting in a bubble chamber . it 's pretty obvious that by eye , easily discernible patterns can be seen . i want very much to know more about how classification and segmentation can be done using neural networks for this type of image . these images are binary . the trajectory is either at a point in the image or it is n't . as can be seen , trajectories cross over one another , some data appears to be missing in otherwise smooth curves , at arbitrary points along those curves . ( my data may be more sparse in this respect . ) a typical paper on bubble chamber analysis that i would find deals with the analysis of the physics after trajectories have been classified and segmented . can anyone identify some papers that address this or something similar in the context of neural networks ? i am not able to find anything recent on automated methods at all , but my google fu may not be up to the challenge . ( by the way , i am less interested in some of the parametric methods like hough transforms . i 'd like to focus on the neural approach . ) ( i posted this previous question which was n't quite as specific as this one . i hope there is some available research in this area related to physics that might give me some insights that are more directly related to my problem . )",8439,,,2019-02-08T19:37:12.240,bubble chamber image analysis using neural network,image-recognition classification detecting-patterns,0,0,
2914,10466,1,10498,2019-02-08T19:56:50.297,1,76,"i 'm a beginner in machine learning and i was trying to make a test neural network for digits recognition from scratch using numpy . i used mnist dataset for training and testing . input layer have 28 * 28 neurons which correspond to each pixel of image that must be recognized . output layer have 10 neurons which correspond to each digit ( 0 - 9 ) , and return values from 0 to 1 which mean chance that the corresponding digit is displayed on the image . class layer represents a separate layer and contains links to previous and next layer ( if prevlayer is none , the current layer is input layer ; if nextlayer is none , the current layer is output ) . the forward ( ) method is responsible for passing data through neural network . the backprop ( ) method is responsible for training of neural network via backpropagation algorithm . a layer object contains weights ( w ) between previous and current layer ( input layer object does n't contain weights ) . ' data_in ' property contains a vector of calculated values before passing them into activation function . property ' data ' contains the values after activation function . but , unfortunately , it does n't work : returned value of loss function does n't decrease during training and neural network returns the same result during testing . i assume that bugs might be associated with backprop ( ) and softmax_derivatime ( ) methods . i tried in vain to find all bugs . here 's my code : import numpy as np def relu(x ) : return np.maximum(0 , x ) def relu_derivative(x ) : return np.greater(x , 0).astype(int ) def softmax(x ) : shift = x - np.max(x ) return np.exp(shift ) / np.sum(np.exp(shift ) ) def softmax_derivative(x ) : sm_array = softmax(x ) j = np.zeros((x.size , x.size ) ) for i in range(x.size ) : for j in range(x.size ) : delta = np.equal(i , j).astype(int ) j[j , i ] = sm_array[0][i ] * ( delta - sm_array[0][j ] ) return j class layer : def _ _ init__(self , size , prev_layer = none ) : self.size = size self.prevlayer = prev_layer self.nextlayer = none self.data = none self.data_in = none if prev_layer is not none : self.prevlayer.nextlayer = self self.w = np.random.random((self.prevlayer.size , size ) ) self.w_bias = np.array([np.random.random(size ) ] ) else : self.w = none self.w_bias = none def forward(self ) : if self.prevlayer is not none : self.data_in = np.dot(self.prevlayer.data , self.w ) self.data_in + = np.dot([[1 ] ] , self.w_bias ) if self.nextlayer is not none : self.data = relu(self.data_in ) self.nextlayer.forward ( ) else : self.data = softmax(self.data_in ) else : self.nextlayer.forward ( ) def backprop(self , expected_output = none , prev_delta = none ) : if prev_delta is none : # print(self.data_in ) delta = np.dot(-(expected_output - self.data ) , softmax_derivative(self.data_in ) ) delta_bias = delta else : delta = np.dot(prev_delta , self.nextlayer.w.t ) * relu_derivative(self.data_in ) delta_bias = np.dot(prev_delta , self.nextlayer.w_bias.t ) * relu_derivative(self.data_in ) training_velocity = 0.1 w_dif = np.dot(self.prevlayer.data.t , delta ) * training_velocity w_bias_dif = np.dot([[1 ] ] , delta_bias ) * training_velocity if self.prevlayer.prevlayer is not none : self.prevlayer.backprop(prev_delta=delta ) self.w -= w_dif self.w_bias -= w_bias_dif f_images = open(""train - images.idx3-ubyte "" , "" br "" ) f_images.seek(4 ) f_labels = open(""train - labels.idx1-ubyte "" , "" br "" ) f_labels.seek(8 ) images_number = int.from_bytes(f_images.read(4 ) , byteorder='big ' ) rows_number = int.from_bytes(f_images.read(4 ) , byteorder='big ' ) cols_number = int.from_bytes(f_images.read(4 ) , byteorder='big ' ) input_layer = layer(rows_number*cols_number ) hidden_layer1 = layer(rows_number*cols_number*7//10 , input_layer ) hidden_layer2 = layer(rows_number*cols_number*7//10 , hidden_layer1 ) output_layer = layer(10 , hidden_layer2 ) digits = np.array([np.zeros(10 ) ] ) input_image = np.array([np.zeros(rows_number * cols_number ) ] ) for k in range(images_number ) : for i in range(rows_number ) : for j in range(cols_number ) : input_image[0][i*cols_number+j ] = int.from_bytes(f_images.read(1 ) , byteorder='big ' ) / 255.0 * 2 - 1 input_layer.data = input_image input_layer.forward ( ) current_digit = int.from_bytes(f_labels.read(1 ) , byteorder='big ' ) digits[0][current_digit ] = 1 output_layer.backprop(expected_output = digits ) print(np.sum((digits - output_layer.data)**2)/2 ) digits[0][current_digit ] = 0 if((k+1 ) % 1000 = = 0 ) : print(str(k+1 ) + "" / "" + str(images_number ) ) f_images.close ( ) f_labels.close ( ) f_images = open(""t10k - images.idx3-ubyte "" , "" br "" ) f_images.seek(4 ) f_labels = open(""t10k - labels.idx1-ubyte "" , "" br "" ) f_labels.seek(8 ) images_number = int.from_bytes(f_images.read(4 ) , byteorder='big ' ) rows_number = int.from_bytes(f_images.read(4 ) , byteorder='big ' ) cols_number = int.from_bytes(f_images.read(4 ) , byteorder='big ' ) for k in range(images_number ) : for i in range(rows_number ) : for j in range(cols_number ) : input_image[0][i*cols_number+j ] = int.from_bytes(f_images.read(1 ) , byteorder='big ' ) input_layer.data = input_image input_layer.forward ( ) current_digit = int.from_bytes(f_labels.read(1 ) , byteorder='big ' ) print(output_layer.data ) f_images.close ( ) f_labels.close ( ) i would appreciate for any help . thanks in advance !",21567,21567,2019-02-10T16:06:42.957,2019-02-10T22:31:52.487,"a neural network for digits recognition does n't work ( mnist , numpy )",neural-networks machine-learning backpropagation object-recognition perceptron,1,1,
2915,10467,1,,2019-02-08T21:04:14.843,3,84,"suppose that we are doing machine translation . we have a conditional language model with attention where we are are trying to predict a sequence $ y_1 , y_2 , \dots , y_j$ from $ x_1 , x_2 , \dots x_i$ : $ $ p(y_1 , y_2 , \dots , y_{j}|x_1 , x_2 , \dots x_i ) = \prod_{j=1}^{j } p(y_j|v_j , y_1 , \dots , y_{j-1})$$ where $ v_j$ is a context vector that is different for each $ y_j$ . using an rnn with a encoder - decoder structure , each element $ x_i$ of the input sequence and $ y_j$ of the output sequence is converted into an embedding $ h_i$ and $ s_j$ respectively : $ $ h_i = f(h_{i-1 } , x_i ) \\ s_j = g(s_{j-1},[y_{j-1 } , v_j])$$ where $ f$ is some function of the previous input state $ h_{i-1}$ and the current input word $ x_i$ and $ g$ is some function of the previous output state $ s_{j-1}$ , the previous output word $ y_{j-1}$ and the context vector $ v_j$ . now , we want the process of predicting $ s_j$ to "" pay attention "" to the correct parts of the encoder states ( context vector $ v_j$ ) . so : $ $ v_j = \sum_{i=1}^{i } \alpha_{ij } h_i$$ where tells us how much weight to put on the $ i^{th}$ state of the source vector when predicting the $ j^{th}$ word of the output vector . since we want the s to be probabilities , we use a softmax function on the similarities between the encoder and decoder states : $ $ now , in additive attention , the similarities of the encoder and decoder states are computed as : $ $ where , and are learned attention parameters using a one - hidden layer feed - forward network . what is the intuition behind this definition ? why use the function ? i know that the idea is to use one layer of a neural network to predict the similarities . added . this description of machine translation / attention is based on the coursera course natural language processing .",22131,22131,2019-02-08T22:26:46.283,2019-02-08T22:26:46.283,what is the intuition behind the calculation of the similarity between encoder and decoder states ?,recurrent-neural-networks attention machine-translation,0,1,2
2916,10471,1,10489,2019-02-09T10:17:59.980,2,109,"i am studying a knowledge base ( kb ) from the book "" artificial intelligence : a modern approach "" ( by stuart russell and peter norvig ) and from this series of slides . a formula is satisfiable if there is some assignment to the variables that makes the formula evaluate to true . for example , if we have the boolean formula $ a \land b$ , then the assignments $ a = true$ and $ b= true$ make it satisfiable . right ? but what does it mean for a kb to be consistent ? the definition ( given at slide 14 of this series of slides ) is : a kb is consistent with formula $ f$ if $ m(kb \cup \ { f \})$ is non - empty ( there is a world in which kb is true and $ f$ is also true ) . can anyone explain this part to me with an example ?",21719,9161,2019-02-10T21:17:17.010,2019-02-11T21:05:45.870,when is knowledge base consistent ?,logic knowledge-representation ai-a-modern-approach,2,0,
2917,10472,1,,2019-02-09T11:24:19.650,0,57,"i 've recently started reading a book about deep learning . the book is titled "" grokking deep learning "" ( by andrew w trask ) . in chapter 3 ( pages 44 and 45 ) , it talks about multiplying vectors using dot product and element - wise multiplication . for instance , taking 3 scalar inputs ( vector ) and 3 vector weights ( matrix ) and multiplying . from my understanding , when multiplying vectors the size needs to be identical . the concept i have a hard time understanding is multiplying vectors by a matrix . the book gives an example of an 1x4 vector being multiplied by 4x3 matrix . the output is an 1x3 vector . i 'm am confused because i assumed multiplying vector by matrix needs the same number of columns , but i have read that the matrices need rows equal to the vectors columns . if i do not have an equal number of columns , how does my deep learning algorithm multiply each input in my vector by a corresponding weight ?",22145,2444,2019-02-13T02:35:43.733,2019-02-13T02:35:43.733,how are vectors and matrices multiplied in supervised machine learning ?,deep-learning math,1,1,
2918,10474,1,,2019-02-09T14:48:50.033,1,120,"in the context of rl , there is the notion of on - policy and off - policy algorithms . i roughly understand the difference between on - policy and off - policy algorithms . moreover , in rl , there 's also the notion of online and offline learning . what is the relation ( including the differences ) between online learning and on - policy algorithms ? similarly , what is the relation between offline learning and off - policy algorithms ? finally , is there any relation between online ( or offline ) learning and off - policy ( or on - policy ) algorithms ? for example , can an on - policy algorithm perform "" offline "" learning ? if yes , can you explain why ?",2444,,,2019-02-10T17:27:19.910,what is the relation between online learning and on - policy algorithms ?,reinforcement-learning terminology definitions on-policy off-policy,1,3,
2919,10476,1,10477,2019-02-09T15:33:02.100,1,81,"the update rules for q - learning and sarsa each are as follows : q learning : $ $ q(s_t , a_t)←q(s_t , a_t)+α[r_{t+1}+γ\max_{a'}q(s_{t+1},a')−q(s_t , a_t)]$$ sarsa : $ $ q(s_t , a_t)←q(s_t , a_t)+α[r_{t+1}+γq(s_{t+1},a_{t+1})−q(s_t , a_t)]$$ i understand the theory that sarsa performs ' on - policy ' updates , and q - learning performs ' off - policy ' updates . at the moment i perform q - learning by calculating the target thusly : target = reward + self.y * np.max(self.action_model.predict(state_prime ) ) here you can see i pick the maximum for the q - function for state prime ( i.e. greedy selection as defined by maxq in the update rule ) . if i were to do a sarsa update and use the same on - policy as used when selecting an action , e.g. ϵ-greedy , would i basically change to this : if np.random.random ( ) & lt ; self.eps : target = reward + self.y * self.action_model.predict(state_prime)[random.randint(0,9 ) ] else : target = reward + self.y * np.max(self.action_model.predict(state_prime ) ) so sometimes it will pick a random future reward based on my epsilon greedy policy ?",20352,,,2019-02-09T18:55:45.803,how do updates in sarsa and q - learning differ in code ?,reinforcement-learning q-learning,1,0,1
2920,10479,1,10480,2019-02-09T23:56:24.377,2,69,"i 'm using q - learning to train an agent to play a board game ( e.g. chess , draughts or go ) . the agent takes an action while in state $ s$ , but then what is the next state ( that is , $ s'$ ) ? is $ s'$ now the board with the piece moved as a result of taking the action , or is $ s'$ the state the agent encounters after the other player has performed his action ( i.e. it 's this agent 's turn again ) ?",20352,2444,2019-02-10T21:18:12.610,2019-02-10T21:18:12.610,what is the next state for a two - player board game ?,reinforcement-learning q-learning,1,0,1
2921,10484,1,,2019-02-10T10:22:22.583,0,49,"i am playing with a deep q - learning algorithm in my own environment . the network can perform well as long as there is only one enemy . my agent can perform the following actions : do_nothing prepare_for(e ) attack(e ) where e is some enemy . in the case of two enemies , the action vector has 5 elements : | 0 | 1 | 2 | 3 | 4 | ----------------------------------------------------------------------------- |do_nothing | prepare_for(e1 ) | attack(e1 ) | prepare_for(e2 ) | attack(e2 ) | ----------------------------------------------------------------------------- after a couple of episodes , the agent always starts picking the first do_nothing action , which is not desired . changing reward for do_nothing action is not helping , even using significantly higher negative reward , than for other actions . there is no problem with the environment with only one enemy . ( only using columns 0 , 1 , 2 ) . i feel like my action encoding can be the issue , but i ca n't figure it out , how to fix it . any suggestions ?",22162,2444,2019-02-10T21:18:55.280,2019-02-10T21:18:55.280,deep q - learning is not performing well when there are several enemies,deep-learning reinforcement-learning deep-rl,1,1,
2922,10487,1,,2019-02-10T11:54:50.650,0,42,"i 've read some surveys about cognitive architectures from the past . soar , act - r , clarion and icarus are prominent examples for realizing a symbolic system which can reasoning about different domains . for a better understanding , i 've created a prototype for a new cognitive architecture from scratch . what the name of the software will be is unclear , but a first screenshot is available . the system contains of a gui in which knowledge containers are visible . each box is a tkinter widget in which normal text can be entered . the content entered into the boxes is oriented to what my understanding of a cognitive architecture is . according to the literature such a system contains of goals , evens , tasks , rules and a linked list which stores procedures . the problem is , that the prototype is n't working . that means , it is only a gui without any functionality . the reason is , that my understanding of cognitive architectures is too low and something apart has to be implemented . my question is : what is the next step ? what should i program to realize a working cognitive architecture ?",11571,,,2019-02-10T11:54:50.650,how to build a cognitive architecture ?,agi,0,1,
2923,10492,1,,2019-02-10T16:34:44.907,1,42,"in the context of reinforcement learning , a policy , , is often defined as a function from the space of states , , to the space of actions , , that is , . this function is the "" solution "" to a problem , which is represented as a markov decision process ( mdp ) , so we often say that is a solution to the mdp . in general , we want to find the optimal policy for each mdp , that is , for each mdp , we want to find the policy which would make the agent behave optimality ( that is , obtain the highest "" cumulative future discounted reward "" , or , in short , the highest "" return "" ) . it is often the case that , in rl algorithms , e.g. q - learning , people often mention "" policies "" like -greedy , greedy , soft - max , etc . , without ever mentioning that these policies are or not solutions to some mdp . it seems to me that these are two different types of policies : for example , the "" greedy policy "" always chooses the action with the highest expected return , no matter which state we are in ; similarly , for the "" -greedy policy "" ; on the other hand , a policy which is a solution to a mdp is a map between states and actions . what is then the relation between a policy which is the solution to a mdp and a policy like -greedy ? is a policy like -greedy a solution to any mdp ? how can we formalise a policy like -greedy in a similar way that i formalised a policy which is the solution to a mdp ? i understand that "" -greedy "" can be called a policy , because , in fact , in algorithms like q - learning , they are used to select actions ( i.e. they allow the agent to behave ) , and this is the fundamental definition of a policy .",2444,,,2019-02-11T08:05:10.490,what is the relation between a policy which is the solution to a mdp and a policy like -greedy ?,reinforcement-learning terminology definitions markov-decision-process policy,1,0,
2924,10493,1,,2019-02-10T16:46:50.767,1,36,"can traditional neural networks be combined with spiking neural networks ? and can there be training algorithms for such hybrid network ? does such hybrid network model biological brains ? as i understand , brains contain only spiking networks and traditional networks are more or less crude approximation of them . but we can imagine that evolutionary computing can surpass the biological evolution and so the new structure can be created that are better than mind . and that is why the question about such tradition - spiking hybrid neural networks should be interesting .",8332,2444,2019-02-10T19:20:42.250,2019-02-10T19:20:42.250,can traditional neural networks be combined with spiking neural networks ?,neural-networks spiking-networks,0,1,
2925,10496,1,,2019-02-10T18:39:19.517,1,18,"i have continual simulated data of million sentences of two simulated persons talking to each other in a room and i want to model one of the persons speech . now , during this period things in the room can change . let 's say , one of them says "" where is the book ? "" the other one responds "" i placed the book on the bookshelf "" . now during time , the position of the book changes , so the question where is the book ? does not have stationary answer i.e the answer changes during time . however , in general the answer has to be "" the book is at some_location "" and not something else . also , the mentioning that the book is placed on the bookshelf can be sometimes 10 , 100 or 1000 sentences before the question "" where is the book ? "" how do you approach this kind of problem ? since the window can be too large i can not split data into training samples of 10 , 100 or 1000 sentences . my guess is that i should use bptt + lstm and train in one shot without shuffling the data . i am not sure this is feasible , so i will greatly appreciate your help ! i have also my doubts what if "" where is the book ? "" appears 20 sentences after ( instead of 10,100 and 1000 ) in the test set ( which is not same as the training set ) ? also , should i use reinforcement learning ( since i can generate the data ) or supervised learning ? thanks a lot !",20378,20378,2019-02-10T18:46:47.927,2019-02-10T18:46:47.927,how to train chat bot on infinite non - stationary data ?,deep-learning reinforcement-learning natural-language-processing lstm,0,0,
2926,10500,1,,2019-02-11T03:34:20.913,1,37,"in the diagram below , there are three variables : x3 is a function of ( depends on ) x1 and x2 , x2 also depends on x1 . more specifically , x3 = f(x1 , x2 ) and x2 = g(x1 ) . therefore , x3 = f(x1 , g(x1 ) ) . if the probabilistic distribution of x1 is known , is it possible to derive the the probabilistic distribution of x3 ?",22184,2444,2019-02-11T21:19:25.473,2019-03-13T23:02:15.593,can we derive the distribution of a random variable based on a dependent random variable 's distribution ?,machine-learning statistical-ai bayes probability-distribution,1,2,
2927,10508,1,,2019-02-11T10:38:55.857,1,52,what is the difference between automatic transcription and automatic speech recognition ? are they the same ? is my following interpretation correct ? automatic transcription : it converts the speech to text by looking at the whole spoken input automatic speech recognition : it converts the speech to text by looking into word by word choices,22195,2444,2019-04-21T13:17:37.423,2019-05-21T14:01:30.450,what is the difference between automatic transcription and automatic speech recognition ?,natural-language-processing difference speech-synthesis,2,0,
2928,10513,1,,2019-02-11T18:41:00.217,1,62,i want to study artificial intelligence but i do n't know how to begin . i want to know study path for artificial intelligence .,22212,,,2019-02-11T20:35:57.560,study path for artificial intelligence,machine-learning,1,1,1
2929,10517,1,,2019-02-12T05:15:40.137,1,50,"suppose $ g_t$ , the discounted return at time $ t$ is defined as : $ $ g_t \triangleq r_t+\gamma r_{t+1}+\gamma^{2}r_{t+2 } + \cdots = \sum_{j=1}^{\infty } \gamma^{k}r_{t+k}$$ where $ r_t$ is the reward at time $ t$ and $ 0 & lt ; \gamma & lt ; 1 $ is a discount factor . let the state - value function $ v(s)$ be defined as : $ $ v_{\pi}(s ) \triangleq \mathbb{e}[g_t|s_{t}=s]$$ in other words , it is the expected discounted return given that we start in state $ s$ with some policy . then $ $ v_{\pi}(s ) = \mathbb{e}_{\pi}[r_t+\gamma g_{t+1}|s_{t}=s]$$ $ $ = \sum_{a } \pi(a|s ) \sum_{s',r } p(r , s'|s , a)[r+\ \gamma v_{\pi}(s')]$$ question 1 . are the states $ s'$ drawn from a from a joint probability distribution $ p_{sa}$ ? in other words , if you are in an initial state $ s$ , take an action , then $ s'$ is the random state you would end up in according to the probability distribution $ p_{sa}$ ? also let $ q_{\pi}(s , a)$ , the action - value function be defined as : $ $ q_{\pi}(s , a ) \triangleq \mathbb{e}_{\pi}[g_t|s_t = s , a_t = a]$$ $ $ = \sum_{s',r } p(r , s'|s , a)[r+\ \gamma v_{\pi}(s')]$$ question 2 . what are the advantages of looking at $ q_{\pi}(s , a)$ versus $ v_{\pi}(s)$ ?",22228,22916,2019-03-18T14:22:24.930,2019-03-18T14:22:24.930,is the next state drawn from the joint distribution of the previous state and action ?,reinforcement-learning math markov-decision-process,1,2,
2930,10520,1,,2019-02-12T08:27:36.637,0,19,"before a robot can act with meaning , some planning is needed . the idea is , that the decision making process is independent from action . the task of figuring out what the best decision is , was introduced under the term cybernetics and is focused on intelligence in animals and artificial systems . in a robot , the act of thinking is realized with computer programs . if we are researching how exactly a computer program determines what the next action for a robot is , the term reinforcement learning is often referenced . the idea is , to store all situations in a q - table and if the robot is in a certain situation he can look into the q - matrix and sees in the row what action is the right one . this is called a policy because it answers the question what action is needed in a certain state . some researchers have recognized that q - learning is not the best way for dealing with complex problems . a policy which is similar to a lookup table will provide only simple formed answers to difficult challenges . the more elaborated form of storing knowledge is called an indirect policy . the idea is here not to store state - action values in a table but use a prediction model . the first thing what the robot software is answering are potential actions and for each action an outcome is calculated . the predicted result of the actions is stored in graph and the best node is identified with a solver . in a concrete situation a direct policy knows what the correct action is , but the indirect policy do n't . the indirect policy has to figure out first what the prediction of future states will be . my question is : is an indirect policy which is using a prediction model superior to a direct policy known as a q - table ?",11571,,,2019-02-12T08:27:36.637,is an indirect policy superior to a normal one ?,q-learning control-problem,0,8,
2931,10525,1,,2019-02-12T10:33:11.910,3,158,"suppose we have a deterministic environment where knowing $ s , a$ determines $ s'$ . is it possible to get two different rewards $ r\neq r'$ in some state $ s_{\text{fixed}}$ ? assume that $ s_{\text{fixed}}$ is a fixed state i get to after taking the action $ a$ . note that we can have situations where in multiple iterations we have : $ $ ( s , a ) \to ( s_1 , r_1 ) \\ ( s , a ) \to ( s_{\text{fixed } } , r_1 ) \\ ( s , a ) \to ( s_{\text{fixed } } , r_2 ) \\ ( s , a ) \to ( s_3 , r_3 ) \\ \vdots$$ my question is , would $ r_1 = r_2 $ ?",22236,2444,2019-02-12T18:05:57.983,2019-02-13T07:43:06.287,can the rewards be stochastic when the transition model is deterministic ?,reinforcement-learning markov-decision-process,3,0,1
2932,10529,1,10538,2019-02-12T12:32:36.097,1,62,"from the reinforcement learning book section 13.3 : using pytorch , i need to calculate a loss , and then the gradient is calculated internally . how to obtain the loss from equations which are stated in the form of an iterative update with respect to the gradient ? in this case : what would be the loss ? and in general , what would be the loss if the update rule were for some general ( derivable ) function $ g$ parameterized by theta ?",21645,,,2019-03-14T19:03:40.740,"how to obtain a formula for loss , when given an iterative update rule in gradient descent ?",machine-learning reinforcement-learning gradient-descent loss-functions,1,6,
2933,10531,1,10537,2019-02-12T15:25:24.263,1,40,"the reinforcement learning book by richard sutton et al , section 13.5 shows an online actor critic algorithm . why do the weights updates depend on the discount factor via $ i$ ? it seems that the more we get closer to the end of the episode , the less we value our newest experience . this seems odd to me . i thought discounting in the recursive formula of itself is enough . why does the weights update become less significant as the episode progresses ? note this is not eligibility traces , as those are discussed separately , later in the same episode .",21645,,,2019-02-12T17:43:25.973,"in online one step actor critic , why does the weights update become less significant as the episode progresses ?",machine-learning reinforcement-learning discount-factor actor-critic,1,0,
2934,10532,1,,2019-02-12T15:35:19.467,0,53,"in my related question , i asked about the one step actor critic from the reinforcement learning book by richard sutton et al , section 13.5 : the learning is becoming less significant as the episode progresses , by means of making $ i$ smaller by the discounting factor , as the episode progresses . how would this discounting generalize to experience replay ? meaning , if we want to update and $ w$ by some experience e= $ ( s , a , r , s')$ , for which we do n't know by how much to discount , how would we accomplish a correct update ? should we remember the discount amount $ i$ in the experience ? please note the critic here is different from the critic here , because it estimates the state - value function $ v(s)$ , rather than the action - state - value function $ q(s , a)$",21645,,,2019-02-12T15:35:19.467,how to correctly discount actor critic with experience replay ?,machine-learning reinforcement-learning discount-factor actor-critic,0,0,
2935,10534,1,10583,2019-02-12T15:49:24.840,0,35,"we recently founded a company in the area of additive manufacturing . our development focuses on making the process easier and liberate time to the user , which i personally believe in as the ultimate promise of any computerised process , including ai , for the public . we have gone through the usual channels including regular job offers and personal contacts , visiting universities and meetups . however there appears to be a lack of participation of real specialists in most of those gatherings as the ones interested in learning about ai are doing just that , and do n't show up somewhere where we can find them , although we believe to have a very interesting task at hand . it appears to be remarkably hard to find specialists in the field , as ai specialists are either employed by huge multinational companies ( instead of startups ) , have no interest in additive manufacturing ( because it implies something else than only code ) and the challenge of machines ( making machines ) or just do not pop up anywhere . how would a specialist of ai look for someone else in the specific field of additive manufacturing / motion planning / creative strategy and its employment ? ( or course in the case they do not already know someone . )",22244,1671,2019-02-14T04:00:47.090,2019-02-15T08:52:22.413,how to find ai specialists interested in additive manufacturing,ai-field,1,0,
2936,10536,1,,2019-02-12T17:13:50.103,2,167,is there any research in this area ?,22251,1671,2019-02-13T02:18:05.090,2019-02-15T14:18:47.947,are there profitable hedge funds using ai ?,machine-learning research algorithmic-trading,3,4,3
2937,10540,1,,2019-02-12T19:22:41.717,2,67,"i am a beginner , just started studying around nlp , specifically various language models . so far , my understanding is that - the goal is to understand / produce natural language . so far the methods i have studied speak about correlation of words , using correct combination to make a meaningful sentence . i also have the sense that the language modeling does not really care about the punctuation marks ( or did i miss it ? ) thus i am curious is there a way they can classify sentence types such as declarative , imperative , interrogative or exclamatory ?",22254,1671,2019-02-14T03:52:18.573,2019-04-15T23:01:20.213,is there a way to understand the type of a sentence ?,natural-language-processing classification computational-linguistics semantics,2,1,
2938,10545,1,,2019-02-13T07:07:05.523,3,32,"for example , you train on dataset 1 with an adaptive optimizer like adam . should you reload the learning schedule etc from the end of training on dataset1 when attempting transfer to dataset2 ? why or why not ?",21158,,,2019-03-15T14:31:14.633,should you reload the optimizer for transfer learning ?,neural-networks deep-learning,2,0,
2939,10546,1,10547,2019-02-13T07:30:20.597,2,170,"i am purchasing titan rtx gpu . everything seems fine with that except float32 & amp ; float64 performance which seems lower vis - a - vis some of its counter parts . i wanted to understand if single precision and double precision performance of gpu affect deep learning training or efficiency ? we work mostly with images , however not limited to that .",17980,9947,2019-02-13T08:01:44.247,2019-02-14T09:03:08.523,does fp32 & fp64 performance of gpu affect deep learning model training ?,deep-learning hardware hardware-evaluation gpu,2,2,
2940,10549,1,10573,2019-02-13T08:33:16.277,2,53,"in the paper "" soft actor - critic : off - policy maximum entropy deep reinforcement learning with a stochastic actor "" , they define the loss function for the policy network as $ $ j_\pi(\phi)=\mathbb e_{s_t\sim \mathcal d}\left[d_{kl}\left(\pi_\phi(\cdot|s_t)\big\vert { \exp(q_\theta(s_t,\cdot)\over z_\theta(s_t)}\right)\right ] $ $ applying the reparameterization trick , let $ a_t = f_\phi(\epsilon_t;s_t)$ , then the objective could be rewritten as $ $ j_\pi(\phi)=\mathbb e_{s_t\sim \mathcal d , \epsilon \sim\mathcal n}[\log \pi_\phi(f_\phi(\epsilon_;s_t)|s_t)-q_\theta(s_t , f_\phi(\epsilon_t;s_t ) ) ] $ $ they compute the gradient of the above objective as follows $ $ \nabla_\phi j_\pi(\phi)=\nabla_\phi\log\pi_\phi(a_t|s_t)+(\nabla_{a_t}\log\pi_\phi(a_t|s_t)-\nabla_{a_t}q(s_t , a_t))\nabla_\phi f_\phi(\epsilon_t;s_t ) $ $ the thing confuses me is the first term in the gradient , where does it come from ? to my best knowledge , the second large term is already the gradient we need , why do they add the first term ?",8689,,,2019-02-14T15:30:03.607,what is the gradient of the objective function in the soft actor - critic paper ?,reinforcement-learning gradient-descent,1,0,1
2941,10551,1,,2019-02-13T10:38:29.573,1,38,"an activation function is a function from $ r \rightarrow r$ . it takes as input the inner products of weights and activations in the previous layer . it outputs the activation . a softmax however , is a function that takes input from $ r^p$ , where $ p$ is the number of possible outcomes that need to be classified . therefore , strictly speaking , it can not be an activation function . yet everywhere on the net it says the softmax is an activation function . am i wrong or are they ?",22273,,,2019-02-13T14:26:05.213,is it a great misconception that the softmax is an activation function ?,neural-networks artificial-neuron activation-function,1,3,
2942,10555,1,,2019-02-13T11:27:58.547,2,63,"i 've noticed that when modelling a continuous action space , the default thing to do is to estimate a mean and a variance where each is parameterized by a neural network or some other model . i also often see that it is one network models both . the reinforce objective can be written as $ $ for discrete action space this makes sense since the output of the network is determined by a softmax . however , if we explicitly model the output of the network as a gaussian , then the gradient of the log likelihood is of a different form , $ $ and the log is : $ $ in the slides provided here ( slide 18 ) : http://www0.cs.ucl.ac.uk/staff/d.silver/web/teaching_files/pg.pdf if the variance is held constant , then we can solve this analytically : $ $ but , are things always modelled assuming a constant variance ? if it 's not constant then we have to account for the inverse of the covariance matrix as well as the determinant ? i 've taken a look at code online and from what i 've seen , most of them assume the variance is constant . @nielslater using the reparameterization trick we would use a normal distribution with fixed parameters 0 and 1 . $ $ a_t \sim \mu_\theta(s_t ) + \sigma_\theta(s_t ) * normal(0 , 1 ) $ $ which is the same as if we had actually sampled directly from a distribution , $ \pi_\theta(a_t | s_t ) = normal(\mu_\theta(s_t ) , \sigma_\theta(s_t))$ and let 's us calculate the corresponding and without having to differentiate through the actual density .",7858,7858,2019-02-13T14:43:54.743,2019-02-15T09:47:37.460,calculating gradient for log policy when variance is not constant,reinforcement-learning policy-gradients,1,7,
2943,10556,1,10558,2019-02-13T11:28:44.470,1,71,"i wonder how self - driving cars determine the path to follow . yes , there 's gps , but gps can have hiccups and a precision larger than expected . suppose the car is supposed to turn right at an intersection on the inner lane , how is the exact path determined ? how does it determine the trajectory of the inner lane ?",13068,,,2019-02-13T15:04:21.553,how do self - driving cars construct paths ?,self-driving path-planning pathfinding,2,0,
2944,10559,1,,2019-02-13T17:04:40.857,1,21,"show which literals can be inferred from the following knowledge bases , using both reasoning patterns and truth tables . show all steps in your reasoning and explain your answers . 1 ) p & amp ; q 2 ) q →r v s 3 ) p → ~r this is from my reasoning pattern tutorial , my text book shows similar question except that there is a single literal with workings , so i can somehow read through but i 'm not familiar with some terms . i do n't understand how i can infer all literals with the above information . i also do n't fully understand what is and - elimination , modus ponens and unit resolution . is there anyone who is kind enough to use the above question as an example so that i can have a clearer picture ?",22282,,,2019-02-13T17:04:40.857,"logic questions : reasoning pattern , infer literals , unit resolution , and - elimination etc",logic,0,1,
2945,10562,1,,2019-02-13T20:37:00.007,0,33,"i 'm new to the whole world of ai and neural networks , and i 'm looking to get started with training a cnn on a cloud service like aws or gcp but i 'm totally overwhelmed by the choices for vm instances . in gcp , there are options that seem to have the same amount of memory but are classifed as standard , highmem , or highcpu -- what 's the difference ? as well what would be some reasons to opt for having a gpu as well ?",22286,1671,2019-02-14T04:06:38.603,2019-03-16T05:02:10.447,choosing an instance in aws or gcp for training a cnn,neural-networks getting-started software-evaluation cloud-services processors,1,0,
2946,10563,1,,2019-02-13T22:52:57.520,1,48,"i am trying to write a genetic algorithm that generates 100 items , assigning random weights and utilities to them . and then try to pick items how out these 100 items while maximising the utility and not picking items over 500ks . the program should return an array of boolean values , where true represents items to be picked and false represents item not to be picked . can someone help with this or point me to a link of something that has been written like this before ?",22288,2444,2019-02-20T18:21:30.357,2019-02-20T18:21:30.357,how do i write a genetic algorithm to solve the knapsack problem ?,genetic-algorithms java,0,1,
2947,10565,1,10605,2019-02-14T03:04:01.177,1,50,"my understanding of the main idea behind a2c / a3c is that we run small segments of an episode to estimate the return using a trainable value function to compensate for the unseen final steps of the episode . while i can see how this could work in continuing tasks with relatively dense rewards , where you can still get some useful immediate rewards from a small experience segment , does this approach work for episodic tasks where the reward is only delivered at the end ? for example , in a game where you only know if you win or lose at the end of the game , does it still make sense to use the a2c / a3c approach ? it 's not clear to me how the algorithm could get any useful signal to learn anything if almost every experience segment has zero reward , except for the last one . this would not be a problem in a pure mc approach for example , except for the fact that we might need a lot of samples . however , it 's not clear to me that arbitrarily truncating episode segments like in a2c / a3c is a good idea in this case .",22297,2444,2019-02-15T17:29:37.227,2019-02-15T19:47:58.540,are a2c or a3c suitable for episodic tasks where the reward is delivered only at the end of the episode ?,reinforcement-learning actor-critic,1,0,
2948,10566,1,,2019-02-14T08:30:40.563,1,40,"one of the most common misconceptions about reinforcement learning ( rl ) applications is that , once you deploy them , they continue to learn . and , usually , i 'm left having to explain this . as part of my explanations , i like to show where it is being used and where not . i 've done a little bit of research on the topic , but the descriptions seem fairly academic , and i 'm left with the opinion that reinforcement learning is not really suitable for financial services in regulated markets . am i wrong ? if so , i would like to know where rl is being used ? also , if it is , how are the rl algorithms governed ?",19484,2444,2019-02-15T00:49:48.360,2019-02-15T00:49:48.360,where are reinforcement algorithms used in financial services ?,reinforcement-learning applications,0,5,
2949,10567,1,,2019-02-14T08:47:05.807,0,103,"which representation is most biologically plausible for actor nodes ? for example , actions represented across several output nodes which may be either mutually exclusive with each other ( e.g. , go north , go south , etc ) , achieved by winner - takes - all . not mutually exclusive with each other ( e.g. left leg forward , right leg forward ) ; these actions may occur concurrently . to go north , the correct combination of nodes must be active . similarly which representation is most plausible for critic output nodes ? a single output node that outputs a real number representing the reward . a set of output nodes each representing a separate value , achieved by winner - takes - all . or do other representations better align with real brains ?",22305,2444,2019-02-15T00:53:02.033,2019-02-15T08:32:37.753,what is the most biologically plausible representation for the actor and critic ?,neural-networks reinforcement-learning actor-critic,1,2,
2950,10568,1,,2019-02-14T09:31:25.837,1,18,"feature visualization allows to better understand neural networks by generating images that maximize the activation of a specific neuron , and therefore understand what are the abstract features that produce a high activation . the examples that i saw so far are related to classification tasks . so my question is : can these concepts be applied to other convolutional neural network tasks , like semantic segmentation or image embedding ( triplet loss ) ? what can i expect if i apply visualization algorithms to these networks ?",16671,,,2019-02-14T09:31:25.837,feature visualization on neural networks which are not for classification,neural-networks convolutional-neural-networks optimization,0,0,
2951,10572,1,10609,2019-02-14T13:46:38.480,0,63,"i have to calculate the affluence in localities of metro city . to calculate affluence , i am considering a parameter per capita income . where i can get a dataset of it ? what are other parameters i should consider for the problem ? any guidance will be fruitful for me .",15368,2444,2019-02-15T20:54:21.607,2019-02-15T23:24:26.520,parameters to calculate affluence in localities of metro city,datasets data-science,1,3,0
2952,10574,1,,2019-02-14T15:40:47.267,0,23,"at present i use decaying ϵ-greedy to pick what action to take , but i 've been looking into ways of better exploring the state - action space . i 've read up on hoeffding 's inequality / ucb , and probability matching , but these methods seem to involve keeping a count of the number of times an action was performed for a given state . but what if the state - space is massive and keeping a record of states and counts is n't feasible ? if ϵ has decayed to near zero already then it will begin to behave greedily and the actions further down the tree which are hit on very rarely could then be ' locked in ' without any future exploration . but if i set the ϵ-decay very small , then i 'll continue to explore state / actions higher up the tree that the agent is already confident about , which is wasteful as instead it would have been better to proceed to exploring the rest of the tree that it 's unsure about . i seem to have somewhat of a paradox of wanting to use ucb or probability matching to ensure my agent better explores because i have a large state - action space , but i 'm unable to use it precisely because i have a large state - action space and recording / looking up counts is n't really feasible . d'oh !",20352,,,2019-02-14T15:40:47.267,"action exploration using probability matching , hoeffding 's inequality ( ucb )",reinforcement-learning,0,0,
2953,10575,1,,2019-02-14T17:03:57.140,1,101,"in reinforcement learning , we often define two functions , the "" state - value function "" $ $ v^\pi(s ) = \mathbb{e}_{\pi}[\sum_{k=0}^{\infty } \gamma^{k}r_{t+k+1}|s_t = s]$$ and the "" state - action value function "" : $ $ q^\pi(s , a ) = \mathbb{e}_{\pi}[\sum_{k=0}^{\infty } \gamma^{k}r_{t+k+1}|s_t = s , a_t = a]$$ where means that these functions are defined as the "" expectation "" with respect to a fixed policy of what is often called the "" return "" , , where is a "" discount factor "" and $ r_{t+k+1}$ is the reward received from the environment ( while the agent interacts with it ) from time $ t$ onwards . so , both the $ v$ and $ q$ functions are defined as expectations of the return ( or the "" cumulative future discounted reward "" ) , but these expectations have different "" conditions "" ( or are conditioned on different variables ) . the $ v$ function is the expectation ( with respect to a fixed policy ) of the return given that the current state ( the state at time $ t$ ) is $ s$ . the $ q$ function is the expectation ( with respect to a fixed policy ) of the return conditioned on the fact that the current state the agent is in is $ s$ and the action the agent takes at $ s$ is $ a$ . i know that the bellman optimality equation for $ v^*$ ( the optimal value function ) can be expressed as the bellman optimality equation for $ q^{\pi^*}$ ( the optimal state - action value function associated with the optimal policy ) as follows $ $ v^*(s ) = \max_{a \in \mathcal{a}(s ) } q^{\pi^*}(s , a ) $ $ this is actually shown ( or proved ) at page 76 of the book "" reinforcement learning : an introduction "" ( 1st edition ) by andrew barto and richard s. sutton . are there any other functions , apart from the $ v$ and $ q$ functions defined above , in the rl context ? if so , how are they related ? for example , i 've heard of the "" advantage "" or "" continuation "" functions . how are these functions related to the $ v$ and $ q$ functions ? when should one be used as opposed to the other ? note that i 'm not just asking about the "" advantage "" or "" continuation "" functions , but , if possible , any existing function that is used in rl that is similar ( in purpose ) to these mentioned functions , and how they are related to each other .",2444,2444,2019-02-14T20:44:45.090,2019-04-14T07:36:45.530,what are the value functions used in reinforcement learning ?,reinforcement-learning rl-an-introduction q-function v-function,1,2,
2954,10578,1,,2019-02-14T21:44:23.633,0,30,"tl;dr : read the bold . the rest are details i am trying to implement reinforcement learning : an introduction , section 13.5 myself : on openai 's cartpole the algorithm seems to be learning something useful ( and not random ) , as shown in these graphs ( different zoom on the same run ) : which show the reward per episode ( y axis is the "" time alive "" , x axis is episode number ) . however , as can be seen , the learning does not seem to stabilize . it looks like every time the reward maxes out ( 200 ) , it immediately drops . my relevant code for reference ( inspired by pytorch 's actor critic ) note : in this question , xp_batch is only the very last ( s , a , r , s ' ) , meaning experience replay is not in use in this code ! the actor and critic are both distinct neural networks which def learn(self , xp_batch):#in this question , xp_batch is only the very last ( s , a , r , s ' ) for s_t , a_t , r_t , s_t1 in xp_batch : expected_reward_from_t = self.critic_nn(s_t ) probs_t = self.actor_nn(s_t ) expected_reward_from_t1 = torch.tensor([[0 ] ] , dtype = torch.float ) if s_t1 is not none : # s_t is not a terminal state , s_t1 exists . expected_reward_from_t1 = self.critic_nn(s_t1 ) m = categorical(probs_t ) log_prob_t = m.log_prob(a_t ) delta = r_t + self.args.gamma * expected_reward_from_t1 - expected_reward_from_t loss_critic = delta * expected_reward_from_t self.critic_optimizer.zero_grad ( ) loss_critic.backward(retain_graph = true ) self.critic_optimizer.step ( ) delta.detach ( ) loss_actor = delta * log_prob_t self.actor_optimizer.zero_grad ( ) loss_actor.backward ( ) self.actor_optimizer.step ( ) def select_action(self , state ) : probs = self.actor_nn(state ) m = categorical(probs ) action = m.sample ( ) return action my questions are : am i doing something wrong , or is this to be expected ? i know this can be improved with eligibility traces / experience replay+off policy learning . before making those upgrades , i want to make sure the current results make sense .",21645,,,2019-02-14T21:44:23.633,"how to make episode ending "" good "" in reinforcement learning ?",machine-learning reinforcement-learning pytorch,0,5,
2955,10579,1,,2019-02-15T04:49:44.107,2,65,"intuitively , how do temporal - difference and monte carlo methods work in reinforcement learning ? how can they be used to solve the reinforcement learning problem ?",22337,2444,2019-02-15T14:34:03.807,2019-02-15T14:34:03.807,what are temporal - difference and monte carlo methods intuitively ?,reinforcement-learning monte-carlo temporal-difference,1,1,
2956,10580,1,10587,2019-02-15T06:36:26.550,2,38,"what is the common representation used for the state in articulated robot environments ? my first guess is that it 's a set of the angles of every joint . is that correct ? my question is motivated by the fact that one common trick that helps training neural nets in general is to normalize the inputs , like setting mean = 0 and std dev = 1 , or scaling all the input values to $ [ 0 , 1]$ , which could be easily done in this case too if all the inputs are angles in $ [ 0 , 2 \pi]$ . but , what about distances ? is it common to , for example , use as input some distance of the agent to the ground , or a distance to some target position ? in that case , the scale of the distances can be arbitrary and vary a lot . what are some common ways to deal with that ?",22297,,,2019-02-15T10:37:28.507,how to normalize the state space for articulated robot environments ?,reinforcement-learning robotics,1,2,
2957,10584,1,,2019-02-15T09:24:46.600,2,34,"in section "" 5.2 monte carlo estimation of action values "" of the second edition of the reinforcement learning book by sutton and barto , this is stated : if a model is not available , then it is particularly useful to estimate action values ( the values of state – action pairs ) rather than state values . with a model , state values alone are sufficient to determine a policy ; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state , as we did in the chapter on dp . however , i do n't see how this is true in practice . i can see how it 'd work trivially for discrete state and action spaces with deterministic environment dynamics , because we could compute by just looking at all possible actions and choosing the best one . as soon as i think about continuous state and action spaces with stochastic environment dynamics , computing the seems to be become very complicated and impractical . for the particular case of continuous states and discrete actions , i think estimating an action value might be more practical to do even if a forward model of the environment dynamics is available , because the becomes easier ( i 'm especially thinking of the approach taken in deep q learning ) . am i correct in thinking this way or is it true that if a model is available it 's not useful to estimate action values if state values are already available ?",17312,2444,2019-02-15T13:44:27.380,2019-03-17T14:00:20.870,why is the state value function sufficient to determine the policy if a model is available ?,reinforcement-learning value-function v-function,1,0,
2958,10586,1,,2019-02-15T10:28:54.233,3,78,"i often see the terms episode , trajectory and rollout to refer to basically the same thing , a list of ( state , action , rewards ) . are there any concrete differences between the terms or can they be used interchangeably ? in the following paragraphs , i 'll summarize my current slightly vague understanding of the terms . please point any inaccuracy or missing details in my definitions . i think episode has a more specific definition in that it begins with an initial state and finishes with a terminal state , where the definition of whether or not a state is initial or terminal is given by the definition of the mdp . also , i understand an episode as a sequence of $ ( s , a , r)$ sampled by interacting with the environment following a particular policy , so it should have a non - zero probability of occurring in the exact same order . with trajectory , the meaning is not as clear to me , but i believe a trajectory could represent only part of an episode and maybe the tuples could also be in an arbitrary order ; even if getting such sequence by interacting with the environment has zero probability , it 'd be ok , because we could say that such trajectory has zero probability of occurring . i think rollout is somewhere in between , since i commonly see it used to refer to a sampled sequence of $ ( s , a , r ) $ from interacting with the environment under a given policy , but it might be only a segment of the episode , or even a segment of a continuing task , where it does n't even make sense to talk about episodes .",12640,2444,2019-02-15T17:21:45.107,2019-03-17T21:00:55.150,"what is the difference between an episode , a trajectory and a rollout ?",reinforcement-learning terminology difference,1,0,1
2959,10590,1,10602,2019-02-15T12:33:20.440,0,48,"the turing - machine is well known in the literature . on the turing - machine it is possible to calculate the busy beaver problem . a more recent approach to implement a computer is a random boolean network . this looks similar to a neural network but contains of logic gates . the advantage over propositional logic is that a feedback loop is in the network , which makes the system turing - powerful . writing the sourcecode for realizing a rbn is not that hard . the problem is similar to neural networks . after executing the demo program , the cpu runs at 100 % but the weights in the network ca n't be found . in the sourcecode , the boolean network should determine the weights for a calculator . some example input / output patterns are given . the idea is , that the 4 first neurons are the input and the 4 last neurons are the output . on a standard pc the network was n't able to find the correct pattern . that means , it 's possible to build with “ and , or , not , xor ” operators a full - adder , but the boolean network does n't find so by it 's own . question : are random boolean networks useless , because there is no algorithm available for determine the parameters ? # ! /usr / bin / env python # -*- coding : utf-8 -*- import random class network : def _ _ init__(self , maxnode ) : self.maxnode=maxnode self.op=(""and"",""or"",""xor"",""not "" ) self.node= [ ] # ( input1 , input2 , operator , boolearnvalue ) def show(self ) : for i in range(len(self.node ) ) : print i , self.node[i ] def randomnetwork(self ) : self.node= [ ] for i in range(self.maxnode ) : nodea = random.randint(0,self.maxnode-1 ) nodeb = random.randint(0,self.maxnode-1 ) op = random.choice(self.op ) self.node.append((nodea,nodeb,op,0 ) ) def setinput(self , listinput ) : for i in range(len(self.node ) ) : temp = self.node[i ] self.node[i]=(temp[0],temp[1],temp[2],listinput[i ] ) def getoutput(self ) : result= [ ] for i in range(len(self.node ) ) : result.append(self.node[i][3 ] ) return result def update(self ) : for i in range(len(self.node ) ) : self.updatenode(i ) def updatenode(self , nodeid ) : # get input pos = self.node[nodeid][0 ] input1= self.node[pos][3 ] pos = self.node[nodeid][1 ] input2= self.node[pos][3 ] # operator operator = self.node[nodeid][2 ] if operator==self.op[0 ] : result = min(input1,input2 ) if operator==self.op[1 ] : result = max(input1,input2 ) if operator==self.op[2 ] : result = input1 ^ input2 # xor if operator==self.op[3 ] : result=1-input1 # stpre result temp = self.node[nodeid ] self.node[nodeid]=(temp[0],temp[1],temp[2],result ) class solver : def _ _ init__(self ) : self.input= [ ] self.output= [ ] # first 4 nodes are input , last 4 nodes are output self.input.append((0,1,0,1,0,0,0,0 ) ) self.output.append((0,0,0,0,0,0,1,0 ) ) # 1 + 1=2 self.input.append((0,1,1,0,0,0,0,0 ) ) self.output.append((0,0,0,0,0,0,1,1 ) ) # 1 + 2=3 self.input.append((0,0,0,1,0,0,0,0 ) ) self.output.append((0,0,0,0,0,0,0,1 ) ) # 0 + 1=1 self.input.append((1,1,0,1,0,0,0,0 ) ) self.output.append((0,0,0,0,0,1,0,0 ) ) # 3 + 1=4 self.mynet=network(8 ) self.run ( ) def run(self ) : # self.mynet.show ( ) for trial in range(1000000 ) : self.mynet.randomnetwork ( ) score = self.getscore(false ) if score&gt;=16 : print trial , score self.getscore(true ) if trial%100000==0 : print trial def getscore(self , verbose ) : result=0 for i in range(len(self.input ) ) : # over all examples self.mynet.setinput(self.input[i ] ) for iupdate in range(10 ) : # iteration self.mynet.update ( ) outputnet = self.mynet.getoutput ( ) similarity = self.similarity(self.output[i],outputnet ) result + = similarity if verbose==true : print i , self.input[i ] print i , outputnet return result def similarity(self , lista , listb ) : count=0 for i in range(4,len(lista ) ) : # only check second half if lista[i]==listb[i ] : count + = 1 return count mysolver = solver ( )",11571,,,2019-03-17T20:00:55.263,are random boolean networks useless ?,machine-learning halting-problem,1,2,
2960,10591,1,,2019-02-15T13:20:50.677,8,711,"is the optimal policy always stochastic ( that is , a map from states to a probability distribution over actions ) if the environment is also stochastic ? intuitively , if the environment is deterministic ( that is , if the agent is in a state $ s$ and takes action $ a$ , then the next state $ s'$ is always the same , no matter which time step ) , then the optimal policy should also be deterministic ( that is , it should be a map from states to actions , and not to a probability distribution over actions ) .",2444,2444,2019-02-15T14:22:21.487,2019-02-16T02:37:23.903,is the optimal policy always stochastic if the environment is also stochastic ?,reinforcement-learning stochastic-policy deterministic-policy policy environment,4,1,3
2961,10603,1,10631,2019-02-15T19:30:11.267,3,204,"this is not meant to be negative or a joke but rather looking for a productive solution on ai development , engineering and its impact on human life : lately with my google searches , the ai model keeps auto filling the ending of my searches with : “ ... in vietnamese ” and “ ... in a vietnamese home ” the issue is i have never searched for that but because of my last name the model is creating this context . the other issue is that i ’m a halfy and my dad is actually third generation , i grew up mainstream american and do n’t even speak vietnamese . i ’m not even sure what a vietnamese home means . my buddy in a similar situation of south asian and noticed the same exact thing more so with youtube recommended videos . we already have enough issues in the us with racism , projections of who others expect us to be based on any number of things , stereotyping and putting people in boxes to limit them - i truly believe ai is adding to the problem , not helping . how can we fix this . moreover , how can we use ai to bring out peoples true self , talents and empower and free them them to create their life how they like ? there is huge potential here to harness ai in ways that can bring us more freedom , joy and beauty so people can be the whole of themselves and with who they really are . then meet peoples needs , wishes , dreams and hope . given them shoulders to stand on to create their reality , not live someone else 's projection of themselves .",22368,22368,2019-03-16T19:29:34.187,2019-03-16T19:29:34.187,"how is it that ai can become biased , and what are the proposals to mitigate this ?",philosophy human-like social,3,8,1
2962,10607,1,,2019-02-15T22:19:54.047,0,58,"i 'd like to use machine learning to guess a mathematical pattern : the input are certain polynomials in four variables $ q_1,q_2,q_3,q_4 $ , the output can be zero or one . allowed polynomials are such that ( i ) all their non - zero coefficients are equal to one , ( ii ) they do not contain monomials of the form $ q_1^j$ for $ j \geq 0 $ , and ( iii ) if an allowed polynomial contains a monomial $ m = q_1^a q_2^b q_3^c q_4^d$ for some non - negative integers $ a , b , c , d$ , then it also contains $ m'=q_1^{a-1 } q_2^b q_3^c q_4^d$ , provided this does not violate ( ii ) and $ a \geq 1 $ ; similarly for $ b \to b-1 $ , $ c \to c-1 $ , and $ d \to d-1 $ . here 's an example batch , given by pairs { input , output } : here 's a second batch : i can construct larger and larger batches using mathematica , and i 'd like to know how to practically go from here , to instructing an ai to guess a simple function of $ q$ 's that reproduces the behavior , namely that can guess the correct output for previously unknown admissible polynomials . what are the typical batch size and computational power required for such a program to succeed ? my idea is to use a function from the space of allowed polynomials to the set , of the form , $ p=\sum_{i \in i } m_i \mapsto \phi(p):= \sum_{i \in i ' \subset i } m_i|_1 \mod 2 $ , where $ m_i|_1 $ means the $ i$ -th monomial inside $ p$ evaluated at $ q_1 = q_2 = q_3 = q_4=1 $ , and come up with the form of $ i'$ as function of $ i$ . notice there 's no linear structure on . remark : of course instead of polynomials one could use punctured solid partitions .",22371,22371,2019-02-18T16:01:50.273,2019-02-18T16:01:50.273,using ai to guess a mathematical pattern of certain polynomials in four variables : practical challenge,neural-networks machine-learning deep-learning python implementation,0,8,1
2963,10610,1,,2019-02-15T23:47:47.353,0,30,"i have been trying to solve a sequential decision - making problem that involves taking "" sub - actions "" . i would like to share my experience and really appreciate any suggestions / insights . suppose we model it as a traveling salesman problem ( tsp ) that has 5 nodes ( however , we do n't need to go back to the starting node in the end ) , and at each node , we also have to decide what to do over 3 possible actions . for example , one possible path is : ( node 2 , action 0 ) ( node 4 , action 2 ) ( node 3 , action 1 ) ( node 1 , action 0 ) ( node 0 , action 2 ) one challenging part is that we can not get the intermediate reward and only the final evaluation score is provided . therefore formulating this problem with td algorithms sounds not that intuitive . methods i have tried includes : genetic algorithm ( ga ) : this method works well overall , but not that "" attractive "" as a piece of research work monte carlo tree search ( mcts ) : i implemented the vanilla version of mcts and set the random rollout to 10k , it can surpass the ga 's performance , but with a tradeoff of 10x computation time i went on to evaluate the cross - entropy method ( cem ) by modeling the sequence generation as a markov chain and sub - action as an independent bernoulli distribution , but the results are rather poor several future work directions i can think of now : speed up the mcts , perhaps by pruning or parallel processing ? methods like seqgan , but was wondering how to deal with the sub - actions , or , how to generate tuples try td approach by assigning 0-reward for the intermediate states and only evaluate the final state ? would really love to hear your opinions and advice ! cheers !",22372,16565,2019-02-18T17:59:20.747,2019-02-18T17:59:20.747,how to approach the sequential decision - making problem with sub - actions with rl ?,reinforcement-learning combinatorial-games,0,1,
2964,10611,1,10614,2019-02-16T00:21:40.397,1,57,"i am in the process of writing my own basic machine learning library in python as an exercise to gain a good conceptual understanding . i have successfully implemented backpropagation for activation functions such as and the sigmoid function . however , these are normalised in their outputs . a function like relu is unbounded so its outputs can blow up really fast . in my understanding , a classification layer , usually using the softmax function , is added at the end to squash the outputs between 0 and 1 . how does backpropagation work with this ? do i just treat the softmax function as another activation function and compute its gradient ? if so , what is that gradient and how would i implement it ? if not , how does the training process work ? if possible , a pseudocode answer is preferred .",22373,,,2019-02-16T15:06:03.950,how does backpropagation with unbounded activation functions such as relu work ?,neural-networks backpropagation relu,1,0,
2965,10615,1,10616,2019-02-16T14:04:33.143,1,99,"this is an excerpt taken from sutton and barto ( pg . 3 ) : another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal - directed agent interacting with an uncertain environment . this is in contrast with many approaches that address subproblems without addressing how they might fit into a larger picture . for example , we have mentioned that much of machine learning research is concerned with supervised learning without explicitly specifying how such an ability would finally be useful . other researchers have developed theories of planning with general goals , but without considering planning 's role in real - time decision- making , or the question of where the predictive models necessary for planning would come from . although these approaches have yielded many useful results , their focus on isolated subproblems is a significant limitation . i have an idea of supervised learning , but what exactly does the author mean by planning ? and how is the rl approach different from planning and supervised learning ? ( illustration with an example would be nice ) .",9947,22424,2019-02-20T14:31:57.527,2019-02-20T14:31:57.527,rl vs supervised learning vs planning,machine-learning reinforcement-learning planning,2,0,
2966,10617,1,10619,2019-02-16T16:09:31.217,0,47,"i am trying to understand how rnns are used for sequence modelling . on a tutorial here , it mentions that if you want to translate say a sentence from english to french you can use an encoder - decoder set - up as they described . however what if you want to do a sequence to sequence modelling where your inputs and outputs are of the same domain but you just want to predict the next output of a sequence . for example if i want to use sequence modelling to learn the sine function . so say i have 20 y - coordinates from $ y = sin(x)$ from 20 evenly spaced out x - coordinates and i want to predict the next 10 or so y - coordinates . would i use an encoder - decoder setup here ?",18312,2444,2019-02-21T11:17:40.413,2019-02-21T11:17:40.413,do i need an encoder - decoder architecture to predict the next item of a sequence ?,deep-learning natural-language-processing recurrent-neural-networks machine-translation,2,0,
2967,10620,1,,2019-02-16T18:59:07.733,0,33,"in reinforcement learning ( rl ) , there are model - based and model - free algorithms . in short , model - based algorithms use a transition model ( e.g. a probability distribution ) and the reward function , even though they do not necessarily compute ( or estimate ) them . on the other hand , model - free algorithms do not use such transition model or reward function , but they directly estimate e.g. the state or state - action value functions by interacting with the environment , which allows the agent to infer the dynamics of the environment . given that model - based rl algorithms do not necessarily estimate or compute the transition model or reward function , in the case these are unknown , how can them be computed or estimated ( so that they can be used by the model - based algorithms ) ? in general , what are examples of algorithms that can be used to estimate the transition model and reward function of the environment ( represented as either an mdp , pomdp , etc . ) ?",2444,,,2019-02-16T18:59:07.733,how can we estimate the transition model and reward function ?,reinforcement-learning algorithm markov-decision-process model-based model-free,0,0,
2968,10621,1,,2019-02-16T19:15:53.957,1,22,"introduction : the notion that various social complex systems ( e.g. society , family , business company , state , etc ) could be regarded as ones exhibiting consistent traits of behaviour of their own - suggesting that they are entities unto themselves , some sort of organisms on their own or even intelligent entities on their own - is not new . i have personally stumbled upon papers of scholars who straightforwardly speak of such systems as if they were already proven to be singular entities . that kind of assumption has entered the vernacular , as well , long time ago - e.g. "" the state wants to ... "" , "" society responds to conflict by ... "" , "" the family dynamics seeks balance through ... "" , etc . therefore , we could even assume at one point , that such social systems are not only organisms of their own , but even some sort of artificial intelligence entities ( as long as we could see them as an artificial product of human activity ) . we are generally used to seeing ourselves as conscious entities and we are also good at exploring entities of less complexity than ourselves . but when it comes to entities which consists of us as mere components , we are not ready to mentally process that idea - it sounds as either too abstract or too sci - fi ( think of stanisław lem 's work ) . question : while the average joe could easily say "" the state wants to ... "" or "" society responds to ... "" , etc , how exactly do we prove ( or at least gather some sort of supporting evidence ) that a complex social system really exhibits a behaviour of its own ? under what conditions could we regard it as some sort of spontaneously born artificial intelligence ? if that were true , how could we predict if that ai would procreate and bring about other social system forms which are also entities unto themselves ? how could we possibly become aware if that has already happened ?",22390,22390,2019-02-18T12:56:58.083,2019-02-18T12:56:58.083,complex systems constituting an entity unto itself,philosophy social artificial-consciousness,0,1,
2969,10622,1,,2019-02-16T19:21:08.867,1,35,"i am reading about the actor - critic architecture . i am confused about how the actor determines the action using the value ( or future reward ) from the critic network . below you have the most popular picture of actor - critic network . it looks like the input of the actor network is only the "" state "" variable ( $ s_t$ ) , it has nothing to do with the critic network . however , from the equation below the actor seems to be related to critic network . i have a few questions does actor network has two inputs , state variable and future reward ( output from critic network ) , or only the state variable ? if the actor network does take future reward as input , how does it use it ? only during the training stage , or in the action making stage ? is there a "" policy iteration "" procedure happens during decision making stage , i.e. for every state $ s_t$ , policy network will make several attempts with critic network , and output the best policy ?",22393,2444,2019-02-17T02:11:45.480,2019-02-17T02:11:45.480,how the actor use the output from the critic to make action in actor - critic network ?,reinforcement-learning actor-critic,0,4,
2970,10623,1,,2019-02-16T20:02:58.273,3,877,what is self - supervision in machine learning ? is it related to supervised learning ? how is it different from supervised learning ?,2444,2444,2019-03-02T22:58:41.137,2019-03-02T22:58:41.137,what is self - supervised learning in machine learning ?,machine-learning supervised-learning self-supervised-learning,2,6,
2971,10625,1,10637,2019-02-16T21:15:07.833,2,43,"in this tutorial from jeremy howard : what is torch.nn really ? he has an example towards the end where he creates a cnn for mnist . in nn.conv2d he makes the inchannels and outchannels : ( 1,16 ) , ( 16,16 ) , ( 16,10 ) . i get that the last one has to be 10 because there are 10 classes and we want ' probabilities ' of each class . but why go up to 16 first ? how do you choose this value ? and why not just go from 1 to 10 , 10 to 10 , and 10 to 10 ? does this have to do with the kernel_size and stride ? all of the images are 28x28 so i ca n't see any correlation between these values and 16 either . class mnist_cnn(nn.module ) : def _ _ init__(self ) : super().__init _ _ ( ) self.conv1 = nn.conv2d(1 , 16 , kernel_size=3 , stride=2 , padding=1 ) self.conv2 = nn.conv2d(16 , 16 , kernel_size=3 , stride=2 , padding=1 ) self.conv3 = nn.conv2d(16 , 10 , kernel_size=3 , stride=2 , padding=1 ) def forward(self , xb ) : xb = xb.view(-1 , 1 , 28 , 28 ) xb = f.relu(self.conv1(xb ) ) xb = f.relu(self.conv2(xb ) ) xb = f.relu(self.conv3(xb ) ) xb = f.avg_pool2d(xb , 4 ) return xb.view(-1 , xb.size(1 ) ) ` ` `",12983,,,2019-02-17T17:07:13.017,mnist cnn architecture,convolutional-neural-networks image-recognition pytorch,1,0,
2972,10626,1,,2019-02-17T04:50:06.717,0,46,"i have the following problem : consider the problem of placing 2 knights and 2 queens on a 4 x 4 chessboard , each piece ( knight or queen ) per row ( denoted by r1 , r2 , r3 , and r4 ) as shown in the figure , such that no two pieces are attacking each other . one knight must be placed in row r1 , the other in r2 , one queen in r3 , and the other queen in r4 . formulate this problem as a constraint satisfaction problem . describe the set of variables , and their domain . list the set of constraints . draw the constraint graph .",22397,1671,2019-02-19T21:12:25.423,2019-02-19T21:12:25.423,how to solve constraint satisfaction problem of two queen and two knights on a 4x4 grid,ai-basics,0,1,
2973,10634,1,,2019-02-17T12:29:10.860,4,74,"i have a system ( like a bank ) that people ( customers ) are entered into the systems by a poisson process , so the time between the arrival of people ( two consecutive customers ) will be a random variable . the state of the problem is related to just the system ( bank ) , and the action , made inside the system , can be e.g. offering the customer promotion or not ( just based on the state of the system , not the status of customers ) . to model the problem through rl , 1 ) it is possible to discretize time horizon into very short time interval ( for example 5 minutes as a stage ) such that in each time interval , just a single customer enter to our system . on the other hand , 2 ) it is possible that stages are defined as the time when a customer enters our system . my questions are : is the second approach an semi - mdp ( smdp ) ? if i want to solve it with rl , should i use hierarchical rl ? in the first approach , if a customer enters in a time interval , it is easy to update the q values . however , what should we do , if we are in state $ s$ and take action $ a$ , but no customer enters our system , so we do not receive any reward for the pair of $ ( s , a)$ ? there would be no difference if we would take action $ a_{1}$ , $ a_{2}$ , and so on . this can happen for several consecutive time intervals . i think it is more challenging when we consider eligibility traces .",10191,2444,2019-02-18T18:11:26.787,2019-03-20T20:01:13.420,should i model my problem as a semi - mdp ?,reinforcement-learning markov-decision-process semi-mdp hierarchical-rl,1,0,1
2974,10636,1,10638,2019-02-17T14:55:39.330,1,143,"first of all , there is a lot of misunderstanding about the graph search and tree search . the difference between these two is not about graph and tree . they have two different algorithms . you can find the difference in the first answer in this link : what is the difference between graph - search and tree search ? . so now i want to be sure that i have understood these two searches well . we have a graph search and a tree search for each searching algorithm . for example bfs graph search and bfs tree search . or dfs graph search and dfs tree search ( dfs tree search is not complete for example ) . iterative deepening graph search and iterative deepening tree search . please answer if you are well informed in this subject because as you see in the above link many users have answered the question wrong ( the difference between graph search and tree search )",22407,9947,2019-02-17T15:07:54.763,2019-02-17T18:34:13.797,difference between graph search and tree search,search breadth-first-search,2,2,
2975,10641,1,,2019-02-17T19:37:42.553,3,53,"i 'm working on my own implementation of neat algorithm based on the original 2002 paper called "" efficient reinforcement learning through evolving neural network topologies "" ( by kenneth o. stanley and risto miikkulainen ) . the way the algorithm is designed it may generate loops in connection of hidden layer . which obviously will cause difficulties in calculating the output . i have searched and came across two types of approaches . one set like this example claim that the value should be calculated like a time series usually seen in rnns and the circular nodes should use "" old "" values as their "" current "" output . but , this seems wrong since the training data is not always ordered and the previous value has nothing to do with current one . a second group like this example claim that the structure should be pruned with some method to avoid loops and cycles . this approach apart from being really expensive to do , is also against the core idea of the algorithm . deleting connections like this may cause later structural changes . i my self have so far tried setting the unknown forward values as 0 and this hides the connection ( as whatever weight it has will have no effect on the result ) but have failed also for two reasons . one is my networks get big quickly destroying the "" smallest network required "" idea and also not good results . what is the correct approach ?",6522,2444,2019-02-17T20:57:51.213,2019-02-17T20:57:51.213,how do you implement neat by taking into account the loops ?,neat,0,0,
2976,10643,1,,2019-02-17T21:58:59.010,1,39,"lets say we have a oracle $ s$ that , given any function $ f$ and desired output $ y$ , can find an input to $ x$ that causes $ f$ to output $ y$ if it exists , or otherwise returns nil . i.e. : $ $ s(f , y ) = x \implies f(x ) = y$$ $ $ s(f , y ) = nil \implies ! \exists x \hspace{10px}s.t.\hspace{10px } f(x ) = y$$ and $ s$ takes $ 1 $ millisecond to run ( plus the amount of time it takes to read the input and write the output ) , regardless of $ f$ or $ y$ . $ f$ is allowed to include calls to $ s$ in itself . clearly with this we can solve any np - complete problem in constant time ( plus the amount of time it takes to read the input and write the output ) , and in fact we can go further and efficiently solve any optimization problem : def ismin(cost , meetsconstraints , x ) : def hassmaller(y ) : return meetsconstraints(x ) and cost(y ) & lt ; cost(x ) and y ! = x return meetsconstraints(x ) and s(hassmaller , true ) = = nil def findmin(cost , meetsconstraints ) : def helper(x ) : return ismin(cost , meetsconstraints , x ) return s(helper , true ) which means we can do something like : def findsmallestrecurrentneuralnetworkthatperfectlyfitsdata(data ) : def meetsconstraints(x ) : return isrecurrentneuralnetwork(x ) and error(x , data ) = = 0 return findmin(numparamaters , meetsconstraints ) and something similar for any other kind of model ( random forest , random ensemble of functions , etc . ) . we can even solve the halting problem with this , which probably means that there is some proof similar to the halting problem proof that shows such an oracle could not exist . lets assume this exists anyway , as a thought experiment . but i 'm not sure how to take it from here to something that achieves endless self improvement . what exactly the "" singularity "" even means i suppose is tricky to define formally , but i 'm interested in any simple definitions , even if they do n't quite capture it . a sidenote , here is one more function we can do : isequivalent(g , h ) : def helper(x ) : return g(x ) ! = h(x ) return p(helper , true ) = = nil",6378,6378,2019-02-17T23:06:36.670,2019-02-17T23:06:36.670,is a very powerful oracle sufficient to trigger the ai singularity ?,algorithm optimization singularity,0,8,2
2977,10644,1,,2019-02-17T23:18:35.250,2,120,"the "" ai singularity "" or "" technological singularity "" is a vague term that roughly seems to refer to the idea of : humans can design algorithms humans can improve algorithms eventually algorithms we design might end up being as good as humans at designing and improving algorithms this might lead to these algorithms designing better versions of themselves , eventually becoming far more intelligent than humans . this improvement would continue to grow at an increasing rate until we reach a "" singularity "" where an ai is capable of making technological progress at a rate far faster than we could ever imagine also known as an intelligence explosion . this rough idea has been heavily debated as to its feasibility , how fast it 'll take ( if it does happen ) , etc . however i 'm not aware of any formal definitions of the concept of "" singularity "" . are there any ? if not , do we have close approximations ? i have seen aixi and the g&ouml;del machine , but these both require some "" reward signal "" & mdash ; it is unclear to me what reward signal one should choose to bring about a singularity , or really how those models are even relevant here . because even if we had an oracle that can solve any formal problem given to it , it 's unclear to me how we could use that to cause a singularity to happen ( see this question for more discussion on that note ) .",6378,1671,2019-02-18T23:14:40.500,2019-03-21T23:05:51.267,can we define the ai singularity mathematically ?,math learning-algorithms singularity,2,6,4
2978,10646,1,,2019-02-18T01:02:16.977,1,145,"i am generating images that consist of points where the object 's location is where the most overlap of points occurs . in this example , the object location is ( 25 , 51 ) . i am trying to train a model to just finds the location , i do n't care about classification . additionally , the shape of the overlapping points where the object is located never changes and will always be that shape . what is a good model for this objective ? many of the potential models i 've been looking at ( cnn , yolo , and rcnn ) are more concerned with classification than location . should i search the image for the overlapping dots , create a bound box around them , then retrieve the boxes coordinates ? thanks :)",22422,22422,2019-02-18T04:26:01.277,2019-02-18T22:45:46.023,"find object location ( x , y ) in an image",neural-networks machine-learning computer-vision object-recognition,2,2,
2979,10649,1,,2019-02-18T11:53:46.140,4,47,"i have some trouble understanding the benefits of bayesian networks . am i correct that the key benefit of the network is that one does not need to use chain rule of probability in order to calculate joint distributions ? so , using the chain rule : $ $ p(a_1 , \dots , a_n ) = \prod_{i=1}^n ( a_i \mid \cap_{j=1}^{i-1 } a_j ) $ $ leads to the same result as the following ( assuming the nodes are structured by an bayesian network ) ? $ $ p(a_1 , \dots , a_n ) = \prod_{i=1}^n p(a_i \mid \text{parents}(a_i ) ) $ $",22433,2444,2019-02-18T17:07:38.850,2019-02-18T17:07:38.850,what are the main benefits of using bayesian networks ?,math bayes bayesian-network,0,2,
2980,10650,1,,2019-02-18T12:44:06.557,3,38,"i would like to implement a variant of policy iteration that can choose one or more actions in each state . an example would be to heal and move in the game of doom . parameterizing the power set of all single actions would be one idea , but i was wondering if somebody achieved good results on a similar problem , perhaps by simply defining some lower bound on the output layer and taking all actions with values larger than that bound ( i.e. with actions and activation values { shoot=0.2 , heal=0.51 , move=0.6 , jump=0.4 } i would choose heal and move if the bound was 0.5 ) another idea was to collect these actions iteratively , i.e. choosing an action from a softmax output based on the state $ s$ ( taking action "" healing "" ) and then constructing and using some temporary state $ s_t$ to evaluate that state to find another action ( e.g. "" moving "" ) . this would require some dummy action that is just used to signal the end of that iteration procedure ( i.e. choosing action $ n+1 $ will not add any other action to the set , but it will lead to the execution of those two actions and transition to the next state $ s'$ .",22161,2444,2019-02-20T18:16:46.657,2019-02-20T18:16:46.657,choosing more than one action in a parameterized policy,reinforcement-learning policy-iteration,0,0,
2981,10651,1,,2019-02-18T14:03:44.797,1,98,"i asked a question a while ago here and since then i 've been solving the issues within my code but i have just one question ... this is the formula for updating the q - matrix in q - learning : $ $ q(s_t , a_t ) = q(s_t , a_t ) + \alpha \times ( r+q(s_{t+1 } , max_a)-q(s_t , a_t))$$ however , i saw a q - learning example that uses a different formula , which i 'm applying to my own problem and i 'm getting good results : $ $ q(s_t , a_t ) = r(s_t , a_t ) + \alpha \times q(s_{t+1 } , max_a)$$ is this valid ?",22088,1641,2019-02-18T20:46:40.370,2019-02-18T20:46:40.370,is there more than one q - matrix update formula ?,reinforcement-learning python q-learning,2,2,
2982,10658,1,11357,2019-02-18T15:49:40.563,5,82,"in a neural network , the number of neurons in the hidden layer corresponds to the complexity of the model generated to map the inputs to output(s ) . more neurons creates a more complex function ( and thus the ability to model more nuanced decision barriers ) than a hidden layer with less nodes . but what of the hidden layers ? what do more hidden layers correspond to in terms of the model generated ?",22424,,,2019-03-21T20:34:32.980,to what does the number of hidden layers in a neural network correspond ?,neural-networks models hidden-layers,3,2,1
2983,10660,1,10662,2019-02-18T16:18:35.610,1,44,"hello i am new to reinforcement learning and robotics . so far i have an understanding of the concept on 2d world . you can make agent move one step in one direction . however , how do you define movement action of a robot arm ? i am a bit lost over here . any useful links or keywords would be very appreciated ! :)",22442,,,2019-02-18T17:12:22.560,robot arm deep q learning actions,deep-learning reinforcement-learning q-learning robotics,1,2,
2984,10663,1,,2019-02-18T17:49:40.567,2,65,i have read a lot on actor critic and i 'm not convinced that there is a qualitative difference doing direct gradient updates on the network and slightly adjusting a soft - max output in the direction of the advantage function and doing gradient descent on the error . can anyone explain why updating the gradient directly is necessary ?,22132,22296,2019-02-20T03:53:15.887,2019-02-27T09:13:36.550,why is gradient ascent necessary when training actor critic agents ?,reinforcement-learning actor-critic,1,0,
2985,10674,1,,2019-02-18T18:26:51.483,1,14,i am toying around with creating a probability of win calculator for proposals that we do . the information on each proposal is housed in our corporate sharepoint ( which i am the admin ) is there a way to pull directly from sharepoint as the data source rather than have to export to xls then upload each time the data updates ?,22446,,,2019-02-18T18:26:51.483,azure ml studio pull directly from sharepoint,machine-learning,0,0,
2986,10675,1,11041,2019-02-18T18:47:52.793,3,119,"in some newer robotics literature , the term system identification is used in a certain meaning . the idea is not to use a fixed model , but to create the model on the fly . so it is equal to a model - free system identification . perhaps a short remark for all , who does n't know what the idea is . system identification means , to create a prediction model , better known as a forward numerical simulation . the model takes the input and calculates the outcome . it 's not exactly the same like a physics engine , but both are operating with a model in the loop which is generating the output in realtime . but what is policy learning ? somewhere , i 've read that policy learning is equal to online system identification . is that correct ? and if yes , then it does n't make much sense , because reinforcement learning has the goal to learn a policy . a policy is something which controls the robot . but if the aim is to do system identification , than the policy is equal to the prediction model . perhaps somebody can lower the confusion about the different terms ... example q - learning is a good example for reinforcement learning . the idea is to construct a q - table and this table controls the robot movements . but , if online - system - identification is equal to policy learning and this is equal to q - learning , then the q - table does n't contains the servo signals for the robot , but it provides only the prediction of the system . that means , the q - table is equal to a box2d physics engine which can say , what x / y coordinates the robot will have . this kind of interpretation does n't make much sense . or does it make sense and the definition of a policy is quite different ?",11571,11571,2019-02-18T18:56:06.737,2019-03-11T19:23:14.953,is policy learning and online system identification the same ?,control-problem,2,0,
2987,10682,1,,2019-02-18T22:10:39.050,3,72,"fuzzy logic is typically used in control theory and engineering applications , but is it connected fundamentally to classification systems ? once i have a trained neural network ( multiple inputs , one output ) , i have a nonlinear function that will turn a set of inputs into a number that will estimate how close my set of given inputs are to the trained set . since my output number characterizes "" closeness "" to the training set as a continuous number , is n't this kind of inherently some sort of fuzzy classifier ? is there a deep connection here in the logic , or am i missing something ?",20685,,,2019-02-19T11:04:28.217,is fuzzy logic connected to neural networks ?,neural-networks fuzzy-logic,1,0,1
2988,10686,1,,2019-02-19T02:50:17.833,1,48,"i have a neural network that is already trained to predict two continuous outputs from a set of 7 continuous features . is there any way to apply the network to predict one of the input features , given other 6 features and the two outputs ?",22452,22296,2019-02-19T04:54:28.690,2019-03-21T18:02:05.923,"is it possible to use a trained neural network to predict a feature , given other features and output ?",matlab deep-learning,1,1,
2989,10719,1,10720,2019-02-19T09:14:56.147,2,59,i am learning deep rl following this tutorial : https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8 i understand everything but one detail : this image shows the difference between a classic q learning table and a dnn . it states that a q table needs a state - action pair as input and outputs the corresponding q value whereas a deep q network needs the state as feature input and outputs the q value for each action that can be made in that state . but should n't the state and the action together be the input to the network and the network just outputs a single q value ?,22460,2444,2019-02-19T09:19:25.167,2019-02-20T11:32:01.903,why does deep q network outputs multiple q values ?,reinforcement-learning q-learning deep-rl,1,0,
2990,10721,1,,2019-02-19T09:28:08.333,0,26,i have two word embeddings $ w_1 $ and $ w_2 $ with dimension 100 as input to a convolutional neural network . it should learn the similarity between these two words . i am now concerned with the applied convolution operation . what is a reasonable filter size ? in which way should the convolution operation perform on the two word embeddings ?,13295,2444,2019-04-16T22:25:56.353,2019-04-16T22:25:56.353,what is the right way to convolve over word embeddings ?,convolutional-neural-networks natural-language-processing word-embedding,0,0,
2991,10723,1,,2019-02-19T10:14:12.223,1,42,"some ( stock market ) traders have the ability to produce a high percentage of winning trades ( 80%+ , positive return ) over years . i had the chance to look into real money trades of two such traders and i also got trading instructions from them for research . now the interesting part is that if you strictly follow their rules then you usually end up with more losers than winners on the long run . but after a while you get some kind of subconscious "" feeling "" for winners which also shows in the results . i assume that this "" feeling "" is a hidden function which can be modeled . my question is : is there work about how to model such "" gut feeling "" and subconscious knowledge by means of machine learning ( especially with little training data sets ) ? is there relevant literature about this topic ? regards ,",22465,,,2019-02-21T11:11:45.267,modelling gut - feeling / subconscious knowledge of stock market traders,neural-networks machine-learning reinforcement-learning,2,1,
2992,10724,1,,2019-02-19T10:42:00.217,2,75,"i 'm looking into using ppo implementations like openai 's spinningup and baselines . however , i fear that these implementations require packages which are not available for windows . so i 'm wondering if in general linux should be used for working with drl algorithms . also , i 'm not planing on using openai 's gym environments . environments , created with the ml - agents toolkit from unity and its gym wrapper are going to be used . what is your opinion on this ? should i rather work on linux or windows ? and what do you think about alternatives such as : vm on windows with ubuntu or subsystem on windows with ubuntu ( wsl ) ?",22467,1671,2019-02-19T20:51:00.887,2019-02-19T20:51:00.887,is windows a bad choice for drl ?,reinforcement-learning software-evaluation,2,4,
2993,10725,1,,2019-02-19T10:53:12.423,0,19,"i just got interested in machine learning . for days , i searched many sites about the prerequisites and skills ( including mathematical skills ) needed to learn machine learning . i want to take my learning to an extremely professional level . so please help me clarify my questions and give me some tips about it .",,2444,2019-02-19T11:03:23.813,2019-02-19T11:03:23.813,what are the prerequisites and skills for studying machine learning ?,machine-learning artificial-consciousness,0,0,
2994,10732,1,10733,2019-02-19T19:46:09.030,1,61,"i am working through the famous rl textbook by sutton & amp ; barto . currently , i am on the value iteration chapter . to gain better understanding , i coded up a small example , inspired by this article . the problem is the following there is a rat ( r ) in a grid . every square is accessible . the goal is to find the cheese ( c ) . however , there is also a trap ( t ) . the game is over whenever the rat either find the cheese or is trapped ( these are my terminal states ) . the rat can move up , down , left , and right ( always by one square ) . i modeled the reward as follows : -1 for every step 5 for finding the cheese -5 for getting trapped i used value iteration for this and it worked out quite nice . however , now i would like to add another cheese to the equation . in order to win the game , the rat has to collect both cheese pieces . i am unsure how to model this scenario . i do n't think it will work when i use both cheese squares and the trap square as terminal states , with rewards for both cheese squares . how can i model this scenario ? should i somehow combine the two cheese states into one ?",10448,2444,2019-02-20T18:27:09.527,2019-02-20T18:33:53.733,how do i apply the value iteration algorithm when there are two goal states ?,reinforcement-learning rl-an-introduction value-iteration,1,0,1
2995,10734,1,,2019-02-20T00:29:52.990,4,44,"does anyone work out ways of relating trained networks by symbolic logic ? for example : if i train a network on pictures of dogs , and i train a network on pictures of shirts . you could imagine that the simplest way of ( without going through the process from scratch ) , identifying "" dog and shirt "" would be to perform an and operation on the last output of the individual cat & amp ; dog neural nets . so "" dog and shirt "" would amount to and'ing the output of two nets ( which i believe is described here ) . but this operation and could be replaced with a more complicated operation . and in principle i could "" train "" a network to act as one of these operations . for instance maybe i could figure out the net that describes some changeable output "" x "" being "" on the shirt . "" this would be sort of like a "" functional "" in mathematics ( in which we are operating are considering the behavior of a network who s input could be any network ) . if i can figure out this "" functional "" then i would be able to use is symbolically and determine queries like "" dog on the shirt "" ? - "" cat on the shirt "" ? it seems like to me there 's a lot of sense to turn specific neural networks into more "" abstract "" objects - and that there would be a lot of power in doing so .",20685,,,2019-03-17T22:31:40.493,"symbolic "" math "" using trained networks",neural-networks,1,0,1
2996,10738,1,,2019-02-20T10:06:35.263,2,31,"the nhs , centre for data ethics , and nuffield council have put together a code of conduct for ai use in health care . my question is whether item 7 as follows is possible and in what ways might it be : 7.1 demonstrate the learning methodology of the algorithm being built . 7.2 aim to show in a clear and transparent way how outcomes are validated .",11893,,,2019-03-23T11:00:57.737,is 7 . of the uk gov al healthcare code of conduct possible ?,ethics healthcare,1,2,
2997,10739,1,,2019-02-20T10:42:03.103,1,22,"i am training a video prediction model . according to the loss plots , the model convergences very fast while the final loss is not small enough and the generation is not good . actually , i have test the lr=1e-04 and lr=1e-05 , the loss plots drop down a little more slowly , but it 's still not ideal . but i think lr=1e-05 should be small enough , is n't it ? how should i fix my model or the hyper parameters ?",22495,,,2019-02-20T10:42:03.103,why is the learning rate is already very small ( 1e-05 ) while the model convergences too fast ?,machine-learning deep-learning computer-vision,0,0,
2998,10740,1,,2019-02-20T11:09:53.073,1,19,i am trying to understand if there is any difference in the the interpretation of accuracy and loss on synthetic data vs real data .,22497,22498,2019-02-20T14:58:58.323,2019-03-22T16:01:19.213,loss / accuracy on synthetic data,data-science,1,1,
2999,10764,1,,2019-02-20T16:17:21.213,0,66,for the unsw - nb15 dataset i receive spikes in the loss function during training . the algorithms see part of this unsw dataset a single time . loss function is plotted after every batch . for other datasets i do n't experience this problem . i 've tried different optimizers and loss functions but this problem remains with this dataset . i 'm using the .fit_generator ( ) function from keras . is there anyone experience this problem using keras with this function ? thanks in advance .,22514,,,2019-04-22T20:01:55.347,loss function spikes,neural-networks keras loss-functions,2,0,
3000,10771,1,,2019-02-20T20:14:27.090,1,21,"state of the art technology in the industry is equal to automation . lots of machines are used which have increased the productivity . for example a crane , a truck , electric light , a food packaging machine , refrigerators , oven and container ships . no factory owner and no worker is motivated to renounce of these technology , because it helps them to increase the profit , to reduce the costs and to make life easier . the shared similarity of automation with high productivity is the absence of software . the cited machines are working all - mechanical . that means , they did n't have an onboard microcontroller , they did n't have neural networks or a vxworks realtime operating system . instead the technology in automation is based on classical engineering , it 's a combination of electric powersource , motors and a mechanic to execute a job . this allows the crane to lift up a load , and the oven to bake bread . in contrast , the robotics revolution and especially artificial intelligence is about technology which goes beyond classical automation . via definition , a robot is a software - defined machine which contains of an operating system , a middleware and lots of ai related software packages for vision , motion planning , symbolic planning , and reinforcement learning . the contrast between a robot like the ev3 mindstorms brick and automation technology used in the industry is huge . the main contrast is the software : the ev3 brick has to be programmed , while the forklift not . what robotics engineers are trying to do is to extend classical automation with robots . the idea is to bring biped walking robots , grasping robots , autonomous cars and humanoid robots into the real life . that means into the factory and into the home . is it possible to shape this transition soft ? soft means , that the world is not surprised by it but can adapt slowly . or let me ask the question from a different perspective : from the failed robotics projects in the past , e.g. halle 54 at vw , or robots at home it is known that introducing robotics into the reality is harder than expected . until now , not a single household robot is produced for a mass market . is it possible that a soft transition is n't possible ?",11571,,,2019-02-20T20:14:27.090,will the transition from automation to robotics become a soft one or abrupt ?,philosophy,0,0,
3001,10772,1,,2019-02-20T20:54:53.377,3,75,"this question is related to what does & quot;stationary&quot ; mean in the context of reinforcement learning ? , but i have a more specific question to clarify the difference between a non - stationary policy and a state that includes time . my understanding is that , in general , a non - stationary policy is a policy that does n't change . my first ( probably incorrect ) interpretation of that was that it meant that the state should n't contain time . for example , in the case of game , we could encode time as the current turn , which increases every time the agent takes an action . however , i think even if we include the turn in the state , the policy is still non - stationary so long as sending the same state ( including turn ) to the policy produces the same action ( in case of a deterministic policy ) or the same probability distribution ( stochastic policy ) . i believe the notion of stationarity assumes an additional implicit background state that counts the number of times we have evaluated the policy , so a more precise way to think about a policy ( i 'll use a deterministic policy for simplicity ) would be : $ $ \pi : \mathbb{n } \times s \rightarrow \mathbb{n } \times a $ $ $ $ \pi : ( i , s_t ) \rightarrow ( i + 1 , s_{t+1 } ) $ $ instead of . so , here is the question : is it true that a stationary policy must satisfy this condition ? $ $ \forall i , j \in \mathbb{n } , s \in s , \pi ( i , s ) = \pi(j , s ) $ $ in other words , the policy must output the same result no matter when we evaluate it ( either the ith or jth time ) . even if the state $ s$ contains a counter of the turn , the policy would still be non - stationary because for the same state ( including turn ) , no matter how many times you evaluate it , it will return the same thing . correct ? as a final note , i want to contrast the difference between a state that includes time , with the background state i called $ i$ in my definition of . for example , when we run an episode of 3 steps , the state $ s$ will contain 0 , 1 , 2 , and the background counter of number of the policy $ i$ will also be set to 2 . once we reset the environment to evaluate the policy again , the turn , which we store in the state , will go back to 0 , but the background number of evaluations wo n't reset and it will be 3 . my understanding is that in this reset is when we could see the non - stationarity of the policy in action . if we get a different result here it 's a non - stationary policy , and if we get the same result it 's a stationary policy , and such property is independent of whether or not we include the turn in the state . correct ?",12640,12640,2019-02-24T22:41:12.190,2019-02-24T22:41:12.190,what is the difference between a non - stationary policy and a state that stores time ?,reinforcement-learning policy,2,0,
3002,10773,1,10780,2019-02-20T21:20:58.600,2,36,"i am confused as to how neural networks consider the different features by that have access to at the input layer . consider this example : i have three features : an image , a dollar amount , and a rating . however , since the one feature is an image , i need to represent it with very high dimensionality , for example with 128x128=16,384 pixel values . ( i am just using ' image ' as an example , my question holds for any feature that needs high dimensional representation : word counts , one - hot encodings , etc . ) will the 16,384 ' features ' representing the image completely overwhelm the other 2 features that are the dollar amount and rating ? ideally , i would think the network would consider each of the three true features relatively equally . would this issue naturally resolve itself in the training process ? would training become much more difficult of a task ?",22525,,,2019-02-21T10:53:30.810,relative importance of input features,neural-networks deep-learning,2,0,
3003,10774,1,,2019-02-20T23:14:31.183,2,52,"the definition machine learning is as follows : a computer program is said to learn from experience e with respect to some task t and performance measure p , if its performance at task t , as measured by p , improves with experience e. here , we talk about what it means for a program to learn rather than a machine , and a program and a machine are n't equivalent , so this is a definition about "" program learning "" rather than "" machine learning "" . how is this consistent ?",22528,2444,2019-02-21T22:03:52.010,2019-02-22T05:26:19.683,"is the definition of machine learning by mitchell in his book "" machine learning "" valid ?",machine-learning ai-basics terminology definitions,2,1,
3004,10775,1,,2019-02-20T23:48:48.603,1,34,"i have a dataset with hundreds of thousands of training examples . there are 27 input variables and one output variable which is always a 0 or a 1 , based on whether an event happened or not . my network therefore has 27 inputs and 1 output . i want the network 's output to be a confidence guess of how likely the event is to happen , for example if the output is 0.23 then that represents that the network thinks the event has a 23 % chance of happening . i am using back propagation to train the neural network . it does appear to work well and the network outputs a higher number when the event is more likely and a lower number when the event is less likely . would it be a valid concern that my training data only has 0 or 1 values as outputs , when this is not truly what i want the network to output ? my concern comes from the fact that back propagation attempts to reduce the square of the error between the network 's output , and the value of the output in the training data , which is always a 0 or a 1 . because it is the square of the error it is trying to reduce , i 'm concerned that it 's probability output may not be a linear mapping to the true probability of the event happening based on the 27 inputs it is seeing . is this a valid concern ? and are there any techniques i can use to get a neural network to output a linear confidence guess between 0 and 1 when my test data only has outputs of 0 or 1 ? i am using the sigmoid activation function for all of my neurons , would there be a better choice of activation function for this problem ? edit : thanks to xpector 's answer , i now understand that not all back propagation aims to reduce the square of the error , it depends on the loss function used . i am including a part of the back propagation code i have used here which calculates the error : var neuronoutput = layeroutputs[i ] ; var error = ( neuronoutput - desiredoutput[i ] ) ; errors[i ] = error * maths.sigmoidderivative(neuronoutput ) ; this is from an open source rprop implementation . i am not sure what loss function is being used here .",21524,21524,2019-02-21T11:40:23.803,2019-02-21T11:40:23.803,training a neural network to output the conditional probability of an event when the training data output is only binary,neural-networks,1,0,
3005,10776,1,,2019-02-20T23:54:50.313,1,30,"i wanna solve a problem of regression to predict a factor . i decide to go with deep neural networks as solution for my problem . the features in this problem represent loop characteristic such us loop nest level , loop sizes . the loops hold also instruction ( operations ) that in itself represent many characteristics like number of variables , loads , stores , etc . those instructions maybe positions in the innermost loop or in the middle or under the outermost loop . we extract here characteristics of computations in tiramisu language . for example , if we have two iterator variables : var i(""i "" , 0 , 20 ) , j(""j "" , 0 , 30 ) ; and we have the following computation declaration : computation s(""s "" , { i , j } , 4 ) ; this is equivalent to writing the following c code : for ( i=0 ; i&lt;20 ; i++ ) for ( j=0 ; j&lt;30 ; j++ ) s(i , j ) = 4 ; the aspect of receptivity here we can have something like this : computation s(""s "" , { i , j } , 4+m ) ; where "" m "" is computation also . we considered those features to represent computations in tiramisu language . / * * computations = loops * * / "" nest_level "" : 3 , // number of nest levels "" perfect_nested "" : 1 , // 1 if the loop is perfectly nested , 0 instead "" loops_sizes "" : [ 200,100,300 ] // sizes of for loops "" lower_bound "" : [ 5,0,0 ] , // bounds of the iterator ( in this e.g [ 2 , 510 ] ) "" upper_bound "" : [ 205,100,300 ] , "" nb_intern_if "" : 1000 , //number of if statements in the nest "" nb_exec_if "" : 300 , // estimation of number if "" prec_if "" : 1 , // 1 = true if the nest is preceded by if statement "" nb_dependencies_intern "" : 5 , // number of dependencies between loops levels in the nest // "" dependencies_extern "" : , // number of extern nest dependencies "" nb_computations "" : 3 , // number of operations ( computations ) in the nest //std::map&lt;std::string , computation_features * & gt ; computations_features ; // list of operations features in the nest and this to represent operations : / * * instructions * * / "" n "" : 1 , & lt;-- number of computations "" compt_array "" : [ { // should we add to which level should belong the instructions ? "" comp_id "" : 1 , // unique i d for the instructions "" nb_var "" : 5 , // number of the variables in the instructions "" nb_const "" : 2 , // number of constantes in the instructions "" nb_operands "" : 3 , // number of operands of the operatiion ( including direct values ) "" histograme_loads "" : [ 2,1,5,8 ] , // number of load ops . i.e. acces to inputs per type "" histograme_stores "" : [ 2,1,5,8 ] , // number of load ops . i.e. acces to inputs per type "" nb_library_call "" : 5 ; // number of the computation library_calls "" wait_library_argument "" : 2 , // number of ar "" operations_histogram "" : [ // number of arithmetic operations per type [ 0 , 2 , 0 , 0 ] , // p_int32 [ 0 , 0 , 0 , 0 ] , // float , for example [ 0 , 0 , 0 , 0 ] , // ... [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , // ... [ 0 , 0 , 0 , 0 ] // boolean ] } ] we may also represent iterator as a characteristic of computation . the problem in those features we have : loops ( computations ) can hold many operations = = > the size of operation vector is variable . instructions ( operations ) can be in the level 2 , 3 under the innermost i mean we can have this situation : for ( i=0 ; i & lt ; 20 ; i++ ) s(i , j ) = 4 ; for ( j=0 ; j & lt ; 30 ; j++ ) ... or this one : for ( i=0 ; i&lt;20 ; i++ ) for ( j=0 ; j&lt;30 ; j++ ) s(i , j ) = 4 ; or many other situations with many instructions = = > their is dependencies between the position of the instruction and the level ( iterator ) in which it is , in the other way operation hold the i d of the iterator : § . the operation on itself can be composed with another computation(loop nest ) which on itself hold instruction and so forth = = > resistivity . after some research i have found that that dnn has fixed input size . rnn , recursive nn can handle with varying length of inputs . but what about others how should i present that as input ?",22526,22296,2019-02-21T01:49:40.607,2019-02-21T01:49:40.607,how to handle varying length of inputs that represent dependencies and recursivity in deep neural networks in case of regression ?,neural-networks deep-network,0,0,
3006,10778,1,,2019-02-21T03:44:13.630,2,56,how to detect liveness of face using face landmark points ? i am getting face landmarks from android camera frames . and i want to detect liveness using these landmark points . how to tell if a human is making a specific movement that can be useful for liveness detection ?,22532,,,2019-03-23T20:49:57.923,face liveness detection using face landmark points,machine-learning,2,0,
3007,10788,1,,2019-02-21T11:22:08.810,3,56,"in the paper "" self - critical sequence training for image captioning "" , on page 3 , they define the loss function ( of the parameters ) of an image captioning system as the negative expected reward of a generated sequence of words ( equation ( 3 ) ) : $ $ l(\theta ) = - \mathbb{e}_{w^s \sim p_{\theta}}[r(w^s)],$$ where $ w^s = ( w_1^s , ... , w_t^s)$ and $ w^s_t$ is the word sampled from the model at time step $ t$ . the derivation of the gradient of $ l(\theta)$ concludes with equation ( 7 ) , where the gradient of $ l(\theta)$ is approximated with a single sample $ w^s \sim p_\theta$ : $ $ where $ b$ is a reward baseline and $ p_\theta(w^s)$ is the probability that sequence $ w^s$ is sampled from the model . up until here i understand what 's going on . however , then they proceed with defining the partial derivative of $ l(\theta)$ w.r.t . the input of the softmax function $ s_t$ ( final layer ) : $ $ i still understand the equation above . and equation ( 8) : $ $ where $ 1_{w^s_t}$ is $ 0 $ everywhere , but $ 1 $ at the $ w^s_t$ ' th entry . how do you arrive at equation ( 8) ? i 'm happy to provide more information if necessary . in the paper "" sequence level training with recurrent neural networks "" , on page 7 , they derive a similar result .",22545,22545,2019-02-21T13:49:01.617,2019-03-23T14:03:50.590,"how is equation 8 derived in the paper "" self - critical sequence training for image captioning "" ?",neural-networks reinforcement-learning loss-functions papers reinforce,1,0,1
3008,10794,1,,2019-02-21T16:27:51.170,2,10,"i 'm creating a schedule for a summer camp . because of the high risk of rain , the higher priority activities need to be attempted first , so there is more time for later attempts if need be ( temporarily ignoring the schedule in that situation ) . camp takes place over four days . my current idea is to map the days to a set of numbers ( 4 , 3 , 2 , 1 ) , and get a correlation between these numbers and the priorities of activities . but i 'm not certain this is the best way to do it , nor what the best way of correlating the two are . i 'm also not sure how i would factor this correlation in with the fitness function , along with the priorities themselves . how should i proceed ?",5526,,,2019-02-21T16:27:51.170,judging a genetic algorithm 's priority - based schedules by how far ahead the higher priority things are done,genetic-algorithms,0,1,
3009,10797,1,10799,2019-02-21T17:00:37.000,2,37,"in an unknown environment , how do i avoid an agent to tend to terminate its trajectory in a negative state when time needs to be taken into account ? suppose the following example to make my question clear : a mouse ( m ) starts in the bottom left of its world its goal is to reach the cheese ( c ) in the top right ( +5 reward ) while also avoiding the trap ( t ) ( -5 reward ) it should do this as quickly as possible , so for every timestep it also receives a penalty ( -1 reward ) if the grid world is sufficiently large , it may actually take the mouse many actions to reach the cheese . is there a scenario where the mouse may choose to prefer the trap ( -1*small + -5 cumulative reward ) versus the cheese ( -1*large + 5 cumulative reward ) ? is this avoidable ? how does this translate to an unknown environment where the number of time steps required to reach the positive terminal state is unknown ?",22525,2444,2019-02-21T18:25:48.223,2019-03-23T20:22:15.007,how do i avoid an agent to tend to terminate in a negative state when time needs to be taken into account ?,reinforcement-learning rewards,1,0,
3010,10798,1,10907,2019-02-21T19:55:43.213,2,233,"sutton and barto state in the 2018-version of "" reinforcement learning : an introduction "" in the context of expected sarsa ( p. 133 ) the following sentences : expected sarsa is more complex computationally than sarsa but , in return , it eliminates the variance due to the random selection of $ a_{t+1}$ . given the same amount of experience we might expect it to perform slightly better than sarsa , and indeed it generally does . i have three questions concerning this statement : why is the action selection random with sarsa ? is n't it on - policy and therefore -greedy ? because expected - sarsa is off - policy the experience it learns from can be from any policy that at least explores everything in the limit e.g. random action - selection with equal probabilities for every action . how can exected - sarsa learning from such policy be generally better than normal sarsa learning from an -greedy policy , especially with the same amount of experience ? probably more general : how can on - policy and off - policy algorithms be compared in such way ( e.g. through variance ) even though their concepts and assumptions are so different ?",21299,2444,2019-02-21T20:19:05.507,2019-02-27T09:00:51.383,"expected sarsa vs sarsa in "" rl : an introduction """,reinforcement-learning rl-an-introduction sarsa expected-sarsa,1,2,2
3011,10806,1,,2019-02-22T01:30:15.920,0,327,"i 've been looking at various bounding box algorithms , like the three versions of rcnn , ssd and yolo , and i have noticed that not even the original papers include pseudocode for their algorithms . i have built a cnn classifier and i am attempting to incorporate bounding box regression , though i am having difficulties in implementation . i was wondering if anyone can whip up some pseudocode for any bounding box classifier or a link to one ( unsuccessful in my search ) to aid my endeavor . note : i do know that there are many pre - built and pre - trained versions of these object classifiers that i can download from various sources , i am interested in building it myself .",22563,,,2019-03-24T19:01:20.303,pseudocode for cnn with bounding box and classifier,convolutional-neural-networks object-recognition,1,0,
3012,10810,1,,2019-02-22T09:02:25.867,0,25,"i am using rasa nlu for training an nlu system to detect intents and slots . now , some languages have word endings with their nouns ( like finnish , e.g. "" in berlin "" - > "" berliinissä "" ) . i have tried to annotate the characters in the training data as entities , but then i run the model , it does n't detect the characters inside the word . when those characters are a separate word , only then they 're detected . i am unable to think of an implementation to effectively detect named entities within a word . suggestions needed .",22574,22574,2019-02-24T07:10:55.897,2019-02-24T07:10:55.897,detect named entities inside words using spacy,natural-language-processing chat-bots,1,0,
3013,10811,1,,2019-02-22T09:07:45.573,1,19,"is there any guidance available for training on very noisy data , when bayes error rate ( lowest possible error rate for any classifier ) is high ? for example , i wonder if deliberately ( not due to memory or numerical stability limitations ) lowering the batch size or learning rate could produce a better classifier . i found so far some general recommendations , not specific for noisy data : tradeoff batch size vs. number of iterations to train a neural network",22544,22544,2019-02-24T19:59:29.450,2019-02-24T19:59:29.450,any guidance on learning rate / batch size for noisy data ( high bayes error rate ) ?,neural-networks machine-learning deep-learning classification optimization,0,1,
3014,10812,1,10818,2019-02-22T09:28:24.537,3,138,i came across this 2 algorithms but i can not understand the difference between these 2 both in terms of implementation as well as intuitionally . so what difference does the second point in both the slides referring to ?,9947,,,2019-02-22T18:57:14.263,what is the difference between first - visit monte - carlo and every - visit monte - carlo policy evaluation ?,reinforcement-learning monte-carlo,1,0,2
3015,10813,1,,2019-02-22T12:11:54.140,0,32,"i 'm going through the paper weight uncertainty in neural networks by google deepmind . in the final line of the proof of proposition 1 , the integral and the derivative are swapped . then the derivative is taken . but this somehow yields 2 derivatives of $ f$ with respect to . i thought that this was the result of a product rule applied to $ q(\epsilon)$ and $ f(w,\theta)$ and then the chain rule . but that does not yield the same outcome as . my question is : does anyone understand how the equation in the last line comes about ?",22273,,,2019-02-22T13:02:14.230,problem with proposition 1 of google deepmind 's ' weight uncertainty in neural networks ',neural-networks bayesian-network expectation,1,0,
3016,10819,1,,2019-02-22T15:10:57.650,0,43,is it possible to build an ai application which would gather information about all the services and applications of a large enterprise ?,22578,2444,2019-03-25T20:48:00.417,2019-03-25T20:48:00.417,ai application to gather information about all the services of an enterprise,ai-basics applications,1,1,
3017,10822,1,10834,2019-02-22T16:29:59.450,1,46,"i have a reinforcement learning agent with both a positive and a negative terminal state . after each episode during training , i am recording whether a success or failure occurred , and then i can compute a running ratio of success to failure . i am seeing a phenomenon where , at some point in time , my agent achieves a reasonably high success rate ( ~80 % ) for a 100-episode running average . however , with further training , it seems to ' train itself out ' of this behavior and ends the training sequence with a very low success rate ( ~10 - 20 % ) . i am using an epsilon - greedy strategy whereby epsilon decays linearly from 1.0 to 0.1 for the first 10 % of episodes and then remains at 0.1 for the remaining 90 % . as such , the ' training out ' appears to occur some time where exploration only occurs with 10 % probability . what could be causing this undesirable behavior ? how can i combat it ?",22525,2444,2019-02-22T17:06:19.537,2019-02-23T00:44:14.250,what is happening when a reinforcement learning agent trains itself out of desired behavior ?,reinforcement-learning q-learning,1,3,
3018,10823,1,,2019-02-22T16:51:21.723,2,33,"i have a questionnaire consisting with over 10 questions . the questionnaire is being answered by a lot of people - which i have manually graded . each question can give the user up to 10 points depending on how they have answered . let 's say that my dataset is big enough , how would i go about using a neural network to automatically grade these questions for me ? i have used neural networks ( cnn ) before in relation to image classifying . but when dealing with text classifying , where should i start ? is there some sort of tutorial out there that covers this with a similar example ? thanks in advance .",22580,,,2019-03-24T21:22:48.900,grading questions using neural networks,neural-networks machine-learning classification,1,0,
3019,10826,1,11002,2019-02-22T18:51:34.127,2,107,"say i have a set of data generated by someone . it could be either bytes from a photo , or readings from bio - sensors , and i have a huge amount of said information , from many people or subjects . which ai algorithms could be used to learn that a set of data belongs to a subject . i would have the information map that a huge set of data belongs to bob , and another belongs to alice to train the system .",22586,,,2019-03-04T18:16:33.570,which ai algorithm to use to identify a subject from many unknown factors,machine-learning,3,1,
3020,10830,1,11206,2019-02-22T20:55:03.887,3,141,"i 'm trying to replicate the deepmind paper results , so i implemented my own dqn . i left it training for more than 4 million frames ( more than 2000 episodes ) on spaceinvaders - v4 ( openai - gym ) and it could n't finish a full episode . i tried two different learning rates ( 0.0001 and 0.00125 ) and seems to work better with 0.0001 , but the median score never raises above 200 . i 'm using a double dqn . here is my code and some photos of the graphs i 'm getting each session . between sessions i 'm saving the network weights ; i 'm updating the target network every 1000 steps . i ca n't see if i 'm doing something wrong , so any help would be appreciated . i 'm using the same cnn construction as the dqn paper . here 's the action selection function ; it uses a batch of 4 80x80 processed experiences in grayscale to select the action ( s_batch means for state batch ) : def action_selection(self , s_batch ) : action_values = self.parallel_model.predict(s_batch ) best_action = np.argmax(action_values ) best_action_value = action_values[0 , best_action ] random_value = np.random.random ( ) if random_value & lt ; ai.epsilon : best_action = np.random.randint(0 , ai.action_size ) return best_action , best_action_value here is my training function . it uses the past experiences as training ; i tried to implement that if it lose any life , it would n't get any extra rewards , so in theory , the agent would try to not die : def training(self , replay_s_batch , replay_ns_batch ) : q_values = [ ] batch_size = len(ai.replay_s_batch ) q_values = np.zeros((batch_size , ai.action_size ) ) for m in range(batch_size ) : q_values[m ] = self.parallel_model.predict(ai.replay_s_batch[m].reshape(ai.batch_shape ) ) new_q = self.parallel_target_model.predict(ai.replay_ns_batch[m].reshape(ai.batch_shape ) ) q_values[m , [ item[0 ] for item in ai.replay_a_batch][m ] ] = ai.replay_r_batch[m ] if np.all(ai.replay_d_batch[m ] = = true ) : q_values[m , [ item[0 ] for item in ai.replay_a_batch][m ] ] = ai.gamma * np.max(new_q ) if lives = = 0 : loss = self.parallel_model.fit(np.asarray(ai.replay_s_batch).reshape(batch_size,80,80,4 ) , q_values , batch_size = batch_size , verbose=0 ) if ai.epsilon & gt ; ai.final_epsilon : ai.epsilon -= ( ai.initial_epsilon-ai.final_epsilon)/ai.epsilon_decay replay_s_batch it 's a batch of ( batch_size ) experience replay states ( packs of 4 experiences ) , and replay_ns_batch it 's full of 4 next states . the batch size is 32 . and here are some results , after training : in blue , the loss ( i think it 's correct ; it 's near - zero ) . red dots are the different match scores ( as you can see , it does sometimes really good matches ) . in green , the median ( near 190 in this training , with learning rate = 0.0001 ) here is the last training , with lr = 0.00125 ; the results are worse ( it 's median it 's about 160 ) . anyway the line it 's almost straight , i do n't see any variation in any case . so anyone can point me to the right direction ? i tried a similar approach with pendulum and it worked properly . i know that with atari games it takes more time but a week or so i think it 's enough , and it seems to be stuck . in case someone need to see another part of my code just tell me . edit : with the suggestions provided , i modified the action_selection function . here it is : def action_selection(self , s_batch ) : if np.random.rand ( ) & lt ; ai.epsilon : best_action = env.action_space.sample ( ) else : action_values = self.parallel_model.predict(s_batch ) best_action = np.argmax(action_values[0 ] ) return best_action to clarify my last edit : with action_values you get the q values ; with best_action you get the action which corresponds to the max q value . should i return that or just the max q value ?",9818,9818,2019-02-23T19:11:36.850,2019-03-13T18:59:34.400,my dqn is stuck and ca n't see where the problem is,deep-learning reinforcement-learning q-learning dqn deep-rl,2,6,1
3021,10833,1,,2019-02-22T23:50:17.017,1,23,"here is what i understand ( what i think i understand ) . we first train out model on our images using transfer learning . so now we have a pre - trained model . for each image in out dataset , we compute selective search on it , which makes 2000 region proposals . these 2000 region proposals are feed through our pre - trained nn , however we only collect the output ( feature maps ) from the last convolution layer . these outputs are saved to a hard - disk . these feature maps are fed into a svm for another round of training , were another label , "" no object "" is added . we also have regression model that trains based on the window coordinates that we also annotated . so we have svn and a regression model ( two models ) that we train . 1)is the above correct ? 2 ) are each of these 2000 region proposals hand - labeled ( correct label ( cat , dog etc ) or no - object ) before feeding it into the svm ? 3 ) is the regression model tied into the svm model ? basically out loss is a combination of both regression coords and svm classification ?",3460,,,2019-02-23T09:36:14.207,having trouble understanding some of the details of r - cnn ( first one ),deep-learning convolutional-neural-networks object-recognition,1,0,
3022,10839,1,,2019-02-23T13:30:07.443,4,200,"in reinforcement learning , in general , successive states ( actions and rewards ) are highly correlated . an "" experience replay "" buffer was used , in the dqn architecture , to avoid training the neural network ( nn ) , which represents the $ q$ function , with correlated ( or non - independent ) data . in statistics , the i.i.d . ( "" independently and identically distributed "" ) assumption is often made . see e.g. this question . this is another related question . intuitively , if consecutive data points are often correlated , then we , as humans , might learn slowly ( because the differences between the consecutive data points are not sufficient to infer more about the associated distribution ) . mathematically , why exactly do ( feed - forward ) neural networks ( or multi - layer perceptrons ) require i.i.d . data ( when being trained ) ? is this only because we use back - propagation to train nns ? if yes , why would back - propagation require i.i.d . data ? or is actually the optimisation algorithm ( like gradient - descent ) which requires i.i.d . data ? back - propagation is just the algorithm used to compute the gradients ( which is e.g. used by gd to update the weights ) , so i think that back - propagation is n't really the problem . when using recurrent neural networks ( rnns ) , we apparently do not make this assumption , given that we expect consecutive data points to be highly correlated . so , why do feed - forward nns required the i.i.d . assumption but not rnns ? i 'm looking for a rigorous answer ( ideally , a proof ) and not just the intuition behind it . if there is a paper which answers this question , you can simply link us to it .",2444,2444,2019-02-23T13:38:49.710,2019-02-24T14:51:25.967,why exactly do neural networks require i.i.d . data ?,neural-networks machine-learning reinforcement-learning math statistical-ai,2,0,1
3023,10842,1,,2019-02-23T17:01:19.063,4,42,"i was reading the paper by kalchbrenner et al . titled a convolutional neural network for modelling sentences and i am struggling to understand their definition of convolutional layer . first , let 's take a step back and describe what i 'd expect the 1d convolution to look like , just as defined in yoon kim ( 2014 ) . sentence . a sentence of length n ( padded where necessary ) is represented as $ x_{1 : n } = x_1 \oplus x_2 \oplus \dots ⊕ x_n,$ ( 1 ) where is the concatenation operator . in general , let $ x_{i : i+j}$ refer to the concatenation of words $ x_i , x_{i+1 } , \dots , x_{i+j}$ . a convolution operation involves a filter $ w \in \mathbb{r}^{hk}$ , which is applied to a window of h words to produce a new feature . for example , a feature ci is generated from a window of words $ x_{i : i+h−1}$ by $ c_i = f(w \cdot x_{i : i+h−1 } + b)$ ( 2 ) . here $ b \in \mathbb{r}$ is a bias term and $ f$ is a non - linear function such as the hyperbolic tangent . this filter is applied to each possible window of words in the sentence to produce a feature map $ c = [ c_1 , c_2 , \dots , c_{n−h+1}]$ , ( 3 ) with $ c \in \mathbb{r}^{n−h+1}$ . meaning a single feature detector transforms every window from the input sequence to a single number , resulting in $ n - h+1 $ activations . whereas in kalchbrenner 's paper , the convolution is described as follows : if we temporarily ignore the pooling layer , we may state how one computes each d - dimensional column a in the matrix a resulting after the convolutional and non - linear layers . define $ m$ to be the matrix of diagonals : $ m = [ diag(m:,1 ) , \dots , diag(m:,m)]$ ( 5 ) where $ m$ are the weights of the d filters of the wide convolution . then after the first pair of a convolutional and a non - linear layer , each column $ a$ in the matrix a is obtained as follows , for some index $ j$ : here $ a$ is a column of first order features . second order features are similarly obtained by applying eq . 6 to a sequence of first order features $ a_j , \dots , a_{j+m'−1}$ with another weight matrix $ m'$ . barring pooling , eq . 6 represents a core aspect of the feature extraction function and has a rather general form that we return to below . together with pooling , the feature function induces position invariance and makes the range of higher - order features variable . as described in this question , the matrix $ m$ has dimensionalty of $ d$ by $ d * m$ and the vector of concatenated $ w$ 's has dimensionality $ d * m$ . thus the multiplication produces a vector of dimensionality d ( for a single convolution of a single window ! ) . architecture visualization from the paper seems to confirm this understanding : the two matrices in the second layer represent two feature maps . each feature map has dimensionality $ ( s + m - 1 ) \times d$ , and not $ ( s + m - 1)$ as i would expect . authors refer to a "" conventional "" model where feature maps have only one dimension as max - tdnn and differentiate it from their own . as the authors point out , feature detectors in different rows are fully independent from each other until the top layer . thus they introduce the folding layer , which merges each pair of rows in the penultimate layer ( by summation ) , reducing their number in half ( from $ d$ to $ d/2 $ ) . sorry for the prolonged introduction , here are my two main questions : what is the possible motivation for this definition of convolution ( as opposed to max - tdnn or e.g. yoon kim 's model ) in the folding layer , why is it satisfying to only have dependence between pairs of corresponding rows ? i do n't understand the gain over no dependence at all .",22602,16565,2019-02-24T11:12:38.093,2019-02-24T11:12:38.093,what is the motivation for row - wise convolution and folding in kalchbrenner et al . ( 2014 ) ?,machine-learning deep-learning natural-language-processing,0,0,
3024,10845,1,,2019-02-23T21:44:58.993,1,20,is it possible to create a voice - recognition virtual assistant using local storage ?,22612,,,2019-02-23T21:44:58.993,create a virtual assistant using local storage,storage,0,4,1
3025,10846,1,,2019-02-23T23:22:55.103,2,38,"abstract i wish to design a neural network that will categorize messages based on criteria i have predefined . it should feature the ability to be proactively trained as it continues its lifecycle . this means a human can intervene in its categorization attempts and determine whether or not it was correct and have it adjust its weights accordingly ( without having to retrain all over again ) . inputs it is know that all input will follow these rules : always of $ n$ length all messages are transformed to eliminate unnecessary complexity here 's a brief overview of how an example message might be processed . starting with a message $ m$ : that 's an interesting perspective . i think that you should consider adding more details to your point about the cat being too silly . the text is then transformed so that extra details are removed : that s an interesting perspective i think that you should consider adding more details to your point about the cat being too silly then it 's converted into a vector ( appending $ 0 $ to reach length $ n$ ) ready to be processed by the neural network : [ 116 , 104 , 97 , 116 , 115 , 32 , 97 , . . . , 0 , 0 , 0 , 0 ] ouputs in my network , i wish all the outputs to be weighted on how well they fit in each category . i need multiple outputs . i 'm not really focusing on one particular category per - say , but how well the message fits in all of them . following the input $ m$ i used as an example , i 'd expect the outputs to look something like this after my vector has fed - forward : suggestive : 0.89042 opinionated : 0.68703 the weight values for each output indicate the strength of the category in the overall message . from message $ m$ : that 's an interesting perspective . would weigh the opinionated category as $ 0.68703 $ . and : i think that you should consider adding more details to your point about the cat being too silly . would weigh the suggestive category as $ 0.89042 $ . summary and questions i 'm interested in the architectural design choices of a network that would support my feature set . the main goal is to be able to train my network to categorize messages based on pre - trained ( and live - trained ) data . i 'd like to know things like : what type of neural network i should use for this purpose ? i 've researched lstm & amp ; recurrent networks ; which have been mentioned to be good at processing sequences ( ie . messages ) . what considerations should i account for when creating this network ? . how can the overall network support live - training so i can tell my network when its wrong and have it ' correct ' itself without having to retrain completely ?",22614,,,2019-02-23T23:22:55.103,how can my neural network categorize message strings ?,neural-networks machine-learning recurrent-neural-networks,0,6,1
3026,10847,1,10851,2019-02-24T06:24:58.143,2,72,"i understand what an admissible heuristic is , i just do n't know how to tell if one heuristic is admissible or not . so , in this case , i 'd like to know why nilsson heuristic is n't admissible .",21832,,,2019-02-24T13:24:40.513,why nilsson 's sequence score is n't an admissible heuristic ?,problem-solving heuristics a-star,1,0,
3027,10849,1,,2019-02-24T08:58:13.917,1,51,"i have a complex wargame already developed in a aging objective - c and i would like to improve the ai i have built the logic for self - play , fitness evaluation and evolution the hard - time is the ability to run a lot of experiences of self - play with limited ressources ( single mac ) . time is a factor but also memory . i am facing some random crashed after a given number of games i was wondering - if people have faced the same issues with a large number of runs with objective - c - if other people have tried to use genetic programming or reinforcement learning with objective - c or c #",20886,20886,2019-02-24T09:46:58.880,2019-02-26T11:58:47.247,genetic programming with objective - c,game-ai genetic-programming,1,2,
3028,10850,1,,2019-02-24T11:14:33.063,2,50,"i have these monte carlo tree search , and i need to expand it , but i do n't understand the step 1 and 2 , why it it goes to the first node and then make a new node ? instead of go to the depthest left leaf ? i thought it need to go to the most probability leaf . montecarlo tree search : monte carlo tree step 1 , it added a new node , now are 9 cases : monte carlo tree step 2 , it added a new node , now are 2 cases under the first node :",17603,2444,2019-02-24T11:44:04.603,2019-02-24T11:44:04.603,understanding an execution of the monte carlo tree search algorithm,algorithm monte-carlo-tree-search,0,3,
3029,10855,1,,2019-02-24T15:56:06.650,2,46,"i am developing an ai tool for anomaly detection in a distributed system . the system supports an interface that combines several individual logs into a single log file generating approx . 7000 entries / min . the logs entries are partially system generated ( d - bus , ipc , … . ) and human written statements ( status not received , initialized successfully , … . ) . the developers use the generated log for debugging . the entries have been configured to have a similar format depending on the generated system ( timestamp , ids , component , context , verbosity level , description , … . ) . background : 1 . the history of the identified anomalies is minimal and not archived . 2 . not many similar event templates in log files . 3 . software execution rules are not clearly documented . 4 . the log events are co - related . what are the recommended algorithms ( statistical , nlp , ml , neural networks ) that can be used to efficiently perform pattern extraction on the entries and identify existing and new anomalous behavior ?",22635,22635,2019-03-02T12:38:43.420,2019-04-01T13:01:06.330,anomaly detection in distributed system using generated log file,neural-networks machine-learning natural-language-processing pattern-recognition,1,4,
3030,10856,1,,2019-02-24T16:09:44.217,0,71,"a bit of clarification on pytorch 's bcewithlogitsloss : i am using : pos_weights = torch.tensor([len_n/(len_n + len_y ) , len_y/(len_n + len_y ) ] ) to initialize the loss , with [ 1.0 , 0.0 ] being the negative class and [ 0.0 , 1.0 ] being the positive class , and len_n , len_y being respectively the length of negative and positive samples . the reason to use bcewithlogitsloss in the first place is precisely because i assume that it is compensating an imbalance between the quantity of positive and negative samples by avoiding the network from simply "" defaulting "" to the most abundant class type in the training set . i want to control the priorization of the loss on detecting the less abundant class correctly . in my case , negative train samples exceed positive samples by a factor of 25 to 1 , so it is very important that the network predicts a high fraction of positive samples correctly , rather than having a high overall prediction rate ( even by defaulting always to negative , that would lead to 96 % prediction if i only cared about that ) . question is it correct my assumption about bcewithlogitsloss using the pos_weights parameter to control training class imbalances ? any insight into how the imbalance is being addressed in the loss evaluation ?",18642,,,2019-02-24T16:09:44.217,` bcewithlogitsloss ` and training class dataset imbalances in pytorch,pytorch categorical-data,0,0,
3031,10860,1,,2019-02-24T18:08:09.497,0,38,"the blocked n - queens is a variant of the n - queens problem . in the blocked n - queens problem , we also have a nxn chess board and n queens . each square can hold at most one queen . some squares on the board are blocked and can not hold any queens . conditionality is that queens do not dare to attack each other . at the entrance to this problem are the queues and blocked areas . how do i model the blocked n queens problem as a search problem , so that i can apply search algorithms like bfs ?",22634,2444,2019-02-24T19:26:46.973,2019-03-27T18:01:05.130,how do i model the blocked n queens problem as a search problem ?,search problem-solving breadth-first-search,1,0,
3032,10861,1,,2019-02-24T19:11:38.163,0,39,"i 'd like to ask you how to use data from range & lt;-1,1 > in neural networks to have the best results ? i came up with three scenarios : use it as it is and check if activation functions work with & lt;-1,1 > data . move it to range & lt;0,1 > with 0.5 as middle value ( taking a role of 0 ) split it into two inputs - one is positive , one negative , and when one keeps the module of previous value second is equal to 0 . what is your experience ?",21171,,,2019-02-24T19:11:38.163,how to use data from range of values in neural networks,neural-networks datasets,0,0,
3033,10864,1,,2019-02-24T20:03:04.503,1,100,"i want to build a dnn model that i will later integrate into a c++ program . i heard that pytorch model is hard to load it on c++ and the integration requires extra code , and it 's complicated . i have searched a bit on the internet and i have found this post : loading a pytorch model in c++ . i am still unsure whether or not i will have any problems in practice if i opt for pytorch .",22526,22526,2019-02-27T20:27:57.697,2019-02-27T20:27:57.697,how to export pytorch deep neural networks trained model to c++ program to use it from c++ ?,pytorch c++,1,2,
3034,10865,1,,2019-02-25T00:57:21.860,1,46,"what would be one good daily life example of sat problem ? i 've thought about this one : the problem of placing a bunch of different kinds of glasses in a shared cabinet in such a way that some constraints would be satisfied , such as putting the longer ones in the back of the shorter ones so it will be easier to take them when we need . is it a good one , or can you think of any other better one ?",21832,1671,2019-02-25T01:17:24.247,2019-02-25T14:32:06.450,daily life example of sat problem,problem-solving satisfiability constraint-satisfaction-problems,1,0,
3035,10866,1,,2019-02-25T01:40:10.680,0,30,"is it a common practice to do 3-dimensional imagary data acquisition and implementing the dataset to certain high resolution motor driven humanoid candidates to mimic the human micro expressions or walk signatures ? if so , what would be the exact law studies that distinguishes the original dataset that determines if a person has committed a certain crime or not rather than the ai itself ?",22647,,,2019-02-25T01:40:10.680,mimicking certain human functions,image-recognition data-science,0,1,
3036,10867,1,,2019-02-25T06:55:59.723,0,28,"we wanna build a dnn model to predict unrolling factor though our features represent variable length of inputs . knowing that we have to give our features at once "" 0 padding "" look like the only solution that may solve our problem . many people suggest to use a masking layer so that padded values are ignored . i have find some posts talking about using 0 padding in rnn , though i do not know if it 's applied same way in case of mlps . how does it work masking layer with 0 padding in case of mlps ? f what are advantages of disadvantages of this solution ? in which cases this solution is useful and when it is not ?",22526,,,2019-02-25T06:55:59.723,how to use 0 padding with mask layer to handle variable lenght of my inputs in case of multi - layers perceptrons ?,neural-networks machine-learning deep-network,0,0,
3037,10869,1,,2019-02-25T09:51:23.810,5,126,"you may have heard of gpt2 , a new language model . it has recently attracted attention from the general public as the foundation that published the paper , openai , ironically refused to share the whole model fearing dangerous implications . along the paper , they also published a manifesto to justify their choice : "" better language models and their implications "" . and soon a lot of media were publishing articles discussing the choice and its effectiveness to actually prevent bad implications . i am not here to discuss the ethical components of this choice but the actual performance of the model . the model got my attention too and i downloaded the small model to play with . to be honest i am far from impressed by the results . some times the first paragraph of the produced text appears to make sense , but nine times out of ten it is giberish by the first or the second sentence . exemples given in the paper seems to be "" lucky "" outputs , cherry picked by human hands . overall , the paper may suffer from a very strong publication bias . however most article we can read on the internet seems to take its powerfullness for granted . the mit technology review wrote : "" the language model can write like a human [ ... ] "" , the guardian wrote "" when used to simply generate new text , gpt2 is capable of writing plausible passages that match what it is given in both style and subject . it rarely shows any of the quirks that mark out previous ai systems , such as forgetting what it is writing about midway through a paragraph , or mangling the syntax of long sentences . "" . the model appears generally qualified as a "" breaktrough "" . these writings do not match my personnal experimentation as produced texts are rarely consistent / syntaxically correct . my question is : whitout the release of the whole model for ethical reasons , how do we know if the model is really that powerfull ?",22654,22654,2019-02-25T11:56:14.210,2019-02-25T13:34:00.603,how do we know if gpt-2 is a better language model ?,natural-language-processing,1,0,1
3038,10888,1,,2019-02-26T08:07:03.373,0,47,why do we use the seed function in the ' pendulum - v0 ' environment ? https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py#l25,22023,22023,2019-03-01T00:13:43.247,2019-03-01T03:14:49.733,what is the use of the seed function in the gym environment ' pendulum - v0 ' ?,reinforcement-learning environment reinforce gym,0,3,
3039,10890,1,,2019-02-26T09:45:47.713,3,37,"i have heard of the concepts of learning by analogy ( which is quite self - explanatory ) , inductive learning and explanation - based learning . i tried to learn about inductive learning and explanation - based learning , but i do n't understand them . how would you explain all these three concepts ? what are the differences between them ? a link to some explanatory article / notes / blog post are appreciated too .",22673,2444,2019-03-28T14:28:09.880,2019-03-28T14:28:09.880,"what are the differences between learning by analogy , inductive learning and explanation based learning ?",machine-learning terminology,1,1,2
3040,10891,1,,2019-02-26T09:53:04.710,0,24,"i order to build better artifical agents ( aa ) we need the right tasks and data to train on . the task i have in mind is the well - know game "" i spy with my little eye "" , where agent a has to guess the thing agent b is seeing by asking questions about it 's nature . i have a mobile game in mind which would likely be popular enough to provide large amounts of data . this data could be used to improve some aspects of artifical agents : ( i ) concise object representations consisting of characteristics humans deem relevant ( ii ) immitate human reasoning process based on the questions being asked to drill down to the object in mind . ( iii ) could be used for other things like chatbots , q&amp;a , ir and to create better embeddings . and since humans and aa 's alike could play it against each other , some interesting scenarios are possible . let it sink in for a second . what do you think about the usefullness of such labeled data and task ?",19693,,,2019-02-26T09:53:04.710,nlp task proposal : i spy with my little eye,natural-language-processing datasets,0,1,
3041,10892,1,,2019-02-26T10:23:02.940,1,24,"how to detect presence of object ( highly occluded ) in a scene ? there are specific features ( small patterns , etc ) , which allow to say that object is present , but it is not enough for detection for yolo or rpcnn . how to detect small specific pattern in a whole image efficiently ?",22677,,,2019-02-26T10:23:02.940,presence of object ( highly occluded vehicle ) in a scene,image-recognition computer-vision object-recognition detecting-patterns,0,3,
3042,10895,1,,2019-02-26T14:01:34.993,1,37,i want to apply the concept that exists in the dialogflow api in my e - commerce website . i get some references in this regard : tokenization part of speech named entity recognition rule based i just saw that i just did n't understand how to implement it on the website . so i still do n't know how the truth is . please give me a method or explanation that can help me create a chatbot for ecommerce that can give action when a user asks for a product and wants to place an order or something else . please give me some explanation or method or references :(,22686,,,2019-03-07T07:01:03.077,how to make chatbot using nlp like dialogflow ?,natural-language-processing chat-bots natural-language,1,0,1
3043,10897,1,,2019-02-26T17:19:08.930,0,25,i am trying to solve the spiral exercise in tensorflow but i can not add features like in this answer https://ai.stackexchange.com/a/10000/22691 . how can i do that ?,22691,2444,2019-02-26T17:19:52.200,2019-02-26T17:19:52.200,adding features in tensorflow playground,tensorflow feature-selection,0,2,
3044,10898,1,,2019-02-26T20:44:25.120,0,37,"i have just found the paper and documentation about gan 2.0 , the new face creator from nvidia . on the website https://thispersondoesnotexist.com/ they have used this approach to create realistic faces . unfortunately , the website does not exist anymore . is there another webpage demonstrating the new face creator from nvidia ?",21103,,,2019-02-27T03:36:09.970,new face generator from nvidia,neural-networks machine-learning,2,1,
3045,10903,1,,2019-02-27T06:07:37.127,2,25,why is the representation of hidden layers made in terms of circles ? what is the difference between a circle and a box in diagrams of neural networks ?,22707,1847,2019-02-27T08:41:47.583,2019-03-29T09:01:20.547,representation of hidden layer in neural network diagrams,neural-networks machine-learning deep-learning hidden-layers,1,1,
3046,10904,1,10905,2019-02-27T06:17:02.450,4,180,"i have already implemented a relatively simple dqn on pacman . now i would like to clearly understand the difference between a dqn and the techniques used by alphago zero / alphazero and i could n't find a place where the features of both approaches are compared . also sometimes , when reading through blogs , i believe different terms might in fact be the same mathematical tool which adds to the difficulty of clearly understanding the differences . for example , variations of dqn e.g. double dqn also uses two networks like alpha zero . has someone a good reference regarding this question ? be it a book or an online ressource .",18845,2444,2019-02-28T22:46:00.120,2019-03-13T09:19:57.707,what is the difference between dqn and alphago zero ?,reinforcement-learning dqn alphazero deep-rl alphago-zero,2,0,1
3047,10909,1,,2019-02-27T09:25:53.430,5,91,"in the context of deep q network , a target network is usually utilized . the target network is a slow changing network with a changing rate as its hyperparameter . this includes both replacement update every $ n$ iterations and slowly update every iteration . since the rate is hard to fine tune manually , is there an alternative technique that can eliminate the use of target network or at least makes it less susceptible to the changing rate ?",9793,9793,2019-02-27T12:19:41.997,2019-04-10T11:42:11.207,is there an alternative to the use of target network ?,reinforcement-learning deep-rl,2,0,
3048,10910,1,,2019-02-27T10:45:11.330,0,51,"a 3-foot - tall monkey is in a room where some bananas are suspended from the 8-foot ceiling . he would like to get the bananas . the room contains two stackable , movable , climbable 3-foot - high crates . give the initial state , goal test , successor function , and cost function for each of the following . choose a formulation that is precise enough to be implemented .",19448,,,2019-02-27T10:45:11.330,a 3-foot - tall monkey is in a room where some bananas are suspended from the 8-foot ceiling . he would like to get the bananas,intelligent-agent artificial-consciousness,0,4,
3049,10911,1,,2019-02-27T11:42:50.690,0,16,"suppose i want to build a neural network regression model that takes one input and return one output . here 's the training data : 0.1 = & gt ; 0.1 0.2 = & gt ; 0.2 0.1 = & gt ; -0.1 you will see that there are 2 inputs 0.1 that matches to different output values 0.1 and -0.1 . so what will happen with most machine learning models is that they will predict the average when 0.1 is fed to the model . e.g. the output of 0.1 will be ( 0.1 + ( -0.1))/2 = 0 . but this 0 as an average answer is an incorrect answer . i want the model to be telling me that the input is ambiguous / insufficient to infer the output . ideally , the model would report it as a form of confidence . how do i report predictability confidence from the input ? the application that i find very useful in many areas is that i could then later ask the model to show me inputs that are easy to predict and inputs that are ambiguous . this would make me able to collect the data that are making sense . one way i know is to train the model then check the error on each training data , if it 's high then it probably means that the input is ambiguous . but if you know any other papers or better techniques , i would be appreciated to know that !",20819,,,2019-02-27T12:34:52.303,how to make machine learning model that reports ambiguity of the input ?,neural-networks machine-learning linear-regression probabilistic,1,0,
3050,10915,1,,2019-02-27T13:30:04.210,1,20,"i am reading the paper "" transformation invariance in pattern recognition – tangent distance and tangent propagation "" , where the tangent vector is calculated for the given curve $ s(p,\alpha)$ at by differentiating with respect to , that is , . for the curve , i have taken one $ 2d$ image and i am rotating it with matrix $ r=\left[\matrix{cos(\alpha)\space -sin(\alpha)\\sin(\alpha ) \space\space\space\space\space cos(\alpha ) } \right]$ . as my image is fixed , the curve is just a function of . therefore , to find the tangent vector , what i am doing is as follows : i am rotating the image by the matrix $ r^{'}$ which is $ r^{'}=\left[\matrix{-sin(\alpha)\space -cos(\alpha)\\cos(\alpha ) \space\space\space\space\space -sin(\alpha ) } \right]$ this rotates the image by $ 90 $ degree , which is not the expected result . i have done the same exercise by differentiating numerically and i am getting the expected answer which is as follows : = 0 "" > = 0 "" > please , help me to understand my mistake in taking derivative of the matrix and multiplying it with image .",22712,2444,2019-02-27T14:30:14.817,2019-02-27T14:30:14.817,"calculating tangent vector of curve s(p , ) at given point = 0",convolutional-neural-networks computer-vision math,0,2,
3051,10916,1,,2019-02-27T15:49:03.373,3,43,"how do you efficiently choose the hyper - parameters of a neural network ( e.g. the learning rate , number of layer , weights , etc . ) ?",22717,2444,2019-02-27T16:47:36.857,2019-02-27T16:47:36.857,how do you efficiently choose the hyper - parameters of a neural network ?,neural-networks machine-learning deep-learning hyper-parameters,1,1,
3052,10917,1,,2019-02-27T16:15:43.010,1,12,"i hope this question is not too broad or general . i have a very large set of images all of which contain text ( some have more , some less ) . all of them have been tagged as containing , say , english text or korean . i wonder if convolutional neural networks would be a good approach to classify these images as containing english vs. korean . or is there any existing literature / method that does this already . crucially though , i am not interested in "" understanding "" the text , so this is not an nlp task but , i suppose , a task of classifying orthographies in the images .",22719,,,2019-03-31T19:03:15.757,using convnet to classify language of text contained in images,convolutional-neural-networks image-recognition classification,1,0,
3053,10919,1,10920,2019-02-27T17:27:13.727,2,58,"what is the fundamental difference between nn for classifying data and generating data ? most examples show how neural networks can be used to classify data . like is it an image of a dog or a cat . however , there are applications where nn are used to create images and even write short stories .",19413,19413,2019-02-28T08:19:09.827,2019-02-28T08:19:09.827,what is the fundamental difference between neural networks for classifying and generating data,neural-networks,1,0,
3054,10921,1,,2019-02-27T19:11:48.233,1,27,"it seems to me that current work in semantics of natural language processing is based on tarski 's book such as "" logic , semantics , metamathematics ; papers from 1923 to 1938 "" , which is far from satisfactory according to michael jordan ( a leading expert in ai ) . any thought on this ?",22182,22182,2019-02-28T03:06:18.227,2019-02-28T03:06:18.227,current status of semantics in natural language,natural-language-processing natural-language semantics,0,2,
3055,10922,1,,2019-02-27T19:13:19.600,1,66,"how would a quantum computer potentially facilitate artificial consciousness , assuming it is possible ?",22725,2444,2019-02-28T15:39:07.467,2019-02-28T15:39:07.467,"how would a quantum computer potentially facilitate artificial consciousness , assuming it is possible ?",philosophy research theory artificial-consciousness quantum-computing,1,4,
3056,10924,1,,2019-02-28T03:56:42.890,2,126,"the use of target network is to reduce the chance of value divergence which could happen with off - policy samples trained with semi - gradient objectives . in deep q network , semi - gradient td is used and with experience replay the training could diverge . target network is a slow changing network designed to slowly track the main value network . in mnih 2013 , it was designed to match the main network every $ n$ steps . there is another way which slowly updates the weight in the direction to match the main network every step . to someone , the latter is called polyak updates . i have done some very limited experiments and seen that with the same update rate , e.g. $ n=10 $ , polyak update would update with the rate of 0.1 , i usually see polyak updates to give smoother progress and converge faster . my experiments are by no means conclusive . i would thence ask if it is known which one to perform better , converge faster or has smoother progress , in a wider range of tasks and settings ?",9793,,,2019-02-28T12:54:08.330,"in dqn , updating target network every n steps or slowly update every step is better ?",reinforcement-learning dqn deep-rl,1,0,1
3057,10930,1,,2019-02-28T23:57:07.183,1,87,"i have a bayesian network , which has the following data : $ p(s ) = 0.07 $ $ p(a ) = 0.01 $ $ p(f \mid s , a ) = 1.0 $ $ p(f \mid s , \lnot a ) = 0.7 $ $ p(f \mid \lnot s , a ) = 0.9 $ $ p(f \mid \lnot s , \lnot a ) = 0.1 $ and i 'm asked to get $ p(f \mid s)$ . is it possible ? how can i deduce it ?",22760,2444,2019-03-01T09:52:04.583,2019-04-03T12:01:40.307,"is it possible to compute $ p ( f \mid s ) $ given $ p(f \mid s , a)$ , $ p(f \mid s , \lnot a)$ ?",math bayes bayesian-network,3,0,
3058,10933,1,,2019-03-01T06:07:50.643,0,23,i am new to machine learning . i 'm trying to identify driving pattern through accelerometer and gyroscope sensor . i have been collecting the data of both the sensors and have been storing them in .csv extension . i am not able to identify a pattern in the datasets since it has a lot of datas . i have three independent variables of accelerometer and with that i need to identify the sudden acceleration and sudden breaking and i have three independent variables of gyroscope and i need to identify aggressive turns . can you suggest as to how i need to analyse the pattern and find a algorithm which suits my requirement . this is how the dataset is,22736,22736,2019-03-01T09:40:47.947,2019-03-01T09:40:47.947,identifying pattern in datasets,classification datasets unsupervised-learning self-driving supervised-learning,0,2,
3059,10934,1,,2019-03-01T08:48:01.860,1,25,"i was wondering whether there is a method ( not a table of recommendations ) that could tell me what activation function to choose if the outputs of the neural network have some interpretation . for example these can be means of some normal distribution , probabilities in multinomial distribution , parameters of exponential distribution and so on .",11359,9947,2019-03-01T09:57:26.123,2019-03-01T09:57:26.123,activation function of the last layer,neural-networks activation-function,0,0,
3060,10935,1,,2019-03-01T08:53:51.203,0,22,"i 'm currently doing a research project related to distributed tracing . my research has led me to a point where i think ml might be suited for our problem . i 'm looking for papers that are similar to this ( even if they have other applications ) : i want to match packets exiting a black - box system ( outputs ) to packets that enter a black box ( inputs ) . i can do that easily in a non concurrent setting which should help me grow a training set ( maybe for supervised learning ) , but i need an algorithm that , in a concurrent setting , can separate the different request "" flows "" if you will . i hope this makes sense . the closest thing to what i 'm looking for is "" aguilera , marcos k. , et al . "" performance debugging for distributed systems of black boxes . "" acm sigops operating systems review . vol . 37 . no . 5 . acm , 2003 . "" but it 's mostly suited for finding the dependency graph of the system , which i already know . thank you",22763,,,2019-03-31T11:08:46.550,machine learning papers for matching packets to request flows,machine-learning,1,0,
3061,10936,1,,2019-03-01T09:15:44.577,0,41,"a generative adversarial network ( gan ) takes a vector of numbers as input and generates an image , based on the input . each element of the vector causes some feature of the image to change , but the mapping between input and output is not clear , as often happens in deep learning . what is the best way to study the correlation between the vector elements and the output image features ? the first approach that comes to mind is to manually change every element and check the result , however i am not sure that this is the best solution .",16671,,,2019-03-01T10:07:47.017,how to study the correlation between gan 's input vector and output image,convolutional-neural-networks generative-adversarial-networks,1,0,
3062,10940,1,10957,2019-03-01T11:15:59.197,0,92,"i 've pieced together this a3c w/ ppo gym pendulum example , but i 'm finding after a while , when attempting to get the action from the model , i get a nan return : a = self.sess.run(self.sample_op , { self.tfs : s})[0 ] it runs okay for a while , but then errors . to me that implies perhaps a invalid update happens at some point that corrupts the model . but i 've put debugging output in the code and everything appears to be fine - it 's not submitting any nans or outliers to the model as far as i can see . after a bit of playing around i find that if i comment out the code to update the actor model then the code executes fine . obviously it does n't learn much , but it appears it 's something to do with this update : [ self.sess.run(self.atrain_op , { self.tfs : s , self.tfa : a , self.tfadv : adv } ) for _ in range(update_step ) ] can anyone see what the problem might be ? i 've been playing around with this for hours and it 's got me stumped . in code example below i set n_workers = 1 so the debug is easier to read , but if you want to hit the error faster then increase that figure to the number of cpu cores you have . "" "" "" dependencies : tensorflow 1.8.0 gym 0.9.2 "" "" "" import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import gym , threading , queue , math ep_max = 600 ep_len = 200 n_worker = 1 # parallel workers gamma = 0.9 # reward discount factor a_lr = 0.0001 # learning rate for actor c_lr = 0.0001 # learning rate for critic min_batch_size = 64 # minimum batch size for updating ppo update_step = 15 # loop update operation n - steps epsilon = 0.2 # for clipping surrogate objective game = ' pendulum - v0 ' env = gym.make(game ) s_dim = env.observation_space.shape[0 ] a_dim = 1 # not available in pendulum env.action_space.n class pponet(object ) : def _ _ init__(self ) : self.sess = tf.session ( ) self.tfs = tf.placeholder(tf.float32 , [ none , s_dim ] , ' state ' ) self.tfa = tf.placeholder(tf.float32 , [ none , ] , ' action ' ) self.tfadv = tf.placeholder(tf.float32 , [ none , 1 ] , ' advantage ' ) # critic with tf.variable_scope('critic ' ) : w_init = tf.random_normal_initializer(0 . , .1 ) lc = tf.layers.dense(self.tfs , 200 , tf.nn.relu , kernel_initializer = w_init , name='layer1-critic ' ) self.v = tf.layers.dense(lc , 1 ) with tf.variable_scope('ctrain ' ) : self.tfdc_r = tf.placeholder(tf.float32 , [ none , 1 ] , ' discounted_r ' ) self.advantage = self.tfdc_r - self.v self.closs = tf.reduce_mean(tf.square(self.advantage ) ) self.ctrain_op = tf.train.adamoptimizer(c_lr).minimize(self.closs ) # actor pi , pi_params = self._build_anet('pi ' , trainable = true ) oldpi , oldpi_params = self._build_anet('oldpi ' , trainable = false ) with tf.variable_scope('sample_action ' ) : self.sample_op = tf.squeeze(pi.sample(1 ) , axis=0 ) # choosing action with tf.variable_scope('update_oldpi ' ) : self.update_oldpi_op = [ oldp.assign(p ) for p , oldp in zip(pi_params , oldpi_params ) ] with tf.variable_scope('loss ' ) : with tf.variable_scope('surrogate_pp ' ) : ratio = pi.prob(self.tfa ) / oldpi.prob(self.tfa ) surr = ratio * self.tfadv self.aloss = -tf.reduce_mean(tf.minimum ( # clipped surrogate objective surr , tf.clip_by_value(ratio , 1 . - epsilon , 1 . + epsilon ) * self.tfadv ) ) with tf.variable_scope('atrain ' ) : self.atrain_op = tf.train.adamoptimizer(a_lr).minimize(self.aloss ) self.sess.run(tf.global_variables_initializer ( ) ) def update(self ) : global global_update_counter while not coord.should_stop ( ) : if global_ep & lt ; ep_max : update_event.wait ( ) # wait until get batch of data self.sess.run(self.update_oldpi_op ) # copy pi to old pi data = [ queue.get ( ) for _ in range(queue.qsize ( ) ) ] # collect data from all workers data = np.vstack(data ) s , a , r = data [ : , : s_dim ] , data [ : , s_dim : s_dim + 1].ravel ( ) , data [ : , -1 : ] adv = self.sess.run(self.advantage , { self.tfs : s , self.tfdc_r : r } ) print(""updating : s : { } , a : { } , r : { } , adv : { } "" .format(s , a , r , adv ) ) # update actor and critic in a update loop [ self.sess.run(self.atrain_op , { self.tfs : s , self.tfa : a , self.tfadv : adv } ) for _ in range(update_step ) ] # err [ self.sess.run(self.ctrain_op , { self.tfs : s , self.tfdc_r : r } ) for _ in range(update_step ) ] update_event.clear ( ) # updating finished global_update_counter = 0 # reset counter rolling_event.set ( ) # set roll - out available def _ build_anet(self , name , trainable ) : # build the current & amp ; hold structure for the policies with tf.variable_scope(name ) : l1 = tf.layers.dense(self.tfs , 200 , tf.nn.relu , trainable = trainable ) mu = 2 * tf.layers.dense(l1 , a_dim , tf.nn.tanh , trainable = trainable , name = ' mu_'+name ) sigma = tf.layers.dense(l1 , a_dim , tf.nn.softplus , trainable = trainable , name = ' sigma_'+name ) norm_dist = tf.distributions.normal(loc=mu , scale = sigma ) # loc is the mean params = tf.get_collection(tf.graphkeys.global_variables , scope = name ) # collects the weights of the layers l1 , mu / 2 , sigma return norm_dist , params def choose_action(self , s ) : s = s[np.newaxis , :] print(""action s : { } "" .format(s ) ) a = self.sess.run(self.sample_op , { self.tfs : s})[0 ] if math.isnan(a ) : print(""action is nan - stopping "" ) exit ( ) return np.clip(a , -2 , 2 ) def get_v(self , s ) : if s.ndim & lt ; 2 : s = s[np.newaxis , :] return self.sess.run(self.v , { self.tfs : s})[0 , 0 ] class worker(object ) : def _ _ init__(self , wid ) : self.wid = wid self.env = gym.make(game).unwrapped self.ppo = global_ppo def work(self ) : global global_ep , global_running_r , global_update_counter while not coord.should_stop ( ) : s = self.env.reset ( ) ep_r = 0 buffer_s , buffer_a , buffer_r = [ ] , [ ] , [ ] for t in range(ep_len ) : if not rolling_event.is_set ( ) : # while global ppo is updating rolling_event.wait ( ) # wait until ppo is updated buffer_s , buffer_a , buffer_r = [ ] , [ ] , [ ] # clear history buffer , use new policy to collect data a = self.ppo.choose_action(s ) s _ , r , done , _ = self.env.step(a ) print(""step returns : s _ : { } , r : { } , done : { } "" .format(s _ , r , done ) ) # self.env.render ( ) buffer_s.append(s ) buffer_a.append(a ) buffer_r.append((r+8)/8 ) # normalize reward , find to be useful s = s _ ep_r + = r global_update_counter + = 1 # count to minimum batch size , no need to wait other workers if t = = ep_len - 1 or global_update_counter & gt;= min_batch_size or done : if done : v_s _ = 0 # end of episode else : v_s _ = self.ppo.get_v(s_ ) discounted_r = [ ] # compute discounted reward for r in buffer_r[::-1 ] : v_s _ = r + gamma * v_s _ discounted_r.append(v_s _ ) discounted_r.reverse ( ) bs , ba , br = np.vstack(buffer_s ) , np.vstack(buffer_a ) , np.array(discounted_r ) [ : , none ] buffer_s , buffer_a , buffer_r = [ ] , [ ] , [ ] queue.put(np.hstack((bs , ba , br ) ) ) # put data in the queue if global_update_counter & gt;= min_batch_size : rolling_event.clear ( ) # stop collecting data update_event.set ( ) # globalppo update if global_ep & gt;= ep_max : # stop training coord.request_stop ( ) break if done : break # record reward changes , plot later if len(global_running_r ) = = 0 : global_running_r.append(ep_r ) else : global_running_r.append(global_running_r[-1]*0.9+ep_r*0.1 ) global_ep + = 1 print('{0:.1f}%'.format(global_ep / ep_max*100 ) , ' |w%i ' % self.wid , ' |ep_r : % .2f ' % ep_r , ) if _ _ name _ _ = = ' _ _ main _ _ ' : global_ppo = pponet ( ) update_event , rolling_event = threading.event ( ) , threading.event ( ) update_event.clear ( ) # not update now rolling_event.set ( ) # start to roll out workers = [ worker(wid = i ) for i in range(n_worker ) ] global_update_counter , global_ep = 0 , 0 global_running_r = [ ] coord = tf.train.coordinator ( ) queue = queue.queue ( ) # workers putting data in this queue threads = [ ] for worker in workers : # worker threads t = threading.thread(target=worker.work , args= ( ) ) t.start ( ) # training threads.append(t ) # add a ppo updating thread threads.append(threading.thread(target=global_ppo.update , ) ) threads[-1].start ( ) coord.join(threads ) # plot reward change and test plt.plot(np.arange(len(global_running_r ) ) , global_running_r ) plt.xlabel('episode ' ) ; plt.ylabel('moving reward ' ) ; plt.ion ( ) ; plt.show ( ) env = gym.make('cartpole-v0 ' ) while true : s = env.reset ( ) for t in range(1000 ) : env.render ( ) s , r , done , info = env.step(global_ppo.choose_action(s ) ) if done : break",20352,,,2019-03-04T10:07:18.733,getting nan from a3c ppo model,reinforcement-learning tensorflow actor-critic proximal-policy-optimization,1,0,
3063,10942,1,,2019-03-01T11:46:15.743,1,19,"usually , when i evaluate ( ) a model , i would get a single loss that is already averaged over all samples . how do i get the loss per each sample and return all of them ? e.g. if my dataset has 100 samples , i would get 100 losses , for each of the sample .",20819,,,2019-03-01T11:46:15.743,how do i get multiple loss per sample in keras evaluate ?,keras loss-functions,0,0,1
3064,10944,1,10945,2019-03-01T14:09:32.353,9,1342,"i 'm interested in knowing whether there exist any neural network , that solves ( with > = 80 % accuracy ) any nontrivial problem , that uses very few nodes ( where 20 nodes is not a hard limit ) . i want to develop an intuition on sizes of neural networks .",22365,2444,2019-03-01T15:58:35.497,2019-03-01T17:43:18.420,are there neural networks with very few nodes that decently solve non - trivial problems ?,neural-networks,1,4,6
3065,10947,1,11044,2019-03-01T18:45:46.870,3,80,"i was reading an article on medium and wanted to make it clear whether a bot created on ibm watson is an intelligent one or unintelligent . simply put , there are 2 types of chatbots — unintelligent ones that act using predefined conversation flows ( algorithms ) written by developers building them and intelligent ones that use machine learning to interact with users .",22676,,,2019-03-06T01:39:18.747,does ibm watson use machine learning ?,machine-learning chat-bots watson,2,1,1
3066,10948,1,10955,2019-03-01T19:33:32.927,1,53,"the reinforcement learning paradigm has the aim to determine the optimal actions for a robot . a typical example is a maze finding robot , but reinforcement learning can also be used for training a robot to play the pong game . the principle is based on a reward function . if the robot is able to solve a problem , he gets a score from the underlying game engine . the score can be positive , if the robot reaches the end of a maze , or it can be negative , if he is colliding with an obstacle . the principle itself is working quite well , that means for simpler applications it is possible to train a robot to play a game with reinforcement learning . chatbots are a different category of artificial intelligence . they are working not with actions but with natural language . person # 1 is opening a dialogue with “ hi , i 'm alice ” , while person # 2 is responding with “ nice to meet you ” . what is missing here is an underlying game which is played . there is no reward available for printing out a certain sentence . in some literature the problem of language grounding was discussed seriously , but with an unclear result . it seems , that a classical game for example pong , and a chatbot conversation does n't have much in common . is it possible to combine reinforcement learning with chatbot design ? the problem is , that a speech - act should be connected to a reward . that means , a well formulated sentence gets +10 points but a weak sentence gets -10 points . how can this be evaluated ?",11571,,,2019-03-01T22:07:48.297,is reinforcement learning possible for chatbots ?,reinforcement-learning chat-bots,1,1,
3067,10950,1,10962,2019-03-01T20:46:51.720,3,54,"the following plot shows error function output based on system weights . two equal local minima are shown in green pointers . note that the red dots are not related to the question . does the right one generalize better compared to the left one ? my assumption is that for the right minimum if the weights change , the overall error increases less compared to the left minimum point . would this somehow mean the system does better generalization if the right one is chosen as the optimum minimum ?",22779,22779,2019-03-01T21:47:14.563,2019-03-02T10:37:56.403,could error surface shape be useful to detect which local minima is better for generalization ?,neural-networks backpropagation optimization gradient-descent loss-functions,2,0,
3068,10951,1,10968,2019-03-01T20:56:32.490,0,47,"i find myself wondering if there exists a data structure with the following properties : stores information conforms to the encoding specificity principle additionally to the existence of such a data structure i 'm specifically wondering if neural networks have been used to model this biological principle at all and if it 's been done in an efficient manner . more broadly , this relates to an interest in neural networks as any type of storage mechanism such as the hopfield network ( although it 's not clear to me wether an hn models this principle , if someone can shed light on this i would appreciate it ) .",22781,,,2019-03-02T13:20:32.413,does a data structure that models the encoding specificity principle in memory exist ?,neural-networks biology structure memory,1,0,
3069,10953,1,10967,2019-03-01T21:30:38.590,0,34,"i want to try and compare different optimization methods in some datasets . i know that in scikit - learn there are some corresponding functions for grid and random search optimizations . however , i also need a package ( or multiple ones ) for different recent bayesian optimization methods . are there any good and stable ones to use ? which packages do you recommend ? ( if any recent for grid / random search , it is also okay . ) thanks in advance .",22784,,,2019-03-02T11:19:30.723,python packages for recent optimization methods,python optimization,1,0,
3070,10954,1,10961,2019-03-01T21:47:19.977,1,37,"the following plot shows error function output based on system weights . two equal local minima are shown in green pointers . note that the red dots are not related to the question . considering the amount of convex in the local minima , is there any way to opt between these two local minima ?",22779,2444,2019-03-01T22:45:34.327,2019-03-02T09:06:44.530,which local minima to choose according to the shape of the error surface ?,neural-networks backpropagation optimization gradient-descent loss-functions,1,0,
3071,10958,1,10964,2019-03-02T06:02:53.117,2,140,"i just read about parse tree for parsing a sentence as an input for nlp task . in my understanding , a valid parse tree of a sentence should have be validated by linguistic expert . so , i concluded , a sentence only has one parse tree structure . but , is that correct ? is it possible a sentence has more than one valid structures of parse tree with the same type ( e.g. constituency - based ) ?",16565,,,2019-03-06T23:43:03.970,a sentence with different parse tree structures,natural-language-processing,3,0,
3072,10963,1,,2019-03-02T08:45:12.107,0,30,"i want to install some libraries such as xgboost , lightgbm , and catboost without connecting to the internet in anaconda because of some security issues . can anybody help me how to do this ? moreover , is there any pre - requisite to install before installation of these libraries like visual studio , etc ?",10191,,,2019-03-02T08:45:12.107,"how to install xgboost , lightgbm , catboost without connecting to the internet",python,0,0,
3073,10965,1,10978,2019-03-02T10:17:24.927,1,38,"i m trying to learn a bit more about why someone would use a neuroevolution algorithm over a traditional machine learning algorithm . i.e what situation would only apply to an algorithm such as neat , but no other machine learning algorithm .",22802,,,2019-03-03T11:34:42.223,how does neat work differently to ordinary machine learning algorithms ?,machine-learning neat,1,1,
3074,10966,1,,2019-03-02T10:31:03.677,0,16,"there are various heuristic search algorithms , like hill climbing , greedy search , a * algorithm , but when it is best preferred to use hill climbing ?",22804,2444,2019-03-02T10:47:08.183,2019-03-02T10:47:08.183,in which situation hill climbing searching will be more appropriate ?,search applications hill-climbing,0,1,
3075,10971,1,,2019-03-02T14:43:03.937,1,33,"i have coded a very basic lstm with forget gates ( no libraries used ) . i 'm trying to predict $ 0.5*sin(t + n)$ given $ 0.5*sin(t)$ as an exercise . i have tweaked the model , changing the output layer activation function , weight initialization , number of memory blocks / cells , etc . however , i still could n't manage to correct the output . the problem is that the output range is much smaller than desired , $ [ -0.2 , 0.2]$ instead of $ [ -0.5 , 0.5]$ . the output also is slightly delayed , meaning it is predicting $ sin(t + n - 1)$ for example . is there something that i 'm missing ? as an example , for output layer activation function as a centered logistic from $ ( -1 , 1)$ , the validation output looks like training output looks like additional information : topology : 1 input layer , 1 hidden layer each with 5 memory blocks each with 1 cell , 1 output layer each with 1 regular neuron . alpha : 1 weights : generated with normal distribution , from $ [ -1 , 1]$ output layer activation function used : logistic $ [ 0 , 1]$ , centered logistic , tanh , relu , leaky relu , $ f(x ) = x$ ( identity )",16609,16565,2019-03-03T23:20:24.267,2019-03-03T23:20:24.267,predicting sine using lstm : small output range and delayed output ?,neural-networks machine-learning recurrent-neural-networks lstm,0,1,
3076,10973,1,,2019-03-02T21:08:57.293,3,61,"to make a2c into a3c you make it asynchronous . from what i understand the ' correct ' way to do that is to thread off workers with a copy of the policy and critic , and then return the state / action / reward tuples to the main thread , which then performs the gradients updates on the main policy and critic , and then repeat the process . i understand why the copying would be necessary in a distributed environment , but if i were to always run it locally then could i just perform the updates on a global variable of the policy and critic , i.e. avoid the need for copying ? provided the concurrency of the updates was handled correctly , would that be fine ?",20352,20352,2019-03-02T23:18:11.650,2019-04-23T14:03:10.620,can a3c update the policy / critic on a local machine without needing to copy ?,reinforcement-learning actor-critic,1,3,
3077,10975,1,,2019-03-03T08:39:20.207,1,45,"so i was wondering , why i have only encountered square loss function also known as mse . the only nice property of mse i am so far aware of is its convex nature . but then all equations of the form $ x^{2n}$ where $ n$ is an integer belongs to the same family . my question is what makes mse the most suitable candidate among this entire family of curves ? why do other curves in the family , even though having steeper slopes , $ ( x & gt;1 ) $ , which might result in better optimisation , not used ? here is a picture to what i mean where red is $ x^4 $ and green is $ x^2 $ :",9947,9947,2019-03-03T09:39:33.463,2019-03-03T09:39:33.463,why is mse used over other quadratic loss functions ?,neural-networks machine-learning deep-learning math loss-functions,1,1,1
3078,10977,1,,2019-03-03T10:31:23.247,2,27,"we have been assigned a project , in which we have to create a chatbot which will ask question , take the replies , analyse them and give an approximate assessment of the current emotional state of the person . there are two aspects of the project , training the bot to choose the next question based on the previous response and analysing the responses individually , to detect . the sentiment . what technology would we have to use and what would be the steps to accomplishing the tasks ? thanks .",22822,,,2019-04-02T12:02:32.737,what would be the steps to create an sentiment analysis chatbot ?,machine-learning sentiment-analysis,1,1,
3079,10980,1,,2019-03-03T12:09:27.253,0,41,i want to use tensorflow to solve a distributed classification problem . i need an example that explains me how can i do this . i have installed tensorflow on windows 7 using anaconda ( 4.6.4 ) and python ( 3.6.4 ) .,22823,2444,2019-04-02T19:25:39.203,2019-05-02T20:01:05.127,how do i create a distributed classification algorithm in tensorflow ?,deep-learning distributed-computing,1,1,
3080,10981,1,10996,2019-03-03T13:05:42.127,4,100,is there some type of neural network that changes the number of neurons while training ? using this idea the network can increase or decrease the amount of neurons when the complexity of the inputs increases or decreases .,19783,2444,2019-03-04T21:01:46.623,2019-03-04T21:01:46.623,is there a neural network with a varying number of neurons ?,neural-networks neat,1,1,2
3081,10982,1,,2019-03-03T13:07:18.340,0,226,"in section 3.6 of the openai gpt-2 paper it mentions summarising text based relates to this , but the method is described in very high - level terms : to induce summarization behavior we add the text tl;dr : after the article and generate 100 tokens with top - k random sampling ( fan et al . , 2018 ) with k=2 which reduces repetition and encourages more abstractive summaries than greedy decoding . we use the first 3 generated sentences in these 100 tokens as the summary . given a corpus of text , in concrete code terms ( python preferred ) , how would i go about generating a summary of it ?",2424,,,2019-03-03T13:07:18.340,how do i use gpt-2 to summarise text ?,open-ai text-summarization,0,2,
3082,10983,1,10985,2019-03-03T13:36:06.670,1,37,"hi guys i 've been studying such combination where the idea is to replace the classic descendant gradient for an algorithm that is less sensitive to local optimum , so the pso tries to select the best weight and bias for the model . but i 've seen everywhere that only one hidden layer is used ( no one explains why just one layer is being used ) and all those codes break when i try to use more than one hidden layer , so the questions are : ca n't pso be used to optimize an artificial neural network with more than one hidden layer ? in that case , why is that ?",15764,16565,2019-03-03T16:00:30.940,2019-03-03T17:18:07.213,training an artificial neural network using pso,neural-networks machine-learning optimization swarm-intelligence,1,0,
3083,10987,1,,2019-03-03T18:32:27.577,1,39,"i 'm wanting to conduct game theoretic analyses of ongoing conflict situations ( e.g. the us / north korea negotiations ; syrian conflict ; etc ) as reported in the news media . i believe that ai may help me do this by helping me to pick out from the text : the parties involved ; the issues over which they are in conflict ; the choices they have ; their preferences . however i 'm not sure whether to approach this using ' modern ' ' deep learning ' approaches or to try something along the lines of the classic work by schank , dejong etc . who used the notion of scripts ( and sketchy scripts ) in their work with conceptual dependency approaches . does anyone have comments , suggestions that may guide my work please ?",22832,,,2019-03-03T18:32:27.577,how can / should i use ai to populate a game ( in the game theory sense ) from text input,deep-learning natural-language-processing game-theory,0,2,
3084,10988,1,,2019-03-03T18:46:48.937,1,39,i 've learned that you can use two perceptrons to ultimately create a classifier for non - linearly separable data . i 'm trying to understand how / if these two perceptrons converge to two different decision boundaries though . source : https://tdb-alcorn.github.io/2017/12/17/seeing-like-a-perceptron.html i do n't understand how the second perceptron creates a different decision boundary when it has the same input as the first perceptron ? i know the weights can be initialized differently but does this second perceptron classify something else ? should n't the decision boundaries be converge to be the the same ultimately after training ? source : https://tdb-alcorn.github.io/2017/12/17/seeing-like-a-perceptron.html,22833,,,2019-03-03T22:02:30.310,how do two perceptrons produce different linear decision boundaries when learning ?,neural-networks activation-function perceptron,0,0,
3085,10989,1,,2019-03-03T19:41:22.113,0,21,"after going through both the "" illustrated transformer "" and "" annotated transformer "" blog posts , i still do n't understand how the sinusoidal encodings are representing the position of elements in the input sequence . is it the fact that since each row ( input token ) in a matrix ( entire input sequence ) has a unique waveform as its encoding , each of which can be expressed as a linear function of any other element in the input sequence , then the transformer can learn relations between these rows via linear functions ? that was the best way i could phrase that question , apologies in advance . thanks !",8062,8062,2019-03-03T21:54:49.680,2019-03-03T21:54:49.680,"how do the sine and cosine functions encode position in the "" attention is all you need "" paper ?",machine-learning deep-learning natural-language-processing attention,0,0,
3086,10991,1,,2019-03-03T21:05:00.483,0,16,"i am a bit confused about how can i visualize clusters in a self organizing map . the input data is a set of images , where each image is an english alphabet in some font . now if i have to visualize the final organization and clusters , where each alphabet / letter that is similar is shown as a cluster , how can i achieve this ? i previously thought that som algorithm does it for you and my current visualization shows the image generated using the weight vector after each iteration for every single output node . i realized my approach to visulize does n't show clusters but it shows how each output node is converging towards some input image . also one or more output nodes may be converging towards different input images . i have read about u - matrix approach but still unclear if i will be able to visualize similar inputs as seperate clusters . my question is that if i have a data set of letters where each letter belongs to the set x={a , b , c , d , e , f } and each letter is in 6 different fonts , hence forming a data set of total 36 letters . how can i visualize the 6 clusters where each cluster has 6 letters in it ? also does the dimension of the lattice be equal to that of the inputs in the dataset for better visualization ? any help will be greatly appreciated . thanks in advance",22837,,,2019-03-03T21:05:00.483,visualizing clusters in self organizing map,neural-networks unsupervised-learning,0,0,
3087,10993,1,,2019-03-04T00:31:47.630,0,57,"i am trying to build a 1d gan able to produce data similar to the input one , which looks like this : i am using pytorch . this is the code for my discriminator , which takes as input a 1d vector of size 256 and outputs a number between 0 and 1 : class convblock(nn.module ) : def _ _ init__(self , ni , no , ks , stride , bn = true , pad = none ) : super().__init _ _ ( ) if pad is none : pad = ks//2//stride self.conv = nn.conv1d(ni , no , ks , stride , padding = pad , bias = false ) self.bn = nn.batchnorm1d(no ) if bn else none self.relu = nn.leakyrelu(0.2 , inplace = true ) def forward(self , x ) : x = self.relu(self.conv(x ) ) return self.bn(x ) if self.bn else x class dcgan_d(nn.module ) : def _ _ init__(self , data_len , nc_input , nc_output ) : super().__init _ _ ( ) # get the number of convolutions needed to get the image to size 1 n_conv = int(np.log2(data_len ) ) # make an initial convolutionar layer which increases the number of channels from 1 to 64 self.initial = convblock(nc_input , nc_output , 6 , 4,pad=1 ) # reduce the size of the input vector until it is of dimension 4 layers = [ ] for i in range(n_conv//4 ) : # add conv layers that reduce the size of the image each time layers = layers + [ convblock(nc_output , nc_output , 6 , 4,pad=1 ) ] self.layers = nn.sequential(*layers ) # add several linear layers at the end until the output is size 1 self.final = convblock(nc_output , 1 , 6 , 4,pad=1,bn = false ) def forward(self , x ) : x = self.initial(x ) x = self.layers(x ) x = self.final(x ) return torch.sigmoid(x).mean ( ) and if it is helpful , here it is the architecture of the discriminator : dcgan_d ( ( initial ) : convblock ( ( conv ) : conv1d(1 , 32 , kernel_size=(6 , ) , stride=(4 , ) , padding=(1 , ) , bias = false ) ( bn ) : batchnorm1d(32 , eps=1e-05 , momentum=0.1 , affine = true , track_running_stats = true ) ( relu ) : leakyrelu(negative_slope=0.2 , inplace ) ) ( layers ) : sequential ( ( 0 ) : convblock ( ( conv ) : conv1d(32 , 32 , kernel_size=(6 , ) , stride=(4 , ) , padding=(1 , ) , bias = false ) ( bn ) : batchnorm1d(32 , eps=1e-05 , momentum=0.1 , affine = true , track_running_stats = true ) ( relu ) : leakyrelu(negative_slope=0.2 , inplace ) ) ( 1 ) : convblock ( ( conv ) : conv1d(32 , 32 , kernel_size=(6 , ) , stride=(4 , ) , padding=(1 , ) , bias = false ) ( bn ) : batchnorm1d(32 , eps=1e-05 , momentum=0.1 , affine = true , track_running_stats = true ) ( relu ) : leakyrelu(negative_slope=0.2 , inplace ) ) ) ( final ) : convblock ( ( conv ) : conv1d(32 , 1 , kernel_size=(6 , ) , stride=(4 , ) , padding=(1 , ) , bias = false ) ( relu ) : leakyrelu(negative_slope=0.2 , inplace ) ) ) this is the code of the generator , which takes as input a gaussian random noise vector of size 32 and outputs a mini - batch of vectors of the same size as the input for the discriminator ( 256 ) : class deconvblock(nn.module ) : def _ _ init__(self , ni , no , ks , stride , pad , bn = true ) : super().__init _ _ ( ) self.conv = nn.convtranspose1d(ni , no , ks , stride , padding = pad , bias = false ) self.bn = nn.batchnorm1d(no ) if bn else none self.relu = nn.relu(inplace=true ) def forward(self , x ) : # x = x[:,none , : ] x = self.relu(self.conv(x ) ) return self.bn(x ) if self.bn else x class dcgan_g(nn.module ) : def _ _ init__(self , data_len , input_size , nc_input , nc_output ) : super().__init _ _ ( ) # make an initial deconvolutionar layer which increases the number of channels from 1 to 64 self.initial = deconvblock(nc_input , nc_output,2,2,0 ) # increase the size of the input vector until it is of dimension 512 layers = [ ] for i in range(int(data_len / input_size/8 ) ) : layers = layers + [ deconvblock(nc_output , nc_output,2,2,0 ) ] self.layers = nn.sequential(*layers ) self.final = deconvblock(nc_output,1,2,2,0,bn = false ) def forward(self , x ) : x = self.initial(x ) x = self.layers(x ) x = self.final(x ) return 2*torch.sigmoid(x ) and this is the generator architecture : dcgan_g ( ( initial ) : deconvblock ( ( conv ) : convtranspose1d(1 , 64 , kernel_size=(2 , ) , stride=(2 , ) , bias = false ) ( bn ) : batchnorm1d(64 , eps=1e-05 , momentum=0.1 , affine = true , track_running_stats = true ) ( relu ) : relu(inplace ) ) ( layers ) : sequential ( ( 0 ) : deconvblock ( ( conv ) : convtranspose1d(64 , 64 , kernel_size=(2 , ) , stride=(2 , ) , bias = false ) ( bn ) : batchnorm1d(64 , eps=1e-05 , momentum=0.1 , affine = true , track_running_stats = true ) ( relu ) : relu(inplace ) ) ( 1 ) : deconvblock ( ( conv ) : convtranspose1d(64 , 64 , kernel_size=(2 , ) , stride=(2 , ) , bias = false ) ( bn ) : batchnorm1d(64 , eps=1e-05 , momentum=0.1 , affine = true , track_running_stats = true ) ( relu ) : relu(inplace ) ) ) ( final ) : deconvblock ( ( conv ) : convtranspose1d(64 , 1 , kernel_size=(2 , ) , stride=(2 , ) , bias = false ) ( relu ) : relu(inplace ) ) ) finally , this is the training loop with the loss functions : def train(niter , first = true ) : gen_iterations = 0 for epoch in trange(niter ) : netd.train ( ) ; netg.train ( ) i , n = 0,400 with tqdm(total = n ) as pbar : while i & lt ; n : set_trainable(netd , true ) set_trainable(netg , false ) # train the discriminator 20 times d_iters_d = 20 j = 0 while ( j & lt ; d_iters_d ) and ( i & lt ; n ) : j + = 1 ; i + = 1 # for p in netd.parameters ( ) : p.data.clamp_(-0.01 , 0.01 ) # create a mini - batch of real data real = create_data(bs , data_len ) real_loss = 1-netd(real ) # i want this close to 1 # create a mini - batch of noise fake = netg(create_noise(bs ) ) fake_loss = netd(fake ) # i want this close to 0 netd.zero_grad ( ) lossd = real_loss+fake_loss lossd.backward ( ) optimizerd.step ( ) pbar.update ( ) set_trainable(netd , false ) set_trainable(netg , true ) d_iters_g = 20 j = 0 while ( j & lt ; d_iters_g ) and ( i & lt ; n ) : j + = 1 ; i + = 1 netg.zero_grad ( ) lossg = 1-netd(netg(create_noise(bs ) ) ) lossg.backward ( ) optimizerg.step ( ) pbar.update ( ) # gen_iterations + = 1 print(f'loss_d { to_np(lossd ) } ; loss_g { to_np(lossg ) } ; ' f'd_real { to_np(real_loss ) } ; loss_d_fake { to_np(fake_loss ) } ' ) these are the definitions of netd and netg : netg = dcgan_g(data_len , data_len/16,1,32).cuda ( ) netd = dcgan_d(data_len,1,32).cuda ( ) and in the training loop i am training them alternatively 20 times each . i am sorry if this is silly , but i am new to ai in general and i 've never written a gan code before , so any suggestion is greatly appreciated . i spent 2 days on it , but i ca n't make it work and i really do n't know why . the discriminator is getting a loss close to 0 while the generator does n't seem to learn . i tried training the generator more than the discriminator , but it is still not working . is my loss function wrong ? is the architecture flawed ? what am i doing wrong ? thank you !",22839,,,2019-03-04T00:31:47.630,1d gan not converging,convolutional-neural-networks generative-adversarial-networks,0,0,
3088,10997,1,11005,2019-03-04T10:59:52.900,2,31,how accurate are neuro - evolution algorithms ( such as neat ) in modelling real organism evolution ?,22802,2444,2019-03-04T16:08:15.110,2019-03-04T23:02:12.290,how accurate are neuroevolution algorithms in modelling organism evolution ?,evolutionary-algorithms neat,1,1,
3089,10999,1,,2019-03-04T12:30:30.353,0,11,i recently started working onto time - series problem and gathered insights onto the working and the logic behind . however i 'm still not clear that how acf and pacf plots help us in building a model ? and checking the quality of generated model by analyzing residuals for the generated model ?,8041,,,2019-03-04T12:30:30.353,how to interpret the generated acf and pacf plot in time - series problem ?,python models statistical-ai forecasting r,0,0,
3090,11000,1,,2019-03-04T16:07:19.913,0,109,"the ( discrete and continuous ) fourier transform ( ft ) is used in signal processing in order to convert a signal ( or function ) in a certain domain ( e.g. the time domain ) to another domain ( e.g. , frequency domain ) . there are several resources on the web that attempt to explain the ft at different levels of complexity . see e.g. this answer or this and this youtube videos . what are examples of ( real - world ) applications of the fourier transform to ai ? i am looking for answers that explain the reason behind the use of the ft in the given application . i suppose that there are several applications of the ft to e.g. ml ( data analysis ) and robotics . i am looking for specific examples .",2444,,,2019-03-06T09:18:29.177,what are examples of applications of the fourier transform to ai ?,machine-learning math applications robotics,2,3,
3091,11004,1,,2019-03-04T21:13:16.217,0,31,"i am doing some experimentation on neural networks , and for that i am trying to program a plain ocr task . i have learned cnns are the best choice , but for the time being and due to my inexperience , i wanna go step by step and start with feedforward nets . so my training data is a set of roughly 400 16 * 16 images extracted from a script that draws every alphabet char in a tiny image for a small set of fonts registered in my computer . then the test data set is extracted from the same procedure , but for all fonts in my computer . well , results are quite bad . get an accuracy of aprox . 45 - 50 % , which is very poor ... but that 's not my question . the point is that i ca n't get the mse below 0.0049 , no matter what hidden layer distribution i apply to the net . i have tried with several architectures and all comes down to this figure . does that mean the net can not learn any further given the data ? this mse value however throws this poor results too . i am using tensorflow api directly , no keras or estimators and for a list of 62 recognizable characters these are examples of the architectures i have used : [ 256,1860,62 ] [ 256,130,62 ] [ 256,256 , 128 , 62 ] [ 256,3600,62 ] .... but never get the mse below 0.0049 , and still results are not over 50 % . any hints are greatly appreciated .",22869,2444,2019-03-05T10:04:03.283,2019-04-04T13:02:00.360,attempting to solve a optical character recognition task using a feed - forward network,feedforward ocr,1,4,
3092,11007,1,11026,2019-03-05T00:25:34.997,1,60,"i have been looking at a patent for the deductive computing machine . the author claims that his machine has been built , but can it ?",22874,2444,2019-03-05T09:36:10.860,2019-03-06T06:02:44.403,the deductive computing machine,machine-learning,1,2,
3093,11008,1,,2019-03-05T01:17:00.723,6,99,it is possible that the view of what is impressive enough in computer behavior to be called intelligence changes with each decade as we adjust to what capabilities are made available in products and services .,4302,4302,2019-03-06T06:50:01.273,2019-04-01T10:46:59.163,would the people of the 19th century call our conventional software today artificial intelligence ?,philosophy social perception,3,3,2
3094,11009,1,,2019-03-05T01:30:25.717,1,54,"conjecture 1 : the smartest chess playing system is the one that wins the tournament . conjecture 2 : the computer system that imitates human dialog is as smart as the human whose dialog was imitated . these are legacy views from the twentieth century . are they correct ? response to comments the question is not about agents . it is about indicators , or metrics if you wish . the question is whether the established game championship and turing 's thought experiment , the dialog centered imitation game , are correctly designed metrics to establish the extent of intellectual ability . that someone voted to close this question on the basis of broadness is irrelevant . this question and ones like it are important questions for ai researchers and enthusiasts to ask about the set of capabilities we call intelligence , which we attempt to realize in artificial systems . there may also be a tendency to think of questions like this as seeking an opinion . it is certainly true that our opinions are not useful to the furtherance of ai , and opinion is not what this question invite when it asks , "" is this [ pair of conjectures that guides much ai research ] correct ? "" correctness must draw on mathematical rigor leading to a yes or a no . some reasoning leading to the conclusion of correctness or incorrectness must be given . the answer may be yes in one context and no in another , but this is not because of the broadness of the question but because of the broadness of the many useless opinions and possibly counterproductive conjecture that remains from early thinking about what intelligence is and how to gauge its presence in computing machinery or in people . questions about intelligence from this author are intended to narrow the terms and concepts so that they can be expressed mathematically . only with a mathematically terse and elegant set of fundamentals can ai progress as a field in the sciences . some interesting opinion will probably need to be discarded and a logically constructed consensus must replace broadness and ambiguity for this to occur .",4302,4302,2019-03-06T06:46:51.553,2019-03-06T06:46:51.553,are winning and imitating the primary indicators of intelligence ?,theory,3,2,1
3095,11014,1,,2019-03-05T09:22:06.333,1,19,"in the paper mastering the game of go with deep neural networks and tree search , the input features of the networks of alphago contains a plane of constant ones and a plane of constant zeros , as following . feature # of planes description stone colour 3 player stone / opponent stone / empty ones 1 a constant plane ﬁlled with 1 turns since 8 how many turns since a move was played liberties 8 number of liberties ( empty adjacent points ) capture size 8 how many opponent stones would be captured self - atari size 8 how many of own stones would be captured liberties after move 8 number of liberties after this move is played ladder capture 1 whether a move at this point is a successful ladder capture ladder escape 1 whether a move at this point is a successful ladder escape sensibleness 1 whether a move is legal and does not ﬁll its own eyes zeros 1 a constant plane ﬁlled with 0 player color 1 whether current player is black i wonder why these features are necessary , because i think a constant plane contains no information and it makes the the network larger and consequently harder to train . what 's more , i do n't understand the sharp sign here . does it mean "" the number "" ? but one number is enough to represent "" the number of turns since a move was played "" , why eight ? thank you very much .",22886,,,2019-03-05T09:22:06.333,why is a constant plane of ones added into the input features of alphago ?,machine-learning alphago,0,0,
3096,11016,1,11018,2019-03-05T10:30:06.413,2,1139,"i wanted to clarify the term ' acting greedily ' . what does it mean ? does it correspond to the immediate reward , future reward or both combined ? i want to know the actions that will be taken in 2 cases : $ v_\pi(s)$ is known and $ r_s$ is also known ( only ) . $ q_{\pi}(s , a)$ is known and $ r_s^a$ is also known ( only ) .",9947,2444,2019-03-05T10:34:05.417,2019-03-05T16:49:17.883,what does ' acting greedily ' mean ?,reinforcement-learning greedy-ai,3,1,
3097,11019,1,,2019-03-05T11:41:49.440,1,69,i have made some research online but i have yet to find a tool where i can input text such as : a bonobo ape climbing a tree and receive a image illustrating the sentence . an api would be interesting too . i have seen other people do this already though so i know it 's possible even if the results are n't always perfect . but where can i find these types of tools ?,22891,16565,2019-03-07T13:05:04.267,2019-04-07T05:43:48.487,is there any online tool or api available for generating images from text ?,image-generation,3,1,
3098,11029,1,,2019-03-05T15:13:20.307,0,18,"question is purely theoretical . i am desiging a machine learning model for classification purposes . i am using gridsearch optimization method to select best hyperparameters and i have written separated evolutionary optimizer for selection of proper features used in the model . my question is how to combine this two optimization tasks , to find the best solution . two options that comes to my mind are : -tuning hyperparameters first on all features model , then choose features using that hyperparameters -selecting best features first on some random hyperparameters and then tune them using previously selected features however , i believe that these ways might not find the best solution , since for example , some features that worked badly on the used hyperparameters set might work better on a different set , but i will never know , because i will choose the best set for chosen hyperparameters values . the solution that comes to my mind is to make one big optimisation that will in the same iteration optimize both features selected and the hyperparameters . or more brutal method , to make "" optimization in optimization "" meaning that for each iteration of features selection i run seperate grid search of hyperparameters tuning . but i am pretty sure that both solutions are extremely computationally expensive . what do you think ? what is the typical "" by the book "" way of dealing with these things ? if i did n't explain what i mean clearly enough , please let me know .",22659,12006,2019-03-05T16:36:56.143,2019-03-05T16:36:56.143,feature selection optimization and hyperparameters optimization for one model,machine-learning optimization hyper-parameters,0,0,
3099,11032,1,,2019-03-05T17:58:07.273,0,21,"are there any ( well validated ) approaches for applying pathfinding algorithms on a graph following specific rules ? to be more specific : i want to introduce a graph with coloured edges . the idea is to apply a well known pathfinding algorithm ( such as dijkstra ) on the graph given the rule : "" only black and red edges "" .",19413,,,2019-03-05T18:25:49.640,which pathfinding algorithms can be applied on coloured graphs ?,pathfinding,1,0,
3100,11038,1,11045,2019-03-05T18:59:04.550,1,50,"i am interested in a framework for learning the similarity of different input representations based on some common context . i have looked into word2vec , svd and siamese networks , all of which are similar to what i want . for example , suppose we have some customers we are sending different advertisements to , and i would like to create a system to map offers to customers . i am thinking in the lines of creating a customer representation , and a representation of the offers , and feeding them in parallel to a neural network that has a label of whether they acted on the advertisement or not . the idea is that i should be able to locate the best offer for any customer given these representations . i have looked into siamese networks and word2vec , both are close to what i want . the problem differs slightly in that for the siamese networks , it tends to be identical parallel networks , which i do n't want because my inputs are not equivalent . and for word2vec the vectors tend to be in the same domain , while i want to apply this in a more general setting . if anyone has any resources on a similar problem statement , i would be very interested in it .",22906,2444,2019-04-03T09:10:43.237,2019-04-03T09:10:43.237,learning similarities between customers and offers representation,deep-learning knowledge-representation word2vec recommender-system,1,0,1
3101,11039,1,,2019-03-05T19:19:06.103,1,12,"the inverted pendulum problem is a famous control task . it can be solved with a technique called system identification . system identification means to formalize the state - action space of a system in a model and this model is used to predict future states . it is similar to creating a physics engine . this is done with data from the past . for example , the behavior of a domain is analyzed from timeframe 0 to 1000 . and the generated model is used to control the system from timeframe 1000 to 10000 . the underlying assumption is , that the model remains the constant . that means , that the system can be identified once and it will work the same in the future . or to explain it more colloquial , that the past will repeat over and over again . for a mechanical system this is equal to the law of physics , which are remaining constant over time . that means , it 's not important in which timeframe an action is executed in the system , because the system will react always the same . my question is : has this assumption an official term ? are systems available which are not repeating themself ?",11571,,,2019-03-05T19:19:06.103,does a mechanical system repeats itself ?,math control-problem chaos,0,0,
3102,11042,1,,2019-03-05T20:39:26.673,1,16,"i would like to train a constrained neural network . i found a paper on this : https://papers.nips.cc/paper/4-constrained-differential-optimization.pdf . however , i do n't really understand how to change my feedforward neural network . i do n't understand exactly the role of the new output neurons , which serve as lagrangian multipliers . can someone explain this to me in more detail ? which steps in backpropagation do i have to change ?",19195,2444,2019-03-05T21:29:56.373,2019-03-05T21:29:56.373,what is the purpose of the new neurons in the constrained neural network ?,neural-networks optimization,0,0,
3103,11043,1,12471,2019-03-05T23:22:24.627,0,57,"i 'm looking to build from scratch an implementation of the wake - sleep algorithm also known as an unsupervised neural network . i plan on doing this in python in order to better understand how it works . in order to facilitate my task i was wondering if anyone could point me to an existing implementation of this concept . as an aside , i also plan to do the same for a cascade correlation neural network however i had better luck and found this so if anyone has any better suggestions please do let me know . guides , open source codebases , articles , etc . are all welcomed .",22781,2444,2019-03-06T12:11:31.250,2019-05-22T14:49:23.757,where can i find an implementation of the wake - sleep algorithm ?,neural-networks machine-learning python unsupervised-learning,2,0,
3104,11047,1,,2019-03-06T08:57:54.400,0,25,"i would like to find a way to isolate the speech of each of the people in an audio record so i can create a file of that form : [ { "" voice_fingerprint "" : "" 701066edd3a0a40a2f53f61eafd0e6ab "" , "" sentences "" : { { "" sentence "" : "" do you like red apples "" , "" position "" : 1.39 // seconds . time position in the audio record } , { "" sentence "" : "" and how do you feel about time shifts "" , "" position "" : 7.21 } } } , { "" voice_fingerprint "" : "" 8ffea051af3e3fb9a80a51a98fe05896 "" , "" sentences "" : { { "" sentence "" : "" yes i do like them "" , "" position "" : 4.54 } , { "" sentence "" : "" i feel well about traveling "" , "" position "" : 10.18 } } } ] this may be an interview record . the problem is not the speech to text , but to isolate the two people 's sentences . preferably in python . have you ever worked on this ? do you have any hints ?",22924,,,2019-03-06T08:57:54.400,isolate the speech of two people in an audio record with two people only,machine-learning natural-language-processing python audio-processing,0,0,
3105,11050,1,11147,2019-03-06T10:41:05.043,2,81,"following deep q - learning from demonstrations , i 'd like to avoid potentially unsafe behavior during early learning by making use of supervised learning with demonstration data . however , the implementation i 'm following still uses an environment . can i train my agent without an environment at all ?",22930,22916,2019-03-11T08:10:48.217,2019-03-11T10:04:56.840,is there a way to train an rl agent without any environment ?,reinforcement-learning q-learning deep-rl environment,1,4,
3106,11053,1,,2019-03-06T11:47:00.090,0,24,"i want to make a simple recommendation system based on reinforcement learning using kerasrl and openai gym . my point is that i 've already created an agent to learn the cartpole environment and now i want to set up my own environment to train against it . my dataframe has the typical columns of [ user , item , relevance , timestamp ] but i do n't know how to deal with the sequences of acts to show to the agent and the reward to give to it . my approach is to give the user i d as the input to make an embedding of it and the output as a softmax of the size of the number of items to make a ranking and multiply a number for the ndcg of the number of ones ( as it means the user has bought the item ) minus the ndcg of the number of ceros ( as it means that the user has viewed the item and not buyed it ) . the problem is that by this way the agent would learn to recommend the items that the user has already bought ( not useful ) and not to recommend the items that has already seen ( useful ) . i hope to make a system that could recommend some items that the user is not going to buy ( the less the better ) hoping to maximize that the user buys a distinct item later ( so the timestamp is very important ) . another problem is the way of showing the data . it is clear that i have to show it ordered by timestamp but , is it relevant to order the data by user before ? where do i calculate the reward ? has anyone any idea or critic to this approach ? can i do this with kerasrl and a gym environment ? has anyone a paper or a video useful to this issue ? i hope i expressed myself correctly and thank you in advance .",22930,,,2019-03-06T11:47:00.090,extend gym environment for recommendation with kerasrl and some questions,reinforcement-learning keras open-ai deep-rl,0,0,
3107,11054,1,,2019-03-06T12:18:15.480,0,12,"in all the courses and literature i 've read on neural networks i ca n't recall it anywhere being said that you could combine your validation and training set when you 're done with training , but for the life of me i ca n't figure out why this would n't be a good idea . the validation set is used to guide training parameters , and after you 're happy with the model 's performance you test it on a completely unseen test set to ensure it really is generalizable . satisfied with this , particularly if training samples are acquired with great difficulty , why would n't any practitioner now train their network a little more with their entire validation set ? this new model could then be tested on the test set again to ensure it 's still good ( which it should be if the validation set was indeed randomly sampled from the same distribution as the training set ) . i can see why we would n't incorporate the test set , since if we do this we have no way to validate the accuracy of this new final model , but why not incorporate the validation set into the actual training set after it 's served its purpose ?",6328,,,2019-03-06T12:18:15.480,incorporating validation set into training set after training ?,neural-networks,0,1,
3108,11055,1,,2019-03-06T13:23:32.007,1,103,"i am working on creating a rl based ai for a certain board game . just as a general overview of the game so that you understand what it 's all about : it 's a discrete turn - based game with a board of size n*n ( n depending on the number of players ) . each player gets an m number of pieces , which the player must place on the board . in the end the one who has the least number of pieces wins . there are of course rules as to how the pieces can be placed so that not all placements are legal at every move . i have the game working in an openai gym environment like fashion ( i.e. control by step function ) , have the board representation as the observation , reward function , and the thing i am struggling with right now is to meaningfully represent the action space . i looked into how alphazero tackles chess . the action space there is 8 * 8 * 73 = 4672 : for every possible tile on the board there are 73 movement - related modalities . so for every move the algorithm comes up with 4672 values , the illegal ones are set to zero and non - zero ones are re - normalized . now i am not sure if such an approach would be feasible for my use - case as my calculations show that i have a theoretical cap of ~30k possible actions ( n * n * m ) if using the same way of calculation . i am not sure if this would still work on , especially considering that i do n't have the deepmind computing resources at hand . therefore my question : is there any other way of doing it apart from selecting the legal actions from all theoretically possible ones ? the legal actions would be just a fraction of the ~30k possible ones . however , at every step the legal actions would change because every new piece determines the new placement possiblities ( also the already placed pieces are not available anymore , i.e. action space generally gets smaller with every step ) . i am thinking of games like starcraft 2 where action space must be larger still and they demonstrate good results , not only by deepmind but also by private enthusiasts with for example dqn . i would appreciate any ideas , hints or readings ! thank you !",21278,,,2019-04-06T00:02:35.613,huge action space size in reinforcement learning,reinforcement-learning dqn open-ai,1,0,
3109,11056,1,,2019-03-06T13:35:58.420,3,52,"some formulations of humanism already grant moral consideration to sentient non - human animals ( e.g. https://humanists.international/what-is-humanism/ ) . does humanism also extend to granting rights to agis , should they become sentient ? sentientism is a closely related philosophy that makes this explicit https://en.wikipedia.org/wiki/sentientism .",22935,22935,2019-03-07T16:23:14.293,2019-03-07T18:26:36.317,does humanism grant moral consideration to sentient artificial general intelligences ?,philosophy ethics,1,7,1
3110,11057,1,11133,2019-03-06T14:07:16.067,7,266,"in mathematics , the word "" operator "" can refer to several distinct but related concepts . an operator can be defined as a function between two vector spaces , it can be defined as function where the domain and the codomain are the same , or it can be defined as a function from functions ( which are vectors ) to other functions ( for example , the differential operator ) , that is , an high - order function ( if you are familiar with functional programming ) . what is the bellman operator in reinforcement learning ? why do we even need it ? where is it used ? why is it called an "" operator "" ? if it is an operator , then it is a map from a function space to another function space . which are these function spaces ? how is the bellman operator related to the bellman equations in rl ?",2444,,,2019-03-10T13:30:01.007,what is the bellman operator in reinforcement learning ?,reinforcement-learning terminology math,1,2,3
3111,11058,1,,2019-03-06T14:21:30.907,0,59,"i have 10 variables as like below v1=1 , v2=2 , v3=3 , v4=4 , v5=5 , v6=6 , v7=7 , v8=8 , v9=9 and v10=10 note : each variable can have any value now i want to select the best 3 variables combination as like below v1v3v4 or v10v1v7 or v5v3v9 etc . the best combination is nothing but the sum of the values of 3 variables in the combination . example : combination 1(v1v2v3 ) : 1 + 2 + 3= > 6 combination 2(v8v9v10 ) : 8 + 9 + 10= > 27 in the above example combination 2(v8v9v10 ) has the highest sum value . so the combination 2(v8v9v10 ) is the best combination here . like this if i have large amount of variables means which machine learning algorithm selects the best combination in all the sense . suggest me the best machine learning algorithm for selecting the best variable combinations . thanks in advance .",22931,2444,2019-03-06T14:23:22.433,2019-04-06T17:02:00.643,what is the best machine learning algorithm to select best 3 variable combinations ?,machine-learning,1,4,
3112,11059,1,11064,2019-03-06T14:44:40.287,3,87,"the question already has some answer . but i am still finding it quite unclear ( also does here mean $ q(s , a)$ ? ): the few things i do not understand are : why the difference between 2 iterations if we are acting greedily in each of them ? as per many sources ' value iteration ' does not have an explicit policy , but here we can see the policy is to act greedily to current $ v(s)$ what exactly does policy improvement mean ? are we acting greedily only at a particular state at a particular iteration or once we act greedily on a particular state we keep on acting greedily on that state and other states are added iteratively until in all states we act greedily ? we can intuitively understand that acting greedily w.r.t $ v(s)$ will lead to $ v^*(s)$ eventually , but does using policy iteration eventually lead to $ v^*(s)$ ? note : i have been thinking of all the algorithms in context of gridworld , but if you think there is a better example to illustrate the difference you are welcome .",9947,2444,2019-03-06T16:41:23.103,2019-03-06T20:16:16.243,a few questions regarding the difference between policy iteration and value iteration,reinforcement-learning policy value-iteration,2,4,
3113,11062,1,,2019-03-06T17:28:51.890,1,18,"i do n't know if this it 's possible but nowadays as almost everything is possible i am asking to see if anyone has any idea . the problem is : regularly i have to import files ( csv , xml , excel , ... ) to an sql server database and after i import the files i have to map the content of the columns of the imported files in a .sql script to run and insert in a table in another db ( this table is the same for all the files imported ) . i spend a lot of time on these mappings as you can imagine , i want to know if it 's possible through artificial intelligence or machine learning one solution that can make the import of the initial data to the final db , the initial import i can make it automatically the problem is only the import of the imported files to the final db . note : the initial files do n't have the same columns names , each file can have different columns names . sorry for the long post but i do n't know if it is possible something like that . i searched and do n't find anything .",22940,16565,2019-03-07T09:07:40.323,2019-03-07T09:07:40.323,automation the import of files to database,machine-learning automation,0,0,
3114,11070,1,,2019-03-07T02:18:47.097,2,39,"i would like to start programming a multi task reinforcement learning model . for this , i need not just one maze or grid world ( or just model - based ) , but many with different reward functions . so , i am wondering if exists a dataset or a generator for such thing , or do i need to code everything by my self ?",22949,2444,2019-03-07T08:51:25.057,2019-03-07T08:51:25.057,is there any grid world dataset or generator for reinforcement learning ?,reinforcement-learning q-learning model-based value-function,1,0,
3115,11074,1,11075,2019-03-07T07:50:52.097,1,108,"i 'm studying how spp ( spatial , pyramid , pooling ) works . spp was invented to tackle the fix input image size in cnn . according to the original paper https://arxiv.org/pdf/1406.4729.pdf , the authors say : convolutional layers do not require a fixed image size and can generate feature maps of any sizes . on the other hand , the fully - connected layers need to have fixed size / length input by their definition . hence , the fixed size constraint comes only from the fully - connected layers , which exist at a deeper stage of the network . why does a fully connected layer only accepts a fixed input size ( but convolutional layers do n't ) ? what 's the real reason behind this definition ?",20612,2444,2019-03-07T08:45:49.117,2019-03-07T10:28:09.877,why does a fully connected layer only accept a fixed input size ?,deep-learning convolutional-neural-networks computer-vision,2,0,
3116,11077,1,,2019-03-07T11:06:06.593,1,42,i have series of sensors ( around 4k ) and each sensor will measure the amplitudes at each point.suppose i train the neural network with sufficent set of 4k values ( n * 4k shape ) . the machine will find a pattern in the series of values.if the values stray away from the pattern ( that is anomaly ) it can detect the point and will be able to say that anomaly is in the ' x'th sensor.is this possible.if so what kind of neural network should i use ?,21936,,,2019-03-12T16:20:50.353,finding anomaly detection by pattern matching in a set of continous data,neural-networks machine-learning deep-learning pattern-recognition detecting-patterns,2,3,
3117,11078,1,11085,2019-03-07T12:00:57.300,1,111,"i would like to know a list of model - based and model - free reinforcement learning algorithms , like q - learning , sarsa , td , dyna - q .",22963,2444,2019-03-09T11:00:59.493,2019-03-09T12:16:44.813,list of model - based and model - free reinforcement learning algorithms,reinforcement-learning model-based model-free,1,6,2
3118,11079,1,11082,2019-03-07T12:27:21.053,2,39,which python packages do you recommend on random search optimization to use ? is there any recent and good one ( better than the one in sci - kit ) ?,22784,1671,2019-05-08T23:05:21.947,2019-05-08T23:05:21.947,recent python packages for random search optimization,optimization software-evaluation,1,0,
3119,11080,1,,2019-03-07T13:10:21.793,0,18,"what are main differences between these 2 ? i understand that automl has a gui and is cloud served , yet under the hood how do they work in terms of transfer learning , hyper parameter choice , architecture choice etc .",22965,,,2019-03-07T13:10:21.793,adanet vs google automl,google,0,0,
3120,11081,1,11086,2019-03-07T15:52:59.950,3,76,"excercise 3.5 the equastions in section 3.1 are for the continuing case and need to be modified ( very slightly ) to apply to episodic tasks . show that you know the modifications needed by giving the modified version of ( 3.3 ) . , for all $ s\in s , a \in a(s)$ ( 3.3 ) is it just about final states ? so for $ s \in s$ when s is not final ?",22826,22826,2019-03-07T16:19:21.257,2019-04-17T18:23:17.717,"difference in continuing and episodic cases in sutton and barto - introduction to rl , exercise 3.5",reinforcement-learning markov-decision-process rl-an-introduction markov-chain,2,0,
3121,11084,1,,2019-03-07T16:11:56.240,4,56,"i 'm writing a virtual environment for a 4-player card game named estimation , and will use deep reinforcement learning to teach an agent to play it . each player gets a hand of 13 cards , and the first phase is for each player to estimate the number of tricks they will collect . the highest player gets to start first , and then after each round , the player who collects the trick starts the next . so basically the first phase is for bidding and the next phase consists of 13 rounds . the state input i 'll use will include all the cards that have been played , the goal and collected tricks , and the available cards . the output for each round will be a vector of length 54 , containing all the cards and then the available card with the highest probability would be played . at first i thought that the bidding phase should use the same input but with zeros everywhere except the available hand , and the output would exclude all the cards with no numbers like king , queen or jack . but then the ability to dash ( estimate that you 'll collect 0 tricks ) would n't be available . also i do n't think it would work really well . should i just use two nns for each phase , or what should i do ? also if anyone has any advice on things i need to watch out for , i 'd really appreciate it if they shared them .",22971,,,2019-03-09T09:01:46.283,teaching a neural network to play a card game with two phases,neural-networks reinforcement-learning,1,0,
3122,11087,1,11091,2019-03-07T16:59:53.727,3,45,"let 's use excercise 3.8 from sutton , barto - introduction to rl : suppose and following sequence of rewards is received $ r_1=-1 $ , $ r_2=2 $ , $ r_3=6 $ , $ r_4=3 $ , $ r_5=2 $ , with $ t=5 $ . what are $ g_0 , g_1 , ... , g_5?$ there is n't $ g_5 $ because $ r_5 $ is last reward . am i understanding it right ? so : $ g_4 = 2 $ $ g_3 = 3 + 0.5 * 2 = 4 $ $ g_2 = 6 + 0.5 * 4 = 8 $ $ g_1 = 2 + 0.5 * 8 = 6 $ $ g_0 = -1 +0.5 * 6 = 2 $",22826,2444,2019-03-07T17:43:48.340,2019-03-07T19:48:32.480,"given specific rewards , how can i calculate the returns for each time step ?",reinforcement-learning rewards return,1,0,
3123,11092,1,,2019-03-07T22:28:01.463,1,65,"dialects differ a lot between cities in my country , syria . people sometimes express themselves using different local phrases and idioms which refer to the same topic . so , i came up with the idea of creating an android application shows a limited set of sentences or expressions while asking you to express them in the local dialect of your region orally , after that this application tries to figure out what your dialect is . for a short period of time , i 'm going to launch an android application in order to collect the needed dataset which will be a new contribution . first of all , i need some helpful answers to my questions : in general , is a period of 6 months enough for such a project to be done by only one student who is a beginner in this field or it is harder than it seems ? are the libraries and tools needed to do this project available for free ? i know that more training data leads to more accurate results . in order to obtain good results , what is the estimated minimum number of training data needed for this model ? how do you advise me to begin ? how much is my suggested project relevant to the project attached in this link ? kindly write down your suggested edits and recommendations if any . edit for the 5th question : also see this paper .",20449,20449,2019-03-09T09:57:00.077,2019-03-09T09:57:00.077,dialects classification using deep learning,deep-learning natural-language-processing classification applications voice-recognition,0,5,
3124,11097,1,,2019-03-08T08:13:15.317,0,19,"how does the process of segmentation of face , using a cnn , in face recognition , work ? how are we able to segment the face ?",22995,2444,2019-03-08T09:25:20.063,2019-03-08T09:25:20.063,how does the process of segmentation of face in face recognition work ?,neural-networks convolutional-neural-networks image-recognition,0,0,
3125,11099,1,,2019-03-08T11:57:23.460,2,24,"i frequently need to translate product names for hundreds of similar products -- and i have a list of past product names . is it possible to train ai to review past translations and translate ? it does n't have any special grammar , simply the name ( with some industry - specific usage that a general machine translator ca n't do . ) what would i need to do to get started ?",23000,9947,2019-03-08T12:14:21.590,2019-03-08T12:14:21.590,translate product names with ai,natural-language-processing ai-basics machine-translation,0,3,
3126,11100,1,,2019-03-08T12:52:43.333,3,47,"i am working on a mlp neural networks , using supervised learning ( 2 classes and multi - class classification problems ) . for the hidden layers , i am using ( which produces an output in the range $ [ -1 , 1]$ ) and for the output layer a softmax ( which gives the probability distribution between $ 0 $ and $ 1 $ ) . as i am working with supervised learning , should be my targets output between 0 and 1 , or $ -1 $ and $ 1 $ ( because of the function ) , or it does not matter ? the loss function is quadratic ( mse ) .",19268,2444,2019-03-08T13:55:40.093,2019-04-07T21:00:05.400,what should the range of the output layer be when performing classification ?,neural-networks classification activation-function supervised-learning,1,1,
3127,11103,1,,2019-03-08T17:37:58.113,1,26,is there a way to run c64 / nintendo gameboy / other platforms in an openai - gym like environment ? seems it 's atari games only .,23009,,,2019-03-08T18:03:45.767,openai gym for other platforms,reinforcement-learning open-ai,1,0,
3128,11107,1,,2019-03-08T20:08:39.423,1,23,"here intelligence is defined as any analytic or decision making process , regardless of strength ( utility ) , and , potentially , any process of computation that produces output , regardless of the medium . the idea that ai is part of an evolutionary process , with humans as merely a vehicle of the next dominant species , has been a staple of science fiction for many decades . it informs our most persistent mythologies related to ai . ( recent examples include terminator / skynet , westworld , ex machina , and alien : covenant ) . what i 'm driving at here is that , although the concept of neural networks have been around since the 1940 's , they have only recently demonstrated strong utility , so it 's not an unreasonable assumption to identify moore 's law as the limiting factor . ( i.e. it is only recently that we have had sufficient processing power and memory to achieve this utility . ) but the idea of ai is ingrained into information technology once automation becomes possible . babbage 's difference engine led to the idea of an analytic engine , and the game of tic - tac - toe was proposed as a means of demonstrating intelligence . what i 'm driving at here is that the idea of analysis and decision making are so fundamental in regard to information technology , that it is difficult to see functions that do n't involve them . and , if the strength of analysis and decision making is largely a function of computing power : can intelligence be understood as a naturally occurring function of information technology ?",1671,1671,2019-03-08T21:54:40.597,2019-03-08T21:54:40.597,is intelligence a naturally occurring function of information technology ?,philosophy theory soft-question mythology-of-ai intelligence,1,0,1
3129,11109,1,,2019-03-08T20:41:23.087,3,20,"as i know , with problem representation is meant the formulation of the problem in a way that it can be programmed and therefore solved ( for ex . you can represent the n - queens problem by using an array of nxn ) . but , what does problem modelling mean , and what do they differ ?",21832,2444,2019-03-08T21:01:25.097,2019-03-08T21:01:25.097,what is the difference between problem modelling and problem representation ?,terminology models difference ai-a-modern-approach,1,6,
3130,11112,1,11113,2019-03-08T21:21:16.363,2,52,"i was reading in the article a tutorial on partially observable markov decision processes ( p. 120 ) , by michael l. littman , that , where $ a$ is the action , $ s'$ the next possible state and $ z$ a certain / specific observation . how come that the observation function $ o(a , s ' , z)$ adds up to $ 1 $ in pomdp ?",19413,2444,2019-03-28T17:40:31.440,2019-03-28T17:40:31.440,does the observation function for pomdp always add up to 1 ?,reinforcement-learning markov-decision-process pomdp,1,0,0
3131,11116,1,11804,2019-03-09T03:05:14.073,2,28,"in this tutorial , they build a speech recognition model to classify a one - second audio clip as one of ten predefined words . suppose that we modified this problem as the following : given an arabic dataset , we aim to build a dialects recognition model to classify a two -second audio clip as one of $ n$ local dialects using ten predefined sentences . i.e. for each of these ten sentences , there are $ x$ different phrases and idioms which refer to the same meaning $ ^*$ . now how can i take advantage of the mentioned tutorial to solve the modified problem ? $ * $ the $ x$ different phrases and idioms for each sentence are not predefined .",20449,,,2019-04-13T20:58:33.217,how much the dialects recognition and speech recognition are relevant ?,deep-learning natural-language-processing classification voice-recognition,1,1,
3132,11117,1,,2019-03-09T05:25:36.703,3,66,"what are the characteristics which make a function difficult for the neural network to approximate ? intuitively , one might think uneven functions might be difficult to approximate , but uneven functions just contain some high frequency terms ( which in case of sigmoid is easy to approximate , by increasing the value of $ w$ ) . so uneven data might not be diffcult to approximate . so my question is what makes a function truly difficult for approximation ? note : by approximation i do not mean things which can be changed by changing the training method ( changing training set size , methods , optimisers ) . by approximation i mean things which require hyperparameters ( size , structure , etc ) of a nn to be changed to approximate to a certain level significantly easily .",9947,2444,2019-05-10T14:41:28.817,2019-05-10T14:41:28.817,what characteristics make it difficult for a neural network to approximate a function ?,neural-networks machine-learning deep-learning math function-approximation,0,4,1
3133,11118,1,,2019-03-09T05:57:32.520,1,24,"can ann with only one neuron in output layer be trained in a way that output neuron ’s value ( 0 - 1 ) can be representation of some real value , like for example height . in other words , can neural network given the inputs predict the height of a person by outputing values from 0 to 1 . zero being the 50 cm and 1 being 250cm.or it will always gravitate to 0 or 1?can it predict a height of 150 cm ( 0.5 ) ?",23022,,,2019-03-11T11:16:28.103,ann ’s single output representing value,neural-networks,1,0,
3134,11119,1,,2019-03-09T06:42:21.613,1,36,"i 'm writing an ai for a board game , and previously i would just create a value maximizing state machine and tune the factors one at a time . however , the issues with this is getting apparent . my last ai did not look into the future , hurting it 's long term chances , and manual tuning of weights is proving to be a chore . i 've looked into minimax algorithms , but with a non perfect information game and elements of random chance i 'm not too confident it will be effective . i 've also looked into traditional neural networks , but evaluating board states is tricky and the game does not split into moves well . the game i 'm writing the ai for is ticket to ride , but i would appreciate tips for any board game with similar mechanics .",23024,,,2019-03-09T11:52:22.073,game ai design for a multiplayer random board game ?,game-ai,1,1,
3135,11124,1,,2019-03-09T13:22:19.807,1,18,"there are many , many literary works in the public domain , along with human translations , many of which have entered the public domain as well . ( public domain = easily available ) in order for me to advance my knowledge of e.g. japanese , i 'd like to read texts in japanese using a yet to be written tool , and when i encounter an unknown word / phrase / sentence , i 'd like to just click on a position in the text and be transferred to the corresponding position in the e.g. english translation ( or original ) of the text in question . let 's also assume we have access to a dictionary that translates a fair amount of words between those two languages ( and there are of course free dictionaries for many language pairs ) . what ways are there to use ai toolkits plus some wiring and perhaps scripting / programming , to auto - correlate the positions of two versions of the same text , in two languages ? the results do not - and in fact in many cases can not - be perfect , but they should be roughly correct . i 'm aware that this is still not a straightforwards task , as there are complicating factors like inflection of verbs and other grammatical properties that make the use of dictionary tools much harder . also , translators will often translate to words that do n't have that mapping in any dictionary . then there is the fact that words are n't delimited by spaces in languages like my example language japanese - ( but if it is easier to work with only space - separated languages like , say , spanish , or russian , i 'd like to hear answers to this simpler problem as well ) . also the order of words and even hole sub - clauses differs from language to language . a simple , non - ai approximation would be to figure out at what relative position in the source language text the user clicked ( e.g. at the character on position 50.234 % ) then go to that same relative position 50.234 % in the target language text this approximation could perhaps be used as the starting point for the ai , which would then use words and dictionaries to make the results more accurate .",23027,,,2019-03-10T16:08:33.820,use ai to auto - correlate the words of human - translated texts ?,algorithm learning-algorithms,1,0,
3136,11125,1,11126,2019-03-09T14:27:23.940,1,40,i 'm looking for a simple chess algorithm that works well on a ~132mhz cpu . i know that minimax would take too long to go deep in order to find good moves . is monte carlo tree search working well on low cpus ? are there other options ? what algorithms did old chess computers use ?,19783,9947,2019-03-09T16:48:06.810,2019-03-09T16:48:06.810,chess engine for low clock cycle cpu,algorithm history monte-carlo-tree-search minimax chess,1,0,
3137,11127,1,,2019-03-09T16:30:14.757,1,39,"i would love to know if an ai model could come up with certain theories of the old like pythagoras ' theorem , euclid 's formulations , newton 's gravity , einstein 's theories if provided and trained with sufficient amount of observable data available at those period of time . if this is possible can unsolved conjectures be proved by ai ? or even better can ai develop new theories or will it fail to come up with even basic mathematical operations by itself ?",23034,,,2019-03-09T17:47:01.493,can ai come up with scientific theories of past when provided with sufficient data available at that time ?,machine-learning philosophy automated-theorem-proving,1,4,
3138,11128,1,,2019-03-09T16:38:17.100,3,25,can ai transform natural language text describing real scenarios to visual images and videos ? how does as ai interprets say a harry potter story if it has to reproduce it in form of videos ? would be useful if anyone can help me with the required literature for understanding text to image transformation by ai,23034,,,2019-03-09T16:38:17.100,how would an ai visualize a story written in natural language ?,natural-language image-generation,0,0,
3139,11129,1,,2019-03-09T17:29:17.463,3,18,"i have a nn i 'd like to train using supervised learning . some samples of the training set , however , have better "" quality "" than others , so i 'd like the algorithm to pay "" special attention "" to them . as general question , how to take this into account in implementation ? being more specific , i 'm working with opencv and noticed that the train method apparently have such parameter : cv2.ann_mlp.train(inputs , outputs , sampleweights [ , sampleidx [ , params [ , flags ] ] ] ) → retval where : sampleweights – ( rprop only ) optional floating - point vector of weights for each sample . some samples may be more important than others for training . you may want to raise the weight of certain classes to find the right balance between hit - rate and false - alarm rate , and so on . however opencv documentation is unclear on this , so how to handle this parameter ?",13087,,,2019-03-09T17:29:17.463,backpropagation : how to take into account different samples quality,machine-learning supervised-learning,0,0,
3140,11131,1,,2019-03-09T18:54:43.750,1,51,"i 'm trying create neural network to predict moves in a card game . i am looking for recommendations on encoding the game state to my input layer . it 's a complex turn based collectible card game ( think magic the gathering ) . i need to represent cards being in various areas of the game board ( deck , discard pile , hand , etc ) . it seems difficult to assign cards to these areas because the number of cards in these areas is never constant . i was thinking of an approach where i assign each card in the game to being in a specific card area . the number of total cards in the game should be constant ( let 's assume that ) . this approach i feel should give me a potentially less sparse input . also , with this approach what is the best way to handle card duplicates ? let 's say i have 3 copies of the exact same card in my deck . maybe 1 of the copies is in my hand and 2 in my discard pile . it does not matter which of the 3 copies is in my hand because they are all the same exact card . there is now multiple ways to represent this same exact game state in my network because each individual card has it 's own state . to me this does not seem good . how much will this effect my neural network 's ability to learn the game ?",23037,,,2019-04-09T02:00:36.283,how to encode card game state into neural network input,neural-networks game-ai,1,0,
3141,11134,1,11138,2019-03-10T06:18:15.020,2,73,"i notice with the recent revelation of severe limitations in some ai domains such as self driving cars that nnets behave with the same sort of errors as in simpler models . ie : they may be ~100 % accuracte on test data however if you throw in a test sample that is slightly different to anything it 's been trained on , it can throw the net off completely . this seems to be the case with self driving cars where nnets are miss - classifying modified / grafitied stop signs , unable to cope with rain or snow flakes , or birds appearing on the road , etc . something it 's never seen before in a unique climate may cause it to make completely unpredictable predictions . these specific examples may be circumvented by training on modified stop signs , or with rain and birds , but that avoids the point : that nn 's seem very limited when it comes to generalizing to an environment that is completely unique to its training samples . and this makes sense of course given the way nns train . the current solution seems to be to manually find out these new things that confuse the netowrk and label them as additional training data . but that is n't an ai at all . that is n't true generalization . i think part of the problem to blame is the term "" ai "" in and of itself . when all we 're doing is finding a global minima to a theoretical ideal function at some perfect point before over - fitting our training data , it 's obvious that the nnet can not generalize anymore than what is possible within its training set . i thought one way that might be possible to get around this is if rather than being static "" one unique calculation at a time "" if instead nnets could remember the last one or two predictions they made , and then their current prediction , and use the result of that to then make a more accurate prediction . ie : a very basic form of short term memory . by doing this perhaps the nn could see that the rain drops or snow flakes are n't static objects , but are simply moving noise . it could determine this by looking at its last couple of predictions and see how those objects move . certainly this would require immense additional computation overhead , but i 'm just looking to the future in terms of when processing power increases how nns might evolve further . similar to how nns were already defined many decades ago but were never a "" thing "" due to the lack of computational power , could this be the same case with something like short term memory ? that we lack the practical computational power for it but that perhaps we could theoretically implement it somehow for sometime in the future when we do have it . of course this short term memory thing would only be useful for when a classification is related to the prior classifications , like with self driving cars . it 's important to know what was observed a few seconds ago in real life when driving , likewise it could be important for an nn to know what was predicted a few live classifications ago . this might also have use in object detection , ie : perhaps a representation could be learned for a moving figure in the distance . speed could now become a representation in the hidden layers and used in assistance for identification of distant objects , something not possible when using a set of single static weights . of course this whole thing would involve somehow getting around the problem of training weights on a live model for the most recent sample . or alternatively perhaps the weights could still remain static but we 'd use two or three different models of weights to represent time somehow . nevertheless i ca n't help but see short term memory of some form as being a requirement if ai is to not be "" so stupid "" when it observes something unique , and if it 's to ever classify things based on time and recent observations . i 'm curious if there 's any research papers or other sources that explore any aspect of incorporating time using multiple recent classifications or something else that could be considered a short term memory of sorts to help reach a more general form of generalization that the nnet does n't necessarily see in its training , ie : making it able to avoid noise using time as a feature to help it do so , or using time from multiple recent classifications as a way to estimate speed and use that as a feature ? i 'd appreciate the answer to include some specific experiment or methodology as to how this sort of thing might be added to nnets ( or list it as a source ) , or if this is not an area of active research , why not . thank you .",6328,6328,2019-03-10T10:11:23.493,2019-03-10T17:17:21.697,neural nets need a short term memory of some sort,theory,1,3,
3142,11135,1,,2019-03-10T10:28:40.553,0,33,"i am working lately on batch normalization for the last couple of days , but i just ca n't seem to solve it on how to apply it . so , to give the full code to you , i will just send the implementation . implementation ddpg code : https://github.com/morvanzhou/reinforcement-learning-with-tensorflow/blob/master/contents/9_deep_deterministic_policy_gradient_ddpg/ddpg_update2.py and fun thing is that this github creator actually also has a batch normalization implementation : https://github.com/morvanzhou/tensorflow-tutorial/blob/master/tutorial-contents/502_batch_normalization.py . however , i do n't get how he does it , since his videos are in chinese . i did a lot of research so far on how to implement it , however i usually get valueerrors for "" nonetype "" or "" false_fn has to return a value "" . the name differences between everything makes me even more confused on where what should be . also , to avoid confusion , i am trying to add this version of batch normalisation : https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization if you have any idea , suggestion on how to implement this , more than welcome .",23050,2444,2019-03-10T10:47:03.037,2019-03-10T10:47:03.037,how do i apply batch normalization to my algorithm ?,machine-learning reinforcement-learning python tensorflow,0,2,2
3143,11139,1,,2019-03-10T17:29:15.723,1,107,"i 'm looking for neural network architecture that excel in counting objects . for example , cnn that can output the number of balls ( or any other object ) in a given image . i already found articles about crowd counting , i 'm looking for articles about different types of objects . thank",23049,,,2019-03-10T18:30:42.397,count number of objects in image using cnn,neural-networks deep-learning convolutional-neural-networks image-recognition,1,0,
3144,11141,1,,2019-03-10T20:54:06.810,3,42,i want to model an smdp such that time is discretized and the transition time between the two states follows an exponential distribution and there would be no reward between the transition . can i know what are the differences between $ q(\lambda)$ and q - learning for this problem ( smdp ) ? i actually want to extend the pseudo - code presented here to an smdp problem with discretization of time horizon .,10191,2444,2019-03-10T21:24:10.793,2019-03-11T17:43:24.233,how to apply or extend the $ q(\lambda)$ algorithm to semi - mdps ?,reinforcement-learning q-learning semi-mdp eligibility-traces,1,0,
3145,11142,1,,2019-03-10T22:05:40.670,0,66,"when training a ai rl agent to play a game there 'll be situations where the ai can not perform certain actions lest they violate the game rules . that 's easy to handle , and i can set illegal actions to some large negative amount so when doing an argmax they wo n't be selected . or if i use softmax i can set probabilities of illegal actions to zero and then re - calculate softmax on the remaining legal states . indeed , i believe this is what david silver was referring to when asked this question at a presentation / lecture of alphazero : https://www.youtube.com/watch?v=wujy7ozvdjk&amp;t=2404s but doing so changes the output from the network and surely changes things when performing the backprop once a reward is known . how does one handle that ? would i set the illegal actions to the mean of the legal actions , or zero ... ?",20352,20352,2019-03-11T12:42:28.323,2019-03-11T12:42:28.323,how to back - propagate illegal actions for policy gradient learning,machine-learning reinforcement-learning game-ai backpropagation,0,11,
3146,11143,1,,2019-03-11T01:14:01.363,0,32,"i am currently trying to create a one - shot network using the siamese architecture for an object that is n't a face . my problem is , in normal face recognition the detecting gadget ( e.g. smartphone ) knows which image it should compare the face , currently trying to unlock the tool , to . in my case , i do n't know the object 's identity so i would have to compare every other object in the database to it . is there a better , more efficient way of checking the identity of the object without comparing it to any other object . a normal classification does n't work in my case , because new unknown objects can be added any time .",23063,,,2019-03-11T01:14:01.363,siamese network for unknown object,convolutional-neural-networks image-recognition classification computer-vision,0,0,
3147,11144,1,,2019-03-11T07:31:54.857,0,28,"i need to extract personal information about a person from a list of documents and summarize it to the user . if there are 2 people with the same name , the correct person should be identified . if the person has a nickname , that also needs to be identified . the input to the program can be the name of the person , address , organization name etc . i have extracted named entities like person , org , location etc from the text using nltk library . the output after extracting the named entities is mentioned below , [ ( ' michael ' , ' nnp ' , ' b - person ' ) , ( ' joseph ' , ' nnp ' , ' b - person ' ) , ( ' jackson ' , ' nnp ' , ' i - person ' ) , ( ' was ' , ' vbd ' , ' o ' ) , ( ' born ' , ' vbn ' , ' o ' ) , ( ' in ' , ' in ' , ' o ' ) , ( ' gary ' , ' nnp ' , ' b - gpe ' ) , ( ' , ' , ' , ' , ' o ' ) , ( ' indiana ' , ' nnp ' , ' b - gpe ' ) .... now , i want to extract relationships between those entities .",23069,23069,2019-03-11T08:44:26.103,2019-03-11T08:44:26.103,extract personal information about a person from a list of documents and summarize it,natural-language-processing,0,4,
3148,11150,1,11157,2019-03-11T11:07:21.470,1,54,"i am dealing with intent classification task on an italian customer service data set . i 've more or less 1.5k sentences and 29 classes ( imbalanced ) . according to the literature , a good choice is to generate synthetic data , oversampling , or undersampling the training data , using for example the smote algorithm . i also want use a cross - validation mechanism ( stratified k -fold ) to be more confident in the obtained result . i also know that accuracy is not the right metric to take into account , i should use precision , recall and confusion matrix . is it possible to combine cross validation k -fold and oversampling or undersampling techniques ?",20780,2193,2019-03-11T15:08:00.590,2019-03-11T18:13:08.523,multi class text classification with imbalanced data,classification,1,0,
3149,11152,1,,2019-03-11T16:02:04.947,2,73,"reading the high - level descriptions of backpropagation and predictive coding , they do n't sound so drastically different . what is the key difference between these techniques ? i am currently reading the following paper if that helps ground the explanation : predictive coding - based deep dynamic neural network for visuomotor learning",20955,2444,2019-03-12T15:36:31.373,2019-03-12T15:36:31.373,what is the difference between backpropagation and predictive coding ?,neural-networks machine-learning backpropagation difference,1,0,
3150,11154,1,,2019-03-11T17:29:07.163,0,29,"glados { glados is responsible for testing and maintenance in the aperture science research facility in all titles . while glados initially appears in the first game to simply be a voice that guides the player , her words and actions become increasingly malicious as she makes her intentions clear . the second game , as well as the valve created comic lab rat , reveals that she was mistreated by the scientists and used a neurotoxin to kill the scientists in the laboratory }",23085,1671,2019-03-11T20:30:15.677,2019-03-11T20:30:15.677,can glados be real ?,mythology-of-ai popular-culture,0,2,
3151,11156,1,,2019-03-11T18:01:38.167,0,46,"i just came accros the "" selu "" function . it looks promising , especially with the header "" self normalizing neural network "" , since i have some serious with exploding gradient within my network with ( leaky ) relu . personally i was n't able to implement batch norm , so i hope this selu could help me . so i have two main questions : does selu work like batch norm , as in normalizing the network , thus avoiding exploding gradients how do i implement selu in my network ? so i am talking about this from the tensorflow documentation : https://www.tensorflow.org/api_docs/python/tf/nn/selu this is how my network currently looks like : net = tf.layers.dense(s , 64 , activation = tf.nn.leaky_relu , name='l1 ' , trainable = trainable ) mid = tf.layers.dense(net , 33 , activation = tf.nn.tanh , name='l2 ' , trainable = trainable ) a = tf.layers.dense(mid , 3 , activation = tf.nn.softmax , name='la ' , trainable = trainable ) so with selu i am expecting it to look more like : net = tf.layers.dense(s , 64 , activation = tf.nn.selu , name='l1 ' , trainable = trainable ) mid = tf.layers.dense(net , 33 , activation = tf.nn.tanh , name='l2 ' , trainable = trainable ) a = tf.layers.dense(mid , 3 , activation = tf.nn.softmax , name='la ' , trainable = trainable ) simple , right ? however i know i am missing something as stated on their documentation : to be used together with initializer = tf.variance_scaling_initializer(factor=1.0 , mode='fan_in ' ) here is where my problem occurs , because i have no idea what they mean with this neither if i need to give more information to the network besides only this line ? i also saw you had to do something with the n_states ( assuming dimension of state ? ) . if anyone could help me out , that would be great ! jan",23050,16565,2019-03-12T21:36:43.773,2019-03-12T21:36:43.773,selu and how to implement it ?,python tensorflow activation-function,0,2,
3152,11158,1,,2019-03-11T19:43:59.410,0,31,"i m trying to solve pong by a dqn approach . these are my hyper parameters : replay_buffer = deque(maxlen=100000 ) batch_size = 50 update_factor = 1000 gamma = 0.99 # discount future rate alpha = 0.0001 # learning rate epsilon = 1.0 epsilon_min = 0.01 decay_rate = 0.00005 and my network looks like : def create_net(self ) : # neural - net model = sequential ( ) model.add(dense(24 , input_dim = self.amount_obs , activation='relu ' ) ) model.add(dense(24 , activation='relu ' ) ) model.add(dense(24 , activation='relu ' ) ) model.add(dense(24 , activation='relu ' ) ) model.add(dense(self.amount_actions , activation='linear ' ) ) model.compile(loss='mse ' , optimizer = adam(lr = self.alpha ) ) return model my logic is as follows : get 5 frames / steps from the environment append these to my replay buffer sample a minibatch from my replay buffer train on this minibatch repeat i update my target network after 1k frames / steps with this approach i get a perfect score on cartpole after 50k/60k frames / steps . but i ca nt even get a slight improvement on pong . and after 150k frames , i have over trained and my agent wo nt even move from the bottom . what can i be doing wrong ? is my logic correct ? what are some common mistakes ? very glad for any help ! code : import gym from gym import spaces import numpy as np import random from collections import deque from keras.optimizers import adam from keras.models import sequential , save_model , load_model from keras.layers import activation , dense import copy import sys # deep q - learning agent class model : def _ _ init__(self , environment , model_name = ' ' ) : self.env = gym.make(environment ) self.global_state = self.env.reset ( ) self.amount_obs = len(self.env.observation_space.sample ( ) ) self.amount_actions = self.env.action_space.n self.f = open('learning_history ' , ' a ' ) self.replay_buffer = deque(maxlen=100000 ) self.batch_size = 50 self.amount_frames = 5 self.update_factor = 1000 self.max_frames = 100000 self.gamma = 0.99 # discount future rate self.alpha = 0.0001 # learning rate # epsilon = explore / exploite factor self.epsilon = 1.0 # 100 % explore initally self.epsilon_min = 0.01 self.decay_rate = 0.00005 if model_name : print(""using existing model "" ) self.network = load_model(model_name ) else : self.network = self.create_net ( ) self.target_network = copy.deepcopy(self.network ) def create_net(self ) : # neural - net model = sequential ( ) model.add(dense(24 , input_dim = self.amount_obs , activation='relu ' ) ) model.add(dense(24 , activation='relu ' ) ) model.add(dense(24 , activation='relu ' ) ) model.add(dense(24 , activation='relu ' ) ) model.add(dense(self.amount_actions , activation='linear ' ) ) model.compile(loss='mse ' , optimizer = adam(lr = self.alpha ) ) return model def act(self , state , always_exploit = false ) : # reshape into a column vector state = np.reshape(state , [ 1 , self.amount_obs ] ) # explore or exploite if random.random ( ) & lt;= self.epsilon and not always_exploit : # random action ( explore ) return random.randrange(self.amount_actions ) # predict an action ( exploite ) possible_actions = self.network.predict(state ) return np.argmax(possible_actions[0 ] ) def generate_data(self ) : state = self.global_state for episode in range(self.amount_frames ) : # decide action action = self.act(state ) next_state , reward , done , _ = self.env.step(action ) # reshape into a column vectors state = np.reshape(state , [ 1 , self.amount_obs ] ) next_state = np.reshape(next_state , [ 1 , self.amount_obs ] ) # save the state self.replay_buffer.append((state , action , reward , next_state , done ) ) if not done : state = next_state else : state = self.env.reset ( ) self.global_state = state def train(self ) : frames = 0 while true : # get training data self.generate_data ( ) # only if we do nt have enough data if len(self.replay_buffer ) & gt ; self.batch_size : states = [ ] target_states = [ ] sample_batch = random.sample(self.replay_buffer , self.batch_size ) for state , action , reward , next_state , done in sample_batch : if done : target = reward else : target = reward + self.gamma * \ np.amax(self.target_network.predict(next_state ) ) target_state = self.network.predict(state ) target_state[0][action ] = target # saves states into a list state = np.squeeze(state ) states.append(state ) # saves target state into a list target_state = np.squeeze(target_state ) target_states.append(target_state ) self.network.fit(np.asarray(states ) , np.asarray(target_states ) , \ batch_size = self.batch_size , \ epochs=2 , verbose=0 ) # decrease epsilon if self.epsilon & gt ; self.epsilon_min : self.epsilon -= self.decay_rate # update target network after each n : th step if frames%self.update_factor==0 and not frames==0 : print ( "" { } frames"".format(frames ) ) print(""update target network and evaluate "" ) # update target network self.target_network = copy.deepcopy(self.network ) # play games to determain how good network is play_average = self.play(5 ) self.f.write(str(frames ) + ' , ' + str(play_average ) + ' \n ' ) print(""new epsilon { } "" .format(self.epsilon ) ) # save network if frames%10000==0 and not frames==0 : # save the episode number and its average score name = "" model _ "" + str(frames ) self.network.save(name ) print ( "" { } saved ... "".format(name ) ) # increment loop frames + = self.amount_frames def play(self , episodes ) : total_reward = 0 for episode in range(1 , episodes+1 ) : state = self.env.reset ( ) episode_reward = 0 done = false while not done : self.env.render ( ) # always get best action action = self.act(state , always_exploit = true ) next_state , reward , done , _ = self.env.step(action ) episode_reward + = reward state = next_state if done : break total_reward + = episode_reward print(""game : { } with score { } "" .format(episode , episode_reward ) ) average = total_reward / episodes print(""average score of { } games : { } "" .format(episodes , average ) ) return average if len(sys.argv ) & gt ; 1 : model = model('pong - ram - v0 ' , sys.argv[1 ] ) else : model = model('pong - ram - v0 ' ) # model.train ( ) model.play(10 )",22349,,,2019-03-11T19:43:59.410,having trouble solving pong . my model is not improving,machine-learning reinforcement-learning dqn,0,4,
3153,11163,1,,2019-03-12T01:11:11.133,1,28,"according to per , we have to multiply the $ q$ error by the importance sampling ratio to correct the bias introduced by the imbalance sampling of per , where importance sampling ratio is defined $ $ w_i=\left({1\over n}{1\over p(i)}\right)^\beta $ $ in which $ 1 / n$ is the probability of drawing a sample uniformly from the buffer , and $ p(i)$ is the probability of drawing a sample from per . i 'm wondering if we have to do the same to the target of the actor when we apply per to ddpg . that is , multiplying $ -q(s_i , \mu(s_i))$ by $ w_i$ , where is the outcome of the actor . in my opinion , it is necessary . and i 've done some experiments in the gym environment bipedalwalker - v2 . the results , however , is quite confusing : i constantly get better performance when i do not apply importance sampling to the actor . why would this be the case ?",8689,2444,2019-04-04T16:37:20.530,2019-04-04T16:37:20.530,should we multiply the target of actor by the importance sampling ratio when prioritized replay is applied to ddpg ?,reinforcement-learning actor-critic experience-replay ddpg,0,0,
3154,11164,1,,2019-03-12T09:50:56.700,0,15,in my project i have around 10k requirements and 60k test cases . i would like to use some ai techniques to handle the requirement to test case mapping . i do n't know which ai methos or techniques i should use . any suggestions / comments please . i am familiar with python and c++ programming languages . sankar b.,23106,,,2019-03-12T09:50:56.700,requirement traceability,machine-learning deep-learning natural-language-processing,0,1,
3155,11165,1,11185,2019-03-12T09:57:37.507,2,27,"i wonder if it would be possible to know the size of a room using image , i do n't see anything about this subject , do you have some idea how it could be done ?",23107,2444,2019-03-12T11:02:33.867,2019-03-13T02:58:19.773,how could we estimate the square footage of a room from an image ?,machine-learning deep-learning computer-vision papers,1,1,
3156,11166,1,,2019-03-12T10:29:49.193,5,285,what is geometric deep learning ( gdl ) ? how is it different from deep learning ? why do we need gdl ? what are some applications of gdl ?,2444,2444,2019-03-13T17:38:02.750,2019-03-19T08:41:01.907,what is geometric deep learning ?,machine-learning deep-learning definitions graphs geometric-deep-learning,2,2,5
3157,11169,1,,2019-03-12T10:46:08.563,1,65,what is a graph neural network ( gnn ) ? how is a gnn different from a nn ? how exactly is a gnn related to graphs ? what are the components of a gnn ? what are the inputs and outputs of gnns ? how can gnns be trained ? can we also use gradient descent with back - propagation to train gnns ?,2444,2444,2019-03-13T17:29:21.047,2019-03-13T17:29:21.047,what is a graph neural network ?,deep-learning definitions graphs geometric-deep-learning,0,1,
3158,11170,1,,2019-03-12T11:30:31.120,0,22,"how is the convolution operation used in convolutional neural networks ( cnns ) a special case of the mathematical convolution operator ? most of us , when we think of the "" convolution operation "" , we think of the main operation associated with cnns , that is , the operation of sliding a kernel ( or a filter ) over an input 2d space ( or 3d volume ) . however , in mathematics , the convolution between two functions $ f$ and $ g$ is defined as $ $ ( f*g)(t)\triangleq \ \int _ { -\infty } ^{\infty } f(t-\tau ) g(\tau ) \,d\tau$$ where the symbol $ * $ is the convolution operation ( between functions $ f$ and $ g$ ) . apparently , this definition has little to do with the convolution operation used in cnns , because this definition is an integral over the real numbers . so , how can we derive the mathematical definition of the convolution operation used in cnns from the definition above ? the integral is with respect to a variable . how does this relates to the definition of convolution used in cnns ? why are there two variables ( $ t$ and ) in the definition above ?",2444,,,2019-03-12T11:30:31.120,how is the convolution operation used in cnns a special case of the convolution operator ?,machine-learning convolutional-neural-networks math geometric-deep-learning convolution,0,1,
3159,11171,1,,2019-03-12T13:31:25.977,0,16,"i have an agent that will play in a multi - agent environment , and i 'd like that agent to play against other agents that have been previously trained . the model for each agent was saved using tf.train.saver ( ) . what is the neatest way to load the opponents models , bearing in mind each agent will ( i suspect ) need its own independent session and graph , otherwise i 'll end up with duplicate model elements on the same graph and tensorflow will throw an error . it seems rather clunky to need lots of graphs and sessions , so i was wondering if there was a neater way to achieve this ?",20352,,,2019-03-12T13:31:25.977,loading multiple trained models for use in multi - agent environment,python tensorflow,0,2,
3160,11172,1,,2019-03-12T13:57:59.017,2,33,"how can the convolution operation used by cnns be implemented as a matrix - vector multiplication ? we often think of the convolution operation in cnns as a kernel that slides across the input . however , rather than sliding this kernel ( e.g. using loops ) , we can perform the convolution operation "" in one step "" using a matrix - vector multiplication , where the matrix is a circulant matrix containing shifted versions of the kernel ( as rows or columns ) and the vector is the input . how exactly can this operation be performed ? i am looking for a detailed step - by - step answer that shows how the convolution operation ( as usually presented ) can be performed using a matrix - vector multiplication . is this the usual way the convolution operations are implemented in cnns ?",2444,,,2019-03-12T13:57:59.017,how can the convolution operation be implemented as a matrix - vector multiplication ?,machine-learning deep-learning convolutional-neural-networks math implementation,0,1,
3161,11173,1,,2019-03-12T16:00:54.587,1,31,"the book by sutton and barto discussed in section 11.8 that the convergence of off - policy td function approximation can be improved by correcting for distribution of states encountered . the section seems to be written in haste and does n't do a good job in explaining why will $ m_t$ , the emphasis , help in getting a state distribution closer to the target policy . my understanding of on - policy distribution is not clear at the moment . i think it is the distribution of states encountered under the target policy ( the policy for which we want to state - action / state values ) . the importance sampling ratio corrects for update distribution ( by multiplying the correction term with the ratio ) , but how is $ m_t$ helping in correcting for state distribution ?",21509,,,2019-03-13T07:29:41.120,on - policy distribution for emphatic td,reinforcement-learning off-policy temporal-difference,1,0,
3162,11174,1,11194,2019-03-12T16:30:55.220,5,135,"i have been using openai retro for awhile , and i wanted to experiment with two player games . by two player games , i mean co - op games like "" tennis - atari2600 "" or even pong , where 2 agents are present in one environment . there is a parameter for players in the openai documentation , but setting this variable to 2 does nothing in terms of the game . how do you properly implement this ? can this even be done ? the end goal is to have 2 separate networks per one environment .",23119,1847,2019-03-13T09:43:52.320,2019-03-15T19:04:34.423,2 player games in openai retro,machine-learning deep-learning reinforcement-learning python open-ai,1,4,
3163,11178,1,,2019-03-12T17:10:04.383,0,32,"in word2vec , the task is to learn to predict which words are most likely to be near each other in some long corpus of text . for each word $ c$ in the corpus , the model outputs the probability distribution $ p(o = o|c = c)$ of how likely each other word $ o$ in the vocabulary is to be within a certain number of words away from $ c$ . we call $ c$ the "" center word "" and $ o$ the "" outside word "" . we choose the softmax distribution as the output of our model : $ $ p(o = o|c = c ) = \frac{\exp(\textbf{u}_{0}^{t } \textbf{v}_{c})}{\sum_{w \in \text{vocab } } \exp(\textbf{u}_{w}^{t } \textbf{v}_c)}$$ where and are vectors that represent the outside and center words respectively . question . what do the vectors and look like ? are they just one - hot - encodings ? do we need to learn them too ? why is this useful ?",23120,2444,2019-04-16T22:26:26.883,2019-04-16T22:26:26.883,what do the vectors of the center and outside word look like in word2vec ?,natural-language-processing word2vec word-embedding,1,0,
3164,11181,1,,2019-03-12T19:51:22.200,2,51,"i want to tackle the problem of detecting similar objects in an image . to illustrate the problem consider this photo of some lego bricks as my "" input "" : the detection routine should identify similar objects . so for the given input , it should e.g. identify the following output : so an object might appear none to multiple times in the input image . for example , there are only two bricks marked with a blue cross , but three bricks marked with a red cross . it can be assumed that all objects are of similar kind , so e.g. only lego bricks or a heap of sweets . my initial idea is to apply a two - phased approach : extract all objects in input image . compare those extracted objects and find similar ones . is this a valid approach or is there already some standard way of solving this kind of problem ? can you give me some pointers how to solve this problem ?",23123,,,2019-03-13T07:47:07.023,how do i detect similar objects in an image ?,object-recognition,2,0,1
3165,11182,1,11183,2019-03-12T21:09:52.673,1,48,"i am trying to understand the value iteration method for markov decision process(mdp ) and i was referring ot uc berkeley 's slides titled markov decision processes and exact solution methods on slide no.9 , we start with the first step : ok ! so , we have the information about the transition function ( desribed elaborately in slide no.5 as well ) , the resting reward is 0 and discount of 0.9 . using this , i am able to compute the utility value of the cell left to terminal state with r = +1 ( green cell ) . the action that is going to be most rewarding at this cell is moving forward , so putting the values in the equation as : 0.0 + 0.9(0.8 * 1 + 0.1 * 0 + 0.1 * 0 ) = 0.72 which seems to be correct : now , using the same algorithm i am able to compute the value of the cells adjacent to this newly obtained utility cell value . however , i really do not know how did they update the value from 0.72 - > 0.78 in the next slide : i have tried searching at various sites and seen some videos but most of them stop at first iteration assuming the next step is same as it is a recursive equation ( and it should have been so ! ) but i am stuck at this !",23126,,,2019-03-12T22:05:07.810,unable to understand the second iteration update in value iteration algorithm for solving mdp,reinforcement-learning ai-basics markov-decision-process value-iteration mdp,1,0,
3166,11189,1,11193,2019-03-13T08:29:19.037,1,27,"i am making an ai model to predict monthly retail sales of a motor cycle spare parts shop , for that to be possible i have to first create a dataset . the problem i am facing is what features should the dataset have ? i already did some research on some other datasets but still i want to know specifically what features should it have other than date , product name , quantity , net amount , gross amount .. ?",23140,9608,2019-03-13T20:58:40.717,2019-03-13T20:58:40.717,what features should a dataset to predict monthly retail sales for a motorcycle spare parts shop have ?,datasets feature-selection data-mining,2,0,
3167,11191,1,,2019-03-13T08:46:32.527,0,13,"suppose we have two independent random variables ( a and b ) that are identically independently distributed gaussian distribution of mean 0 and standard deviation 1 . how can the probability that a is less than b be calculated ? application : a is a lab test measurement at time 1 and has value 1.2 ; and b is a lab test measurement at time 2 and has value 1.4 . how can i calculate the probability that lab test a is actually less than lab test b ( rather than b > = a ) ? i.e. how sure am i that the lab test increased ( and this is not just random variation ) ? my first guess is that the answer is the area under the curve ( aoc ) in interval ( a , infinity ) * aoc in interval ( -infinity , b ) , i.e. the aoc greater than a times the aoc less than b , but this is a wag and i can not find this type of problem in my statistics texts .",23143,,,2019-03-13T08:46:32.527,comparing normally distributed lab values,fuzzy-logic,0,0,
3168,11196,1,,2019-03-13T11:20:27.733,5,128,"i am looking for a source that really discusses the classic rules of learning in depth . so classical conditioning , operant conditioning , imitation learning ... i have found an infinite number of books that supposedly discuss these topics , but have not yet found anything that summarizes all the important findings on this topic . i am familiar with the basics , but i am explicitly interested in a detailed presentation of the topic . can anyone tell me a good source about the different forms of classical learning in mammals ? i consider "" reinforcement learning : an introduction "" comprehensive and detailed regarding rl . a comparable book on biological learning systems would be great . sources in german and english would fit .",17658,1847,2019-03-14T07:44:41.517,2019-03-18T12:47:02.480,looking for comprehensive material on learning in mammals,reinforcement-learning biology,2,6,3
3169,11199,1,,2019-03-13T16:07:11.727,0,11,"suppose is a sequence of words $ w_1 , \dots , w_k$ . suppose we want to find $ p(\textbf{w})$ . in an $ n$ -gram language model , $ $ p(\textbf{w } ) = \prod_{i=1}^{k+1 } p(w_{i}|w_{i - n+1}^{i-1})$$ when you want to train this model , does the training data consist of the individual words $ w_1 , \dots , w_k$ or multiple sequences ? are the parameters to be estimated the conditional probabilities of each of the words given the previous $ n-1 $ words ?",23120,23120,2019-03-13T16:12:57.800,2019-03-13T16:12:57.800,training data of $ n$-gram language models,natural-language-processing,0,0,
3170,11200,1,,2019-03-13T16:36:18.250,0,20,"i am looking for keywords / papers / communities : my group is working on a ml - model that can work with little data ( and bad accuracy ) as long as there actually is little data available but can easily be extended as soon as said data becomes available ( think of interpolating existing models and then creating an individual predictor ) . this is due to a business requirement of the relevant application ( new plants get installed over time but need to be integrated seamlessly ) . transforming from a coarse yet cheap prediction to an accurate but expensive prediction takes labor effort that the company can invest as desired . my question is , are there research areas that take into account such "" evolutionary transformation processes "" which enable a tradeoff between costs and accuracy ? what are the right keywords to look for ?",23158,,,2019-04-13T05:02:15.463,are there communities dealing with costs - vs - accuracy tradeoffs in machine learning ?,machine-learning,1,1,
3171,11202,1,,2019-03-13T18:03:26.477,1,25,"suppose we want to classify a review as good ( $ 1 $ ) or bad ( $ 0 $ ) . we have a training data set of $ 10,000 $ reviews . also suppose we have a vocabulary of $ 100,000 $ words $ w_1 , \dots , w_{100,000}$ . so the data is a matrix of dimension $ 100,000 \times 10,000 $ . let 's represent each of the words in the reviews using a bag - of - words approach over tf - idf values . also we normalize the rows such that they sum to $ 1 $ . in a logistic regression approach would we have $ 10,000 $ different logistic regression models as follows : $ $ \log \left(\frac{p}{1-p } \right)_{1 } = \beta_{0_{1 } } + \beta_{1_{1}}w_{11 } + \dots + \beta_{100,000_{1}}w_{100,000 } \\ \vdots \\ \log \left(\frac{p}{1-p } \right)_{10,000 } = \beta_{0_{10,000 } } + \beta_{1_{10,000}}w_{11 } + \dots + \beta_{100,000_{10,000}}w_{100,000}$$ so are we estimating $ 100,000 \times 10,000 $ coefficients ?",23120,23120,2019-03-14T14:45:10.187,2019-04-13T18:02:30.500,sentiment analysis and logistic regression,natural-language-processing,1,0,
3172,11203,1,,2019-03-13T18:08:28.337,1,65,"i want to make an ai with deep learning which can adapt itself from user to user . let 's say we have food combiner ai which suggests a food to eat with another food as you give as input . this is the most personalized case i found to ask here . for example the ai suggested a food for me . however , the food ai suggested for me might not be good choice for another person . so another person will let the ai know like "" i do n't like that food to eat with this . etc . when the user let the ai know that , it should affect ai 's further combination food suggestions . how can i build that ai ? where should i start from ? which area or topics should i research ?",16864,2444,2019-03-13T20:51:18.713,2019-03-13T21:08:15.427,how create an ai that continuously adapts to different users ?,machine-learning deep-learning,1,2,
3173,11204,1,,2019-03-13T18:10:49.323,0,43,"currently i 'm feeding spectrogram of audio to the cnn with 3 convolution . each convolution is followed by a max pool of filter size 2 . first - > 5x5x4 second - > 5x5x8 third - > 5x5x16 and final layer is a fully connected with 512 unit . but while training with dropout of 0.25 , getting train accuracy of 0.97 with 150 iterations . and on test data accuracy is just 0.60 . tell me how to improve the results . yes both train and test data come from same distribution .",18428,,,2019-04-12T22:02:15.450,how to reduce over - fitting on training set ?,machine-learning deep-learning convolutional-neural-networks,1,2,
3174,11207,1,,2019-03-13T19:24:03.697,0,15,"i am trying to generate a word vector representation of the textual descriptions of events from the log file in a distributed system . the logged events are time series data and correlated . during the pre - processing , i have removed the unwanted entries(columns representing timestamp , ids , log level , ... ) and special characters in the descriptions part . i now need to convert the filtered textual data to a vector representation to perform feature extraction and clustering . the goal is to perform an unsupervised anomaly detection from the log data . would tf - idf be the ideal choice in such a scenario ? will a sparse matrix impact the succeeding steps ? note : the log file also contains vocabulary specific to the system which is not contained in the english dictionary .",23161,,,2019-03-13T19:24:03.697,word vector representation for feature extraction from log file,machine-learning natural-language-processing,0,0,
3175,11208,1,,2019-03-13T19:33:08.647,2,34,"max tegmark discusses the topic consciousness in his book life 3.0 and comes to the conclusion , that consciousness is substrate independent . if his analysis is correct , it should be possible to create artificial consciousness . the integrated information theory ( iit ) , while currently only just a theory , also points in this direction . this leads me to the question , which fields of ai research are currently actively engaged in this domain , if any . so far i 've only found research concerning consciousness in neuroscience and discussions of experts in philosophy . are there any projects publicly known concerning artificial consciousness or organizations who are active in this regard ?",9161,,,2019-03-14T02:26:27.853,which fields of ai are actively researching consciousness ?,artificial-consciousness,1,0,
3176,11209,1,,2019-03-13T20:00:59.333,3,28,"i am reading the paper regret minimization in games with incomplete information on cfr algorithm . on page 4 , the paper defines $ r^{t,+}_{i,\text{imm}}=\max\{r^{t}_{i,\text{imm } } , 0\}$ after equation ( 5 ) . i am confused why it is necessary ? it seems to me that since in the definition of $ r^{t}_{i,\text{imm}}$ the regret is computed with respect to the optimal action . as everything is in expectation , is mixed - action going to make any difference ? is n't $ r^{t}_{i,\text{imm}}$ always nonzero already ?",23163,1671,2019-03-13T21:01:56.900,2019-03-13T21:01:56.900,negative counterfactual regret,reinforcement-learning game-ai game-theory poker,0,0,
3177,11212,1,11404,2019-03-13T21:17:24.320,0,31,"i 'm trying to replicate the deepmind dqn paper , and actually i 'm using the openai - gym enviroment . i 'm trying to get a decent score with space invaders ( using spaceinvaders - v4 enviroment ) . i checked the number of actions available with env.unwrapped.get_action_meanings ( ) and i get this : [ ' noop ' , ' fire ' , ' right ' , ' left ' , ' rightfire ' , ' leftfire ' ] checking the number of actions with env.action_space.n gives me a number of 6 actions . the ' rightfire ' and ' leftfire ' actions i suppose is n't used , am i right ? if so , restricting the action size to the 4 first actions would improve my learning ?",9818,,,2019-03-23T15:04:42.993,openai - gym excess of actions,deep-learning reinforcement-learning game-ai dqn open-ai,1,6,
3178,11214,1,,2019-03-14T02:31:53.637,1,30,"i just recently got into machine learning , and have been hitting a lot of obstacles understanding the algorithms involved in the programmings . my issue is nt with the programming , but how they 're translated from math to code . ml is popular with python , and that 's okay , but i do nt like python , and i do nt want to have to learn it to be able to use the programming language of my choice , to do the exact same thing but in a way i feel comfortable ( i do nt care if python is popular for math majors , because it 's easier for them to understand -- it is nt for me , when nothing being done is explained thoroughly . ) . i 'm trying to decipher this model this is the breakdown for the algorithm model this is the math i was able to decipher for this particular model ( left is terminology and their usage , the middle in black was something to do with programming arrays ... below it is the equation used in bottom left , but more elaborate , and underneath that is a image that says the same thing the algorithm is doing .. because sometimes pictures are easier to understand that words vectorarray(value ) * vectorarray(weight ) + singleunit(bias ) = neuron(node ) ) but then everything stops at the middle layer of the second image . how do i get the full output to give me a yes or no response ? how do i enter in the variables and tables to go thru the math steps ? is my understanding correct , or am i lacking somewhere ? this user is also sharing the same algorithm but our math do nt look the same how do i go from what i have , to what [ s]he has ? at the end of all of these questions , i 'm going to write everything into a programming script , that 'll use a different language from python ( and i would need to manually create resources from scratch , because no one else thinks machine learning should be done in other languages -- it seems ... ) . i want to be able to understand the process itself , without just doing cookie - cutter actions ( tools made by users for those too lazy to do the work -- which circumvent the learning / understanding process of what 's going on behind scenes ) .",23171,23171,2019-03-14T07:54:43.193,2019-03-14T07:54:43.193,"how to translate algorithm from logic to equation , and back ?",neural-networks machine-learning algorithm,0,10,
3179,11215,1,,2019-03-14T04:07:41.590,0,20,i am building a mobile camera app that needs to show a real time preview of the processed image . so i was thinking of reducing the image size to 100 x 100 . but as for the weights and number of layers . is there a maximum known value for the processing time to be less than 20 fps .,22876,,,2019-03-14T04:07:41.590,maximum number of nodes for cnn for mobile phones,convolutional-neural-networks image-generation,0,3,
3180,11218,1,11224,2019-03-14T05:02:29.717,2,56,"i need to create model which will find suspicious entries or anomalies in a network , whose characteristics or features are the asset_id , user_id , ip accessed from and time_stamp . which unsupervised anomaly detection algorithms or models should i use to solve this task ?",23174,2444,2019-03-14T10:47:21.273,2019-03-14T10:47:21.273,which unsupervised anomaly detection algorithms are there ?,unsupervised-learning anomaly-detection,2,0,
3181,11219,1,11221,2019-03-14T05:49:49.220,3,83,"i read top articles on google search about deep q - learning : https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8 https://skymind.ai/wiki/deep-reinforcement-learning https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/ q - learning page on wikipedia * and then i noticed that they all use cnn as approximator . if deep learning has a broader definition than just cnn , can we use the term "" deep q - learning "" on our model if we do n't use cnn ? or is there a more appropriate definition for that kind of q - learning model ? for example , if my model only using deep fully - connected layer . * it does n't say explicitly deep rl means cnn on rl , but it uses the deepmind ( that uses cnn ) as an example on deep q - learning",16565,16565,2019-03-15T03:50:47.987,2019-03-22T09:10:32.753,do we have to use cnn for deep q learning ?,reinforcement-learning definitions deep-rl,2,0,
3182,11220,1,,2019-03-14T06:37:24.773,0,27,"i am not new to ai and did some work for few months but completely new to text to audio . yes i used text to audio tools a decade back ... but i would like to know where exactly we stand in terms of text to audio today . i already did some research , it seems like traditional way of text to audio is fading away and speech cloning seems to be emerging but my impression on this might be completely wrong . what are the current open source text - to - audio libraries ?",23176,23176,2019-03-14T12:17:04.083,2019-04-13T23:01:45.443,what are the current open source text - to - audio libraries ?,audio-processing,1,0,
3183,11223,1,,2019-03-14T08:38:13.693,0,64,"this is in continuation of my last question here . i am just starting to build an os which will be based on ai or rather synthetic super intelligence . i also wrote a paper on it for which i am working on the drafts to get it published ( read it here , still unfinished ) . i believe such an agent will work best when implemented as an os . i am researching for it and also tried to get a boot loader working ( here , here ) . the os i am developing is research based ( wherever ai can help with science , mathematics , engineering etc . ) . the ai will also look after processes , input / output and other os modules . i also have some ui designs . i need help and resources for it . please share links , experiences and your thoughts about such work . it will be great if i could get to read about building os ground up with kernels , driver development , process management as well as building ai applications like siri by apple or cortana by microsoft using deep learning and as such . i will keep the developments posted here .",23074,23074,2019-03-17T09:44:53.973,2019-03-17T09:44:53.973,building ai as an operating system,neural-networks ai-design research superintelligence,0,2,1
3184,11226,1,,2019-03-14T10:20:51.617,1,446,"what is non - euclidean data ? where does this type of data arises ? apparently , graphs and manifolds are non - euclidean data . why exactly is that the case ? what is the difference between non - euclidean and euclidean data ? how would a dataset of non - euclidean data look like ?",2444,2444,2019-03-14T10:27:17.293,2019-03-15T16:57:49.573,what is non - euclidean data ?,machine-learning deep-learning datasets graphs geometric-deep-learning,2,3,1
3185,11227,1,11240,2019-03-14T10:39:13.217,0,23,"i have features like state , city and location . currently i am inserting this in respective tables in db and tranform it using its primary key . eg . country , state , city = in , maharastra , pune = 2,5,10 ( primary key in db ) transforming it as 002005010 is this approach correct ? if not suggest me correct one",23174,,,2019-03-14T18:39:07.727,transform location data into int which will be used as input to ml model,machine-learning datasets,1,0,
3186,11229,1,,2019-03-14T10:55:33.147,3,56,"i could n't find gui for precise "" artificial neural - network alike "" structures , which could supports neuron naming , synapse naming , import of external functions or code fragments and debugging . it would be ideal if synapses also could pass not only float values , but user - defined structures too . optimization , gpu - computing is irrelevant ( and probably impossible with such features ) . is such thing exists ? i 'm thinking about writing one myself , for my needs . and most probably i will ... why do i need such features ? for testing concept of construction of kind - of - neural logic programming . i 'm sure there is also should exists programming languages with such paradigm , but i also do n't know how to find them , google would mostly give me info about common artificial neural networks .",23183,23183,2019-03-14T12:22:29.410,2019-03-14T12:22:29.410,is there any gui for per - neuron editing,neural-networks programming-languages,0,5,
3187,11230,1,,2019-03-14T11:59:25.710,0,17,"i 'm a researcher and my research involves using ml / dl network , and me with my advisor are evaluating the available fpga boards available that fit our budget . we came across these 2 boards , pynq and zynq . from what i read , the main difference is that pynq has a python interface to control the fpga part from the processor . how does that affect the ml / dl part ? does this make it easier ? does it provide more libraries support or so ? i 'm trying to evaluate which one is better in terms of ml / dl applications so we can buy it and start working .",5873,,,2019-03-14T11:59:25.710,what is the difference between pynq and zynq boards for ml / dl applications ?,machine-learning deep-learning,0,0,
3188,11231,1,,2019-03-14T12:52:07.787,1,27,there are several version of ddqn floating around . sutton gives one that is a simple symmetric random update of the two q functions . i think other papers ( silver paper for example ) use a kind of delayed and split update rule . is there anything systematic describing the properties of the bias corrections and their respective advantages ?,23001,2444,2019-03-14T14:59:00.233,2019-03-14T14:59:00.233,comparison and understanding of different version of ddqn ?,reinforcement-learning q-learning dqn deep-rl,0,1,
3189,11232,1,,2019-03-14T12:53:22.040,0,17,how to serve a deep q network using tensorflow serving . i have built a deep q network using multilayer perceptron . is it possible to serve it ?,23185,,,2019-03-14T12:53:22.040,how to serve a deep q network using tensorflow serving ?,deep-learning tensorflow q-learning deep-rl,0,6,
3190,11233,1,,2019-03-14T13:02:27.273,4,78,"one of the common conceptions in ai is the idea of game theory . we see that in the predominance of chess and other games in the literature as metrics of ai success . we see it in the names of machine learning concepts such as generative adversarial networks . we see it in academia in that a limited number of seats in teaching and administrative structure are available . we see it in perceptions of biological history . we see it in sports and finance and geopolitical endeavors . is our current perception skewed ? when mathematician john von neumann and economist oskar morgenstern wrote theory of games and economic behavior , it was 1943 . nazi forces were devastating europe and japanese forces had taken korea and much of china . the context of games was intended to be economic , but von neumann was also involved in the technological revolution needed to oppose these forces in an adversarial game called wwii . the idea of games changed from play to preservation of civilization . yet civilization , although it may rely on defensive action to counter what may or may not be destructive offense , it is more defined by collaboration for mutual benefit ( finnish win - win - tilanne ) . civilization is a game of preservation and the avoidance of loss , as john donne intoned in his famous poem when he wrote , "" if a clod be washed away by the sea , europe is the less , "" implying that the loss of one person is a loss for all , based on the golden rule ( jesus ) . it is this higher thinking of the objective of the game that preserves civilization , not adversarialism . lives families ideas it is often the unique , original ideas that forward civilization in the area of ideas . it is often the lives and families of those who , at the time the idea arises , are not in power that change the thought collective ( ludwik fleck ) . these are a few examples of those that disturbed cultural norms ( including scientific ones ) and led to a forward step for us all . socrates saul of tarsus hypatia of alexandria galileo isaac newton huang yuanyong ( 黃遠庸 ) mahatma gandhi martin luther king richard stallman reconciliation of game theory with concepts of civilization nobel laureate john forbes nash jr . contributed to the mathematical idea that in economic systems real world curvature in relations , the game is not a zero sum game . this is the basis for the question . as gaming continues to penetrate further into mobile device and internet usage and as ai games continue to gain degrees of angle in the pie chart of most national defense spending budgets , the definition of games that are not based on the wwii model of what a game must be gains importance . how can designers of educational , entertainment centered , economic , financial , and military games transition their thinking toward cherishing alternative thought and re - prioritizing concepts like detente , playing for fun , employee collaboration , and educational objectives that are not zero sum gpa ( grade point average ) games based on # 2 pencil multiple guess assessments ? mathematics leading to game design concepts first must come a mathematical idea of value , where the value between the individual and the body of all humans is tied . the mathematics may be theoretical ( and probably should be at first ) , but must also be applied such that no one can achieve a high score in a game without developing an increase in the velocity of achievement in class of those with low achievement velocities . this requires a curved ( non - linear ) system , much like occurs in nature . it is a more realistic game than where all the adversaries are killed and the champion is now alone in the game space such that , had the game application not ended , no real value was gained — only loss . incentivization in this context , we can return to the title question . how can a collaboration game be defined mathematically ? what kinds of value functions incentivize collaboration and frustrate the actions of those that try to trounce everyone else to get ahead ? how can we characterize the curvature of the required functions such that too many individual losses block the ultimate achievements of current leaders ? if the golden rule , mutual appreciation , and collaboration are truly the ways civilization progresses forward , how can scoring be designed to reflect this reality ? what qualities , mathematically , must the model have ? the objective is to define mathematically a very specific thing indicated in the title question . that some background is give is solely to encourage out of the box thinking in responding specifically and correctly to a narrow and well defined question .",4302,1671,2019-03-14T20:43:32.790,2019-04-14T04:04:50.923,how can a collaboration game be defined mathematically ?,math models game-theory value-function incentivization,3,2,1
3191,11234,1,,2019-03-14T13:03:06.537,2,16,"i have a log file of the format , index , date , timestamp , module , app , context , session , verbosity level , description the log file can be considered as a master log , which consists of individual logs from several modules constituting a distributed system . the individual log can be identified using the corresponding module+app+context tags . the verbosity level(info , warn , error , … ) and the descriptions(system generated + print statements added by developers ) contain further information on the log events necessary for debugging . i need to perform an unsupervised anomaly detection with the log file as input . the output should be the functionality and timestamp of the identified anomalies . since the log is mostly textual , i plan to use nlp algorithm ( bag of words / tf - idf ) to convert the data into word vectors and then perform a generative learning method to identify the normal pattern . can someone suggest if my approach in the right direction ? which headers of the log file would be relevant for the word - vector representation and further analysis ?",23161,,,2019-03-14T13:03:06.537,how to perform unsupervised anomaly detection from log file with mostly textual data ?,neural-networks machine-learning natural-language-processing word2vec anomaly-detection,0,0,
3192,11235,1,,2019-03-14T14:15:54.450,0,37,"i 'm looking for the bert model you can find here . as i look to the attention mechanism , i do n't understand why in the bert encoder we have an intermediate layer between the attention and neural network layers with a bigger output ( 4*h where h is the hidden size ) . perhaps it is the layer normalization but by looking at the code i 'm not certain .",23154,23154,2019-03-14T14:21:37.887,2019-03-14T14:21:37.887,bert intermediate layer utility,deep-learning natural-language-processing google attention,0,0,
3193,11236,1,,2019-03-14T14:43:58.590,0,29,"suppose we are using word2vec and have embeddings of individual words $ w_1 , \dots , w_{10}$ . let 's say we wanted to analyze $ 2 $ grams or $ 3 $ grams . question 1 why would adding all the possible or embeddings be "" worse "" then using 1d - convolutions ? question 2 also for each of the $ 2 $ -grams and $ 3 $ -grams , would you try to learn some large number of $ 2 \times 2 $ filters and $ 3 \times 3 $ filters so that you can convolve it with the word2vec embedding ? how do you learn these filters ?",23120,,,2019-04-13T22:00:47.723,"1d convolutions , word2vec , and $ n$-grams",natural-language-processing,1,1,
3194,11243,1,,2019-03-14T20:47:32.873,3,41,"what is a generalized mdp ? how is it different than a "" regular "" mdp ? how does it generalise the notion of an mdp ? why do we need a generalised mdp ? do generalised mdps have some practical usefulness or they are just theoretical tools ?",2444,,,2019-03-14T20:47:32.873,what is a generalized mdp ?,reinforcement-learning definitions markov-decision-process,0,1,
3195,11244,1,,2019-03-14T20:50:46.340,3,40,"in certain reinforcement learning ( rl ) proofs , the operators involved are assumed to be non - expansive . why is the ( and the ) , which is e.g. used in q - learning , a non - expansive operator ? what is a non - expansive operator ?",2444,,,2019-03-15T15:53:00.423,why is the max a non - expansive operator ?,reinforcement-learning q-learning proofs,1,3,
3196,11246,1,,2019-03-14T21:12:04.403,4,163,"apparently , we can solve an mdp ( that is , we can find the optimal policy for a given mdp ) using a linear programming formulation . what 's the basic idea behind this approach ? i think you should start by explaining the basic idea behind a linear programming formulation and which algorithms can be used to solve such constrained optimisation problems .",2444,,,2019-03-15T02:42:16.287,how can we use linear programming to solve an mdp ?,reinforcement-learning optimization markov-decision-process linear-programming,1,3,
3197,11256,1,,2019-03-15T14:11:31.047,0,19,"i have a corpus of around 2000 texts , that are relatively short ( + - 150 words ) . all of these texts are news articles about accidents that happened in the netherlands . i 'd like to extract the exact location where the accident happened from the texts . for this i 've tried using a named entity recognizer , and only using the location entities . this works but not as good as i want it to work . one example news article is : een automobilist is vanochtend gewond geraakt nadat hij met zijn voertuig op zijn kant terecht was gekomen . dat gebeurde op een rotonde op de jan van kuikweg in heemskerk .volgens een woordvoerder van de politie is de inzittende uit de auto bevrijd en met verwondingen naar het ziekenhuis gebracht.het slachtoffer zou voorafgaand aan het ongeluk een andere auto hebben geraakt , waarna zijn eigen voertuig omsloeg . volgens een getuige was de voorruit van de auto nog gedeeltelijk bevroren . mogelijk heeft dat een rol gespeeld bij de botsing . beide voertuigen worden geborgen . tot die tijd is de weg afgesloten , waardoor het verkeer vastloopt . here , the bold text is the location i 'd like to extract from the text ( it translates to : at a roundabout on the jan van kuikweg in heemskerk ) . a named entity recognizer will only find the word "" heemskerk "" , which is not ( entirely ) what i want . one other thing i 've tried using is searching for certain patterns in the words , using a part of speech tagger . for example : & lt;adp|adv&gt;+&lt;det&gt;+&lt;noun&gt;&lt;adp&gt;&lt;noun&gt ; the problem here is that the locations from the texts come in a lot of formats . the example above will also tag entirely other sentences with the same structure , but a very different meaning / context which is of course undesirable . are there any other models i could use to find the locations ?",23217,,,2019-03-15T14:11:31.047,"detecting an entity ( location ) in a text , using a corpus of smaller texts",natural-language-processing,0,0,
3198,11261,1,11263,2019-03-15T17:44:50.580,2,58,"two words can be similar if they co - occur "" a lot "" together . they can also be similar if they have similar vectors . this similarity can be captured using cosine similarity . let $ a$ be a $ n \times n$ matrix counting how often $ w_i$ occurs with $ w_k$ for $ i , k = 1 , \dots , n$ . since computing the cosine similarity between $ w_i$ and $ w_k$ might be expensive , we approximate $ a$ using truncated svd with $ k$ components as : $ $ a \approx w_k \sigma w^{t}_{k } = cd$$ where $ $ c = w_{k } \sigma \\ d = w^{t}_{k}$$ where are the cosine similarities between the words $ w_i$ and $ w_k$ captured ? in the $ c$ matrix or the $ d$ matrix ?",23220,2444,2019-03-15T18:06:55.537,2019-03-16T11:57:28.173,which matrix represents the similarity between words when using svd ?,machine-learning natural-language-processing math,1,1,
3199,11266,1,,2019-03-16T08:14:47.073,2,73,"i do n't know anything about neural networks , but i got the information that making a chatbot with a neural network is very good . is this really true ? what do i need to know in order to build a chat bot using neural networks ? i have understood a little about text preprocessing ( such as tokenization , stemming , stop word lists , named entity recognition ) , but when i want to try classification using the neural network i am confused .",22686,2444,2019-04-15T20:25:44.523,2019-05-15T21:02:25.460,how do i use neural networks to implement a chatbot ?,neural-networks natural-language-processing chat-bots,1,9,
3200,11272,1,,2019-03-16T19:20:13.503,0,17,"i intend to use an ai method for modeling to predict road crashes and safety sorrogate measures . the dependent variables are crash frequency and other measures that are nonnegative integers . the independent variables are characteristics of road such as lane width , shoulder width , radius of horizontal curves , etc . i searched through the review articles that investigated various ai methods for prediction . i found two articles that were about predicting the energy use of buildings . in those review articles it was concluded that the best prediction method is the hybrid of svm an swarm intelligence methods . does anybody know whether i can utilize this method for my research or not ? thanks .",23246,,,2019-03-16T21:58:54.173,the proper ai method for modeling to predict road crashes and safety surrogate measures,prediction swarm-intelligence svm,1,0,
3201,11277,1,,2019-03-16T22:24:45.287,1,80,"i was reading about the grounding problem after seeing it mentioned in another answer today . the article states that , in order to avoid the "" infinite regress "" of defining all words with other words , we must ground the meaning of some words in the "" sensorimotor . "" to be grounded , the symbol system would have to be augmented with nonsymbolic , sensorimotor capacities — the capacity to interact autonomously with that world of objects , events , actions , properties and states that its symbols are systematically interpretable ( by us ) as referring to . obviously , this made me think of reinforcement learning . but i 'm not exactly sure what counts as "" interaction . "" would this necessarily imply an mdp - like formulation with rewards , state transitions , etc ? or could some form of grounding be accomplished with supervised learning ? this seems like a pretty fundamental problem of ai . does anyone know of research being done on grounding words / symbols within an rl agent ?",22916,22916,2019-03-17T10:12:25.127,2019-03-17T17:43:38.947,is reinforcement learning the future of natural language processing ?,reinforcement-learning natural-language-processing,1,3,
3202,11278,1,,2019-03-16T22:30:25.790,1,18,"robotics is a technology which is formalized in academic papers . it contains of sub disciplines like vision , motion control and path planning . on the other hand , robotics is done by real people . for example a university professor can held a lecture about artificial intelligence , or a school can run a robotics competition . in a recent study “ haring , kerstin sophie : cultural differences in perception and attitude towards robots . "" it was researched if robotics is treated different by countries , for example japan vs. europe . but this kind of survey does n't answer the question what the perception of robotics is among different social classes , for example low income , middle class and higher income . what we can say for sure is , that as default no social class is motivated to research robotics in detail . otherwise the traffic here , in the forum would be higher . but a certain class of people is motivated to discuss the subject in detail , otherwise the amount of academic papers about the subject would be lower question 1 : which social class is motivated most to research artificial intelligence ? is this equal with phd students which are around 15 million in the world ? question 2 : can the collective ignorance of robotics explained with the current education system , which means , that apart from university professor nobody is educated on university level and has access to latest research papers ? question 3 : if this question is put on hold , can the moderator move it to academia.stackexchange and convince the admins there to not put the question on hold too ?",11571,22603,2019-03-17T08:38:59.533,2019-03-17T08:38:59.533,robotics and social perception,social,0,0,
3203,11279,1,,2019-03-17T01:21:18.437,0,29,"the last week i 've been looking for freelancers who are able to do this project for me but they were n't that experienced in it , so i would like to know whether my idea is complicated or is it their lake of experience . scenario : 1 . the facial recognition system will be installed on a vertical screen where a camera would be attached to it and it will be assigned on the entrance of the room . 2 . once a visitor come to the entrance , and looks at the screen , a text on the screen would say : "" welcome ! it seems like it 's your first visit ! please enter your name then a keyboard should popup "" 3 . the visitor would enter his first name into the keyboard , and it would be saved alongside his face in the database , and it would say thank you , { name}. 4 . if the same visitor visits again , the system should say : "" welcome back { name } , happy to see you again "" ! that 's it .",23252,,,2019-04-16T16:03:25.670,facial recognition + database + compare & identify - is it complicated ?,machine-learning ai-basics detecting-patterns facial-recognition,1,0,
3204,11280,1,11282,2019-03-17T08:56:00.903,0,28,"i 'm implementing ppo myself strictly follow the steps : sample transitions randomly shuffle the sampled transitions compute gradients and update networks using the sampled transitions drop transitions and repeat the above steps i observe a strange phenomenon that randomly shuffling transitions makes the algorithm perform significantly worse than keeping it as it is . this is very strange to me . to my best understanding , neural networks perform badly when the input data are correlated . to decorrelate transitions , algorithms like dqn introduce replay buffer and randomly sample from it . but this seems not the same story to policy - based methods . i 'm wondering why policy - based methods do not require to decorrelate the input data ?",8689,,,2019-03-17T10:10:38.300,why do n't we decorrelate transitions for policy - based data ?,reinforcement-learning proximal-policy-optimization,1,0,
3205,11283,1,,2019-03-17T10:01:31.383,0,2,"i need to use lightgbm for improve the auc score of my binary classification algorithm . i have used bayessearchcv with score:""roc - auc "" but i do nt know how can i make it able to search the best parameters based on auc test not auc train .",23254,,,2019-03-17T10:01:31.383,how to improve the auc - test of lightboost based on bayessearchcv,machine-learning data-science supervised-learning,0,0,
3206,11284,1,,2019-03-17T11:18:03.417,0,34,"floorball is a type of floor hockey . during the game , substitutions can be made . the team is also allowed to change players any time in the game ; usually , they change the whole team . individual substitution happens sometimes , but it usually happens when a player is exhausted or is hurt . i would like to use an rnn to predict when the next substitution will happen for a team . however , i have no pre - existing dataset to train on . is there a way that i can start predicting without a dataset and continually improve accuracy as more games are played ?",23258,22916,2019-03-18T16:45:56.873,2019-03-18T16:45:56.873,is there a rnn that can predict the next substitute in a floorball match ?,neural-networks training datasets,0,4,
3207,11285,1,,2019-03-17T11:56:36.493,5,125,"in general , the word "" latent "" means "" hidden "" and "" to embed "" means "" to incorporate "" . in machine learning , the expressions "" hidden ( or latent ) space "" and "" embedding space "" occur in several contexts . more specifically , an embedding can refer to a vector representation of a word . an embedding space can refer to a subspace of a bigger space , so we say that the subspace is "" embedded "" in the bigger space . the word "" latent "" comes up in contexts like hidden markov models ( hmms ) or auto - encoders . what is the difference between these spaces ? in some contexts , do these two expressions refer to the same concept ?",2444,2444,2019-03-17T12:06:50.413,2019-05-24T12:22:09.307,what is the difference between latent and embedding spaces ?,machine-learning terminology,2,0,
3208,11286,1,11291,2019-03-17T12:25:06.293,1,58,so i am simulating a tic tac toe game with a human opponent . the way the rl trains is through policy / value iterations for a fixed number of iterations all specified by the user . now whether the human player has turn 1 or turn 2 will decide the starting state ( 1st move by human or empty ) . now the starting states for the 1st case can differ as the human can make 9 different moves . so my questions are : in tic tac toe what is the effect of starting state in the state and action value function ? does it converge to the same stable value for all starting states ? will the value functions change if the starting players are changed ? ( human vs rl to rl vs human ) note : i will be enumerating all states since there are approximately 20000 states which i believe is not a big number and thus convergence should not be a problem .,9947,9947,2019-03-17T16:23:18.053,2019-03-17T17:39:29.453,importance of starting state and player in rl for tic tac toe,reinforcement-learning gaming,1,11,
3209,11290,1,,2019-03-17T15:47:30.717,3,32,"i am developing an algorithm that , in certain moment , must explore an exponential number of objects derived from a graph : for o in my_graph.getderivedobjects ( ) : if haspropertyx(o ) : process(o ) break ; if one of the derived objects has property $ x$ , then the algorithm process it and then stops . the theory ensures that at least one of these derived objects has property $ x$ . now , i strongly suspect that there is a strong correlation between some topological aspects of the graph , and which derived objects actually have property $ x$ . i want to predict some of the derived objects that have property $ x$ using machine learning . so , the idea is : 1 ) predict a derived object $ o$ that supposedly has property $ x$ - or maybe predict $ n$ of them for some number $ n$ . 2 ) if any of them is useful , i use them . if not , i run the exponential algorithm . of course , this is n't an optimization in the worst - case complexity of the algorithm . also , i suppose i should also develop some statistical tests in order to show that the prediction algorithm actually works . what i wanted to ask is if these type of optimizations are common . could you please provide some examples ? literature on the subject would also be greatly appreciated . thank you .",22365,22365,2019-03-17T15:53:44.160,2019-04-20T19:01:25.210,is it common to use machine learning to improve the average case complexity of an algorithm ? any exmples ?,algorithm time-complexity,1,1,1
3210,11293,1,11294,2019-03-17T19:05:22.787,0,60,i am trying to understand how weights are actually gotten . what is generating them in a neural network ? what is the algorithm that gives them certain values ?,23262,22916,2019-03-17T20:08:08.840,2019-03-17T21:44:44.843,what determines the values of weights in a neural network ?,neural-networks machine-learning neurons,2,0,
3211,11298,1,,2019-03-18T02:26:26.940,1,54,"i am attempting to train a network to do something i thought would be a relatively simple case to learn with : identify whether the back of a scanned vintage postcard has one of ' no postage stamp ' , a ' 1 cent stamp ' , or a ' 2 cent stamp . ' the images are 250px by about 150px , rgb color , and there are about two thousand of them . ballpark 75 % of them are no - stamp , 20 % 1-cent , and 10 % 2-cent . when i attempt to train the network it seems like it is starting at 70 + /- 1 % accurate and hovers in that range for 50 epochs , never improving . i 'm not sure i 'm reading the metrics correctly , though , as this does n't seem quite right . i set this up by following the tutorial on the keras blog : https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html i have n't implemented the latter part of the tutorial , where a pre - trained network is used , because i have n't found one that seems like it would be a similar problem . my training and validation sets are here : https://drive.google.com/open?id=1-txekvvvp7rufc7kfgh7wt5a8z8qgtr3 and my google colab jupyter notebook is here : https://colab.research.google.com/drive/1uukdf1wdwylxszb2ahirygrnfcs2d_sd",23264,,,2019-03-18T23:04:02.620,why does n't my image classification network get better with training ?,convolutional-neural-networks image-recognition classification keras,1,3,
3212,11299,1,,2019-03-18T03:50:45.247,1,11,"i learned how to build recurrent cnn model for text classification and sketched out my initial implementation . however , i am wondering how to transform recurrent cnn model for sentence classification . i am curious how can i come up better implementation of recurrent cnn model for sentence classification task . here is part of keras solution that i used : import gensim import numpy as np import string import gensim from gensim.models import word2vec from gensim.utils import simple_preprocess from gensim.models.keyedvectors import keyedvectors word2vec = keyedvectors.load_word2vec_format('googlenews-vectors-negative300.bin ' , limit= 500000,binary = true ) embeddings = np.zeros((word2vec.syn0.shape[0 ] + 1 , word2vec.syn0.shape[1 ] ) , dtype = "" float32 "" ) embeddings[:word2vec.syn0.shape[0 ] ] = word2vec.syn0 max_tokens = word2vec.syn0.shape[0 ] embedding_dim = word2vec.syn0.shape[1 ] hidden_dim_1 = 200 hidden_dim_2 = 100 num_classes = 10 problem i want to learn sentence classification task by using recurrent cnn ( rcnn ) model . the fact that some people used rcnn for object recognition problem . and it is not very intuitive for me how to transform same idea to list of short sentences . here is the code that i want to make them work for sentence classification task : document = input(shape = ( none , ) , dtype = "" int32 "" ) left_context = input(shape = ( none , ) , dtype = "" int32 "" ) right_context = input(shape = ( none , ) , dtype = "" int32 "" ) embedder = embedding(max_tokens + 1 , embedding_dim , weights = [ embeddings ] , trainable = false ) doc_embedding = embedder(document ) l_embedding = embedder(left_context ) r_embedding = embedder(right_context ) continuation of my code i am struggling to make above code in problem section for sentence classification problem . can anyone give me possible idea how to make it work for sentences classification ? if there is efficient transformation on above code , i 'd like to continue my pipeline as follow to build rcnn model for sentence classification . forward = lstm(hidden_dim_1 , return_sequences = true)(l_embedding ) backward = lstm(hidden_dim_1 , return_sequences = true , go_backwards = true)(r_embedding ) backward = lambda(lambda x : k.reverse(x , axes = 1))(backward ) together = concatenate([forward , doc_embedding , backward ] , axis = 2 ) semantic = conv1d(hidden_dim_2 , kernel_size = 1 , activation = "" tanh"")(together ) pool_rnn = lambda(lambda x : k.max(x , axis = 1 ) , output_shape = ( hidden_dim_2 , ) ) ( semantic ) model_output = dense(num_classes , input_dim = hidden_dim_2 , activation = "" softmax"")(pool_rnn ) model_rcnn = model(inputs = [ document , left_context , right_context ] , outputs = model_output ) maybe i need to tokenize all sentences and create array for right / left context for each sentences , but i did n't get solid idea on that . any more thoughts ? question how can i realistically create input matrix for sentences list , right / left context of each sentence ? any workaround to get this done ? any efficient sketch solution to use recurrent cnn model for sentence classification ? thanks in advance !",21595,21595,2019-03-21T22:16:39.377,2019-03-21T22:16:39.377,any workaround to manipulate / transform recurrent cnn for sentence classification ?,deep-learning convolutional-neural-networks natural-language-processing,0,0,
3213,11302,1,11305,2019-03-18T08:47:45.250,1,34,"for the case of crowd density estimation using cnn , using datasets like shanhaitech or ucf , why there has n't been attempts to tackle this type of task as a classification problem ? all current papers i 've seen are related to regression , not classification . for example , if i have the crowd images labeled based on their crowd density ( low , moderate , high ) , and i 'm not interested in the count , but the density class ( low , moderate , high ) , ca n't i train the network to classify the data based on these classes as a classification network ?",23268,2444,2019-03-18T16:39:17.823,2019-03-18T16:39:46.237,why do n't we perform classification of crowd density ?,convolutional-neural-networks classification,1,0,
3214,11309,1,,2019-03-18T16:08:15.043,1,26,"i 've got classification problem on image , i have 10 classes and when i fine tuned my model on it ( i tried vgg , xception , resnet etc ) i have approximatly 83 % validation accuracy . i was wondering if doing lot of binary model with 1 class represented and the other as ' other ' and then using them to classify my image would be good and efficient ? ( i obtain more than 90 % val acc for each class doing this ) except for memory consumption and training time does this method have drawback ?",23107,,,2019-03-18T16:08:15.043,is making lot of 1 versus other model efficient ?,deep-learning convolutional-neural-networks classification,0,1,
3215,11310,1,,2019-03-18T19:00:35.433,2,18,"consider a shop owner who has to deal with having to buy for one week from a different supplier with several different brands . another week a brand is removed or added from the market . yet another week , the manager decides to skip three assortments of fruit soda and exchange them with a different selection of three assortments of fruit sods , and so on and so forth . is dealing with such issues possible with existing public implementations of reinforcement learning ? the only research that i saw dealing with dynamically changing stuff like this is the deepmind ftw bot playing quake capture the flag . it deals with changing layouts to the map being played , but the implementation is not public and it does n't resemble the inventory management situation i outlined .",21724,2444,2019-03-18T20:23:59.450,2019-03-18T20:23:59.450,reinforcement learning for inventory management with dynamic changes to available products,machine-learning reinforcement-learning,0,0,
3216,11312,1,,2019-03-18T22:38:21.627,0,24,"i am working on intent classification task ( chatbot engine ) , 2k sentences , 24 classes . major class is composed of about 150 sentences , minor class of about 35 sentences , the others are more or less balanced ( 70 sentences each one ) . i used fast - text pre trained embedding and then feed into cnn using keras . i 've done a grid search cross validation to choose best parameter for the model , then i trained model with these parameters on train ( smote applied to balance classes ) and then evaluate on test ( hold - out ) . i got an accuracy and micro avg f - score of 0.91 what are other good metrics to evaluate performance of model ? confusion metrics ? roc curve ? classification report ? i tried also precision - recall obtained the results below in the image . how should be the values for each class to be considered good ?",20780,,,2019-03-18T22:38:21.627,metrics evaluation multiclass classification,convolutional-neural-networks classification,0,0,
3217,11313,1,11319,2019-03-18T23:16:00.460,5,80,"i have read a lot about rl recently . as far as i understood , most rl applications have much more states than there are actions to choose from . i am thinking about using rl for a problem where i have got a lot of actions to choose from , but only very few states . to give a handy example : the algo should render ( for whatever reason ) a sentence with three words . i always want to have a sentence with three words , but i have many words to choose from . after choosing the words , i get some sort of reward . are rl algorithms an efficient way to solve this ? i am thinking about using policy gradients with an ε - greedy algorithm to explore a lot of the possible actions before exploiting the knowledge gained .",23288,2444,2019-03-18T23:24:28.367,2019-03-19T08:59:47.867,reinforcement learning with more actions than states,reinforcement-learning policy-gradients greedy-ai,1,3,
3218,11316,1,,2019-03-19T07:23:56.957,0,18,"i want to cite the reference paper of the models appeared in this webpage : https://github.com/ruotianluo/imagecaptioning.pytorch/blob/master/models/ init .py but which papers do these four models ' all_img ' , ' att2in2 ' , ' adaattmo ' and ' topdown ' originate from ? and which of the above models are created by the author of the github page ? thanks !",21613,,,2019-03-19T07:23:56.957,"what are the reference papers of image captioning model ' all_img ' , ' att2in2 ' , ' adaattmo ' and ' topdown ' ?",machine-learning natural-language-processing computer-vision,0,2,
3219,11317,1,,2019-03-19T07:46:36.403,1,51,"this is a simple version of nim : two players alternately remove one , two or three coins from a stack initially containing 5 coins . the player who picks up the last coin loses . what does alpha - beta pruning look like on the game tree for this game ?",23298,1847,2019-03-19T09:52:48.520,2019-03-19T09:52:48.520,understanding alpha - beta pruning for simplified nim,game-ai minimax,0,3,
3220,11318,1,,2019-03-19T08:30:30.273,2,60,i am writing a field report on ai . i was wondering what the technological challenges are that ai is facing today . i have written the following so far . ai needs common sense like a human common ai needs curiosity ai needs to understand cause and effect ai needs to be creative is there any other hardware - tech related obstacles ?,23300,2444,2019-03-20T08:59:28.327,2019-03-20T08:59:28.327,what are the technological challenges that ai faces today ?,machine-learning artificial-consciousness hardware challenges,1,2,
3221,11321,1,,2019-03-19T09:46:54.090,0,28,"is there such a thing as facebook 's dr qa with multiple choice questions ? where the algorithm selects the most likely from 3 possible responses ? the idea is to solve the following problem automatically . here are 12 questions . followed by 12 * 3 answers . the first 3 correspond to the first question , the second three correspond to the second .. i have a dataset of 2500 multiple choice questions with their answers . the questions are in spanish but can be translated automatically . the questions ( common knowledge questions ) are unrelated and there is no text to find the answers in . the english translation powered by google . [ "" which of these options does not correspond to a traditional sweet ? "" , "" what autonomous community do the provinces of huesca , teruel and zaragoza form ? "" , "" what name does the object that is exchanged between the members of the same team in a relay race ? "" "" which of these options does not correspond to a megalithic monument ? "" , "" what do we mean when we talk about the "" 2000 effect "" ? "" , "" which of these options does the term "" alamar "" refer to ? "" , "" how many eyelids can we find in the eye of a camel ? "" , "" in which of these constellations is the star group of the seven sisters ? "" "" how many cents does it cost to make a euro ? "" "" which of these filmmakers directed the film "" once upon a time in america "" ? "" , "" which of these singers lent his image for a video game ? "" , "" until what year was it prohibited in spain to translate or have a copy of the bible in spanish ? "" ] , [ "" cachopo "" , "" torrija "" , "" mona de pascua "" , "" aragon "" , "" catalonia "" , "" the rioja "" , "" dorsal "" , "" taco "" , "" witness "" , "" baldaquino "" , "" dolmen "" , "" menhir "" , "" end of the mayan calendar "" , "" informatical issue "" , "" increase in the hole in the ozone layer "" , "" a kind of cuttlefish "" , "" an arab bath "" , "" a type of eyelet "" , "" one "" , "" two "" , "" 3 "" , "" aquarius "" , "" taurus "" , "" sagittarius "" , "" 3 "" , "" 4,5 "" , "" 5.2 "" , "" francis ford coppola "" , "" martin scorsese "" , "" sergio leone "" , "" david bowie "" , "" freddie mercury "" , "" mick jagger "" , "" 1783 "" , "" 1812 "" , "" 1823 "" ] , here is the set of all questions and answers",23302,23302,2019-03-19T12:24:34.610,2019-03-19T12:24:34.610,facebook 's dr qa with multiple choice questions ?,natural-language-processing watson,0,1,
3222,11322,1,,2019-03-19T11:51:12.200,0,49,"i have the following part of code - ... model.add(generic_act_func ( ) ) print('layer 1 output ' ) print(model.layers[1].output ) # here layers[1 ] is generic_act_func ( ) layer specified above model.add(lambda(lambda x : tf.spectral.irfft2d(x ) ) ) ... which when run produces the following output and error respectively - output - layer 1 output tensor(""generic_act_func_1 / complex:0 "" , shape= ( ? , 28 , 28 , 9 ) , dtype = complex64 ) error - valueerror : tensor conversion requested dtype complex64 for tensor with dtype float32 : ' tensor(""lambda_1 / placeholder:0 "" , shape= ( ? , 28 , 28 , 9 ) , dtype = float32 ) ' at line model.add(lambda(lambda x : tf.spectral.irfft2d(x ) ) ) tf.spectral.irfft2d ( ) requires a complex64 input which the previous layer i.e. , generic_act_func ( ) layer is already producing ( as printed by layer 1 output ) , hence i am not able to figure out why this error . thanks in advance for any help for solving the error . edit - could there be a mistake wrt passing previous layer outputs to the lambda layer ? like i have used a variable ' x ' in the lambda function assuming the previous layer output will be captured in this variable . is this the correct way of adding the lambda layer ? ( i am a beginner in keras , so will be really grateful for any help )",23306,23306,2019-03-20T06:36:42.970,2019-03-20T06:36:42.970,what is the meaning of valueerror in keras ? - ' tensor conversion requested dtype complex64 for tensor with dtype float32 ',computer-vision keras,0,9,
3223,11323,1,,2019-03-19T12:28:04.870,0,16,"i ’m in this problem and have n’t found a sound solution to it . been like 20 days now . i have a dataset that looks like this : x = image y1= current_zoom ( 0,25,50,75 ) y2= predicted_zoom ( 0,25,50,75 ) y1 will have equal images for all classes . also , i will know x and y1 when i test the model . y2 will have variation with it because it has the predicted zoom level . i tried to train on mtl model , i used two outputs from the model - y1 and y2 . now , y2 overfits ( still not sure why , but my best guess is class imbalance ) . and y1 accuracy is around 0.99 in validation . now the thing is when i deploy this on production , i ’ll always have the current zoom ( y1 ) with the image . so , i wanted to incorporate this to my model . first , i tried with two inputs and one output model , but loss was too much . here , i concatenated the y1 to the output of last conv layer before it flattens and goes to dense . and second i tried was to concatenate y1 after flattening the output from last conv . both did n’t work . any ideas on how can i with data like this . models used : resnet-18 , vgg , alexnet size of data : approx 7000 images in total .",15633,,,2019-03-19T12:28:04.870,how to work with multi - labels or two inputs and a output,deep-learning convolutional-neural-networks computer-vision,0,2,
3224,11324,1,,2019-03-19T15:50:12.097,0,12,"let 's say i have a number of videos , and i want to train an ssd / yolo ( or frcnn ) to detect objects . in the case of a large amount of videos , there will be a lot of frames extracted and transferred to images . can you take , for example , only the fifth frame everytime and thus lower the amount of memory required ? if the frames contain a similar info , we can skip some and not impact the results ? this is mainly to train faster ..",23311,,,2019-03-19T15:50:12.097,consecutive frames can be discarded when training an ssd / yolo ?,deep-learning training datasets object-recognition,0,2,
3225,11325,1,,2019-03-19T15:59:36.103,2,21,"what if there are multiple goals ? for example , let 's consider bit - flipping environment as described in the paper her with one small change : now , goal is not some specific configuration , but let 's say for last m bits ( e.g m=2 ) i do not really care if there is 1 or 0 . in the paper , there is section 3.2 multi - goal rl , where they mention example with two - dimensional coordinates ( x and y ) , but they are interested only in x coordinate , so they use only x coordinate as a goal . applying this strategy to my example would result in cutting last m bits from goal and only use the other bits , is this logic correct ? another approach i could think of would be to train with all possible goals configurations , as there are not many in my case . but this seems impractical as the number of goals configurations grows .",22162,,,2019-03-19T15:59:36.103,hindsight experience replay with multiple goals,deep-learning reinforcement-learning q-learning,0,0,
3226,11326,1,11358,2019-03-19T17:03:32.260,1,42,"i have a 2d plane , with a fixed height and width of 10 m . the plane has a robot in the point ( 1,2.2 ) , and an electric outlet in the point ( 8.2 , 9.1 ) . the plane has a series of obstacles like polygons and implemented with a lot of points in the edges . but not have routes between two points , this is not like a map exactly . is there an algorithm to find the shortest path between the robot and the electricoutlet ? and if the point has a fixed wingspan ? for example , that the space between o and n is smaller than the robot , and then the robot can not cross ?",22082,2444,2019-03-19T17:27:05.780,2019-03-21T17:37:10.530,how can i calculate the shortest route between two 2d vector points with obstacles ?,algorithm evolutionary-algorithms robotics pathfinding,1,6,
3227,11327,1,,2019-03-19T19:09:31.427,1,26,"nowadays there are too much data for humans to work on alone and it is very normal for data analysts to use a.i techniques to treat and process these data so it can lead to a faster and more accurate result . but many data analysts and decision makers still do n't trust a.i methods or techniques and are reluctant to use them . how can we encourage them to accept or prefer these a.i solutions ? for example : if a.i . gives advice to solve a problem , then decision - makers must trust the results and data analysts must trust the mechanism . so that decision - makers can be confident in the a.i work and also data analysts can concentrate their activities on added value . my question is which nudges to or how can we encourage data analysts & amp ; decision - makers to accept / prefer a.i . work ? thank you in advance !",23315,,,2019-03-19T20:07:02.870,how can we encourage data analysts & decision - makers to accept / prefer a.i . work,ai-basics getting-started ai-community soft-question,2,0,
3228,11328,1,,2019-03-19T19:25:00.353,1,43,"suppose we want to predict context words $ w_{i - h } , \dots , w_{i+h}$ given a target word $ w_i$ for a window size $ h$ around the target word $ w_i$ . we can represent this as : $ $ p(w_{i - h } , \dots , w_{i+h}|w_i ) = \prod_{-h \leq k \leq h , \ k \neq 0 } p(w_{i+k}|w_i)$$ where we model the probabilities of a word $ u$ given another word $ v$ as $ $ p(u|v ) = \frac{\exp(\left&lt;\phi_u , \theta_v \right&gt;)}{\sum_{u ' \in w } \exp(\left&lt;\phi_{u ' } , \theta_v \right&gt;)}$$ where are some vector representations for words $ u$ and $ v$ respectively and is the dot product between these vector representations ( which represents some sort of similarity between the words ) and $ w$ is a matrix of all the words . in skip - gram negative sampling , we want to learn the embeddings that maximize the following : $ $ question . how exactly does this work ? for example , suppose $ k=5 $ , the target word $ w_i$ is and we want to find $ p(\text{pie}| \text{apple})$ . let $ n_{uv } = 10 $ ( number of times pie co - occurs with apple ) . then we sample $ 5 $ random words that did not occur with and whichever term in the sum is bigger is the one we predict ? for example , if the first term in the sum is larger than the second term then we would predict that $ p(\text{pie}| \text{apple } ) \approx 1 $ ? otherwise we predict that $ p(\text{pie}| \text{apple } ) \approx 0 $ ? is this the correct intuition ? source . here at around the 10:05 mark .",23220,2444,2019-04-16T22:23:58.053,2019-04-16T22:23:58.053,skip - gram model training,natural-language-processing word2vec word-embedding,1,0,
3229,11332,1,,2019-03-19T20:47:49.283,0,53,"i am training a model using the dnn regressor estimator from the tensorflow api to predict prices based on 1035 features . my dataset contains a little over 500 millions inputs and none of the target output is negative . all prices are positive . after training for 3 epochs , my nn is still making negative predictions . is this normal ? should n't the network learn that prices are always positive",23318,,,2019-03-19T20:47:49.283,neural network regression predicting negative values,neural-networks tensorflow,0,3,
3230,11333,1,11441,2019-03-19T21:18:47.090,2,30,"agent can have reasoning skills ( prediction , taking calculated guesses , etc . ) and those skills can help reinforcement learning of this agent . of course , reinforcement learning itself can help to develop reasoning skills . are there research that explores this impact of reasoning and consciousness on the effectivenes of reinforcement learning . or maybe people just sit and wait such skills to emerge during reinforcement learning ?",8332,,,2019-03-25T13:18:21.157,how agent 's reasoning skills can improve its reinforcement learning ?,reinforcement-learning artificial-consciousness reasoning,1,1,
3231,11335,1,,2019-03-20T01:18:15.157,1,14,"i would like to use a 3d convolutional network on a 2000x2000x2000 volume for segmentation . i know i can break the volume into chunks that can fit in vram , but i was wondering if there was a way to analyze the entire 3d volume at once .",23323,,,2019-03-20T01:18:15.157,very large 3d input size,convolutional-neural-networks,0,1,
3232,11336,1,,2019-03-20T02:19:57.160,0,31,if we have classified 1000 people 's faces ; how do we ensure the network tells us when it encounters a new person ?,23324,,,2019-04-19T07:04:15.790,how do we classify an unrecognised face in face recognition ?,image-recognition,1,2,
3233,11337,1,,2019-03-20T04:51:46.497,1,44,"i am writing my first lstm network and i would really appreciate if someone can tell me if it is right ( the loss seems to go down very slowly and before playing around with hyper parameters i want to make sure that the code is actually doing what i want ) . the code is meant to go through some time series and label each point according to some categories . in the version i am putting here there are just 2 categories : 0 , if the value of the point is 1 and 1 otherwise ( i know it ’s a bit weird , but i did n’t choose the labels ) . so this is the code : import torch import torch.nn as nn import torch.nn.functional as f from torch.autograd import variable import torch.optim as optim import numpy as np from fastai.learner import * torch.manual_seed(1 ) torch.cuda.set_device(0 ) bs = 2 x_trn = torch.tensor([[1.0000 , 1.0000 ] , [ 1.0000 , 0.9870 ] , [ 0.9962 , 0.9848 ] , [ 1.0000 , 1.0000]]).cuda ( ) y_trn = torch.tensor([[0 , 0 ] , [ 0 , 1 ] , [ 1 , 1 ] , [ 0 , 0]]).cuda ( ) n_hidden = 5 n_classes = 2 class tess_lstm(nn.module ) : def _ _ init__(self , nl ) : super().__init _ _ ( ) self.nl = nl self.rnn = nn.lstm(1 , n_hidden , nl ) self.l_out = nn.linear(n_hidden , n_classes ) self.init_hidden(bs ) def forward(self , input ) : outp , h = self.rnn(input.view(len(input ) , bs , -1 ) , self.h ) return f.log_softmax(self.l_out(outp),dim=1 ) def init_hidden(self , bs ) : self.h = ( v(torch.zeros(self.nl , bs , n_hidden ) ) , v(torch.zeros(self.nl , bs , n_hidden ) ) ) model = tess_lstm(1).cuda ( ) loss_function = nn.nllloss ( ) optimizer = optim.adam(model.parameters ( ) , lr=0.01 ) for epoch in range(10000 ) : model.zero_grad ( ) tag_scores = model(x_trn ) loss = loss_function(tag_scores.reshape(4*bs , n_classes ) , y_trn.reshape(4*bs ) ) loss.backward ( ) optimizer.step ( ) if epoch%1000==0 : print(""loss at epoch % d = "" % epoch , loss ) print(model(x_trn ) , y_trn ) the ( super reduced in size ) time series should be [ 1,1 , 0.9962,1 ] , with labels [ 0,0,1,0 ] and [ 1 , 0.9870 , 0.9848,1 ] with labels [ 0,1,1,0 ] and the batch size should be 2 . i really hope i did n’t mess up the dimensionalities , but i tried to make it in a shape accepted by the lstm . this is the output : loss at epoch 0 = tensor(1.3929 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 1000 = tensor(0.8939 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 2000 = tensor(0.8664 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 3000 = tensor(0.8390 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 4000 = tensor(0.8339 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 5000 = tensor(0.8288 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 6000 = tensor(0.8246 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 7000 = tensor(0.8202 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 8000 = tensor(0.8143 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) loss at epoch 9000 = tensor(0.8108 , device='cuda:0 ' , grad_fn=&amp;lt;nlllossbackward&amp;gt ; ) ( tensor([[[-9.0142e-01 , -1.2631e+01 ] , [ -9.3762e-01 , -9.6707e+00 ] ] , [ [ -1.3467e+00 , -3.9542e+00 ] , [ -2.2005e+00 , -7.6977e-01 ] ] , [ [ -2.4500e+01 , -1.9363e-02 ] , [ -2.3349e+01 , -6.2210e-01 ] ] , [ [ -1.0969e+00 , -2.1953e+01 ] , [ -6.9776e-01 , -1.8608e+01 ] ] ] , device='cuda:0 ' , grad_fn=&lt;logsoftmaxbackward&gt ; ) , tensor([[0 , 0 ] , [ 0 , 1 ] , [ 1 , 1 ] , [ 0 , 0 ] ] , device='cuda:0 ' ) ) the loss does n’t go down too fast ( i expected it to overfit and go really close to zero ) . the actual values are okish ( the smaller one is always the right one ) , but they can be definitely improved . can someone tell me if my code is doing what i want ( and maybe suggest why is the loss still big - maybe i need a smaller lr ? )",22839,,,2019-03-21T15:50:24.007,lstm is not converging,lstm convergence pytorch,1,2,
3234,11340,1,,2019-03-20T09:46:16.050,4,35,"this is my first project using machine learning so i 'm looking for some guidance . i am extending a model - based testing ( mbt ) system to a learning - based testing system by integrating a machine learning algorithm , for automation purposes . the mbt system executes tests on some system under test ( sut ) and generates test verdicts . the ml algorithm i want to integrate , is to take test verdicts from the mbt system as input and gain understanding of the sut 's behaviour , in order to generate new test cases . the test verdict ( input ) is a text - file , containing information about the previous test and whether it passed or failed . the output is a new test case , also a text file , containing variable values . i was thinking that supervised learning would be suitable since the input file contains both variable values ( features ) and the test verdict ( class ) . however , i have doubts since i am not looking to solve a classification problem . i would appreciate ideas of what type of algorithm i should use ( supervised / unsupervised / reinforcement etc . ) , and where i could find such ( open - source ) algorithms . thanks , adam",23331,23331,2019-03-20T10:08:43.060,2019-03-20T10:08:43.060,choosing machine learning algorithm : learning - based testing,machine-learning model-based open-source,0,5,1
3235,11342,1,11343,2019-03-20T10:59:56.310,1,40,"i am fine - tuning a vgg16 model on 20 classes with 500k images i was wondering how do you chose the size of the dense layer ( the one before the prediction layer which has a size 20 ) . i would prefer not to do a grid search seeing how long it take to train my model . also how many dense layer should i put after my global average pooling ? base_model = keras.applications.vgg16(weights='imagenet ' , include_top = false ) x = base_model.output x = globalaveragepooling2d()(x ) x = dense ( ? ? ? , activation='relu')(x ) x = dropout(0.5 , name='drop_fc1')(x ) prediction_layer = dense(class_number , activation='softmax')(x ) i have n't see particular rules about how its done , are there any ? is it link with the size of the convolution layer ?",23107,23107,2019-03-20T11:07:30.250,2019-03-20T11:20:55.723,how to chose dense layer size ?,deep-learning convolutional-neural-networks hidden-layers,1,0,
3236,11345,1,,2019-03-20T13:47:59.457,1,38,"so i have been playing around with neat - python . i made a program , applying neat , to play pinball on the atari 2600 . the code for that can be found in the file test2.py here now based on that , i would like to do the same , but on a 2 player game . i have already set up the environment to play a 2 player game , which pong using openai retro . what i have no clue how to do , is run 2 nets at the same time , on the same observation . the way that neat - python works , is you get the observation from a single function that goes through each genome and runs the environment . how would you create 2 eval_genome functions that can take in the same observation real - time ? this means that they train based off of the same images and environmenrs . help ?",23119,,,2019-03-20T13:47:59.457,running 2 neat nets on the same observations,deep-learning reinforcement-learning python neat open-ai,0,3,
3237,11347,1,,2019-03-20T16:55:39.990,2,71,"there are several ( family of ) algorithms that can be used to cluster a set of $ d$ -dimensional points : for example , k - means , k - medoids , hierarchical clustering ( agglomerative or divisive ) . what is graph - based clustering ? are we clustering the nodes or edges of a graph instead of a set of $ d$ -dimensional ( as e.g. in k - means ) ? could n't we just use k - means to also cluster a set of nodes ?",2444,,,2019-03-21T10:25:47.193,what is graph clustering ?,unsupervised-learning clustering k-means k-medoids hierarchical-clustering,1,5,
3238,11349,1,,2019-03-20T18:31:41.823,0,18,"i create a model based on ela [ error level analysis ] for image forgery detection i use the following code : def convert_to_ela_image(path , quality ) : filename = path resaved_filename = filename.split('.')[0 ] + ' .resaved.jpg ' i m = image.open(filename).convert('rgb ' ) im.save(resaved_filename , ' jpeg ' , quality = quality ) resaved_im = image.open(resaved_filename ) ela_im = imagechops.difference(im , resaved_im ) extrema = ela_im.getextrema ( ) max_diff = max([ex[1 ] for ex in extrema ] ) if max_diff = = 0 : max_diff = 1 scale = 255.0 / max_diff ela_im = imageenhance.brightness(ela_im).enhance(scale ) return ela_im dataset = pd.read_csv('micc2000.csv ' ) x = [ ] y = [ ] for index , row in dataset.iterrows ( ) : x.append(array(convert_to_ela_image(row[0 ] , 90).resize((128 , 128))).flatten ( ) / 255.0 ) y.append(row[1 ] ) x = np.array(x ) y = to_categorical(y , 2 ) x = x.reshape(-1 , 128 , 128 , 3 ) x_train , x_val , y_train , y_val = train_test_split(x , y , test_size = 0.50 , random_state=5 , shuffle = true ) model = sequential ( ) model.add(conv2d(filters = 32 , kernel_size = ( 5,5),padding = ' valid ' , activation = ' relu ' , input_shape = ( 128,128,3 ) ) ) model.add(conv2d(filters = 64 , kernel_size = ( 5,5 ) , strides=(2,2 ) , padding = ' valid ' , activation = ' relu ' ) ) model.add(conv2d(filters = 128 , kernel_size = ( 5,5),padding = ' valid ' , activation = ' relu ' ) ) model.add(conv2d(filters = 256 , kernel_size = ( 5,5),strides=(2,2),padding = ' valid ' , activation = ' relu ' ) ) model.add(dropout(0.25 ) ) model.add(flatten ( ) ) model.add(dense(256 , activation = "" relu "" ) ) model.add(dropout(0.5 ) ) model.add(dense(2 , activation = "" softmax "" ) ) model.summary ( ) how i can change in this code so i can add spp layer on it ?",23343,,,2019-03-20T18:31:41.823,how to add spp(spatial pyramid pool ) layer to cnn network ?,machine-learning convolutional-neural-networks,0,0,
3239,11350,1,11352,2019-03-20T21:53:38.110,2,51,"i have read a lot about rl algorithms , that update the action - value function at each step with the currently gained reward . the requirement here is , that the reward is obtained after each step . i have a case , where i have three steps , that have to be passed in a specific order . at each step the agent has to make a choice between a range of actions . the actions are specific for each step . to give an example for my problem : i want the algorithm to render a sentence of three words . for the first word the agent may choose a word out of [ ' i ' , ' trees ' ] , the second word might be [ ' am ' , ' are ' ] and the last word could be chosen from [ ' nice ' , ' high ' ] . after the agent has made its choices , the reward is obtained once for the whole sentence . does anyone know which algorithms to use in this kind of problem ? to give a bit more detail on what i already tried : i thought that using value iteration would be an reasonable approach to test . my problem here is that i do n't know how to assign the discounted reward for the chosen operations . for example after the last choice i get a reward of 0.9 . but how do i update the value for the first action ( choosing out of i and trees in my example ) ?",23288,1847,2019-03-21T07:36:47.693,2019-03-21T08:11:38.287,reinforcement learning with long term rewards and fixed states and actions,reinforcement-learning rewards,2,0,
3240,11353,1,,2019-03-21T08:15:52.560,1,47,"i 'm learning a bit about the use of the surprise library and i have a set of data with users and ratings . i 'm training a network with this library , using knnbasic and knnwithmeans , this last algorithm is the same as knn but averages the ratings before calculating the distances between the points . if i do n't use any measure of similarity , i.e. using the two algorithms with the default parameters , knnbasic predicts the results better than using knnwithmeans . but if i train the nets using subsets , 10 folds , where the algorithm iterates over 9 oh them for training and the other one for validating , knnwithmeans gives better results . do you know why this can happen ? why knnbasic is better in the first case , and increasing the number of folds is better knnwithmeans ?",23347,23347,2019-03-22T07:39:38.047,2019-03-22T07:39:38.047,knnbasic vs knnwithmeans,algorithm ai-basics training,0,0,1
3241,11356,1,,2019-03-21T16:53:19.580,0,15,"i have trained a recurrent neural network based on 1 stack of lstm cells . i use it to solve a classification problem . the rnn cell has 48 hidden states . the output of the last unfolded lstm cell is linearly projected into two dimensions corresponding to two mutually exclusive classes . i train with softmax cross - entropy loss . i also know that both my train and test sets are mislabeled ( ! ) to a certain extent . possibly about 10 % of items labelled as class 1 are actually class 0 . and the same hold in other direction . what puzzles me is this . every time i train the network from scratch and plot a precision recall curve for the validation set in the end . and every time it is different ! especially in the very beginning in the range that corresponds to high precision levels . why is this happening ( this instability ) ? i tried various numbers of epochs , training rates , number of hidden states in lstm . every time it is the same , but for some combinations of these parameters the variability is less ( e.g. 8 out of 10 train / test runs i see more or less the same precision recall curve and 2 times it is severely different and worse ) .",11417,,,2019-03-21T16:53:19.580,why validation performance is unstable for my lstm based model ( labelling problems ) ?,classification training recurrent-neural-networks lstm,0,0,
3242,11360,1,,2019-03-21T18:23:47.867,1,30,"suppose that we have unlabeled data . that is , all we have are a collection of emails and want to determine whether any of them is spam or not . let 's say we have $ 1,000 $ rules to determine whether a particular email is spam or not . for example , one rule could be that a sender 's email address should not contain the text no_reply . if an email does contain this text , then it would be classified as spam . question . what are the advantages / disadvantages of a rules - based approach for detecting spam vs. a non - rules - based approach / unsupervised methods for detecting spam ? would there even be a point in constructing a non - rules based model given that we already have a rules based model ? could we use the rules - based model to create some labeled training data and then apply supervised techniques ?",23362,,,2019-04-21T09:03:45.743,are there any advantages of using rules - based approaches versus models for detecting spam ?,models rule-acquisition meta-rules,1,0,
3243,11361,1,11362,2019-03-21T21:38:12.243,2,66,"i want to solve a problem using reinforcement learning on a 20x20 board . an agent ( a mouse ) has to get the highest possible rewards as fast as possible by collecting cheese , which there are 10 in total . the agent has a fixed amount of time for solving this problem , namely 500 steps per game . the problem is , the cheeses will be randomly assigned to the fields on the board , the agent knows however , where the cheeses is located . is there any way how this could be solved using only reinforcement learning ( and not training for an absurd amount of time ) ? or is the only way to solve this problem to use algorithms like the a*-algorithm ? i 've tried many different ( deep)-q - learning models , but it always failed miserably . edit : i could not get any meaningful behavior after 6 hours of learning while using a gtx 950 m . maybe my implementation was off , but i do n't think so .",23005,23005,2019-03-21T22:19:07.793,2019-03-22T08:10:57.177,"can reinforcement learning solve problems , where certain elements in the environement are randomly located ?",deep-learning reinforcement-learning rewards,2,2,
3244,11363,1,11625,2019-03-21T22:24:49.743,2,31,"i know the difference between content - based and collaborative filtering approach in recommender systems . i also know some of the articles said collaborative filtering have some advantages than content - based , some of them also suggest to use both method ( hybrid ) to make a better system recommendation . is there a specific case where the use of one method ( content - based , specifically ) is better than another ? because if there is no case at all , why both methods are considered to be on the same "" level "" , why not focus on just one method ? for example , focus on collaborative filtering or hybrid method ( as an extension for collaborative filtering ) .",16565,2444,2019-04-03T08:30:26.390,2019-04-03T15:16:10.700,when is content - based more appropriate than collaborative filtering ?,research definitions difference recommender-system,1,0,
3245,11366,1,,2019-03-22T01:38:14.430,0,22,"i 'm a rails developer with a lot of web experience , but none ( still ) in ai . i 'm working in a web text editor that judges use to writing their sentences . the goal is to start to use ai to help the judge rule the case , either based on his own previous rulings , either based on his colleagues rulings . the judge would provide the text for the plaintiffs and defendants petitions , and based on these two inputs the system would recommend previous rulings that apply for the case . i already have a considerable dataset of judges rulings inside the database , and they can be easily attached to the plaintiffs and defendants petitions for training ( so this plaintiff petition + this defendant petition = this ruling ) . this is specially challenging because the complaints can contain different subjects combined into the same petition ; but the fact is that many offices use the same standardized petitions , as the defendants do as well , so i think the system can have a great chance of prediction success . what algorithms or strategies should i start studying to tackle this problem ? any similar articles , white papers or repositories that could help in my goal ?",23370,23370,2019-03-22T02:29:19.187,2019-03-22T08:09:34.633,algorithms and strategies to help judges rule cases,natural-language-processing text-summarization,1,2,
3246,11373,1,11379,2019-03-22T09:07:26.303,2,21,can deep learning be applied to computational fluid dynamics ( cfd ) to develop turbulence models that are less computationally expensive compared to traditional cfd modeling ?,23276,,,2019-03-22T14:42:35.523,can deep learning be applied to computational fluid dynamics,neural-networks deep-learning,1,0,
3247,11374,1,,2019-03-22T10:45:48.367,4,78,"i 've started working on anomaly detection in python . my dataset is a time series one . the data is being collected by some sensors which record and collect data on semiconductor making machines . my dataset looks like this : contextid time_ms ar_flow_sccm backsgas_flow_sccm 7289973 09:12:48.502 49.56054688 1.953125 7289973 09:12:48.603 49.56054688 2.05078125 7289973 09:12:48.934 99.85351563 2.05078125 7289973 09:12:49.924 351.3183594 2.05078125 7289973 09:12:50.924 382.8125 1.953125 7289973 09:12:51.924 382.8125 1.7578125 7289973 09:12:52.934 382.8125 1.7578125 7289999 09:15:36.434 50.04882813 1.7578125 7289999 09:15:36.654 50.04882813 1.7578125 7289999 09:15:36.820 50.04882813 1.66015625 7289999 09:15:37.904 333.2519531 1.85546875 7289999 09:15:38.924 377.1972656 1.953125 7289999 09:15:39.994 377.1972656 1.7578125 7289999 09:15:41.94 388.671875 1.85546875 7289999 09:15:42.136 388.671875 1.85546875 7290025 09:18:00.429 381.5917969 1.85546875 7290025 09:18:01.448 381.5917969 1.85546875 7290025 09:18:02.488 381.5917969 1.953125 7290025 09:18:03.549 381.5917969 14.453125 7290025 09:18:04.589 381.5917969 46.77734375 what i have to do is to apply some unsupervised learning technique on each and every parameter column individually and find any anomalies that might exist in there . the contextid is more like a product number . i would like to know which unsupervised learning techniques can be used for this kind of task at hand since the problem is a bit unique : it has temporal values since it has temporal values , each product will have many ( similar or different ) values as it can be seen in the dataset above . thanks",23380,,,2019-03-23T11:36:14.220,unsupervised learning for anomaly detection,neural-networks machine-learning deep-learning unsupervised-learning anomaly-detection,1,0,
3248,11375,1,11377,2019-03-22T10:57:57.820,4,74,"coming from a process ( optimal ) control background , i have begun studying the field of deep reinforcement learning . sutton & amp ; barto ( 2015 ) state that particularly important ( to the writing of the text ) have been the contributions establishing and developing the relationships to the theory of optimal control and dynamic programming with emphasis on the elements of reinforcement learning - that is , policy , agent , environment , etc . , what are the key differences between ( deep ) rl and optimal control theory ? in optimal control we have , controllers , sensors , actuators , plants , etc , as elements . are these different names for similar elements in deep rl ? for example , would an optimal control plant be called an environment in deep rl ?",23276,2444,2019-04-21T13:11:49.610,2019-04-21T13:11:49.610,what is the difference between reinforcement learning and optimal control ?,reinforcement-learning difference rl-an-introduction control-theory,2,3,1
3249,11380,1,,2019-03-22T14:44:10.680,1,12,"geoffrey hinton 's coursera mooc was recently discontinued : https://twitter.com/geoffreyhinton/status/1085325734044991489?lang=en the videos however are still available at both on youtube and on hinton 's webpage : https://www.cs.toronto.edu/~hinton/coursera_lectures.html however in his mooc hinton also had some papers as required ( or recommended , i ca n't really remember ) readings after each lecture . they were generally old , seminal papers which provided some good insights and intuitions and were worth reading imho for learning purposes . i could n't find these papers as a reading list online . does anyone have this list ?",23386,,,2019-03-22T14:44:10.680,hinton 's reading list from the removed coursera mooc,deep-learning,0,0,
3250,11381,1,,2019-03-22T15:11:47.503,3,46,"judea pearl won the 2011 turing award for fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning . he is credited with the invention of bayesian networks and a framework for causal inference . why should we study causation and causal inference in artificial intelligence ? how would causation integrate into other topics like machine learning ? there are facts or relations that can not be retrieved from data ( e.g. cause - effect relations ) , which is the driving force of ml . are there other reasons for studying causation ?",2444,,,2019-03-25T12:59:08.513,why should we study causation in artificial intelligence ?,causation,1,0,2
3251,11388,1,,2019-03-22T20:42:21.047,1,36,"do we have to consider if ( s is given ) an action a can lead to s ' when defining a reward function ? to be more specific : let 's say i have a 1d map like : |a|b|c|d| to define a reward function , i simply defined a matrix for every action , where the columns and rows represent the states ( a - d ) and the entries represent the reward . but i made it even simpler . as reaching a specific state gives a reward of 1 i just assigned the reward to a column ( c ) . $ $ \begin{matrix } -1&amp;0&amp;1&amp;0\\ -1&amp;0&amp;1&amp;0\\-1&amp;0&amp;1&amp;0\\-1&amp;0&amp;1&amp;0 \end{matrix } $ $ however , lets say the matrix is specified for the action "" going right "" . now there are entries reading : d = = & gt ; going right = = & gt ; c getting reward of 1 . this is actually not possible . however , i thought the transition function would handle this issue since i will define there what is possible and what is not . but anyway it is said that for a horizon of one the immidiate reward is given and the transition function is n't even considered . this leads to arbitrary result . so do i have to consider the "" physics "" of my world ?",19413,19413,2019-03-22T23:10:07.127,2019-04-22T12:02:30.480,do we have to consider the feasability of an action when defining the reward function of a mdp ?,rewards mdp,1,1,1
3252,11389,1,11400,2019-03-22T21:54:33.107,1,26,"consider an iterative deepening search using a transposition table . whenever the transposition table is full , what are common strategies applied to replace entries in the table ? i 'm aware of two strategies : replace the oldest entry whenever a new one is found . replace an entry if the new one has a higher depth . i 'm curious about other replacement approaches .",22369,22916,2019-03-23T03:58:54.010,2019-03-23T09:49:36.470,what are the common techniques one could use to deal with collisions in a transposition table ?,search dynamic-programming,1,0,
3253,11390,1,,2019-03-23T02:00:07.433,0,16,"i 'm working on a project where i need to forecast sales data where i have history of 1 year ( 2017 ) daily data . i am new on artificial intelligence topic and after searching for a while , i think arima or multiple linear regression is a good model for seasonal forecasting ( correct me if i 'm wrong ) . but then i think that my history data is exclusive for 2017 because on 2018 and 2019 , holiday date is changing . what model i have to used to forecast based on new holiday setup ? is arima or multiple linear regression still can be used ? or i need another model ? where do i have to start on this ?",23393,,,2019-03-23T02:00:07.433,what model used for forecasting sales with dynamic holiday,neural-networks linear-regression forecasting,0,0,1
3254,11394,1,,2019-03-23T06:26:44.727,0,48,"i have a set of topics and each topic consist of set of words . i want to make meaningful english sentences from these words . each topic is consist of five to ten words and these words are relevant to each other , like { code , language , test , write and function } and { class , public , method , string , int } are two sets.i want to generate sentence from these set of words using api .",23399,,,2019-04-22T21:01:30.207,how can i make meaningful english sentences from given set of words in python ?,machine-learning natural-language-processing,2,2,
3255,11401,1,,2019-03-23T12:38:31.063,2,51,"there are several variants of the dqn model . for example , double dqn , duelling dqn , prioritized dqn , distributed prioritized dqn , episodic memory dqn , asynchronous n - step dqn and multiple dqn . what are the differences between all these variants ? what are their advantages and disadvantages ? when should i use one over the other ? i am looking for an answer that ( briefly ) describes all the variants ( that we are aware of ) and then compares them .",2444,2444,2019-03-23T12:52:23.623,2019-04-22T19:02:50.557,what are the differences between the dqn variants ?,reinforcement-learning q-learning dqn difference deep-rl,1,0,
3256,11405,1,,2019-03-23T15:53:23.880,8,713,"autoencoders are neural networks that learn a compressed representation of the input in order to later reconstruct it , so they can be used for dimensionality reduction . they are composed of an encoder and a decoder ( which can be separate neural networks ) . dimensionality reduction can be useful in order to deal with or attenuate the issues related to the curse of dimensionality , where data becomes sparse and it is more difficult to obtain "" statistical significance "" . so , autoencoders ( and algorithms like pca ) can be used to deal with the curse of dimensionality . why do we care about dimensionality reduction specifically using autoencoders ? why ca n't we simply use pca , if the purpose is dimensionality reduction ? why do we need to decompress the latent representation of the input if we just want to perform dimensionality reduction , or why do we need the decoder part in an autoencoder ? what are the use cases ? in general , why do we need to compress the input to later decompress it ? would n't it be better to just use the original input ( to start with ) ?",2444,,,2019-03-24T00:26:08.930,what are the purposes of autoencoders ?,machine-learning autoencoders dimensionality-reduction curse-of-dimensionality,3,1,1
3257,11407,1,11428,2019-03-23T16:03:49.737,5,99,"raul rojas ' neural networks a systematic introduction , section 8.1.2 relates off - line backpropagation and on - line backpropagation with gauss - jacobi and gauss - seidel methods for finding the intersection of two lines . what i ca n't understand is how the iterations of on - line backpropagation are perpendicular to the ( current ) constraint . more specifically , how is 's gradient , $ ( x_1,x_2)$ , normal to the constraint $ x_1w_1 + x_2w_2 = y$ ?",14892,23790,2019-04-07T20:52:00.720,2019-04-07T20:52:00.720,are on - line backpropagation iterations perpendicular to the constraint ?,backpropagation math gradient-descent,2,0,
3258,11417,1,,2019-03-24T05:25:16.717,1,27,"i 'm trying to train a dc - gan on cifar-10 dataset . i 'm using binary cross entropy as my loss function for both discriminator and generator ( appended with non - trainable discriminator ) . if i train using adam optimizer , the gan is training fine . but if i replace the optimizer by sgd , the training is going haywire . the generator accuracy starts at some higher point and with iterations , it goes to 0 and stays there . the discriminator accuracy starts at some lower point and reaches somewhere around 0.5 ( expected , right ? ) . the peculiar thing is the generator loss function is increasing with iterations . i though may be the step is too high . i tried changing the step size . i tried using momentum with sgd . in all these cases , the generator may or may not decrease in the beginning , but then increases for sure . so , i think there is something inherently wrong in my model . i know training deep models is difficult and gans still more , but there has to be some reason / heuristic as to why this is happening . any inputs in appreciated . i 'm new to neural networks , deep learning and hence new to gans as well . here is my code : cifar10models.py from keras import sequential from keras.initializers import truncatednormal from keras.layers import activation , batchnormalization , conv2d , conv2dtranspose , dense , flatten , leakyrelu , reshape from keras.optimizers import sgd class dcgan : def _ _ init__(self , print_model_summary : bool = false ) : self.generator_model = none self.discriminator_model = none self.concatenated_model = none self.print_model_summary = print_model_summary def build_generator_model(self ) : if self.generator_model : return self.generator_model self.generator_model = sequential ( ) self.generator_model.add(dense(4 * 4 * 512 , input_dim=100 , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(batchnormalization(momentum=0.5 ) ) self.generator_model.add(activation('relu ' ) ) self.generator_model.add(reshape((4 , 4 , 512 ) ) ) self.generator_model.add(conv2dtranspose(256 , 3 , strides=2 , padding='same ' , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(batchnormalization(momentum=0.5 ) ) self.generator_model.add(activation('relu ' ) ) self.generator_model.add(conv2dtranspose(128 , 3 , strides=2 , padding='same ' , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(batchnormalization(momentum=0.5 ) ) self.generator_model.add(activation('relu ' ) ) self.generator_model.add(conv2dtranspose(64 , 3 , strides=2 , padding='same ' , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(batchnormalization(momentum=0.5 ) ) self.generator_model.add(activation('relu ' ) ) self.generator_model.add(conv2d(3 , 3 , padding='same ' , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(activation('tanh ' ) ) if self.print_model_summary : self.generator_model.summary ( ) return self.generator_model def build_discriminator_model(self ) : if self.discriminator_model : return self.discriminator_model self.discriminator_model = sequential ( ) self.discriminator_model.add(conv2d(128 , 3 , strides=2 , input_shape=(32 , 32 , 3 ) , padding='same ' , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.discriminator_model.add(leakyrelu(alpha=0.2 ) ) self.discriminator_model.add(conv2d(256 , 3 , strides=2 , padding='same ' , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(batchnormalization(momentum=0.5 ) ) self.discriminator_model.add(leakyrelu(alpha=0.2 ) ) self.discriminator_model.add(conv2d(512 , 3 , strides=2 , padding='same ' , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(batchnormalization(momentum=0.5 ) ) self.discriminator_model.add(leakyrelu(alpha=0.2 ) ) self.discriminator_model.add(conv2d(1024 , 3 , strides=2 , padding='same ' , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(batchnormalization(momentum=0.5 ) ) self.discriminator_model.add(leakyrelu(alpha=0.2 ) ) self.discriminator_model.add(flatten ( ) ) self.discriminator_model.add(dense(1 , kernel_initializer = truncatednormal(mean=0.0 , stddev=0.02 ) ) ) self.generator_model.add(batchnormalization(momentum=0.5 ) ) self.discriminator_model.add(activation('sigmoid ' ) ) if self.print_model_summary : self.discriminator_model.summary ( ) return self.discriminator_model def build_concatenated_model(self ) : if self.concatenated_model : return self.concatenated_model self.concatenated_model = sequential ( ) self.concatenated_model.add(self.generator_model ) self.concatenated_model.add(self.discriminator_model ) if self.print_model_summary : self.concatenated_model.summary ( ) return self.concatenated_model def build_dc_gan(self ) : self.build_generator_model ( ) self.build_discriminator_model ( ) self.build_concatenated_model ( ) self.discriminator_model.trainable = true optimizer = sgd(lr=0.0002 ) self.discriminator_model.compile(loss='binary_crossentropy ' , optimizer = optimizer , metrics=['accuracy ' ] ) self.discriminator_model.trainable = false optimizer = sgd(lr=0.0001 ) self.concatenated_model.compile(loss='binary_crossentropy ' , optimizer = optimizer , metrics=['accuracy ' ] ) self.discriminator_model.trainable = true cifar10trainer.py : # shree krishnaya namaha # based on https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0 import os import datetime import numpy import time from keras.datasets import cifar10 from keras.utils import np_utils from matplotlib import pyplot as plt import cifar10models log_file_name = ' logs.csv ' class cifar10trainer : def _ _ init__(self ) : self.x_train , self.y_train = self.get_train_and_test_data ( ) self.dc_gan = cifar10models.dcgan ( ) self.dc_gan.build_dc_gan ( ) @staticmethod def get_train_and_test_data ( ) : ( x_train , y_train ) , _ = cifar10.load_data ( ) x_train = x_train.reshape(x_train.shape[0 ] , x_train.shape[1 ] , x_train.shape[2 ] , 3 ) # generator output has tanh activation whose range is [ -1,1 ] x_train = ( x_train.astype('float32 ' ) * 2 / 255 ) - 1 y_train = np_utils.to_categorical(y_train , 10 ) return x_train , y_train def train(self , train_steps=10000 , batch_size=128 , log_interval=10 , save_interval=100 , output_folder_path='./trained_models/ ' ) : self.initialize_log(output_folder_path ) self.sample_real_images(output_folder_path ) for i in range(train_steps ) : # get real ( database ) images images_real = self.x_train[numpy.random.randint(0 , self.x_train.shape[0 ] , size = batch_size ) , : , : , :] # generate fake images noise = numpy.random.uniform(-1.0 , 1.0 , size=[batch_size , 100 ] ) images_fake = self.dc_gan.generator_model.predict(noise ) # train discriminator on both real and fake images x = numpy.concatenate((images_real , images_fake ) , axis=0 ) y = numpy.ones([2 * batch_size , 1 ] ) y[batch_size : , :] = 0 d_loss = self.dc_gan.discriminator_model.train_on_batch(x , y ) # train generator i.e. concatenated model noise = numpy.random.uniform(-1.0 , 1.0 , size=[batch_size , 100 ] ) y = numpy.ones([batch_size , 1 ] ) g_loss = self.dc_gan.concatenated_model.train_on_batch(noise , y ) # print logs , save models , generate sample images if ( i + 1 ) % log_interval = = 0 : self.log_progress(output_folder_path , i + 1 , g_loss , d_loss ) if ( i + 1 ) % save_interval = = 0 : self.save_models(output_folder_path , i + 1 ) self.generate_images(output_folder_path , i + 1 ) @staticmethod def initialize_log(output_folder_path ) : log_line = ' iteration no , generator loss , generator accuracy , discriminator loss , discriminator accuracy , ' \ ' time\n ' with open(os.path.join(output_folder_path , log_file_name ) , ' w ' ) as log_file : log_file.write(log_line ) @staticmethod def log_progress(output_folder_path , iteration_no , g_loss , d_loss ) : log_line = ' { 0:05},{1:2.4f},{2:0.4f},{3:2.4f},{4:0.4f},{5}\n ' \ .format(iteration_no , g_loss[0 ] , g_loss[1 ] , d_loss[0 ] , d_loss[1 ] , datetime.datetime.now().strftime('%y-%m-%d % h:%m:%s ' ) ) with open(os.path.join(output_folder_path , log_file_name ) , ' a ' ) as log_file : log_file.write(log_line ) print(log_line ) def save_models(self , output_folder_path , iteration_no ) : self.dc_gan.generator_model.save ( os.path.join(output_folder_path , ' generator_model_{0}.h5'.format(iteration_no ) ) ) self.dc_gan.discriminator_model.save ( os.path.join(output_folder_path , ' discriminator_model_{0}.h5'.format(iteration_no ) ) ) self.dc_gan.concatenated_model.save ( os.path.join(output_folder_path , ' concatenated_model_{0}.h5'.format(iteration_no ) ) ) def sample_real_images(self , output_folder_path ) : filepath = os.path.join(output_folder_path , ' cifar10_sample_real_images.png ' ) i = numpy.random.randint(0 , self.x_train.shape[0 ] , 16 ) images = self.x_train[i , : , : , :] plt.figure(figsize=(10 , 10 ) ) for i in range(16 ) : plt.subplot(4 , 4 , i + 1 ) image = images[i , : , : , :] image = numpy.reshape(image , [ 32 , 32 , 3 ] ) plt.imshow(image ) plt.axis('off ' ) plt.tight_layout ( ) plt.savefig(filepath ) plt.close('all ' ) def generate_images(self , output_folder_path , iteration_no , noise = none ) : filepath = os.path.join(output_folder_path , ' cifar10_gen_image{0}.png'.format(iteration_no ) ) if noise is none : noise = numpy.random.uniform(-1 , 1 , size=[16 , 100 ] ) # generator output has tanh activation whose range is [ -1,1 ] images = ( self.dc_gan.generator_model.predict(noise ) + 1 ) / 2 plt.figure(figsize=(10 , 10 ) ) for i in range(16 ) : plt.subplot(4 , 4 , i + 1 ) image = images[i , : , : , :] image = numpy.reshape(image , [ 32 , 32 , 3 ] ) plt.imshow(image ) plt.axis('off ' ) plt.tight_layout ( ) plt.savefig(filepath ) plt.close('all ' ) def main ( ) : cifar10_trainer = cifar10trainer ( ) cifar10_trainer.train(train_steps=10000 , log_interval=10 , save_interval=100 ) del cifar10_trainer.dc_gan return if _ _ name _ _ = = ' _ _ main _ _ ' : start_time = time.time ( ) print('program started at { 0}'.format(time.strftime('%y-%m-%d % h:%m:%s ' , time.localtime(start_time ) ) ) ) main ( ) end_time = time.time ( ) print('program ended at { 0}'.format(time.strftime('%y-%m-%d % h:%m:%s ' , time.localtime(end_time ) ) ) ) print('total execution time : { 0}s'.format(datetime.timedelta(seconds = end_time - start_time ) ) ) some of the graphs are as below : discriminator optimizer : sgd(lr=0.0001 , beta1=0.5 ) generator optimizer : adam(lr=0.0001 , beta1=0.5 ) discriminator optimizer : sgd(lr=0.0001 ) generator optimizer : sgd(lr=0.0001 ) discriminator optimizer : sgd(lr=0.0001 ) generator optimizer : sgd(lr=0.001 ) discriminator optimizer : sgd(lr=0.0001 ) generator optimizer : sgd(lr=0.0005 ) note : this question was originally asked in stackoverflow and then re - asked here as per suggestions in so",23421,,,2019-03-24T05:25:16.717,why is my generator loss function increasing with iterations ?,deep-learning python keras optimization generative-adversarial-networks,0,3,
3259,11418,1,,2019-03-24T05:26:49.420,3,39,"td3 is inspired from both double q - learning and double dqn . in double q - learning , i understand that q1 and q2 are independent because they are trained on different samples . in double dqn , i understand that target q and current q are relatively independent because their parameters are quite different . but in td3 , q1 and q2 are trained on exactly the same target . if their parameters are initialized the same , there will be no difference in their output and the algorithm will be equal to dqn . the only source of independence / difference of q2 to q1 i can tell is the randomness in the initialization of their parameters . but with training on the same target , i thought this independence will become smaller and smaller as they converge to the same target values . so i do n't quite understand why td3 works in combating overestimation in q - learning .",23420,,,2019-04-26T04:05:51.160,why q2 is a more or less independant estimate in twin delayed ddpg ( td3 ) ?,q-learning,1,0,
3260,11419,1,,2019-03-24T06:52:59.050,0,25,can i compare mae and mse loss results of a regression cnn with categorical_crossentropy loss of a classification cnn if they both have similar tasks ? is yes how to ?,23268,,,2019-03-24T08:29:48.243,can we compare mae mse results with categorical_crossentropy ?,convolutional-neural-networks classification linear-regression,0,2,
3261,11421,1,,2019-03-24T08:40:01.430,5,75,"i am training a modified vgg-16 to classify crowd density ( empty , low , moderate , high ) . 2 dropout layers were added at the end on the network each one after one of the last 2 fc layers . network settings : training data contain 4381 images categorized under 4 categories ( empty , low , moderate , high ) , 20 % of the training data is set for validation . test data has 2589 images . training is done for 50 epochs.(training validation accuracy drops after 50 epochs ) lr=0.001 , decay=0.0005 , momentum=0.9 loss= categorical_crossentropy augmentation for ( training , validation and testing data ) : rescale=1./255 , brightness_range=(0.2,0.9 ) , horizontal_flip with the above stated settings , i get the following results : training evaluation loss : 0.59 , accuracy : 0.77 testing accuracy 77.5 ( correct predictions 2007 out of 2589 ) regarding this i have two concerns : is there anything else i could do to improve accuracy for both training and testing ? how can i know if this is the best accuracy i can get ?",23268,22916,2019-03-25T19:14:52.807,2019-03-26T07:59:20.003,how do i improve accuracy and know when to stop training ?,convolutional-neural-networks classification,2,1,
3262,11427,1,,2019-03-24T19:37:20.193,6,49,"background context : in the past i 've heavily applied various "" code quality metrics "" to statically analyze code to provide an inkling of how "" maintainable "" it is and using things like the maintainability index alluded to here . however , a problem that i face is whether a language has libraries that effectively measure such metrics - only then is it usable else it 's rather subjective / arbitrary . given the plethora of languages that one has to deal with in an enterprise system , this is can get rather unwieldy . proposed idea : build and train an artificial neural network that "" ingests a folder of code "" ( i.e. , all files within that folder / package are assumed to house the "" project "" whose quality metrics we 'd like to compute ) . this may again be language dependent but let 's assume it exists for a language that i 'm having the hardest time with ( for measuring "" maintainability "" ) : scala . using numeric metrics like mccabe 's complexity or cyclomatic complexity maybe "" convention "" but are not entirely relevant . few things like class / method length are almost always relevant no matter the language . thus , providing a few "" numeric metrics "" + abstract notion of readability by subjective evaluation to train an ann would be a good balance of "" inputs "" to the ann . the output being either a classification of maintainability like low , medium , high etc . , or a number between 0 and 1 . question : has this been tried and are there any references ? i spent some time digging via google scholar but did n't find anything "" usable "" or worthwhile . it 's okay if it 's not scala , but have anns been used for measuring code quality ( i.e. , static analysis ) and what are the benefits or disadvantages of something like this ? ps : hopefully , the question is n't too broad , but if so , please let me know in the comments and i 'll try make it as specific as possible .",23432,,,2019-03-25T15:19:15.470,are there existing examples of using neural networks for static code analysis ?,neural-networks machine-learning quality-control,1,1,1
3263,11431,1,,2019-03-24T23:20:19.493,4,21,"i was looking around , a promising approach was this : https://github.com/mpralat/notesrecognizer the problem is : it does n't seem good enough . one should be able to read musical notes with lower quality images . you can see in her : "" bad "" images folder just tiny variations of lightning can already cause her problems with her high res images . others are here , they all use high res sharp images : https://github.com/suyalcinkaya/music-note-recognition https://github.com/suyalcinkaya/music-note-recognition/blob/master/input_images/im2s.jpg?raw=true https://github.com/nikolalsvk/note-play https://github.com/nikolalsvk/note-play/blob/master/images/notes-1.png?raw=true now this is unsatisfying if you want to snap a photo of some tunes , and want them to be recognized . so what could one do to achieve a good solution ? i was thinking about treating the musical notes just like written letters . the computer can easily learn written characters with the arabic symbols . i wonder though , how easy would it be for a none arabic ? e.g. in chinese or japanese , several characters combine into one . the same applies to music notes , they can be connected and form something slightly different through that e.g. or : in contrast to just simple notes like : what would be a good approach to try ones luck , with interpreting those symbols successfully , even for slightly low res images or bit blurry deformed images . i 'm not saying to read out a symfony out of a thumbnail . but less than optimal captures . any subjective ideas or comments are more than welcome",23435,,,2019-03-25T06:01:46.900,an approach on reading musical notes from photos,image-recognition ocr,1,1,
3264,11433,1,11451,2019-03-25T04:26:05.393,3,32,"i am trying to reproduce the results for the simple grid - world environment in [ 1]. but it turns out that using a dynamically learned pba makes the performance worse and i can not obtain the results shown in figure 1 ( a ) in [ 1 ] ( with the same hyperparameters ) . here is the result i got : the issue i found is that the learning procedure is stuck due to bad pba in the early stages of training . without pba , sarsa can converge well . did anyone try the method before ? i am really puzzled and how the authors obtain these good results ? there are some top conference papers using the same method in [ 1 ] , for example , [ 2 ] and [ 3]. [ 1 ] expressing arbitrary reward functions as potential - based advice [ 2 ] learning from demonstration for shaping through inverse reinforcement learning [ 3 ] policy transfer using reward shaping is the method itself defective or anything wrong with my code ? here is part of my codes : import copy import numpy as np import pandas as pd def expert_reward(s , action ) : if ( action = = right ) or ( action = = down ) : return 1.0 return 0.0 class dynamicpba : def _ _ init__(self , actions , learning_rate=0.1 , reward_decay=0.99 ) : self.lr = learning_rate self.gamma = reward_decay self.actions = actions self.q_table = pd.dataframe(columns=self.actions , dtype = np.float64 ) # q table for current time step self.q_table _ = pd.dataframe(columns=self.actions , dtype = np.float64 ) # q table for the current time step self.check_state_exist(str((0,0 ) ) ) def learn(self , s , a , r , s _ , a _ ) : # ( s , a ) denotes current state and action , r denotes reward , ( s _ , a _ ) denotes the next state and action self.check_state_exist(s_ ) q_predict = self.q_table.loc[s , a ] q_target = r + self.gamma * self.q_table.loc[s _ , a _ ] self.q_table.loc[s , a ] = self.q_table.loc[s , a ] + self.lr * ( q_target - q_predict ) def update(self ) : self.q_table = copy.deepcopy(self.q_table_ ) def check_state_exist(self , state ) : if state not in self.q_table.index : # append new state to q table self.q_table = self.q_table.append ( pd.series ( [ 0]*len(self.actions ) , index = self.q_table.columns , name = state , ) ) self.q_table _ = self.q_table_.append ( pd.series ( [ 0]*len(self.actions ) , index = self.q_table_.columns , name = state , ) ) # # # # # # # main part rl = sarsatable(actions = list(range(len(actions_dict ) ) ) , reward_decay=0.99 , learning_rate=0.05 ) expert = dynamicpba(actions = list(range(len(actions_dict ) ) ) , learning_rate=0.1 , reward_decay=0.99 ) for episode in range(100 ) : # initial observation s = ( 0,0 ) env.reset(s ) action = rl.choose_action(str(s ) ) r_episode_s = 0 r_episode = 0 current_step = 0 while true : # rl take action and get next observation and reward s _ , _ , reward , status = env.step(action ) current_step + = 1 action _ = rl.choose_action(str(s_ ) ) # update dynamic potentials expert_r = -expert_reward(s , action ) expert.learn(str(s ) , action , expert_r , str(s _ ) , action _ ) # compute pba f = expert.gamma * expert.q_table_.loc[str(s _ ) , action _ ] - expert.q_table.loc[str(s ) , action ] # update expert pba table expert.update ( ) rl.learn(str(s ) , action , reward+f , str(s _ ) , action _ , status ) # swap observation s = s _ action = action _ # break while loop when end of this episode if status ! = ' not_over ' : break if current_step&gt;10000 : print(episode , r_episode , r_episode_s , current_step ) break # learning rate decay rl.lr = rl.lr*0.999 # expert.update ( )",23429,,,2019-03-25T19:47:58.837,expressing arbitrary reward functions as potential - based advice ( pba ),machine-learning reinforcement-learning,1,0,
3265,11436,1,,2019-03-25T09:19:38.147,4,31,"i am no expert in the field of ai so i apologize if this is a simple / easy question . i was trying to implement a network similar to openai 's for another game and i noticed that i did not fully understand how the network worked . below is the image of openai five 's network basic question how is the data concatenated/ what is the dimension of the data right after the concatenation before the lstms or just before entering the lstms ? below are my thoughts which i provide for clarification 's sake . 1st interpretation in the blue area for units , my initial understanding was that for each visible unit , the output of the max - pool is concatenated along the columns . so , assuming the number of rows is 1(as a 1d max - pool is being applied for each unit ) , the number of columns is n and there are n visible units , the final size of the matrix when concatenated is $ ( 1,n\cdot n)$ with few extra columns given by the pickups and the like as shown on the left - hand side of the model . problem with this interpretation as the number of units a player can see per each turn is not constant , under this interpretation , i suspect that the fully connected layer after the concatenation layer can not do its job as matrix multiplication becomes impossible with a variable number of columns . possible solution one possible solution to this is to set a maximum to the number of observed units as $ n_{max}$ and pad with constants if some units are not observed . is this the case ? 2nd interpretation my 2nd interpretation is that the data is concatenated along the rows . in this case , i can see that the data can pass through a fully connected layer because the number of columns can remain constant . under this assumption , i decided that right before going through the lstm , the data is reshaped to ( batch size , number of rows , number of columns ) . problems with this interpretation while i found this interpretation to be more appealing , i noticed that under this train of thought , the lstm is used just to associate the input data and is not associated with time(the time step for the lstm is simply the next row of data rather than actual time ) . i know that this is not especially a problem but i thought that there is no special need to use an lstm here as in this second interpretation , the order of the data holds no special meaning . but is this the case ? i apologize in advance for any unclear points . please tell me in the comments and i 'll try to clarify as best as i can !",23443,,,2019-03-25T09:19:38.147,how did the openai 5 for dota concatenate units ?,deep-learning open-ai deep-rl,0,0,1
3266,11438,1,,2019-03-25T09:55:07.263,0,55,"i have been looking for bert for many tasks and i would like to compare performance to answer a faq using bert semantic similarity and bert q / a . however , i 'm not sure it is a good idea to use semantic similarity for this task . if it is , do you think it is possible to find a dataset to fine - tune my algorithm ?",23154,,,2019-05-21T18:29:45.637,use bert to answer a faq with semantic similarity,neural-networks deep-learning natural-language-processing,1,0,
3267,11439,1,,2019-03-25T11:22:41.433,2,64,"i 'm interested in building a ( deep ) rl agent for solving a continuous problem ( which splits something into portions ) . in all examples i 've seen so far , e.g. , solving the continuous lunar lander , always a output layer activation was used , which produces values between $ -1 $ and $ +1 $ . is this just because it fits the use case or is this a general rule for rl agents with continuous action spaces ? what if i just want values between $ 0 $ and $ 1 $ ? could i simply use a activation for my output layer ?",19928,2444,2019-05-15T15:11:45.990,2019-05-16T03:41:37.453,regarding the output layer 's activation function for continuous action space problems,neural-networks reinforcement-learning deep-rl activation-function,1,0,1
3268,11442,1,11448,2019-03-25T13:48:27.580,2,43,"from what i know , ai / ml uses a large amount of data to train an algorithm to solve problems . but since it ’s an algorithm , i was wondering if it 's possible to export it . if i trained an ai with r , could i export a mathematical algorithm that could be imported by other users to use in their application , whether it ’s written in r or another language ? so it ’s like i ’ve discovered a secret message decoding method . i do n’t need to share the whole program for others to decode it . i just need to tell them the steps ( algorithm ) to decode it , and they can implement it in whatever application they want .",23017,22916,2019-03-25T19:22:44.047,2019-03-25T19:22:44.047,export trained ai / ml model,machine-learning training,2,0,
3269,11447,1,,2019-03-25T15:49:31.813,1,11,"i have a dataset about ( 240000,23 ) . for my task , i have to use an unsupervised learning method and apply it on every single column separately in order to detect anomalies that might exist . i have pre - processed the data and i am visualizing the timeelapsed vs the parameter in python ( the graphs look like the one shown in an earlier post by me here ) . i am wondering if there is a way wherein after the graphs are plotted , the graphs are compared with each other and then clustered together based on their similarities . example : if i have temporal data of about 200 products , i plot the graphs of all the 200 products ( as can be seen in the link provided above ) and these 200 graphs must be compared with each other and must be separately plotted as 200 different points on a scatter plot ( by using some unsupervised learning techniques ) based on how similar are the graphs of different products to each other . i do n't know if the code that i have would be helpful in guiding me , but the code that i have is here : import pandas as pd import numpy as np import matplotlib.pyplot as plt np.random.seed(1234 ) dataset = pd.read_csv('temporal_datatable.csv ' , header = 0 ) dataset ! = 0 ( dataset ! = 0).any(axis=0 ) dataset = dataset.loc [ : , ( dataset ! = 0).any(axis=0 ) ] dataset['product_number ' ] = pd.factorize(dataset.context)[0]+1 dataset['index ' ] = pd.factorize(dataset.context)[0]+1 cols = list(dataset.columns.values ) cols.pop(cols.index('stepid ' ) ) cols.pop(cols.index('context ' ) ) cols.pop(cols.index('product_number ' ) ) cols.pop(cols.index('index ' ) ) dataset = dataset[['index','product_number','context','stepid']+cols ] dataset = dataset.set_index('index ' ) max_time_id_without_drop = dataset.groupby(['product_number'])['timeelapsed ' , ' stepid'].max ( ) avg_time_without_drop = np.average(max_time_id_without_drop['timeelapsed ' ] ) dataset_drop = dataset.drop(index = [ 128 , 133 , 140 , 143 , 199 ] ) max_time_id_with_drop = dataset_drop.groupby(['product_number'])['timeelapsed ' , ' stepid'].max ( ) avg_time_with_drop = np.average(max_time_id_with_drop['timeelapsed ' ] ) dataset = dataset.drop(columns=['timestamp ' ] ) dataset_drop = dataset_drop.drop(columns=['timestamp ' ] ) grouped = dataset.groupby('product_number ' ) ncols = 4 nrows = int(np.ceil(grouped.ngroups/40 ) ) for i in range(10 ) : fig , axes = plt.subplots(figsize=(12,4 ) , nrows = nrows , ncols = ncols ) for ( key , ax ) in zip(grouped.groups.keys ( ) , axes.flatten ( ) ) : grouped.get_group((20*i)+key).plot(x='timeelapsed ' , y=['flow_ar - edge ' ] , ax = ax , sharex = true , sharey = true ) ax.set_title('product_number=%d'%((20*i)+key ) ) ax.get_legend().remove ( ) handles , labels = ax.get_legend_handles_labels ( ) fig.legend(handles , labels , loc='upper center ' ) plt.show ( ) thanks in advance for the help .",21233,,,2019-03-25T15:49:31.813,is there a way to compare the similarities among different graphs and then cluster them using unsupervised learning ?,machine-learning deep-learning python unsupervised-learning,0,1,
3270,11450,1,,2019-03-25T19:12:42.337,0,10,"it seems like most applications of active learning is to use humans to label data . i 'm wondering if there are established applications of active learning to contexts where the oracle is a computer itself . links to published papers are necessary to be a good answer . i can think of conjectural applications , but the issue i am having is an inability to find papers documenting anyone doing this .",12732,,,2019-03-25T19:12:42.337,active learning with a computer oracle,active-learning,0,0,
3271,11452,1,,2019-03-25T21:52:23.613,-2,53,"recently , in an online obituary , it was announced that henry white pierce died at the age of 88 . according to the news - headline , he was a researcher in the topic of organ transplantation , in - vitro fertilization , and artificial intelligence . according to google scholar the name “ hw pierce ” is listed as an author in the database who has published decades ago about some topics . but the fulltext is stored in jstor , so i ca n't read it . my question is : does anybody know him personally ? has he published a lot about artificial intelligence ? do we have to remember of him ? the reason why i 'm asking is simple . academic progress can only be made on the shoulder of giants . the lifespan of a single person is not enough to research a topic in depth . what the later generation can do is to take the scientific legacy of a professor and try to remember him . if the work of a person is lost , it 's not possible to reference to papers from him . what i do n't understand is , why somebody can die and all his work get lost and nobody remembers him .",11571,1671,2019-04-04T21:07:49.610,2019-04-04T21:07:49.610,who was henry white pierce ?,history journalism,1,1,
3272,11453,1,,2019-03-25T22:03:20.690,0,39,"from this article , i read that "" to accurately classify data with neural networks , wide layers are sometimes necessary . "" however , i have seen many implementations and discussions on deep - learning , such as this , mention the concept of depth . what is the difference in the context of neural networks ? how does width vs depth impact a neural network 's performance ?",22424,,,2019-03-25T22:11:24.217,"what does "" wide "" vs. "" deep "" mean in the context of neural networks ?",neural-networks deep-learning,1,0,
3273,11455,1,11463,2019-03-25T22:46:28.447,5,101,"as far as i know , stochastic gradient descent is an optimization algorithm which belongs to the the category of algorithms where hyper - parameters have to be defined beforehand . they are useful in many cases , but there are some cases that the adaptive learning algorithms ( like adagrad or adam ) might be preferable . when are algorithms like adam and adagrad preferred over sgd ? what are the cons and pros of adaptive algorithms , like adam , when we compare them with learning algorithms like sgd ?",23460,2444,2019-03-26T14:15:08.243,2019-03-26T14:15:08.243,when should we use algorithms like adam as opposed to sgd ?,machine-learning optimization,1,1,
3274,11457,1,,2019-03-26T08:22:35.057,0,15,"to implement a specific function , i need "" input_channels "" number of kernels in my layer , each having only a single channel depth , and not depth = "" input_channels "" . i need to convolve one kernel with one channel of the input , thus the output of the layer would have "" input_channels "" number of kernels . image attached for reference - thanks in advance for any help . ( if anyone wishes to know what all i have tried yet - in the conv2d function of tensorflow , if i specify number of kernels = 1 to do this , then it will sum over all input_channels and number of output_channels will be 1 , since it always initialises kernel depth = "" input_channels "" . another option is to specify number of number of kernels = input_channels in conv2d function but this would create "" input_channels "" number of kernels of depth "" input_channels "" , thus adding lot of complexity and incorrect implementation of my layer . yet another thing i tried was to initialise a kernel of volume ( kernel_height , kernel_width , input_channels ) and loop over the third dimension to convolve only a single input channel with a single kernel . but the tensorflow conv2d function requires a rank 4 kernel to work and gives the following error - valueerror : shape must be rank 4 but is rank 3 for ' generic_act_func_4 / conv2d ' ( op : ' conv2d ' ) with input shapes : [ ? , 28,28 ] , [ 28,28]. )",23306,,,2019-03-26T08:22:35.057,controlling number of channels in weight / kernel in tensorflow,python tensorflow,0,0,
3275,11460,1,,2019-03-26T09:27:52.700,3,24,"i recently listened to the byzantine generals ’ problem , poisoning , and distributed machine learning with el mahdi el mhamdi ( beneficial agi 2019 ) . today , i was laughing with a friend at "" quantum computing "" being mentioned in a job offer . considering the probabilistic nature of results in quantum computers , what would be the advantages to using byzantine resistant neural networks on quantum computer ? feel free to improve the question if my wording is not correct .",23466,1671,2019-03-27T20:09:44.913,2019-03-27T20:09:44.913,can there be applications of byzantine neural networks on quantum computers ?,neural-networks quantum-computing,0,0,
3276,11461,1,,2019-03-26T09:40:36.617,0,37,"i need a pathfinding algorithm that considers the history of visited nodes and varies its path depending on some rules ( like already visited ) . are there good approaches serving this purpose ? to be more specific : let 's say i have a graph representing a map . i want to find a route from b to d : once i am in d following b->c->d , i want to calculate a new path , let 's say to a. the shortest path would be d->c->b->a . but i want a path with unvisited nodes even if it 's longer than the shortest possible path . the new path should be the shortest among all possible paths according to the rules . another example is the game "" snake "" . seeing the grid as a graph i can not visit already visited nodes as fas as the corpus ( of the snake ) is in that node ( or i have visited the node t time steps ago ) maybe the problem is too specific and i have to implement some basic pathfinding algorithm in a wider algorithm .",19413,19413,2019-03-26T15:53:49.180,2019-03-26T16:25:46.187,are there any pathfinding algorithms that take customized rules into account when determining the shortest path ?,pathfinding,1,2,
3277,11462,1,,2019-03-26T09:57:58.457,1,49,"i try to implement rl to a case something like this : this game consist of several rounds . every round the players need to generate a maze that consists of rooms . there are around 1000 different available rooms with different properties . at the beginning of a round , each player will be given 10 rooms one - by - one ( the sequence is same for each player ) from 1000 that available , then he / she try to create a maze from the taken rooms ( by arranging each room ) . after the maze is done there is a game master who will judge the maze ( give a score between 0 - 100 ) . the player never know how the game master judges the maze , it can be judged based on the level of difficulty that is produced , the order of the room we compose , or others . the player who got the best score for the given rooms will win this round . in this case , i have around 100,000 "" perfect "" mazes that have been created from different room combination and got a perfect score . i use this maze as episodes and try to train rl - agent to find the pattern of how the game master judges a maze . for your information , there are rooms that not exist in the 100,000 perfect mazes , but i hope the rl - agent can use its properties to find similar rooms that exist in the "" perfect "" mazes , and make it as a reference this case is different from other rl environments that i 've ever met before , generating an episode is not an easy task because it needs an expert to validate it ( the game master ) . so you could say , i can only build the rl agent using that 100,000 episodes . but , even though it only consists of 100,000 episodes , my case has millions of states , so i plan to use q - learning with neural net as approximator . my question is : in this case , am i still need the experience replay process ( i am afraid i do n't need it because of the small number of available episodes ) ? has this case ever happened before ? what is the best approach to deal with cases where the number of episodes is limited ?",16565,16565,2019-05-03T03:22:29.857,2019-05-03T03:22:29.857,reinforcement learning with limited number of episodes,reinforcement-learning q-learning experience-replay,0,11,
3278,11464,1,,2019-03-26T10:33:47.713,2,32,"i am wondering about a problem that i was given to solve : given circular trail divided by n segments(n>2 ) labeled 0 .. n-1 . in the beginning an agent is at the start of segment number 0 ( the edge between segments n-1 and 0 ) and the agent 's speed ( m ) is 0 segments per minute . at the start of each minute the agent take one of three actions : speed up : the agent 's speed increases by 1 segment per minute . slow down : the agent 's speed decreases by 1 segment per minute . keep the same speed : stay on same speed . the action slow down can not be used if current speed is 0 segments per minute . the cost of each action is 1 . the goal of the agent is to drive around the trail k times ( $ 1 \leq k$ ) and then park in the beginning spot ( at speed 0 of course ) . the agent needs to do that in the minimum amount of actions . the heuristic is given as : if agent is in segment z then : n - z if $ z \not = 0 $ or 0 if $ z=0 $ i need to find if the given heuristic is complete and consistent . i think : regarding consistency : a heuristic is consistent if its estimate is always estimated distance from any given neighbour vertex to goal plus cost of reaching goal . so in the given problem it is consistent because ( n>2 ) so heuristic function is well defined and because of circular trail divided by n segments with a constant price of 1 for each action , then the given definition of consistency holds because estimated distance from any given neighbour vertex to goal can be looked on as a difference between segments until reaching a goal and it is consistent because again the function is well defined . regarding admissibility : an admissible heuristic is one that the cost to reach goal is never more than the lowest possible cost from current point to reach the goal . i am not sure if the given heuristic is admissible because it does not help much to know the difference between trail size ( n = segment size ) and current place . but it does not create flaws so it is probably admissible . i am not sure this is a proof . is my idea correct ? how could i write it as a proof ?",23467,1847,2019-03-26T20:55:19.457,2019-03-26T20:55:19.457,admissible and consistent heuristics,ai-design ai-basics problem-solving heuristics,1,0,
3279,11465,1,,2019-03-26T11:04:09.213,0,21,"i have a column with links to websites and another column with keywords from those websites . i have to find a link between these two such that for a new input of a website - link , i can generate keywords . example : training sample could be website link : chocolate.com keywords : milk , dark we can tell from this example that my keywords are types of chocolates . testing : career.com result : it , medicine ... i have a feeling that some sort of supervised neural networks could be used here . which approach would be most suited here ?",23468,23468,2019-03-27T11:30:00.273,2019-03-27T11:30:00.273,how would one go about find patterns in text files when keywords are given ?,neural-networks detecting-patterns text-summarization supervised-learning,0,2,
3280,11466,1,,2019-03-26T13:14:18.797,1,43,"i was going through this implementation of reinforcement learning where model is being trained to manage the number of bikes at a station . here , line 78 represents the loop over all episodes ( if i understood correctly ) . in line 92 , the dqn agent is defined meaning after each episode , the agent will be reset to default parameters . but should n't we define the model before the loop starts because the after each episode , wo n't all the previous learning be lost if we initialize the class object in each iteration ? am i misinterpreting anything ?",19244,,,2019-03-26T13:14:18.797,do we need to reset the dqn network after every episode ?,reinforcement-learning q-learning dqn,0,4,
3281,11469,1,,2019-03-26T20:43:07.783,5,55,"in the documentary about the match , it is said that after losing the 4th game , alphago came back stronger and started to play in a weird way ( not human - like ) and it was pretty impossible to be beaten . why and how did that happen ?",21832,1671,2019-03-26T20:56:47.093,2019-03-27T04:01:42.990,"why did n't champion of the go game manage to win the last game against alphago , after winning the 4th one ?",game-ai soft-question alphago monte-carlo,1,1,1
3282,11471,1,,2019-03-27T03:11:54.003,1,17,"i was researching about hierarchical object detection , and end up reading that yolo v3 is the state of art for that kind of tasks , besides , the inference time make it one of the best for run it on live video . so , what i have in mind , is to run a pose estimation technology over the live video ( likeopenpose ) , then focus only in the rectangles near the hands of the estimated pose in order to detect the object . the previous approach sounds good , but i feel like i 'm not taking advantage of the temporal features on the video , for example , yolov3 could not be very sure that someone has a cellphone with only the rectangle of the hands , but if i add up , the movement of the estimated pose ( hand near to the head for several frames ) , i could be more sure that he has a phone . but i can not find a paper , approach or something close to the idea i have on mind , so i was wondering if someone here could give me a little clue about what path should i follow . thanks in advance for any help !",23491,,,2019-03-27T03:11:54.003,live video object detection with pose estimation,computer-vision,0,0,
3283,11474,1,,2019-03-27T07:34:42.463,1,43,"i am trying to create a chatbot whose dialogue policy model will be trained through reinforcement learning . dialogue policy is responsible for selecting the action to take based on the given state of the conversation . all implementations i see for rl are trained from an environment taken from gym or created manually . these environments provide the next state , rewards etc to the model based on which it is trained . since i am creating a dialogue policy model which will be trained through real user conversations , i can not provide a "" pre - defined "" environment which can provide the states and rewards . i am planning to train it myself by talking to it and providing rewards and next state ( which i think is called interactive learning ) . but i was not able to find any implementations , tutorials or articles on interactive learning . i am not able to figure out how to create such a model , how to take care of the episodes , sessions etc . this will be a continuous learning that will go on for months maybe . i have to save the model each day and continue training the next day by loading the model from that same state . can anyone guide me in the correct direction on how to approach this ? any githubs links , articles , tutorials of such implementations will be highly appreciated . i am aware this question seems too broad , but some hints will be very helpful for a newbie like me .",19244,,,2019-03-27T18:14:25.177,how to build a dqn agent which can be trained through interactive learning ?,reinforcement-learning q-learning chat-bots dqn,1,0,0
3284,11476,1,,2019-03-27T10:48:49.873,0,8,"i am reading thesis https://tel.archives-ouvertes.fr/tel-00850289v2 about use of mean field theory for the stochastic approximation of neural networks . there are lot of such research in arxiv cond - nn section , in condensed matter physics . such research reveals that emergence of distinct behavior is possible in approxiamted neural networks . but what about full , non - approximated neural networs as researched in computer science ? does such non - approximated networks have some emergence ? sometimes i am thinking that attention mechanisms are something how emergence is manifested in non - approximated neural networks , is it so ? are there other kinds of emergence ? does neural machine translation can be considered as the emergent structure ? or maybe emergence has been detected only in the approximated neural networks as researched in condensed matter physics / complexity theory ?",8332,,,2019-03-27T10:48:49.873,emergence and attention in non - approximated deep neural networks ?,neural-networks attention emergence,0,0,
3285,11478,1,,2019-03-27T12:13:29.973,3,136,"is there a concept called neural network that edits neural networks ? the neural network is editing itself . i think we can combine this with evolutionary algorithm to create strong intelligence . for example the b neural network edits the a neural network , if you 've edited it successfully , you 'll survive . the neural network d trying to edit the a neural network is blocked by certain conditions or the f neural network that edits the b neural network on the condition of the a neural network , or the neural network that creates this editing neural network . it 's about interacting with all kinds of neural networks that function .",23500,,,2019-03-28T23:51:32.793,is there a concept of neural networks that edit neural networks ?,neural-networks,2,1,1
3286,11479,1,,2019-03-27T12:59:53.067,2,18,"svm is designed for two - class classification problem . if the data is not linear - separable , a kernel function is used . i want to know if there is exists any method that will indicate if the data is linearly separable or not .",23501,,,2019-03-27T12:59:53.067,is there any formal test for linear separability of 2-class data ?,neural-networks machine-learning,0,1,
3287,11480,1,,2019-03-27T13:01:19.460,3,62,"i do n't know what people mean by ' vanilla policy gradient ' , but what comes to mind is reinforce , which is the simplest policy gradient algorithm i can think of . is this an accurate statement ? by reinforce i mean this surrogate objective $ $ \frac{1}{m } \sum_i \sum_t log(\pi(a_t|s_t ) ) r_i $ $ where i indices over the $ m$ episodes and $ t$ over time steps , and $ r_i$ is the total reward of the episode . it 's also common to replace $ r_i$ with something else , like a baselined version $ r_i - b$ or use the future return , potentially also with a baseline $ g_{it } - b$ . however , i think even with these modifications to the multiplicative term , people would still call this ' vanilla policy gradient ' . is that correct ?",17312,2444,2019-05-31T21:54:23.450,2019-05-31T21:54:23.450,is reinforce the same as ' vanilla policy gradient ' ?,reinforcement-learning terminology policy-gradients relation,2,2,
3288,11481,1,,2019-03-27T13:30:57.687,0,59,"i 'm trying to implement my own dqn . so far i think my code is good , but my q - values ( i 'm getting the mean of all the values for every episode ) tends to converge near - zero but negatively . it is normal ? or there is something wrong in my implementation ? my exploration vs explotation greedy strategy goes from 1.0 to 0.1 in 1 million steps ( as deepmind does ) , my learning rate is 0.00025 and my gamma 0.99 . i read here that "" the mean q - values should smoothly converge towards a value proportionnal to the mean expected reward . "" so , it 's my agent expecting a negative reward ? if so , how can i fix it ? here is a graph of the first training session : you can see how the q - values tend to converge near - zero after about 1300 episodes ( 1120000 steps aproximately ) . actually it 's showing values like -0.0117 , -0.0145 , etc . also , the agent seems very "" static "" after epsilon gets near 0.1 , and when it reaches it does n't move so much . ( i 'm training with pongdeterministic - v4 )",9818,,,2019-05-09T14:01:45.313,dqn q - mean values converge negatively,reinforcement-learning q-learning dqn deep-rl,0,2,
3289,11483,1,,2019-03-27T13:47:21.110,2,35,"in order to update the belief state in a pomdp , the following formula is used : $ $ b'(s')=\frac{o(a , s ' , z ) \sum_{s\in s } b(s)t(s , a , s')}{\mathbb{p}(z \mid b , a)}$$ where $ s$ is a specific state in the set of states $ s$ $ b'(s')$ is the updated belief state of being in the next state $ s'$ $ t(s , a , s ' ) = \mathbb{p}(s ' \mid s , a)$ is the propability ( function ) of having been in $ s$ and ending up in $ s'$ by taking action $ a$ ; $ o(a , s ' , z ) = \mathbb{p}(z \mid s ' , a)$ the probability ( function ) of observing $ z$ , performing action $ a$ and ending up in $ s'$ is defined as follows looking at it is possible that the result is $ 0 $ . this would be the case if the agent is in a state where no further actions are possible . but , in that case , there is a problem with updating $ b'(s')$ , since this causes a zero division . is this a common problem and is the only possibility to avoid that a programming solution like an if - statement ? or is always non - zero ?",19413,2444,2019-03-28T14:52:48.933,2019-03-28T21:11:41.690,can the normalization factor for the belief state update be zero ?,reinforcement-learning pomdp probability-distribution,1,0,
3290,11484,1,,2019-03-27T14:32:48.177,-2,45,"models are used by economists for simulating complicated processes . a well - known example is a population model which consists of individuals which are born on a certain day and get older each year . the model can answer questions such as how many individuals there are in the population , what their average age is , and if the population is growing or not . realizing an economic model in software is easier than it looks like . in most cases a simple python class which holds the variables plus an update function is enough to simulate a complete population . such a model can be extended with graphical output which results into a full - blown realistic simulation . but i 'm not here to promote the subject of statistics , but to discuss analyzing models about human thinking . so called brain models are used in psychology to teach human thinking . similar to a population model they are simplified versions of reality with the aim of testing hypotheses . they can be realized with the python programming language as well . in cognitive science , a handful of components are known which describe how the human brain works . usually , they are long - term memory , a short term memory , sensory input and motor output . how can this vague theory be realized in a working model ? are examples available which have tried to simulate the human brain with a simplified model ?",11571,2193,2019-03-27T15:18:04.460,2019-03-28T09:26:45.130,how to create a brain model from scratch ?,models human-like brain,1,0,
3291,11485,1,11486,2019-03-27T15:06:57.007,1,40,"as my first ai model i have decided to make an ai model to predict multiplication of two numbers ex - [ 2,4 ] = [ 8]. i wrote the following code , but the loss is very high , around thousands , and it 's very inaccurate . how do i make it more accurate ? import torch import torch.nn as nn import torch.nn.functional as f data = torch.tensor([[2,4],[3,6],[3,3],[4,4],[100,5]],dtype=torch.float ) values = torch.tensor([[8],[18],[9],[16],[500]],dtype=torch.float ) lossfun = torch.nn.mseloss ( ) model = net ( ) optim = torch.optim.adam(model.parameters(),lr=0.5 ) class net(nn.module ) : def _ _ init__(self ) : super(net , self).__init _ _ ( ) ; self.fc1 = nn.linear(in_features=2,out_features=3 ) self.fc2 = nn.linear(in_features=3,out_features=6 ) self.out = nn.linear(in_features=6,out_features=1 ) def forward(self , x ) : x = self.fc1(x ) x = f.relu(x ) x = self.fc2(x ) x = f.relu(x ) x = self.out(x ) return x for epoch in range(1000 ) : y_pred = model.forward(data ) loss = lossfun(y_pred , values ) print(loss.item ( ) ) loss.backward ( ) optim.step ( ) note : i am a newbie in ai and ml .",23507,16229,2019-03-30T15:51:48.930,2019-03-30T15:51:48.930,heavy loss and inaccurate answer in pytorch,loss-functions pytorch,1,1,
3292,11487,1,,2019-03-27T17:29:05.870,3,31,"suppose i have a deep feed - forward neural network with sigmoid activation already trained on a dataset $ s$ . let 's consider a training point $ x_i \in s$ . i want to analyze the entries of a hidden layer $ h_{i , l}$ , where $ $ h_{i , l } = \sigma(w_l ( \sigma ( w_{l-1 } \sigma ( \dots \sigma ( w_1 \cdot x_i))\dots ) . $ $ my intuition would be that , since gradient descend has passed many times on the point $ x_i$ updating the weights at every iteration , the entries of every hidden layer computed on $ x_i$ would be either very close to zero or very close to one ( thanks to the effect of the sigmoid activation ) . is this true ? is there a theoretical result in the literature which shows anything similar to this ? is there an empirical result which shows that ?",21338,2444,2019-03-27T21:19:55.887,2019-03-27T21:19:55.887,how do intermediate layers of a trained neural network look like ?,neural-networks machine-learning deep-learning activation-function,0,1,
3293,11488,1,,2019-03-27T17:57:59.900,0,16,"i 'm trying to implement dqn using tf - agents for simple environment . so far i have from tf_agents.specs import array_spec action_spec = array_spec.boundedarrayspec ( shape= ( ) , dtype = np.int32 , minimum=0 , maximum=10 , name='action ' , ) and just 10 states for the environment . but in some states agent is able to perform [ 1 - 5 ] actions not the whole range [ 1 - 10]. how can i specify that in practice ? right now i 've came up only with the idea of giving really high negative reward when being in the state and performing illegal action . is there a right way how to accomplish that ?",23434,,,2019-03-27T17:57:59.900,how to limit actions based on a state,reinforcement-learning tensorflow q-learning dqn deep-rl,0,1,
3294,11490,1,,2019-03-27T19:42:47.843,1,15,"in the paper "" deep inside convolutional networks : visualising image classification models and saliency maps "" , https://arxiv.org/abs/1312.6034 , at part 3 , there is a first - order taylor expansion(formula number 3 & amp ; 4 ) that i ca n't understand the logic behind it and how they are obtained . those formulas are about computing saliency map in convolutional neural networks .",10051,,,2019-03-27T19:42:47.843,image - specific class saliency visualisation,neural-networks convolutional-neural-networks computer-vision,0,0,
3295,11491,1,,2019-03-28T03:07:34.627,3,23,"i am attempting to implement yolo v3 in tensorflow - keras from scratch , with the aim of training my own model on a custom dataset . by that , i mean without using pretrained weights . i have gone through all three papers for yolov1 , yolov2(yolo9000 ) and yolov3 , and find that although darknet53 is used as a feature extractor for yolov3 , i am unable to point out the complete architecture which extends after that - the "" detection "" layers talked about here . after a lot of reading on blog posts from medium , kdnuggets and other similar sites , i ended up with a few significant questions : have i have missed the complete architecture of the detection layers ( that extend after darknet53 used for feature extraction ) in yolov3 paper somewhere ? the author seems to use different image sizes at different stages of training . does the network automatically do this upscaling / downscaling of images ? for preprocessing the images , is it really just enough to resize them and then normalize it ( dividing by 255 ) ? please be kind enough to point me in the right direction . i appreciate the help !",21513,21513,2019-03-28T03:18:11.627,2019-03-28T03:18:11.627,yolo v3 complete architecture,tensorflow computer-vision keras object-recognition,0,0,
3296,11498,1,,2019-03-28T11:13:50.207,0,41,"i have trouble with the reinforce algorithm in keras with atari games . after round about 30 episodes the network converges to one action . but the same algorithm is working with cartpole - v1 and converges with mean reward 495,0 after round 350 episodes . why are there problems with atari games ? i do n't know what i 'm doing wrong with the loss function . here is my code : policy examples episode 0 : p = [ 0.15498623 0.16416906 0.15513565 0.18847148 0.16070205 0.17653547 ] .... episode 30 : p = [ 0 . 0 . 0 . 0 . 1 . 0 . ] .... episode 40 : p = [ 0 . 0 . 0 . 0 . 1 . 0 . ] .... network from keras.layers import * from keras.models import model from keras.optimizers import adam from keras.backend.tensorflow_backend import set_session import tensorflow as tf import os config = tf.configproto ( ) config.gpu_options.allow_growth = true sess = tf.session(config=config ) set_session(sess ) class pgatarinetwork : def _ _ init__(self , state_space , action_space , lr ) : input = input(shape = state_space , name='inputs ' ) rewards = input(shape=(1 , ) , name='rewards ' ) conv1 = conv2d(filters=64 , kernel_size=(8 , 8) , strides=(4 , 4 ) , activation='relu ' , name='conv1')(input ) conv2 = conv2d(filters=128 , kernel_size=(4 , 4 ) , strides=(2 , 2 ) , activation='relu ' , name='conv2')(conv1 ) conv3 = conv2d(filters=256 , kernel_size=(4 , 4 ) , strides=(2 , 2 ) , activation='relu ' , name='conv3')(conv2 ) flatten = flatten()(conv3 ) fc1 = dense(units=512 , activation='relu ' , name='fc1')(flatten ) fc2 = dense(units=256 , activation='relu ' , name='fc2')(fc1 ) p = dense(units = action_space , activation='softmax')(fc2 ) def policy_loss(r ) : def pol_loss(y_true , y_pred ) : log_prob = k.log(k.sum(y_pred * y_true , axis=1 , keepdims = true ) + 1e-10 ) return -log_prob * k.stop_gradient(r ) return pol_loss return pol_loss self.model = model(inputs=[input , rewards ] , outputs = p ) self.model.compile(loss=policy_loss(rewards ) , optimizer = adam(lr = lr ) ) self.model.summary ( ) def predict(self , s ) : s = s[np.newaxis , :] return self.model.predict([s , np.array([1 ] ) ] ) def update_model(self , target ) : self.model.set_weights(target.get_weights ( ) ) def train(self , s , a , r ) : self.model.train_on_batch([s , r ] , a ) def save_weights(self , path ) : self.model.save_weights(path ) def load_weights(self , path ) : if os.path.isfile(path ) : self.model.load_weights(path ) training import gym import numpy as np import matplotlib.pyplot as plt from atari_wrapper import * from pg_arari_network import * class agent : def _ _ init__(self , env ) : self.env = env self.state_space = env.observation_space.shape self.action_space = env.action_space.n # hyperparameter self.gamma = 0.97 self.lr = 0.001 # environment self.mean_stop_train = 19.5 self.save_model_episode = 10 self.show_policy = false # network self.model_path = ' pong.h5 ' self.model = pgatarinetwork(self.state_space , self.action_space , self.lr ) self.model.load_weights(self.model_path ) # lists self.rewards = [ ] def train(self , episodes ) : for episode in range(episodes ) : time_steps = 0 states , actions , episode_rewards = [ ] , [ ] , [ ] episode_reward = 0 s = self.env.reset ( ) s = np.array(s ) while true : time_steps + = 1 if episode % 10 = = 0 : self.show_policy = true a = self.get_action(s ) self.show_policy = false s _ , r , d , i = self.env.step(a ) s _ = np.array(s_ ) episode_reward + = r action = np.zeros(self.action_space ) action[a ] = 1 actions.append(action ) states.append(s ) episode_rewards.append(r ) if d : discounted_episode_rewards = self.discount_rewards(episode_rewards ) self.update_policy(states , actions , discounted_episode_rewards ) self.rewards.append(episode_reward ) mean_rewards = np.mean(self.rewards[-min(len(self.rewards ) , 10 ) : ] ) print('episode : { } \treward : { } \tmean : { } \tsteps : { } ' .format ( episode , episode_reward , mean_rewards , time_steps ) ) if mean_rewards & gt;= self.mean_stop_train : self.model.save_weights(self.model_path ) return break s = s _ def get_action(self , s ) : p = self.model.predict(s)[0 ] if self.show_policy : print(p ) a = np.random.choice(self.action_space , p = p ) return a def discount_rewards(self , episode_rewards ) : discounted_episode_rewards = np.zeros_like(episode_rewards ) cumulative = 0.0 for i in reversed(range(len(episode_rewards ) ) ) : cumulative = cumulative * self.gamma + episode_rewards[i ] discounted_episode_rewards[i ] = cumulative mean = np.mean(discounted_episode_rewards ) std = np.std(discounted_episode_rewards ) discounted_episode_rewards = ( discounted_episode_rewards - mean ) / std return discounted_episode_rewards def update_policy(self , states , actions , rewards ) : s = np.array(states ) a = np.vstack(np.array(actions ) ) r = np.vstack(np.array(rewards ) ) self.model.train(s , a , r ) if _ _ name _ _ = = ' _ _ main _ _ ' : env = make_atari('pongnoframeskip - v0 ' ) env = wrap_deepmind(env , episode_life = true , clip_rewards = true , frame_stack = true , scale = true ) agent = agent(env ) agent.train(30000 ) plt.plot(range(len(agent.rewards ) ) , agent.rewards , color='blue ' ) plt.title('atari policy gradient ' ) plt.show ( )",23522,23522,2019-03-28T11:32:03.693,2019-03-28T11:32:03.693,policy gradient in keras predicts only one action,reinforcement-learning keras policy-gradients,0,1,1
3297,11500,1,11581,2019-03-28T12:56:12.917,3,103,"can ai be used as a tool to investigate our minds ? to be more precise , what am i specifically asking for here are examples of discoveries on artificial intelligence ( so algorithms , programs and computers that try to implement intelligent systems ) that brought to light facts about intelligence and cognition in general . have this ever happened ? is it frequent ? how influential and important were these discoveries , if any ? a possible example of what i mean could be the pssh , which states that a formal system is sufficient to simulate general intelligent behaviour . i believe that this is relevant to cognitive science in general because it entails our understanding of this phenomena . ( of course , this is just an hypotesis , but i believe that its importance in the ai debate makes it a really compelling result ) .",23527,23527,2019-03-28T14:53:11.307,2019-04-01T11:42:04.067,can ai research lead to new findings in general cognitive science ?,philosophy cognitive-science,3,2,1
3298,11504,1,,2019-03-28T16:08:25.933,5,63,"i am very beginner to this world . i still learning the basics of machine learning and ai but i have a problem at hand and i am not sure which technique or algorithm can be applied on it . i am working on click - fraud detection in advertising . i need to predict fraud and learn new frauds with ml . the dataset i have is the view and click logs from adserver(service provider ) . this data have some fields few of them are listed below : "" auction_log_bid_id "" : null , "" banner "" : 9407521 , "" browser "" : 0 , "" campaign "" : 2981976 , "" city "" : 94965 , "" clickword "" : null , "" content_unit "" : 4335438 , "" country "" : 1 , "" external_profiledata "" : { } , "" external_user_id "" : null , "" flash_version "" : null , "" i d "" : 6665230893362053181 , "" ip_address "" : "" 80.187.103.98 "" , "" is_ssl "" : true , "" keyword "" : "" string "" "" mobile_device "" : -1 , "" mobile_device_class "" : -1 , "" network "" : 268 , "" new_user_id "" : 6665230893362118717 , "" operating_system "" : 14 , "" profile_data "" : { } , "" referrer "" : null , "" screen_resolution "" : null , "" server_id "" : 61 , "" state "" : 7 , "" target_url "" : "" string "" "" timestamp "" : 1551870000 , "" type "" : "" click_command "" , "" user_agent "" : "" mozilla/5.0 ( iphone ; cpu iphone os 12_1_4 like mac os x ) applewebkit/605.1.15 ( khtml , like gecko ) mobile/16d57 "" , "" user_id "" : null , "" view_log_id "" : null there are other fields . i need to analyse these logs to find patterns for possible frauds but i am not sure where to start and which technique to use . e.g. supervised , unsupervised semi - supervised or reinforcement learning .",23531,23531,2019-03-29T11:12:23.627,2019-04-28T14:01:55.347,how to detect frauds in advertising business using machine learning ?,machine-learning models applications,1,5,1
3299,11507,1,,2019-03-28T21:45:10.530,1,34,i am working on a supervised machine learning problem where i have more than 10 probably 50 or 100 predicting label categories . which type of model can be used to work on this type of problem in anaconda python .,23544,23544,2019-03-29T13:09:26.597,2019-03-29T20:36:41.443,what types of machine learning model would fit ?,machine-learning python data-science,1,2,
3300,11508,1,,2019-03-28T21:51:28.413,0,14,"i have an rc car with a camera , i have implemented so that i can detect lanes on my track ( think like a nascar track ) . i want to get this car to be able to go around the track autonomous . but i am quite unsure what my next step should be . either i can do an algorithm that detects so that i stay in the middle of the lane ( if i get to close to either of the lines i steer towards the center ) . or perhaps go around the track manually and save the coordinates of the detected lines as well as the actions i take ( steering ) and try a dqn approach . i 'm trying to minimize my ' trial and error ' time a little here . perhaps there are some important steps in between here , or a solution that i have not thought of ( i.e. am i missing something ) ? i 'm doing this as a proof of concept therefore i can only spend maximum of a month on this , so what would you do here ?",22349,,,2019-03-28T21:51:28.413,next step after lane detection in vehicle automation,computer-vision dqn autonomous-vehicles,0,6,
3301,11510,1,11531,2019-03-28T23:55:54.163,0,47,"this is my first post so please forgive me for any mistakes . i am working on an object detection algorithm that can detect abnormalities in an x - ray . as a prototype , i will be using yolov3 ( more about yolo here : ' https://pjreddie.com/darknet/yolo/ ' ) however , one radiologist mentioned that in order to produce a good result you need to take into account the demographics of the patient . in order to do that , my neural network must take into account both text an an image . some suggestions have been made by other people for this question . for example , someone recommended taking the result of a convolution neural network and a seperate text neural network . here is an image for clarification : image credits : this image ( https://cdn-images-1.medium.com/max/1600/1 * oilg3c3 - 7ocklg9_xubrrw.jpeg ) from christopher bonnett 's article ( https://blog.insightdatascience.com/classifying-e-commerce-products-based-on-images-and-text-14b3f98f899e ) for more details , please refer to above - mentioned article . it has explained how e - commerce products can be classified into various category hierarchies using both image and text data . however , a when convolution neural network is mention it usssualy means it is used for classification instead of detection https://www.quora.com/what-is-the-difference-between-detection-and-classification-in-computer-vision ( link for comparison between detection and classification ) in my case , when i am using yolov3 , how would it work . would i be using yolov3 output vector which would be like this format class , center_x , center_y , width and height my main question is how would the overall structure of my neural network be like if i have both image and text as input while using yolov3 . thank you for taking the time to read this .",23546,,,2019-03-31T17:05:14.057,detecting abnormalities in x - rays while taking into account demographics of a patient -automated,neural-networks convolutional-neural-networks computer-vision object-recognition automation,2,0,1
3302,11511,1,,2019-03-29T00:44:19.540,3,19,"a sampled softmax function is like a regular softmax but randomly selects a given number of ' negative ' samples . this is difference than nce loss , which does n't use a softmax at all , it uses a logistic binary classifier for the context / labels . in nlp , ' negative sampling ' basically refers to the nce - based approach . more details here : https://www.tensorflow.org/extras/candidate_sampling.pdf . i have tested both and they both give pretty much the same results . but in word embedding literature , they always use nce loss , and never sampled softmax . is there any reason why this is ? the sampled softmax seems like the more obvious solution to prevent applying a softmax to all the classes , so i imagine there must be some good reason for the nce loss .",18358,2444,2019-04-16T22:24:32.703,2019-04-16T22:24:32.703,why does all of nlp literature use noise contrastive estimation loss for negative sampling instead of sampled softmax loss ?,natural-language-processing word2vec word-embedding,0,0,
3303,11513,1,,2019-03-29T04:25:36.700,1,20,"the model that we develop in artificial intelligence.what is its purpose , and what training data is relevant to it .",23529,,,2019-05-25T07:41:00.003,what is the goal of the model and is the training data relevant to that ?,training generative-model goal-based,1,0,
3304,11514,1,,2019-03-29T05:24:16.763,1,24,"a multi - agent system is composed of multiple interacting intelligent agents on which kqml ( knowledge query and manipulation language ) is implemented . but i am confused on the nature of agents and why we need them . for examples , what are the difference between agents and classes in oop as well as tables in database ? why is kqml implemented by agents rather than classes ?",22182,2444,2019-03-29T09:49:33.213,2019-03-29T09:49:33.213,why do we need agents in knowledge query and manipulation language ?,natural-language-processing intelligent-agent multi-agent-systems,0,0,
3305,11515,1,11516,2019-03-29T06:15:05.543,1,37,"since keras api as defined as layers , how would it be used to implement the word2vec ?",23549,,,2019-03-29T06:38:41.773,how to implement word2vec using tensorflow 2.0 keras api ?,tensorflow keras word2vec,1,3,
3306,11517,1,11529,2019-03-29T07:12:37.200,3,71,"according to the wikipedia page of the pssh , this hypothesis seems to be a vividly debated topic in philosophy of ai . but , since it 's about formal systems , should n't it be already disproven by gödel 's theorem ? my question arises specifically because the pssh was elaborated in the 1950s , while gödel came much earlier , so at the time the incompleteness theorems were already known ; in which way does the pssh deal with this fact ? how does it "" escape "" the theorem ? or , in other words , how can it try to explain intelligence given the deep limitations of such formal systems ?",23527,16909,2019-03-29T13:35:32.773,2019-05-07T22:04:12.257,should n't gödel 's incompleteness theorems disprove the physical symbol system hypothesis ?,philosophy symbolic-ai,3,4,1
3307,11519,1,11530,2019-03-29T08:30:21.667,0,44,how could we solve the tsp using an hill - climbing approach ?,19448,2444,2019-03-29T09:47:04.110,2019-03-29T15:33:52.450,how could we solve the tsp using an hill - climbing approach ?,ai-design optimization search applications hill-climbing,1,1,0
3308,11520,1,,2019-03-29T08:44:52.503,1,12,"i ’ve created a variational autoencoder to encode 1-dimensional arrays . the encoding is done through 3 1d - convolutional layers . then , after the sampling trick , i reconstruct the series using 3 fully connected layers . here are my questions in case some can shed some light on it : i think it would be better if i use 1d - deconvolutional layers instead of fully connected , but i can not understand precisely why . it ’s because it would bring better results ? but then , if the fc layer is complex enough should be able to archive the same results , right ? it ’s because would be more efficient ? this is , would get the same results as a complex enough fc layer but with less training and parameters ? or it ’s because of other reasons that i ’m missing . thanks .",22066,,,2019-03-29T08:44:52.503,one dimension deconvolutions or fully connected layers ?,deep-learning convolutional-neural-networks,0,0,
3309,11523,1,,2019-03-29T10:43:00.937,5,112,"as a piece of computer code , it 's tempting to think that artificially intelligent systems ca n't have feelings . so , can an artificial intelligence suffer ?",23529,2444,2019-03-31T12:43:25.360,2019-03-31T12:43:25.360,can an artificial intelligence suffer ?,philosophy emotional-intelligence artificial-life,3,2,
3310,11528,1,,2019-03-29T13:27:52.300,1,42,do you see any ga application that could support project management ? i thought about task dispatching . i am curious about your ideas .,23557,2444,2019-03-29T14:00:42.107,2019-04-28T21:02:30.500,applications of genetic algorithms in project management,genetic-algorithms applications,1,0,
3311,11534,1,,2019-03-29T23:50:12.453,1,21,"super comes from the latin and means "" above "" . university of oxford philosopher nick bostrom defines superintelligence as "" any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest "" . ( wiki ) bostrom 's definition could be taken to imply this is a quantitative measure of degrees as a numeric relationship . ( under this definition , we have achieved narrow superintelligence , reduced to competency in a single task . ) gibson , famously , sheds light on an another aspect via wintermute & amp ; neuromancer , where , once superintelligence is achieved , the ai just f- 's off and does it 's own thing , motivations beyond human comprehensions . ( essentially , "" next - level "" thinking . ) the second measure is discrete and ordinal . is superintelligence discrete or continuous ?",1671,,,2019-03-30T11:48:43.743,is superintelligence a function of strength or a category ?,terminology concepts,1,2,
3312,11535,1,11549,2019-03-29T23:57:01.980,3,85,"i 'm working on understanding vaes , mostly through video lectures of stanford cs231n , in particular lecture 13 tackles on this topic and i think i have a good theoretical grasp . however , when looking at actual code of implementations , such as this code from this blog of vaes i see some differences which i ca n't quite understand . please take a look at this vae architecture visualization from the class , specifically the decoder part . from the way it is presented here i understand that the decoder network outputs mean and covariance for the data distribution . to get an actual output ( i.e. image ) we need to sample from the distribution that is parametrized by mean and covariance - the outputs of the decoder . now if you look at the code from the keras blog vae implementation , you will see that there is no such thing . a decoder takes in a sample from latent space and directly maps its input ( sampled z ) to an output ( e.g. image ) , not to parameters of a distribution from which an output is to be sampled . am i missing something or does this implementation not correspond to the one presented in the lecture ? i 've been trying to make sense of it for quite some time now but still ca n't seem to understand the discrepancy .",21278,2444,2019-03-31T18:44:58.253,2019-04-02T09:10:57.480,do we also need to model a probability distribution for the decoder of a vae ?,computer-vision generative-model autoencoders latent-variable,2,1,
3313,11539,1,,2019-03-30T05:31:04.133,4,68,"often times i see the term deep reinforcement learning to refer to rl algorithms that use neural networks , regardless of whether or not the networks are deep . for example , ppo ( https://arxiv.org/pdf/1707.06347.pdf ) is often considered a deep rl algorithm , but using a deep network is not really part of the algorithm . in fact , the example they report in the paper says that they used a network with only 2 layers . this siggraph project "" deepmimic : example - guided deep reinforcement learning of physics - based character skills "" https://xbpeng.github.io/projects/deepmimic/2018_tog_deepmimic.pdf has the name deep in it and the title even says ' deep reinforcement learning ' , but if you read the paper , you 'll see that their network uses only 2 layers . "" learning to walk via deep reinforcement learning "" https://arxiv.org/pdf/1812.11103.pdf by researchers from google and berkeley . again , deep rl in the title , but if you read the paper , you 'll see they used 2 hidden layers . another siggraph project with deep rl in the title : https://www.cc.gatech.edu/~aclegg3/projects/learning-dress-synthesizing.pdf and if you read it , surprise , 2 hidden layers . "" soft actor - critic : off - policy maximum entropy deep reinforcement learning with a stochastic actor "" https://arxiv.org/pdf/1801.01290.pdf if you read table 1 with the hyperparameters they used : 2 hidden layers . is it standard to just call deep rl to any rl algorithm that uses a neural net ?",17312,2444,2019-03-31T12:26:54.623,2019-03-31T19:30:06.863,is reinforcement learning using shallow neural networks still deep reinforcement learning ?,deep-learning reinforcement-learning terminology deep-rl,2,0,2
3314,11542,1,12084,2019-03-30T08:57:53.047,0,91,"if the ai goal is to serve humans and protect them ( if this ever happens ) and ai someday realizes that humans destroy themselves , will it try to control people for their own good , that is , will it control man 's will to not destroy himself ?",23569,2444,2019-03-30T11:23:54.263,2019-05-01T08:09:43.377,"if the ai goal is the protection of humans , will it always pursue this goal ?",philosophy agi ethics ai-safety,3,1,
3315,11543,1,,2019-03-30T10:23:56.050,0,19,"tl;dr below . you entered a online mini video game . it 's a fast - paced hypercasual game . but it 's a ranked one - top 10 get prizes . the leaderboard resets and prizes are handed out every couple of days . but it 's too time - consuming and brain - exhausting to game to the top spots . so ... you decided to write an ai . at first , the prototype failed to start at all due to runtime bugs . with some coding , it went from nowhere to a snail - crawling speed but stil scoring nothing at all . with more experiment , it finally got a few hundred score . you did some research and optimized the crap out of it then ran it again , it scored a whopping 3500 points , leaving behind the second place just several hundreds of points . hours later , you logged in again and found yourself overwhelmed in joy that you just won the biggest prize . ding ! you got an idea : "" why not just wait until just several hours before the leaderboard closes then i run the ai ? "" . meanwhile , you did some research on the game : some 's been banned due to reaching an unbelievably high scores ( ten - thousands points plus ) messaging to the mod claiming they did it all by themselves . fast - forward , it 's time ! closing all the browser tabs and your code editor hoping to score higher , you realized your ai may have reached superhuman level when it scored 5000 points , almost 2 times that of the second place , twice , before you had to shut it down afraid of being banned . a few minutes after sharing the screenshot of your highscore with your friends , with them telling you "" you 're gunna be banned for sure , man ! "" , you checked in again and ... it happened , you 've been banned ! in panic , you asked yourself : "" what i 've done wrong ? "" . topping the leaderboard in just 2 runs ? or twice in a row ? you entered the game under another name , this time developing a play strategy to not get banned . what would you do ? tl;dr : playing a hypercasual game want to top the leaderboard for prizes leaderboard resets every couple of days wrote a ai to do that ai slowly got better won once leaving the second place a few hundred points found out other players who got banned of an unbelievably highscore ai reached superhuman level ran ai just a few hours before the leaderboard closes scored almost twice the points of second place got banned enter the game again with better scheme what scheme ? suggestions / what i 'm doing : climb up the leaderboard over a couple - of - day period stop the ai around a manually given score range randomly at some point run only few times spaced out every few hours surpass the top score only a few hundred points what i 'm not doing : intentionally not get the top 1 spot extra alternate problem : this time , it 's not a hypercasual game but a board game ( think go ) . players are matched with each other . winner gets points . you write a neural network and train it to play the game automatically . would it be suspicious to the mod ? ( i doubt it ) given that it 's much more complicated than a hypercasual game as it 's a board game , matching with real players , points go up slowly .",23571,,,2019-03-30T10:23:56.050,superhuman ai to disguise as human players,ai-design game-ai theory human-like gaming,0,2,
3316,11546,1,11561,2019-03-30T11:47:15.987,0,154,"can the inputs and outputs of a neural network ( nn ) be a neural network ( that is , neurons and connections ) , so that "" if some nn exist , then edit any nn "" . i think that by creating nns with various inputs and outputs , interacting with each other , and optimizing them with evolution , we can create strong intelligence .",23500,23500,2019-03-31T13:59:53.300,2019-03-31T22:02:29.490,can the inputs and outputs of a neural network be a neural network ?,neural-networks,1,1,
3317,11548,1,,2019-03-30T12:55:25.737,0,5,"for https://github.com/google-research/bert/issues/355 , why does fasttext have out - of - vocabulary and computational complexity problems while wordpiece does not ?",20570,2444,2019-03-30T13:42:20.280,2019-03-30T13:42:20.280,why does fasttext have out - of - vocabulary and computational complexity problems while wordpiece does not ?,natural-language-processing,0,0,
3318,11550,1,,2019-03-30T17:02:03.673,2,26,"in machine learning , there are several metrics to assess the quality of the models : accuracy , precision , recall , f measure , roc ( auc ) , etc . there are cases when certain metrics are more appropriate than others . for example , accuracy is not a good metric when there is class imbalance in the dataset ( although there are ways of attenuate this issue , e.g. resampling ) . roc curves are typically used in binary classifications problems , even though they can also be used in multi - class classification problems ( with some tricks ) . the confusion matrix is also a nice way to visualise the performance of the model ( in the case of multi - class classification ) . i am looking for an as much as possible comprehensive ( which can also be concise ) explanation of when a metric ( at least , the ones mentioned above ) can or should ( or not ) be used , depending on the dataset ( balanced or unbalanced ) , type of problem ( classification or regression ) , type of classification ( binary or multi - class ) , etc . you can eventually add more details and metrics .",2444,2444,2019-03-30T17:10:28.363,2019-03-30T17:10:28.363,when to use which metric in machine learning ?,machine-learning classification supervised-learning metric,0,0,1
3319,11552,1,,2019-03-30T18:09:27.287,2,56,"artificial neural networks ( ann ) are computing systems vaguely inspired by the biological neural networks that constitute animal brains , how do they relate to ai ?",23529,,,2019-04-30T23:00:53.327,what are neural networks and how do they relate to ai ?,neural-networks artificial-neuron,1,2,
3320,11553,1,,2019-03-31T00:08:42.173,1,40,"i 've implemented a2c , but i 'm now wondering why we have multiple actors walk around the environment and gather rewards , why not just have a single agent run in an environment vector ? i personally think this will be more efficient since now all actions can be calculated together by only going through the network once . i 've done some tests , and this seems to work fine in my test . one reason i can think of to use multiple actors is implementing the algorithm across many machines , in which case we can have one agent on a machine . what else reason should we prefer multiple actors ? as an example of environment vector based on openai gym class gymenvvec : def _ _ init__(self , name , n_envs , seed ) : self.envs = [ gym.make(name ) for i in range(n_envs ) ] [ env.seed(seed + 10 * i ) for i , env in enumerate(self.envs ) ] def reset(self ) : return [ env.reset ( ) for env in self.envs ] def step(self , actions ) : return list(zip(*[env.step(a ) for env , a in zip(self.envs , actions ) ] ) )",8689,8689,2019-04-04T13:00:21.883,2019-04-04T13:00:21.883,what is the difference between a2c and running an agent in an environment vector ?,reinforcement-learning,1,2,
3321,11554,1,,2019-03-31T03:04:51.420,1,43,"i have billions of anonymized location coordinates of people movement collected from app . i want to improve user experience by using location data . for example identify if user is at home or at office so that what they view in app changes . where will they be tomorrow at particular hour - so that i can suggest to secure their homes from iot device if they are out . regarding the first point , i tried to use the following rule : at night they are at home , and at day they are at work or school . regarding the 2nd point , i have no idea how to proceed . is there any way i could use ai to predict future location and home location ?",23578,2444,2019-03-31T20:31:42.523,2019-03-31T20:36:31.950,how to predict human future location ?,machine-learning models prediction,1,3,
3322,11557,1,,2019-03-31T10:03:05.573,2,45,"in the paper a simple neural attentive meta - learner , the authors mentioned right before section 3.1 : we preserve the internal state of a snail across episode boundaries , which allows it to have memory that spans multiple episodes . the observations also contain a binary input that indicates episode termination . as far as i can understand , snail uses temporal convolutions to aggregate contextual information , from which causal attention learns to distill specific pieces of information . temporal convolutions does not seems to maintain any internal state , and neither does the attention mechanism they use after this paper . this makes me wonder : "" what is the internal state of a snail ? """,8689,8689,2019-04-01T02:33:01.623,2019-05-31T13:02:00.203,what is the internal state of a simple neural attentive meta - learner(snail ) ?,deep-learning reinforcement-learning meta-learning,1,0,
3323,11563,1,,2019-03-31T18:10:45.487,1,43,"i am trying to understand the similarities and differences between : ( i ) the uct algorithm in kocsis and szepesvári ( 2006 ) ; ( ii ) the uct algorithm in section 3.3 of browne et al ( 2012 ) ; ( iii ) the mcts algorithm in silver et al . ( 2016 ) ; ( iv ) the mcts algorithm in silver et al . ( 2017 ) . i would be really grateful for some help identifying the similarities and differences in these papers , i am doing some research and really struggling right now . ( i ) http://ggp.stanford.edu/readings/uct.pdf ( ii ) http://mcts.ai/pubs/mcts-survey-master.pdf ( section 3.3 ) ( iii ) https://storage.googleapis.com/deepmind-media/alphago/alphagonaturepaper.pdf ( iv ) https://deepmind.com/documents/119/agz_unformatted_nature.pdf",23589,2444,2019-04-01T21:53:00.140,2019-04-01T21:53:00.140,"similarities and differences between uct algorithms in ( i ) , ( ii ) , ( iii ) and ( iv ) ?",algorithm monte-carlo-tree-search alphago alphazero monte-carlo,1,1,
3324,11565,1,11583,2019-03-31T20:18:51.570,1,30,"i am following this tensorflow js tutorial where you load car data . the data looks like this : [ { x:100 , y:20 } , { x:80 , y:33 } ] x is the horsepower of a car , y is the expected miles per gallon usage . after creating the model i save it locally using : async function savemodel ( ) { await model.save('downloads://cars-model ' ) ; } next , i load the model in a separate project , to make predictions without needing the original data . new project async function app ( ) { let model = await tf.loadlayersmodel('./cars-model.json ' ) ; console.log(""car model is loaded ! "" ) ; } i expect to be able to run predict here , on a single number ( say , 120 ) model.predict(tf.tensor2d([120 ] , [ 1 , 1 ] ) ) question i think the number 120 needs to be normalised to a number between 0 - 1 , just like the training data was . but how do i know the inputmin , inputmax , labelmin , labelmax values from the loaded model ? to un - normalise the prediction ( in this case 0.6 ) i also need those original values . how do i normalise / un - normalise data when loading a model ? original prediction code uses label and input values from the original data function testmodel(model , inputdata , normalizationdata ) { const { inputmax , inputmin , labelmin , labelmax } = normalizationdata ; // generate predictions for a uniform range of numbers between 0 and 1 ; // we un - normalize the data by doing the inverse of the min - max scaling // that we did earlier . const [ xs , preds ] = tf.tidy ( ( ) = & gt ; { const xs = tf.linspace(0 , 1 , 100 ) ; const preds = model.predict(xs.reshape([100 , 1 ] ) ) ; const unnormxs = xs .mul(inputmax.sub(inputmin ) ) .add(inputmin ) ; const unnormpreds = preds .mul(labelmax.sub(labelmin ) ) .add(labelmin ) ; // un - normalize the data return [ unnormxs.datasync ( ) , unnormpreds.datasync ( ) ] ; } ) ; const predictedpoints = array.from(xs).map((val , i ) = & gt ; { return { x : val , y : preds[i ] } } ) ; }",11620,11620,2019-04-01T13:06:30.747,2019-04-01T13:06:30.747,how do i normalise / un - normalise data when loading a model ?,tensorflow models javascript,1,0,
3325,11567,1,,2019-03-31T21:07:24.583,8,114,i 've pondered this for a while without developing an intuition for the math behind the cause of this . so what causes a model to need a low learning rate ?,20257,,,2019-04-02T03:19:05.670,what causes a model to require a low learning rate ?,machine-learning hyper-parameters,1,1,1
3326,11571,1,,2019-04-01T00:36:19.820,1,20,"description logic is a fragment of first order logic , but description logic is decidable and first order logic not decidable . why is that ? what is the role of variables in first order logic to make it undecidable ?",23590,2444,2019-04-01T13:25:36.273,2019-04-01T13:25:36.273,why is description logic decidable but first order logic is not decidable ?,logic,1,0,
3327,11574,1,,2019-04-01T03:39:55.907,2,17,"i 'm using lstm to categorize medium - sized pieces of text . each item to be categorized has several free - form text fields , in addition to several categorical fields . what is the best approach to using all this information for categorization ? i see two options : concatenate the text from all fields , preceding each field content with a special token . run concatenated text through lstm . train one model per field . concatenate output from each model in a hidden layer and pass into subsequent layers . what are the benefits of each of the approaches ? is there an alternative i 'm missing ?",23599,,,2019-04-01T03:39:55.907,multi - field text input for lstm,natural-language-processing lstm hidden-layers architecture,0,1,
3328,11575,1,,2019-04-01T04:53:30.843,4,27,"i ’m looking to match two pieces of text - e.g. imdb movie descriptions and each person ’s description of the type of movies they like . i have an existing set of ~5000 matches between the two . i particularly want to overcome the cold - start problem : what movies to recommend to a new user ? when a new movie comes out , to which users should it be recommended ? i see two options : run each description of a person through an lstm ; do the same for each movie description ; concatenate the results for some subset of possible combinations of people and movies , and attach to a dense net to then predict whether it ’s a match or not attempt to augment collaborative filtering with the output from running the movie description and person description through a text learner . are these tractable approaches ?",23599,16565,2019-04-04T20:24:16.393,2019-05-09T21:27:16.143,cold start collaborative filtering with nlp,natural-language-processing recommender-system,1,0,
3329,11576,1,11592,2019-04-01T06:29:19.637,4,91,"is decision tree learning a deterministic algorithm ? given a fixed dataset , does it always produce a tree of a same topology ? what about random forest ?",23601,9947,2019-04-01T06:42:26.160,2019-04-02T08:20:20.393,is decision tree learning a deterministic algorithm ?,decision-tree random-variable,2,0,
3330,11588,1,,2019-04-01T18:41:45.310,1,74,"i am taking ai this semester and we have a semester project . we can choose just about anything . i was wondering if anyone has a creative idea that i might be able to do . nothing that is so extensive that it can not be finished in four weeks . any help would be so appreciated ! some background information : i am a graduate student in cs , but this is my first ai course . my research area is in the space of data mining and analytics . i am open to doing anything that seems interesting and creative .",23622,1671,2019-04-04T17:50:53.600,2019-04-04T17:50:53.600,creative ai semester project ( 4 week time - frame ),ai-design game-ai ai-basics ai-community,2,6,1
3331,11593,1,11601,2019-04-02T06:28:09.127,1,28,"i 'm trying to generate images at minimum of size 128 x 128 with a generative adversarial network . i already tried a sagan pytorch implementation , but i 'm not very happy with results . the images look cool but and i see some correct shape but without explanation you would n't know what the images are about . i have a dataset of 4000 images . lightness , colors and shapes vary a lot , but they are similar in style and on what they portray . with a google cloud v100 gpu the gan would run a week to two with default parameters . does this sound realistic time for this kind of dataset ? it 's definitely not feasible for me . is 4000 images enough to train a gan from scratch ? is there any implementation with pytorch / keras that would be good to get nice results with ?",3579,,,2019-04-02T16:01:08.697,how to get good results with gan and some thousands of images ?,generative-model generative-adversarial-networks,1,0,
3332,11595,1,,2019-04-02T11:15:33.117,0,41,"i 've been working on genetic algorithms & amp ; evolutionary strategies for a while now in a research context . across the vast majority of the articles and content i 've read , every single one of them will either use python , matlab , or java / c++ to build & amp ; benchmark their algorithms . is there an objective reason for these languages to be the single ones used in a research environment ? mainly in contrast with other languages like c # , or javascript , that are almost never used ( despite being some of the most used programming languages in other areas ) , whereas it would definitely be possible to code in practice all current algorithms in them .",23637,,,2019-05-02T16:01:15.730,is there a reason evolutionary algorithms are language - bound in research material ?,genetic-algorithms evolutionary-algorithms programming-languages,1,2,
3333,11596,1,11604,2019-04-02T12:11:14.293,1,38,"i am working on a ddqn with 5 lstm layers and 3 actions as output and state space of 21 features . i am dividing the dataset into episodes of 720 timesteps , for each episode the agent acts greedily for the first 480 steps without training , collecting a replay memory , and then update the parameters each step for the subsequent 240 steps using a window size ( of 96 steps ) randomly sampled from the replay memory ( that always saves the last 480 ) . my problem is that so far the agent learned the optimal policy just once , and it looks like this ( on the test set , training is off ) where , as you can see , the agent dynamically changes its evaluation of the state and acts greedily accordingly . all works fine and the performances are optimal , however , i have to slightly change the normalization of the database and rerun the training to get new parameters fitted to the new database . trying to get to the same result has been proven impossible so far , ( even keeping all settings the same ! ) because most of the time the agent learns to keep its q - values static such as . note : this is an extract of the end of an episode and the beginnig of a new one , the noisy behaviour at the extreme is due to the model being trained for each step , in the middle the training is off and the agent acts greedily ( as i explained above ) . the problem is that the learned parameters give rise to static q values that do not change while the state does and inevitably make the agent stuck in suboptimal strategy , as they never changes actions , even on longer sequences on the test set . the middle part of the second picture , where trainig is off , should look like the first picture , however , i am unable to get back to that optimal behaviour , even keeping all the parameters as they were . any idea on what can be the cause of this anomalous behaviour ?",23638,,,2019-04-02T16:32:12.813,dqn q - values are static,reinforcement-learning q-learning lstm dqn convergence,1,4,1
3334,11599,1,,2019-04-02T14:28:09.703,1,24,"i was thinking about what i can do for my thesis in upcoming academic semester , and i came across an idea . the idea is like : "" if there is any kind of system that generates website designs itself . "" if no , then i can go for it , and i will be lucky if anyone has the same idea . we can collaborate . in case , if there 's any project or system ( open source or not ) that can do this or has been initiated in this context , i want to contribute solely . if anyone has any clue or knowledge over this kind of system , please do inform me . as i have n't done anything like this before , i want to learn . any kind of suggestion or assistance on this idea will be so helpful for me .",23643,2444,2019-04-02T16:23:01.167,2019-04-02T16:23:01.167,is there any system that generates website designs ?,research open-source creativity,1,0,
3335,11609,1,11611,2019-04-02T20:57:54.037,4,97,"i coded a tic tac toe program , but i do n't know if i can call it artificial intelligence . here 's what i did . there is a random player , which always makes random valid moves . and then there is the ai player , which will receive input before every move , that input is the state of the board , and all the posibilities . the ai , will try any move that it has n't tried before . but if it knows every possibility , it will select the one that has the higher value . this value is assigned by the outcome of the match , if the match was won , +1 , 0 for draw , -1 for lose . every move , will be stored in a database , or updated if it 's known . eventually it will know every possible move . i also added a threshold to compare the best moves , so it really select the best move . example , two moves with a value of 100 , the ai will keep trying them both , randomly until one has surpased the other by the threshold , say 50 . it takes about 20.000 games to make the ai perfect , it never loses a game , just draws and wins . i 'm new to ai , and i 'm wondering , could this be really considered artificial intelligence ? and how is this different from a neural network approach ? ( i 'm been reading about it , but i still do n't quite get it . )",23389,,,2019-04-27T11:51:59.593,can this tic tac toe program be considered ai ?,neural-networks game-ai,1,0,1
3336,11610,1,,2019-04-02T22:28:55.777,1,20,"now i 'm using tensorflow.keras to implement the fcn-16s , this picture may be different with others , you should focus this it add pool4 to 2x conv7 not 2x pool5 in fcn-16s . actually the conv7 's output_shape is ( w/32,h/32,classes_number ) in my implementation . i know conv7 should multiply 2 to achieve the same shape ( 1st,2nd ) with pool4 . but my question is the pool4 's shape is ( w/16,h/16,512 ) , ps 512 is the output_channel of the last conv - layer . pool4 and 2x conv7 have not same shape(3rd ) , how to add ? and another question is what are the train - labels for the fcn , i think a train - label is just a image which has the same width and same height with input_image and has classes_number channel to show each pixel 's possibility belong to each class . this idea is derived from the conv7 's output_shape , is it right ? i hope you can give me more and clear explanations . thanks ! ! !",23656,,,2019-04-02T22:28:55.777,how to add the pool4 to the 2 x conv7 in fcn-16s using keras ?,deep-learning convolutional-neural-networks tensorflow keras,0,2,
3337,11612,1,,2019-04-03T02:40:29.227,3,55,"can q - learning ( and sarsa ) be directly used in a partially observable markov decision process ( pomdp ) ? if not , why not ? my intuition is that the policies learned will be terrible because of partial observability . are there ways to transform these algorithms so that they can be easily used in a pomdp ?",4042,12509,2019-04-03T11:39:05.597,2019-04-03T11:43:06.913,can q - learning be used in a pomdp ?,reinforcement-learning q-learning pomdp mdp sarsa,2,9,
3338,11613,1,,2019-04-03T02:45:21.020,2,17,"i am struggling to understand the use of the convolutional sequence to sequence ( conv - seq2seq ) model . the image below is take directly from the paper and is the nearly canonical diagram of the parallel training procedure . after puzzling over it for quite some time , it has come to seem straight forward to me : an input sentence of n tokens can be encoded in one step because the input sentence exists prior to the start of training , and therefore the token - wise convolution can be trivially parallelized . ( compare to rnn encoders , which require n steps ) during training , an output sentence can similarly be parallelized in the decoder because during training , the entire output sentence is known . therefore , during training , the attention function can be fully parallelized in the two dimensional array of dot products shown below finally , during training , the attention is used to weight the input embeddings and encodings , combined with the output training encodings ( as such ) and the final output assembled . this is clearly not the case after the network is trained and evaluation input sequences are translated without reference outputs . i understand from various resources ( including but not limited to gehring 's conference presentation ) that post - training , output sequences are generated token by token in a fashion vaguely similar to earlier architectures , but i can not find a clear description of that process . ( i speculate that this is because the parallel training routine was so revolutionary at the time , that the focus of the publications was rightly on the training routines . ) can someone please help me understand the post - training generation algorithm , if possible in terms of the training diagram ? my current non - confident understanding is that the sentence below would be handled something like the following : prime the decoder with a default token string of & lt;p&gt ; & lt;p&gt ; & lt;s&gt ; , this results ( due to convolution ) in a single decoder encoding ( as such ) input into the attention function , and would hopefully generate the single token & lt;'sie'&gt ; as output restart the decoder with the token string & lt;p&gt ; & lt;p&gt ; & lt;s&gt ; & lt;'sie'&gt ; which would generate two inputs into the attention function and hopefully output & lt;'sie'&gt ; & lt;'stimmen'&gt ; proceed with lengthening input sentences until the final token generated output ends with a & lt;/s&gt ; token signifying the end of the sentence . if that is a correct understanding , can someone confirm it ? if close , can someone correct me ? convolutional sequence to sequence learning , jonas gehring , michael auli , david grangier , denis yarats , yann n. dauphin , 2017",15020,,,2019-04-03T02:45:21.020,convolutional sequence to sequence learning : training vs generation,convolutional-neural-networks sequence-modelling,0,0,
3339,11614,1,,2019-04-03T07:56:29.663,1,14,"cognitive architectures are used in computational psychology to simulate the thinking process . the aim is to build dedicated brain models which are behaving similar to human models which allows the researcher to check and reject previous theories . typical examples for cognitive architectures are the general problem solver ( late 1950s ) , soar ( 1980s ) and act - r ( 1970s-1990s ) . the internal realization of a cognitive architecture is divided into different memory regions . the sensory and working memory can store information for a short amount of time which is less than 1 minute . while the longterm memory consists of a declarative , episodic and semantic memory which are storing facts and events for an unlimited timespan . before a cognitive architecture can take a decision , it needs a prediction model of the environment . for example , a robot in a maze needs to anticipate what will happen , if he is moving forward . the prediction model is located in the simulated brain , because there is not other place for doing so . but at which place exactly ? does it fit to the episodic , semantic or declarative memory ?",11571,,,2019-04-03T07:56:29.663,where is the predictive model located in a cognitive architecture ?,strong-ai cognitive-science,0,0,
3340,11617,1,,2019-04-03T09:55:20.223,1,36,"i will explain my question in relation to chess , but it should be relevant for other games as well : in short terms : is it possible to combine the techniques used by alphazero with those used by , say , stockfish ? and if so , has it been attempted ? i have only a brief knowledge about how alphazero works , but from what i 've understood , it basically takes the board state as input to a neural net , possibly combined with monte carlo methods , and outputs a board evaluation or prefered move . to me , this really resembles the heuristic function used by traditional chess engines like stockfish . so , from this i will conclude ( correct me if i 'm wrong ) that alphazero evaluates the current position , but uses a very powerful heuristic . stockfish on the other hand searches through lots of positions from the current one first , and then uses a less powerful heuristic when a certain depth is reached . is it therefore possible to combine these approaches by first using alpha - beta pruning , and then using alphazero as some kind of heuristic when the max depth is reached ? to me it seems like this would be better than just evaluating the current position like ( i think ) alphazero does . will it take too much time to evaluate ? or is it something i have misunderstood ? if it 's possible , has anyone attempted it ?",17488,,,2019-04-03T09:55:20.223,combining deep reinforcement learning with alpha - beta pruning,deep-learning chess alphazero alpha-beta-pruning,0,0,1
3341,11618,1,,2019-04-03T10:36:45.377,1,19,"tltr : i 'm developing a cnn for a classification task . the data contains multiple classes some of which are very similar to each other and i know these meta - classes . in such a situation is it a good approach to use 2 levels of cnns : 1 . level detect the meta - classes . 2 . level detect the classes within the classified meta class ( of level 1 ) . example : suppose i try to classify the following 9 classes : apple tree , plum tree , cherry tree , sports car , suv car , coupe car , dog , cat , wolf now i could of course use one network on these classes and get a classification output for all of them . but the output ( softmax ) percentage e.g. for an apple tree would be for probably high for any tree class . thus is it a good approach to train and use 2 level of cnns , like this : level classify tree , car , animal -- > trained with all images level classify what kind of tree , car , animal -- > trained only with the subsample of trees , cars , animals so images are checked by cnn level 1 and then depending on its classification with appropriate cnn level 2 . so the questions are : is this a good approach ? does it help in terms of prediction quality / accuracy of the subclasses ? is it easier for an cnn to detect the specific features of a subclass if the input is limited ( like in level 2 ) ? or use another approach ? thanks swad",23665,23665,2019-04-03T13:05:11.527,2019-04-03T13:05:11.527,classification of classes within meta - classes,convolutional-neural-networks classification,0,2,
3342,11621,1,,2019-04-03T13:15:15.903,1,23,"i have a dataset of 100000 documents each labelled with a topic to it . i want to create a model such that given a topic , the model can generate a document from it . i came across language models gpt , gpt-2 , bert . i learned that they can be used for generation purposes . but i did not find anywhere that whether they can generate sentences given only a word . i am inclined to use gpt for my task but am not sure how to proceed with it . i wanted to know whether it is possible or not ? it would be helpful if anyone can help me give a start in the right direction .",19244,19244,2019-04-04T05:21:58.723,2019-04-04T05:21:58.723,generate text from single word / topic using pre - trained language models such as gpt or bert ?,natural-language-processing open-ai,0,2,
3343,11622,1,,2019-04-03T13:28:05.830,1,25,"i have a small dataset ( 117 training examples ) and many features ( 4005 ) . each of the training examples is binary labeled ( healthy / diseased ) . each feature represents the connectivity between two different brain regions . the goal is to assign subjects to one of the two groups based on their brain activity . what methods are there for generating new artificial training examples based on the existing training examples ? an example i could think of would be smote . however , this technique is usually only used to balance unbalanced datasets . this would not be necessary for my set , since it has about the same number of training examples for both label classes .",23672,,,2019-04-03T13:28:05.830,what methods are there to generate artificial training examples based on existing training examples ?,neural-networks machine-learning deep-learning training datasets,0,3,
3344,11623,1,,2019-04-03T13:57:01.983,3,104,"the turing award is sometimes called computer sceince 's nobel prize . this year 's award goes to bengio , hinton , and lecun for their work on artificial neural networks . the actual work contributed by these authors is , of course , quite technical . it centers around the development of deep neural networks , convolutional neural networks , and effective training techniques . the lay press will tend to simplify these results to the point that they lose meaning . i would like to have a concise , and yet still precise , explanation of their contributions to share with a lay audience . so , what is a simplified way to explain the contributions of these researchers ? i have my own ideas and will add them if no other satisfactory answer appears . for a "" lay "" audience , i want to assume someone who had taken a college level course in something scientific but not necessarily computer science . explanations that are suitable for those with even less background are better still though , as long as they do n't lose too much precision .",16909,22365,2019-04-03T18:15:39.653,2019-04-03T18:15:39.653,"what is a simplified way to explain why the ai researchers bengio , hinton , and lecun , won the 2019 turing award ?",neural-networks convolutional-neural-networks soft-question,1,6,2
3345,11626,1,,2019-04-03T15:42:04.977,3,24,"it seems that stacking lstm layers can be beneficial for some problem settings in order to learn higher levels of abstraction of temporal relationships in the data . there is already some discussion on selecting the number of hidden layers and number of cells per layer . my question : is there any guidance for the relative number of cells from one lstm layer to a subsequent lstm layer in the stack ? i am specifically interested in problems involving timeseries forecasting ( given a stretch of temporal data , predict the trend of that data over some time window into the future ) , but i 'd also be curious to know for other problem settings . for example , say i am stacking 3 lstm layers on top of each other : lstm1 , lstm2 , lstm3 , where lstm1 is closer to the input and lstm3 is closer to the output . are any of the following relationships expected to improve performance ? num_cells(lstm1 ) > num_cells(lstm2 ) > num_cells(lstm3 ) [ sizes decrease input to output ] num_cells(lstm1 ) & lt ; num_cells(lstm2 ) & lt ; num_cells(lstm3 ) [ sizes increase input to output ] num_cells(lstm1 ) & lt ; num_cells(lstm2 ) > num_cells(lstm3 ) [ middle layer is largest ] obviously there are other combinations , but those seem to me salient patterns . i know the answer is probably "" it depends on your problem , there is no general guidance "" , but i 'm looking for some indication of what kind of behavior i could expect from these different configurations .",20955,,,2019-04-03T15:42:04.977,how do the relative number of cells between neighboring stacked lstm layers affect the network 's behavior ?,neural-networks machine-learning recurrent-neural-networks lstm,0,0,1
3346,11627,1,,2019-04-03T18:50:29.097,1,86,"in my thesis i dealt with the question how a computer can recognize lego bricks . with multiple object detection , i chose a deep learning approach . i also looked at an existing training set of lego brick images and tried to optimize it . my approach by using tensorflow 's object detection api on a dataset of specifically generated images ( created with blender ) i was able to detect 73.3 % of multiple lego bricks in one foto . one of the main problems i noticed was , that i tried to distinguish three different 2x4 bricks . however , colors are difficult to distinguish , especially in different lighting conditions . a better approach would have been to distinguish a 2x4 from a 2x2 and a 2x6 lego brick . furthermore , i have noticed that the training set should best consist of "" normal "" and synthetically generated images . the synthetic images give variations in the lighting conditions , the backgrounds , etc . , which the photographed images do not give . however , when using the trained neural network , photos and not synthetic images are examined . therefore , photos should also be included in the training data set . one last point that would probably lead to even better results is that you train the neural network with pictures that show more than one lego brick . because this is exactly what is required by the neural network when it is in use . are there other ways i could improve upon this ? ( can you see any further potential for improvement for the neural network ? how would you approach the issue ? do any of my approaches seem poor ? how do you solve the problem ? )",23682,1671,2019-04-04T20:42:22.207,2019-05-04T21:01:50.793,how to detect lego bricks by using a deep learning approach ?,deep-learning image-recognition tensorflow datasets object-recognition,1,0,
3347,11628,1,,2019-04-03T19:09:11.937,0,33,"lately , i have been working on yolov3 and have been trying to train it on x - ray images to detect a fracture . however , i have decided that i would want to increase the number of convolution layers for the neural network to be more accurate . in fact , i am thinking about doubling the number of convolution layers . how should i approach this problem , should i start creating my own object detection algorithm or is there a way to double the number of convolution layers in yolov3 . i want to prioritize accuracy over speed since i want to minimize the possibility of a false - positive or a false - negative . thank you",23546,,,2019-04-03T19:09:11.937,double convolution layers in yolov3,deep-learning convolutional-neural-networks image-recognition optimization object-recognition,0,0,
3348,11629,1,11630,2019-04-03T23:36:00.570,1,63,"i am new to deep learning . suppose that we have a neural network with one input layer , one output layer , and one hidden layer . let 's refer to the weights from input to hidden as $ w$ and the weights from hidden to output as $ v$ . suppose that we have initialized $ w$ and $ v$ , and ran them through the neural network via the feedforward algorithm . suppose that we have calculated $ v$ via backpropagation . when estimating the ideal weights for $ w$ , do we keep the weights $ v$ constant when updating $ w$ via gradient descent given we already calculated $ v$ , or do we allow $ v$ to update along with $ w$ ? so , in the code , which i am trying to do from scratch , do we include $ v$ in the for loop that will be used for gradient descent to find $ w$ ? in other words , do we simply use the same $ v$ for every iteration of gradient descent ?",23687,22916,2019-04-04T20:29:57.210,2019-04-04T20:29:57.210,does backpropagation update weights one layer at a time ?,neural-networks deep-learning,1,1,
3349,11632,1,,2019-04-04T03:23:06.147,0,28,"i am trying to solve a mapping problem on a grid ( 100x100 ) where i have few points , say 10 , where i know the values of a tensor . i have a scalar field , $ v$ , which is related to the tensor field . by related i mean , if the values of is given at each node on the grid , you can get the exact value of $ v$ at each node by solving a pde . i want to train a model to predict $ v$ on the grid based on those 10 values of . is it possible to do so using neural networks ? if so , which kind of network should i begin to play with ? any relevant literature would be appreciated .",23689,23689,2019-04-04T05:16:50.347,2019-04-04T05:16:50.347,"which neural network to use for mapping a vector of size m to a vector of size n , where n > > m ?",neural-networks convolutional-neural-networks recurrent-neural-networks mapping-space,0,0,
3350,11633,1,,2019-04-04T04:02:07.013,1,31,"in section 10.4 of sutton and barto 's rl book , they argue that the discount rate has no effect in continuing settings . they show ( at least for one objective function ) that the average of the discounted return is proportional to the undiscounted average reward $ r(\pi)$ under the given policy . $ ^*$ they then advocate using average rewards rather than the usual returns of the discounted setting . i 've never encountered someone using average rewards ( and no discounting ) in the wild , though . am i just ignorant of some use case , or is pretty much everyone sticking to discounting anyways ? $ $ r(\pi)=\sum_s \mu_\pi ( s ) \sum_a \pi(a|s ) \sum_{s',r}p(s',r|s , a)r$$ is the stationary state distribution while following policy . $ ^*$ their proof did use the fact that the mdp was ergodic . i 'm not sure how often that assumption holds in practice .",22916,,,2019-04-04T04:02:07.013,does everyone still use discount rates ?,reinforcement-learning discount-factor mdp,0,0,
3351,11634,1,,2019-04-04T04:19:09.840,1,31,"is any classifier not subject to fooling as in here ? i agree that the question is related to the other one as commented by philip . but i guess it is not completely a duplication as pointed out by hisairnessag3 . what i wanted to ask is that any classifiers inherently do not subject ( or less prone ) to attack . i have a feeling that non - linear classifiers should be less susceptible to attack . btw , any benchmark on say , simple k - nearest neighbor classifiers is available ?",23688,23688,2019-04-04T20:07:12.727,2019-05-05T00:01:26.280,is any classifier not subject ( or less susceptible ) to fooling ?,machine-learning,1,1,
3352,11636,1,,2019-04-04T04:48:12.557,2,46,"i am working with a dataset where each input sample is a matrix , and the output corresponding to each input is also a matrix ( of shape ( 400 , 10 ) ) . the input samples do not have translation invariance . each output image has shape ( 16 , 16 ) . the output matrices have translation invariance . i want to build a neural network which can learn how to predict the output images from the aforementioned data . it seems to me that one needs to think of a neural network here which does regression on the output images to learn . presently , i am using 1000 data samples for learning in the neural network ( input samples and corresponding output images ) . what is the best way to build a neural network for this task ? presently , i am using multi - layer perceptron ( mlp ) with mean square error ( mse ) loss for this task . i flatten the input matrices before i feed them into the mlp , and use a standard mlp with multiple hidden layers ( 4 - 5 ) with many hidden units for this task . while a visual inspection shows that the true and predicted output images are in relatively good agreement for training data , i find a mismatch between true and predicted ( reconstructed ) output images for validation data . the bottom plot shows pictures of true and predicted ( reconstructed ) output images for training and validation data for chosen samples . presently , i am using training and validation loss curves ( with respect to iteration ) to measure performance . i want to have a robust metric for comparison which can tell me whether the prediction is a random image or not . how can i get the model to generalize to the validation set better ? the python code that i am using for this mlp and the required data can be found here and here respectively .",22566,2444,2019-04-04T10:14:23.660,2019-04-04T10:14:23.660,how to build a neural network that can learn to predict output images ?,neural-networks machine-learning deep-learning,0,3,
3353,11637,1,,2019-04-04T08:12:22.690,1,33,"iqn paper ( https://arxiv.org/abs/1806.06923 ) uses distributional bellman target : $ $ \delta^{\tau,\tau'}_t = r_t + \gamma z_{\tau'}(x_{t+1 } , \pi_{\beta}(x_{t+1 } ) ) - z_{\tau}(x_t , a_t ) $ $ and optimizes : $ $ l = \frac{1}{n ' } \sum^{n}_i \sum^{n'}_j \rho^\kappa_{\tau_i } \delta^{\tau_i,\tau_j}_t $ $ but similar quantiles can be got just from q values , when doing so : $ $ \delta^\tau_t = r_t + \gamma \frac{1}{n ' } \sum_{j}^{n ' } z_{\tau_j}(x_{t+1 } , \pi_{\beta}(x_{t+1 } ) ) - z_\tau(x_t , a_t ) \\ = r_t + \gamma q ( x_{t+1 } , \pi_\beta(x_{t+1 } ) ) - z_\tau(x_t , a_t ) $ $ optimizing : $ $ l = \sum^n_i \rho^{\kappa}_{\tau_i } \delta^{\tau_i}_t $ $ both lead to similar performance on cartpole env . the loss function of the 2nd one is more simpler and intuitive ( atleast to me ) . so i was thinking if there are any obvious reason why authors didin't use it ?",18808,18808,2019-04-04T10:40:30.217,2019-04-04T11:27:03.897,iqn bellman target : using z vs using q,deep-learning reinforcement-learning papers atari-games,1,0,
3354,11639,1,,2019-04-04T14:22:51.743,2,24,"policy learning refers to mapping an agent state onto an action to maximize reward . a linear policy , such as the one used in the augmented random search paper , refers to learning a linear mapping between state and reward . when the entire state changes at each time - step , for example in the continuous mountain car openai gym , the position and speed of the car changes at each time - step . however , assume we also wanted to communicate the constant position of one or more goals . by "" constant "" , i mean does not change within a training episode , but may change between episodes . for example , if there was a goal on the left and right of the mountain car . are there examples of how this constant / static information be communicated from the environment other than appending the location of the two goals to the state vector ? can static / constant state be differentiated from state which changes with each action ?",23703,23703,2019-04-05T15:03:44.873,2019-04-05T15:03:44.873,inform policy learning of environment constants,reinforcement-learning,1,3,
3355,11640,1,,2019-04-04T14:40:34.553,1,51,"i 'm learning ddpg algorithm by following the following link : open ai spinning up document on ddpg , where it is written in order for the algorithm to have stable behavior , the replay buffer should be large enough to contain a wide range of experiences , but it may not always be good to keep everything . what does this mean ? is it related to the tuning of the parameter of the batch size in the algorithm ?",23707,2444,2019-04-04T18:59:24.590,2019-05-04T22:40:36.117,how large should the replay buffer be ?,reinforcement-learning deep-rl policy-gradients experience-replay ddpg,1,5,
3356,11643,1,11653,2019-04-04T15:26:30.523,0,24,i am making a nn library without any other external nn lib and is implementing the flatten layer . i know the forward implementation of flatten layer but is the backward just reshaping it or not ? if yes is it i can just call a simple numpy reshape function to reshape it ?,23713,,,2019-04-04T19:52:23.580,back propagation on flatten layer in cnn,neural-networks convolutional-neural-networks backpropagation,1,0,
3357,11648,1,11652,2019-04-04T17:43:07.613,1,44,"what is the motivation behind using a deterministic policy ? given that the environment is uncertain , it seems stochastic policy makes more sense .",23707,2444,2019-04-04T19:12:14.107,2019-04-04T19:32:49.473,what is the motivation behind using a deterministic policy ?,reinforcement-learning deterministic-policy,1,1,
3358,11654,1,,2019-04-04T19:56:50.540,0,11,"my question is only with regards to the feedforward part of an rnn . i am following these steps . i am working on prediction of a time series . the time series is a toy model generated by me . it is composed by 200 numbers : 150 for train and 50 for validation . given a sequence of 50 numbers , it should predict the 51st number . if x1=1,2, .... 50 , then y1=51 . if x2=2,3, ..... 51 , then y2=52 and so on . i have 100 inputs and 100 outputs . i do n't understand how this sequence is related to the simple architecture of an rnn . in this architecture the hidden(t ) is obtained by input * hiddenmat * hiddenmat(t-1 ) . do i have to sequentially feed each input to the different rnns extended in time and calculate all the outputs with the global loss over the time span ? then i need 100 input neutrons ? given an input sequence length , what is the number of input neurons i need ? thank you for your help ! it seems a silly question but i got stuck conceptually on this .",23717,22916,2019-04-04T20:38:29.337,2019-04-04T20:38:29.337,how is the length of an input sequence related to the structure of an rnn ?,deep-learning recurrent-neural-networks loss-functions feedforward,0,0,
3359,11655,1,,2019-04-04T20:10:23.970,2,44,"that has been discussion on this . maybe from hinton himself . and i heard that many max pooling layers have been replaced by conv layers in recent years , is that true ?",23688,,,2019-04-05T05:55:17.730,is max pooling really bad ?,neural-networks machine-learning deep-learning,2,0,
3360,11659,1,,2019-04-04T22:25:48.643,0,11,"i want to restore missing parts from a character . you can see some examples after the pages are edited with scantailor : note that i want to keep the same font , so i do n't want just to ocr and create the pages of the scanned book with some new font . my question is : can i use machine learning to solve this problem ?",23715,,,2019-04-04T22:25:48.643,denoising and improving the quality of scanned books,machine-learning,0,2,
3361,11661,1,,2019-04-05T03:26:43.337,1,21,"i wanted to get some opinions from the community for a certain problem that i will be approaching . the problem is to provide feedback to a user based on a image of the upper male torso . the image would either reflect something positive like increasing muscle mass or decreasing muscle mass or both and gaining adipose tissue would be seen as negative as well as muscle atrophy . using the users input such as ( sleep data , food , training routine ) among some other data i would like to provide feedback such as "" no john , this exercise has not yielded desirable results "" or "" a combination of your recent dietary change has caused strength loss "" obviously this is a complex issue and has a lot of interconnected variables and potentials but you get the idea high - level at least and if you do n't - please ask . so my idea so far would be to use a cnn that holds the picture of the torso , using a softmax function we could run this through a model to estimate bodyfat and doing the same with a model trained on muscle mass using those two models we could paint a pretty accurate picture of someones physique if they 're going in the right direction or not ; we could then go on to analyse what that user may have done / has not done to yield a result - obviously there would be connected models here and many different combinations of algorithms applied such as cnn , rnn and others . really curious to hear your response(s ) thank you in advance .",23726,23726,2019-04-05T13:45:16.197,2019-04-05T13:45:16.197,architecture and use of different algorithms for health goal feedback,machine-learning architecture model-based,0,1,
3362,11666,1,,2019-04-05T06:34:04.157,4,103,"in computer vision is very common to use supervised tasks , where datasets have to be manually annotated by humans . some examples are object classification ( class labels ) , detection ( bounding boxes ) and segmentation ( pixel - level masks ) . these datasets are essentially pairs of inputs - outputs which are used to train convolutional neural networks to learn the mapping from inputs to outputs , via gradient descent optimization . but animals do n't need anybody to show them bounding boxes or masks on top of things in order for them to learn to detect objects and make sense of the visual world around them . this leads me to think that brains must be performing some sort of self - supervision to train themselves to see . what does current research say about the learning paradigm used by brains to achieve such an outstanding level of visual competence ? which tasks do brains use to train themselves to be so good at processing visual information and making sense of the visual world around them ? or said in other words : how does the brain manage to train its neural networks without having access to manually annotated datasets like imagenet , coco , etc . ( i.e. what does the brain use as ground truth , what is the loss function the brain is optimizing ) ? finally , can we apply these insights in computer vision ? update : i posted a related question on psychology & amp ; neuroscience stackexchange , which i think complements the question i posted here : check it out",12746,12746,2019-04-11T21:03:17.333,2019-04-11T21:03:17.333,which loss function is the brain optimizing in order to learn advanced visual skills without expert / human supervision ?,machine-learning computer-vision cognitive-science biology,2,6,1
3363,11667,1,11675,2019-04-05T08:34:13.667,1,60,"i am new to deep learning and trying to understand the concept of back propagation . i have a doubt on when the back propagation is applied . assume that i have a training data set of 1000 images for handwritten letters , does back propagation applied immediately after getting the result for each input data or after getting the result data for all items in a batch ? does back propagation applied n times till the neural network gives satisfactory result for an input data before going to work on the next input data ?",23734,2444,2019-04-05T14:19:37.673,2019-04-05T14:38:12.567,is back propagation applied for each data point or for a batch of data points ?,neural-networks machine-learning deep-learning backpropagation gradient-descent,1,0,
3364,11669,1,,2019-04-05T09:08:56.710,0,94,forward kl divergence ( also known as cross entropy loss ) is a standard loss function in supervised learning problems . i understand why it is so : matching a known a trained distribution to a known distribution fits $ p \log(p / q)$ where $ p$ is the known distribution . why is n't the reverse kl divergence commonly used in supervised learning ?,23723,2444,2019-04-05T12:21:04.883,2019-04-08T08:19:02.583,why is n't the reverse kl divergence commonly used in supervised learning ?,machine-learning optimization loss-functions supervised-learning,1,2,1
3365,11671,1,,2019-04-05T10:06:49.173,0,20,"conventional nvidia gpus , such as the titan and or gt1080 , are used to train ai models . why would a jetson nano board be less than ideal as a substitute for a conventional gpu ? context i would like to run simple concept demonstrator training exercises with and without a gpu : that being said , i am not interested in buying a desktop and outfitting it with a $ 1000 + gpu for said exercise .",18819,18819,2019-04-05T10:59:14.080,2019-04-05T10:59:14.080,why is a nvidia single board computer less than ideal for ai model training ?,training gpu,1,2,
3366,11672,1,,2019-04-05T10:47:53.840,2,56,"i read that functions are used as activation functions only when they are differentiable . what about the unit step activation function ? so , is there any other reason a function can be used as an activation function ( apart from being differentiable ) ?",23501,2444,2019-04-05T19:45:31.297,2019-05-22T04:10:16.957,what kind of functions can be used as activation functions ?,neural-networks machine-learning activation-function,1,0,
3367,11674,1,,2019-04-05T11:29:49.410,2,25,"suppose a model m classifies apples and oranges . can m be extended to classify a third class of objects , e.g. , pears , such that the new images for ' retraining ' only have pears annotated and apples and oranges ignored ? that is , since m already classifies apples and oranges , can the old weights be somehow preserved and let the retraining focus specifically on learning about pears ? methods such as fine - tuning and learning without forgetting seem to require all objects in the new images annotated though .",13068,,,2019-04-06T23:52:04.330,extending a neural network to classify new objects,neural-networks deep-learning convolutional-neural-networks,1,0,
3368,11676,1,11682,2019-04-05T14:23:21.970,3,42,"in sutton & amp ; barto 's book ( 2nd ed ) page 149 , there is the equation 7.11 i am having a hard time understanding this equation . i would have thought that we should be moving $ q$ towards $ g$ , where $ g$ would be corrected by importance sampling , but only $ g$ , not $ g - q$ , therefore i would have thought that the correct equation would be of the form $ q \leftarrow q + \alpha ( \rho g - q)$ and not $ q \leftarrow q + \alpha \rho ( g - q)$ i do n't get why the entire update is weighted by and not only the sampled return $ g$ .",22003,2444,2019-04-05T14:33:50.430,2019-04-05T20:44:26.283,understanding the n - step off - policy sarsa update,reinforcement-learning rl-an-introduction off-policy temporal-difference sarsa,1,3,
3369,11678,1,,2019-04-05T16:13:24.633,1,59,"i am new to nlp realm . if you have an input text "" the price of orange has increased "" and output text "" increase the production of orange "" . can we make our rnn model to predict the output text ? or what algorithm should i use ?",23743,2444,2019-04-05T16:29:01.640,2019-05-12T06:01:20.427,which algorithm should i use to map an input sentence to an output sentence ?,machine-learning natural-language-processing recurrent-neural-networks,2,0,
3370,11679,1,,2019-04-05T18:23:46.233,8,129,"the tabular q - learning algorithm is guaranteed to find the optimal $ q$ function , $ q^*$ , provided the following conditions regarding the learning rate are satisfied where means the learning rate used when updating the $ q$ value associated with state $ s$ and action $ a$ at time time step $ t$ , where $ 0 \leq \alpha_t(s , a ) & lt ; 1 $ is assumed to be true , for all states $ s$ and actions $ a$ . apparently , given that $ 0 \leq \alpha_t(s , a ) & lt ; 1 $ , in order for the two conditions to be true , all state - action pairs must be visited infinitely often : this is also stated in the book reinforcement learning : an introduction , apart from the fact that is "" widely "" known , and it is rationale behind the usage of the -greedy policy ( or similar ) during training . a complete proof that shows that $ q$ -learning finds the optimal $ q$ function can be found in the paper convergence of q - learning : a simple proof ( by francisco s. melo ) . he uses concepts like contraction mapping in order to define the optimal $ q$ function ( see also what is the bellman operator in reinforcement learning ? ) , which is a fixed point of this contraction operator . he also uses a theorem ( n. 2 ) regarding random process that converges to $ 0 $ , given a few assumptions . ( the proof might not be easy to follow , if you are not a math guy . ) i have heard that when we use a neural network to represent the $ q$ function , the convergence guarantees of $ q$ -learning do not hold anymore . why exactly is that ? why does n't q - learning converge when using function approximation ? is there a formal proof of such non - convergence of $ q$ -learning using function approximation ? i am looking for different types of answers , from those that give just the intuition behind the non - convergence of $ q$ -learning when using function approximation to those that provide a formal proof ( or a link to a paper with a formal proof ) .",2444,,,2019-04-06T09:16:00.397,why does n't q - learning converge when using function approximation ?,reinforcement-learning q-learning deep-rl proofs function-approximation,2,2,4
3371,11683,1,,2019-04-06T00:50:34.073,5,67,"does anyone know what specific tasks the opencog environment is capable of performing ? i have glanced though their wiki and a few of the pages on goertzel 's site and the ai.se . so far i could only find some technical documentation regarding theory and engineering , but nothing on concrete results . from the technical description of atomspaces it seems that opencog is capable of some "" representational inference "" , but i have n't come across any sources that concretely describes what it is capable of doing . apparently there is some collaboration between sophia the robot and opencog , but to what extent i am unclear . i am aware however that the dialogue functions is powered by chatscript ( though i also suspect that the high profile interviews sophia gives are completely scripted ... ) does anyone know what feats the opencog environment is capable of ?",6779,2444,2019-04-06T08:18:46.867,2019-04-06T23:41:24.293,what can opencog do ?,applications agi open-cog,1,1,
3372,11684,1,,2019-04-06T03:15:00.873,3,33,i have not seen a neuron that uses both a bias and a threshold . why is this ?,23501,22916,2019-04-06T19:55:00.393,2019-04-06T19:55:00.393,can a neuron have both a bias and a threshold ?,neural-networks neurons perceptron,1,3,
3373,11685,1,,2019-04-06T06:52:05.100,2,21,post pruning is start from downward discarding subtree and include leaf node performance . so what is the best point or condition of the tree where we have to stop further pruning .,23501,,,2019-04-06T17:29:52.440,at which point we have to stop post pruning in decision tree ?,machine-learning decision-tree,1,4,
3374,11689,1,,2019-04-06T09:02:28.557,3,121,"i 'm trying to minimize the power consumption in wireless networks and i have some constraints such as that the sinr should not pass the threshold and the power should be between the 0 and maximum power and other constraints . i 'm trying to formulate my problem to be similar to equation 8 in this paper ' distributed optimization for coordinated beamforming in multicell multigroup multicast systems : power minimization and sinr balancing . \begin{equation } \min \sum_{i=1}^{n}p(i ) \end{equation } subject to \begin{equation } \sum_{i=1}^{n } status_i\leq n \nonumber , \end{equation } \begin{equation } status_i= \begin{cases } 1 , & amp ; \text{if node i is active } \\ 0 , & amp ; \text{otherwise } \end{cases } \nonumber , \end{equation } \begin{equation } 1-p(c_{i , k}&gt;t_f ) \leq p_{cov } \nonumber . \end{equation } \begin{equation } 0\le p(i ) \le p_{max } \end{equation } \begin{equation } \gamma_{k_i , k_j } \le \theta \end{equation } where $ n$ represents the number of the nodes while $ status_i$ shows if the node is active or not . $ t_f$ and $ \theta $ represent the sinr threshold for the node . the coverage probability $ p_{cov } = p[\gamma_{k_i , k_j}&gt ; \theta ] $ . i need to convert this problem to be represented as multi - agent reinforcement learning . the problem is that i do n't know how to define the state , the action , and the reward . can someone help me ?",21181,21181,2019-04-07T05:41:36.567,2019-04-08T14:01:21.497,how can i convert the problem formulation to multi - agent reinforcement learning ?,reinforcement-learning models applications multi-agent-systems,0,11,
3375,11691,1,,2019-04-06T10:03:57.300,1,30,"i want to see if i can make my software defined radio , sdr , to classify unknown radio signals with the help of an artificial neural network . that is , my sdr outputs a sequence of complex numbers ( iq - data ) , which i want to use to determine if the receieved signal is , for instance , fm or am modulated . this approach was used in a paper ( https://arxiv.org/pdf/1712.04578.pdf ) and they created and used a freely downloadable dataset ( https://github.com/sofwerx/deepsig_datasets/blob/master/readme.md ) . being new to both sdr and deep learning i have now tried for a couple of months to create an lstm network , train it on the dataset and then use it for classification , but have sadly failed . i have concluded that it is likely due to the fact that i do not seem to understand how the dataset is structured . i have not been able to find any documentation concerning this dataset ( other than a text file with a list of the modulation forms used in the dataset ) though . my hope is that someone on this forum has some prior experience to share about how to use it . the dataset ( deepsig dataset : radioml 2016.10a ) is split in three matrixes , x : 2x1024x2555904 cells , y : 24x2555904 cells and z : 1x2555904 cells . my belief has up until now been that "" x "" contains the complex time series , i.e. one row for the real component and one row for the imaginary component both in a sequence of 1024 samples . the "" y "" to contain the corresponding 24 classes , whereas the "" z "" somehow contain the signal to noise level for each signal . after training and deploying the network in matlab i found out that it does not even manage to classify the local fm radio station . i then looked at the data i had used for training and plotted one of ( what i thought to be ) the fm sample sequences from the dataset in the complex plane , but i did not get the "" circle "" i had expected from a frequency or phase modulated signal . i am thus totally lost , the dataset has been used for a high class scientific paper so the problem lies with me , and my lack of understanding . i apologize for the long and still very unconcise question , but i do n't want to infer any of my own misconceptions into the query . thanks for any help !",23760,,,2019-04-06T10:03:57.300,deep learning for radio signal classification with deepsig dataset,datasets matlab,0,0,
3376,11694,1,,2019-04-06T18:44:13.170,2,37,"i trained a recurrent neural network ( if it matters - it contains three cudnnlstm cells and 3 dense layers , dropout = 0.2 ) . the result of data preparation is one array of ~330.000 sequences . each contains 256 time steps and 24 features in each time step . this array is normalized , shuffled and balanced . then it is split into two arrays - train array contains 90 % of data ( so ~297k ) and validation array contains ~10 % . during training process ( adam optimizer , 128 or 256 batches ) max accuracy of validation data set is 90 % . epoch accuracy is 94 % . then i run my additional validation test script with more realistic , unbalanced data set . the accuracy of test is just ~50 % . i checked this second test if it is correct and there is no errors in the code , but when i run it on data , that was included in the training set , the accuracy was 87 % , so it looks good . what is going on ? i suppose , that wrong architecture is used . here is a graph of epoch accuracy during training : here is a graph of validation accuracy during training thank you for support . update : today i trained the network one more time . i used the unbalanced data from second test as validation data set in traininge process . you can see the results below.i stopped training after 29 epochs . blue : balanced valitadion data set orange : unbalanced validation data set epoch accuracy : validation accuracy validation loss it does n't look good at all .",21171,21171,2019-04-07T15:11:26.233,2019-04-07T15:11:26.233,rnn : different test results on balanced and unbalanced data,neural-networks tensorflow lstm intelligence-testing,0,2,
3377,11701,1,,2019-04-07T13:47:59.910,2,37,"there are lot of researches about face detection in pictures , but is it the only way one can say "" this person i 'm looking for is here in this picture "" ? are n't there algorithms that you can provide with information like lateral or back pictures of a person and making calculations on the height , the width , the anatomic distance between parts of its body , the analysis of the hair can determine : "" there 's a x per cent chance this is the person you 're looking for "" ? is it possible to accomplish ?",15764,2444,2019-04-07T16:37:10.513,2019-04-07T16:37:10.513,algorithms to indentify people in pictures without using face recognition,neural-networks machine-learning convolutional-neural-networks image-recognition,0,0,
3378,11702,1,,2019-04-07T14:06:11.917,1,60,"there 's been a lot of apocalyptic hype ( on the media ) about cm1k and other similar technologies . elon musk has voiced fears about it taking over the world , which unfortunately are not unfounded . my understanding of these devices is that there 's no software , that they more or less act on their own volition , which makes them almost actually alive . but i 've never used one , so that 's probably not entirely correct . has anyone here ever used a cm1k ( or something similar ) ? if so , what exactly does it do ?",,9947,2019-04-09T11:34:26.560,2019-04-09T11:34:26.560,what does the cm1k chip ( which uses zisc ) exactly do ?,pattern-recognition hardware neuromorphic-engineering,0,7,
3379,11703,1,,2019-04-07T16:07:04.643,1,36,"i have multiple image sequences , each of which contains an animation of two moving dots . the trajectory of the dots in a sequence is always cyclic ( not necessarily circular ) . there are two types of sequences . in some sequences the two dots are moving in phase but in the other sequences they are moving out of phase . is it possible to classify these two types of sequences using a neural network ? what is the simple and feasible neural network structure for this classification ? here 's an example set of animations . the in - phase patterns are in the above row and the out - of - phase patterns are in the bottom row .",23789,23789,2019-04-21T07:46:19.947,2019-04-21T07:46:19.947,what is the feasible neural network structure that can learn to identify types of trajectory of moving dots ?,recurrent-neural-networks deep-network,0,0,
3380,11704,1,,2019-04-07T16:07:58.180,1,16,"a simple word search seems too simple to solve with a computer ai . but what i 'm interested is how human 's solve it . they build up strategies over the course of solving the puzzle . for example : 1 ) first just look to see if any words "" jump out "" . 2 ) look for the horizontal words . 3 ) look for the letter "" o "" 4 ) look for the letter "" p "" next to a letter "" o "" 5 ) methodically look along rows and columns and diagonals . 6 ) cross off words 7 ) take the first letter of a word in the list and put it in memory . things of that sort . i would like to build such a program . with built in search capabilities , some of which are faster than others . and the ai can combine different search methods and try to solve the puzzle as fast as possible . it should also store strategies that work . or think about why strategies work sometimes and not others . i would like to read a bit more about ai and strategies if you know any good references ?",4199,,,2019-04-07T16:07:58.180,has any research been done to solve word searches with ai ?,ai-design,0,0,
3381,11706,1,,2019-04-07T17:45:38.380,4,21,"recently , some work has been done planning and learning in non - markovian decision processes , that is , decision - making with temporally extended rewards . in these settings , a particular reward is received only when a particular temporal logic formula is satisfied ( ltl or ctl formula ) . however , i can not find any work about learning which rewards correspond to which temporally extended behavior . in my searches , i came across k - order mdps ( which are non - markovian ) . i did not find rl research done on k - order mdps .",23792,16920,2019-04-08T09:23:29.140,2019-04-08T09:23:29.140,what research has been done on learning non - markovian reward functions ?,reinforcement-learning rewards markov-property,0,3,
3382,11708,1,,2019-04-07T19:34:07.950,1,38,"if i have a dqn , and i care a lot about future rewards ( moreso than current rewards ) , can i set gamma to a number greater than 1 ? like 1.1 perhaps ?",23719,,,2019-04-08T06:39:54.933,can gamma be greater than 1 in a dqn ?,q-learning dqn,2,0,
3383,11710,1,11712,2019-04-08T04:40:34.507,2,21,"i 've been reading a lot of tutorials on dqns for cartpole . in many of them , they have the funnel layer of the neural net be a linear activation . why is this ? is it just a choice made by the implementer ? is this choice specific to cartpole , or do most control task dqns use it ? thanks .",23803,,,2019-04-08T07:18:07.650,why do dqns use linear activations on cartpole ?,neural-networks dqn,1,0,
3384,11716,1,11728,2019-04-08T09:08:16.597,4,64,"i 'm training both kind of agents against an environment but dqn performs significantly better than double dqn . as i 've saw here , double dqn use to perform better than dqn . am i doing something wrong or is it possible ?",22930,,,2019-04-09T07:35:29.923,can dqn perform better than double dqn ?,reinforcement-learning q-learning keras dqn,2,0,
3385,11717,1,,2019-04-08T10:04:21.293,3,70,"in the paper deep recurrent q - learning for partially observable mdps , the drqn is described as dqn with the first post - convolutional fully - connected layer replaced by a recurrent lstm . i have dqn implementation with only two dense layers . i want to change this into drqn with the first layer as an lstm and leave the second dense layer untouched . if i understood correctly , i would also need to change the input data appropriately . are there any other things that need to be modified in order to make drqn work ?",22162,2444,2019-04-09T17:16:00.457,2019-04-09T17:16:00.457,what can be considered a deep recurrent neural network ?,reinforcement-learning ai-design lstm dqn deep-rl,0,2,
3386,11718,1,11721,2019-04-08T13:09:17.977,2,53,"i am currently using nvidia gtx1050 with 640 cuda cores and 2 gb gddr5 for deep neural network training . i want to buy a new gpu for training , but i am not sure how much performance improvement i can get . i wonder if there is a way to roughly calculate the training performance improvement by just comparing gpus ' specification ? assuming all training parameters are the same . i wonder if i can roughly assume the training performance improvement is x times because the cuda core number and memory size increased x times ? for example , is rtx2070 with 2304 cuda cores and 8 gb gddr6 roughly 4 times faster than gtx1050 ? and is rtx2080ti with 4352 cuda cores and 11 gb gddr6 roughly 7 times faster than gtx1050 ? thanks .",21213,,,2019-04-10T10:39:20.223,can i calculate the training performance of gpus by comparing their specification ?,training gpu,1,0,
3387,11719,1,11722,2019-04-08T13:21:05.430,0,28,"i 'm a bit of a cnn newbie , and i 'm trying to train one to image classify pictures of pretty similar looking particles . i 'm making the inputs and labels by hand from a set of 48x48 grayscale images , and labeling them with a one - hot vector based on their position in the sequence ( for example , the 400/1000th image might have a one - hot in the 4th position if i have 10 categories in the run ) . i 'm using sigmoidal output activation and categorical cross entropy loss . i 've played around with a few different optimizers , as well . i 'm implementing in python keras . unfortunately , although i have pretty good accuracy numbers for the training and validation , when i actually look at the outputs being produced , it generally gives multiple categories , which is not at all what i want . for example , if i have 6 categories and a label of 3 , it might give the following probability vector : [ .99 .98 1.0 .99 0.02 0.05 ] it was my understanding that categorical cross entropy would not allow this type of categorization , and yet it is prevalent in my code . i am under the impression that i 'm doing something fundamentally wrong , but i ca nt figure out what . any help would be appreciated .",23812,22916,2019-04-11T21:02:53.103,2019-04-11T21:02:53.103,cnn output generally has more than one category in one - hot categorization ?,convolutional-neural-networks image-recognition training categorical-data,1,3,
3388,11724,1,,2019-04-08T15:59:38.870,0,32,"it must be a very stupid question , but since i have not such sufficient know ledge storage and having no more time to search the answer of it , i have to put it here to ask for help . i generated a training dataset of images of simple geometric shapes as triangles , squares , diamonds etc . by programs and constructed a cnn with two convolutional layers and one pooling layer also a final fully connected layer to learn the classifications of these shapes . but the network just does not to learn it . i mean the loss just does not decrease . what is the cause ?",14948,,,2019-04-08T15:59:38.870,cnn does n't learning simple geometric patterns,convolutional-neural-networks,0,4,
3389,11725,1,,2019-04-08T16:11:48.077,0,27,"how can the bias , $ b_0 $ , and the coefficient for the single input value , $ b_1 $ , be calculated in logistic regression ? i am trying to calculate these coefficients with pure python . can anybody provide some example or formulas ? ps : problem was solved with : https://machinelearningmastery.com/logistic-regression-for-machine-learning/ https://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf",21708,21708,2019-04-08T20:06:40.873,2019-04-08T20:06:40.873,how can the bias and the coefficient be calculated in logistic regression ?,machine-learning python logistic-regression,0,3,
3390,11726,1,11727,2019-04-08T16:29:47.503,0,29,i am new to deep learning . i have doubts on modifying bias values during back propagation . my doubts are does the back propagation algorithm modifies the weigh values and bias values in the same pass ? how does the algorithm decide whether it has to change the weight value or bias value to reduce the error in a pass ? will the learning rate same for bias and weights ? thanks !,23734,,,2019-04-08T16:57:43.260,when is bias values updated in back propagation ?,deep-learning convolutional-neural-networks backpropagation gradient-descent,1,0,
3391,11729,1,,2019-04-09T05:10:55.027,1,50,gradient descent can get stuck into local optimum . which techniques are there to reach global optimum ?,23501,2444,2019-04-27T16:18:50.447,2019-04-27T16:18:50.447,how can we reach global optimum ?,machine-learning optimization gradient-descent,1,1,
3392,11732,1,,2019-04-09T08:59:57.053,0,15,"yesterday i tried to transform a picture in the artistic style using cnns based on a neural algorithm of artistic style by leon a. gatys , alexander s. ecker , and matthias bethge using a recent torch implemenation , as it is explained here : https://github.com/mbartoli/neural-animation it started the conversion correctly , the problem is that the process is very time consuming , after 1 hour of elaboration a simple picture was not fully transformed . and i have to trasform 1615 pictures . what 's the solution here ? can i use the google cloud platform to make this operation faster ? or some other kind of cloud service ? using my home pc is not the right solution . if i can use the cloud power , how can i configure everything ? let me know , thanks .",23825,,,2019-04-09T09:23:06.560,using the cloud service to trasform a picture using a neural algorithm ?,neural-networks convolutional-neural-networks recurrent-neural-networks google-cloud cloud-services,0,0,
3393,11738,1,,2019-04-09T15:33:33.017,1,13,"i want to train a neural network to predict what my favourite home - work route will be for a particular day . i have these features for routes on a day : temperature , humidity , congestion , distance , duration i have come up with this concept of training / testing a network : // // training features result in route 1,2 or 3 // network.train([30,10,12,20,12 ] , 1 ) network.train([20,10,22,20,14 ] , 3 ) network.train([23,10,2,20,10 ] , 2 ) network.train([20,10,22,20,12 ] , 2 ) // // on a new day , predict which route the user is most likely to take : // var route = network.test([25,8,12,22,12 ] ) my question is : is this a viable approach ? can i make relevant predictions this way if i have enough training data ? can i generate an outcome between 1 and 3 this way ?",11620,,,2019-04-09T15:33:33.017,how to predict a preferred route based on weather and distance,neural-networks machine-learning ai-basics,0,1,
3394,11740,1,,2019-04-09T17:11:03.973,2,40,could a better algorithm other than monte carlo be used for the alphago computer ? why did n't the deepmind team think of choosing another kind of algorithm rather than spending time on their neural nets ?,21832,21832,2019-04-09T23:44:08.303,2019-04-09T23:44:08.303,why is monte carlo used as the tree search algorithm for alphago ?,monte-carlo-tree-search alphago monte-carlo alphago-zero,0,0,1
3395,11741,1,,2019-04-09T17:21:45.803,3,34,"i 'm working in a computer vision project , where the goal is to detect some specific parasites , but now that i have the images , i noticed that they have a watermark that specifies the microscope graduation . i have some ideas of how to remove this noise , like detecting the numbers and replace for the most common background or split the image but if i split the image i 'll lose information . but i would like to hear some recommendations and guidelines of experts . i added an example image below .",23836,23836,2019-04-09T17:34:49.240,2019-04-09T17:34:49.240,how do i denoize a microscopic image ?,convolutional-neural-networks image-recognition computer-vision,0,4,
3396,11743,1,,2019-04-09T19:10:46.760,0,81,i 've decided to make my bachelor thesis in rl . i am currently struggling in finding a proper problematic . i am interested in multi - agent rl with the dilemma between selfishness and cooperation . i only have 2 months to complete this and i 'm afraid that multi - agent rl is too difficult and i do n't have the knowledge and time to nicely learn this topic . do you have any problematics for a bachelor level student ? i 've done a tiny q - learning algorithm to solve open - ai text based environements . thanks in advance for the help !,23838,1671,2019-04-11T21:05:41.067,2019-04-13T20:45:37.070,bachelor thesis in reinforcement learning,reinforcement-learning open-ai academia,1,2,
3397,11744,1,,2019-04-09T19:59:37.873,2,12,"how one can model physiological reward mechanisms occuring in the brain using artificial neural networks ? e.g. are there efforts to use the notion of dopamine or similar substances in the artificial neural networks . maybe introduction of the physiological reward mechanism can lead to the emergence of consciousness or at least enhance the effectiveness of reinforcement learning ? essentially - how neural network models reward ? people 's brain perceive money as the ultimate reward because almost everything other can be bought by this . so - mental perception of owning money gives reward . but how this notion of reward is modeled in artificial neural networks ? how networks know that some money is assigned to the network 's account and so , the network should feel happy and rewarded and should strive to repeat successful behavior ? i am reading https://www.ncbi.nlm.nih.gov/pmc/articles/pmc5293493/pdf/elife-21492.pdf and i hope that it will move me in the right direction . it is quite confusing . the old - school neural networks expect that there are 2 separate phases : training and inference . so , the network receives all the feedback ( let it be called reward ) in the training phase and network receives nothing in the inference phase . but maybe network should receive some reward during acting - inference phase as well , kind a lifelong learning .",8332,8332,2019-04-09T20:19:47.167,2019-04-09T20:19:47.167,models of reward ( possibly mimicking dopamine ) in artificial neural networks ?,neural-networks brain,0,1,
3398,11745,1,,2019-04-09T20:55:12.253,2,19,"one nash equilibrium of every gans model has is when the generator creates perfect samples indistinguishable from the training data and the discriminator just output 1 with probability 1/2 . and i think this is the desirable outcome since we are most interested in the generator part of the gan model . i know that we probably try to converge to this equilibrium with some hacks in training such as "" mode collapse avoidance "" and so on . but is there any theoretical work trying to go in another ways ( say , by reduce the number of nash equilibria somehow ? )",23688,,,2019-04-09T20:55:12.253,how do we ensure that training gans will fall in the desirable nash equilibrium ?,neural-networks machine-learning generative-model,0,0,
3399,11746,1,,2019-04-09T23:51:08.227,1,25,"how can we use the ability of alphago zero computer , to do something in any other life important related field ? is it possible to make something important besides having created something so smart that can play mind games way better than humans ?",21832,,,2019-04-10T07:04:46.153,how do the achievements met in the gaming field ( ex . alphago zero ) impact other fields of application ?,machine-learning gaming alphago-zero,1,0,
3400,11747,1,,2019-04-10T02:18:19.513,2,11,"i 've a rather simple question for a school project . we 're developing a ga solution for the following problem : chromosome : a location with lat - lon coords . there are two types of locations - up to 15 waypoints from user input , and a dataset of about 3 - 400 stations . gene : a route consisting of all waypoints ( incl . a fixed start and end ) + 1 station . fitness function : shortest path . stop condition : run duration - configurable , default 3 seconds . we 're discussing two possible implementations : a problem set of all waypoints and all stations , kinda like soccer team assignment or nurse rostering design . run ga once on all of that . a problem set of all waypoints and one station , do tsp . run ga for number - of - stations - in - dataset iterations . which is better , in terms of design , efficiency , and performance ?",23844,,,2019-04-10T02:18:19.513,shortest route ga : one loop through one dataset vs multiple loops through subsets of the same data ?,genetic-algorithms evolutionary-algorithms genetic-programming,0,0,
3401,11748,1,,2019-04-10T05:11:27.350,1,31,"i have set of topics generated using lda and like { code , language , test , write , function } , { class , public , method , string , int } etc and i want to make meaningful sentence / sentences from these words using api or libraries . how do i implement this with nltk and(or ) machine learning ? any suggestions as to how i should go about this ?",23509,,,2019-05-10T11:01:30.680,how to make meaningful sentences from a set of words ?,machine-learning natural-language-processing,1,0,
3402,11752,1,,2019-04-10T09:17:39.993,0,19,i want to create a semantic search for a set documents with terms that could not appear on the set . is there some code for that ? i 'm working on python .,23854,,,2019-04-10T09:17:39.993,semantic search engine for a set of documents in python,natural-language-processing python,0,4,
3403,11755,1,,2019-04-10T10:02:36.443,1,44,"before the release of bert we used to say that it is not possible to train bidirectional models by simply conditioning each word on its previous and next words , since this would allow the word that ’s being predicted to indirectly “ see itself ” in a multi - layer model . how does this happen ?",23350,22916,2019-04-11T21:02:58.827,2019-04-11T21:02:58.827,"how does bidirectional encoding allow the predicted word to indirectly "" see itself "" ?",deep-learning natural-language-processing recurrent-neural-networks,0,0,
3404,11759,1,,2019-04-10T13:53:28.567,1,32,"let 's suppose i have a set of heuristics $ s$ = { $ h_1 , h_2 , ... , h_n$ } . if all heuristics in $ s$ are admissible , does that mean that a heuristic that takes the $ min(s)$ ( or $ max$ for that matter ) is also admissible ? if all heuristics in $ s$ are consistent , does that mean that a heuristic that takes the $ min(s)$ ( or $ max$ for that matter ) is also consistent ? i 'm thinking about a search problem in a bi - dimensional grid that every iteration of an algorithm , the agent will have to find a different goal . therefore , depending on the goal node a certain heuristic can possibly better guide the agent then the others ( hence the use of $ min$ and $ max$ ) .",22369,22369,2019-04-10T17:58:53.320,2019-04-10T17:58:53.320,is the minimum and maximum of a set of admissible and consistent heuristics also consistent and admissible ?,search heuristics proofs,0,0,
3405,11760,1,11764,2019-04-10T15:42:57.157,3,45,"i am training a modified vgg16 network for classification ( adding 0.5 dropout after each of the last fc layers ) . in the following plot i am training for a small number of epochs as an example , and it shows the accuracy and loss curves of training process on both training and validation datasets . my training set size is $ 1725 $ , and $ 429 $ for validation . also i am training with weights = none my question is about the validation curves , why do not they appear to be as smooth as the training ones ? is this normal during the training stage ?",23268,9947,2019-04-10T17:57:04.163,2019-04-10T17:57:04.163,why are not validation accuracy and loss as smooth as train accuracy and loss ?,neural-networks convolutional-neural-networks training,1,0,
3406,11761,1,,2019-04-10T16:18:36.993,3,35,"i 'm trying to tackle the problem of feature selection as an rl problem , inspired by the paper feature selection as a one - player game . i know monte - carlo tree search ( mcts ) is hardly rl . so , i used mcts for this problem , where nodes are subsets of features and edges are ( "" adding a feature to the subset "" ) and it does converge to the optimal subset slowly . i have a few questions is there a clever way to speed up the convergence of mcts , besides parallelizing the rollout phase ? adding nodes to the tree takes time and memory for datasets with a large number of features , for 10000 features , it takes up all my ram ( 8 gb ) from the second iteration , ( although it runs for 2000 + iterations for a dataset with 40 features which does n't make sense to me ) . is this expected or is my implementation likely wrong ? are there any workarounds for this ? what are your opinions on using mcts for this task ? can you think of a better approach ? the main problem of this approach is running the svm as an evaluation function which may make the algorithm impractically slow for large datasets ( a large number of training examples ) . i was thinking of trying to come up with a heuristic function to evaluate subsets instead of the svm . but i 'm kind of lost and do n't know how to do that . any help would be really appreciated .",23866,2444,2019-04-10T17:21:30.833,2019-04-10T17:21:30.833,feature selection using monte carlo tree search,reinforcement-learning monte-carlo-tree-search feature-selection,0,0,
3407,11762,1,,2019-04-10T17:33:43.483,1,39,"what are the key differences between cellular neural networks and convolutional neural networks in terms of working principle , implementation , potential performance , and applicability ?",23868,22916,2019-04-11T01:58:06.170,2019-04-11T01:58:06.170,what are the key differences between cellular neural network and convolutional neural network ?,neural-networks machine-learning convolutional-neural-networks difference,0,0,
3408,11763,1,11765,2019-04-10T17:54:27.137,4,68,"i want to make a network , specifically a cnn for image recognition , that takes an input , processes it the same way for several layers , and then at some point splits before coming to two different outputs . is it possible to create a network such as this ? it would look something like this : input - > conv - > pool - > conv - > pool --------- > dense - > output 1 || ----&gt ; dense -&gt ; output 2 i.e. it splits off after the second pooling layer into separate fully connected layers . of course , it has to train to both outputs , so that it is producing minimal error on both separate outputs using these common convolutional layers . also , i am using python keras , and it would help if there was some way to do this using keras in some way . thank you !",23812,,,2019-04-11T05:17:19.863,is it possible to make a ' forked path ' neural network ?,neural-networks machine-learning convolutional-neural-networks image-recognition,1,0,
3409,11766,1,,2019-04-10T19:22:24.713,1,22,"please read the following page of the sklearn documentation . the figure shown there ( see below ) illustrates why c should be scaled when using a svm with ' l1 ' penalty , whereas it should n't be scaled c when using one with ' l2 ' penalty . the scaling however does not change the scores of the models examined within the gridsearch . so what exactly is this scaling - step good for ?",23672,,,2019-04-10T19:22:24.713,what is the benefit of scaling the hyperparameter c of an svm ?,hyper-parameters regularization svm,0,0,
3410,11768,1,,2019-04-11T02:34:42.343,2,37,"i think i do n't understand group convolutions well . say you have 2 groups . this means that the number of parameters would be reduced in half . so assuming u have an image and 100 channels , with a filter size of 3x3 , you would have 900 parameters ( ignore the bias for this example ) . if you separate this in 2 groups , if i understand it well , you would have 2 groups of 50 channels . this can be made faster , by running the 2 groups in parallel , but how does the number of parameters gets halved ? is n't each group having 50 * 9=450 parameters , so in total you still have 900 parameters . do they mean that the number of parameters that the backpropagation goes over ( in each branch ) gets halved ? because overall , i do n't see how it can get reduced . also , is there a downside in using more groups ( even going to 100 groups of 1 channel each ) ? thank you !",23871,,,2019-04-12T00:58:05.953,confused about group convolution,convolutional-neural-networks,1,1,
3411,11769,1,,2019-04-11T06:36:25.450,0,32,"i 'm attempting to implement the actor - critic algorithm on matlab using radial basis function , local linear regression , and shallow neural network for inverted pendulum system . the state space and the action space are continuous . states are the angle x_1 wrapped into [ -pi pi ] and the angle velocity x_2 in [ -8*pi 8*pi ] the continuous action u , which is bound between [ -3 3]. reward function is quadrat rho = x'q x+u'r u where q = diag(1,5 ) and r=0.1 the desired point is upright position [ 0 0 ] ' some notes will be added the used solver is ode45 . the sampling time 0.03 . it explores random u every step , with normal distribution zero mean sigma=1 model of the system ( to save place the parameters of the model are not written ) function dy = pendulum(y , u ) dy(1,1)=y(2 ) ; dy(2,1)=1 / j*(m*g*l*sin(y(1))-(b+k^2 / r)*y(2)+k / r*u ) ; end_function function to calculate rbf : the idea is to define centers and widths for n rbfs which cover the entire state space to approximate the value function and policy separately . the rbf is normalized . function phi = rbf(x , c , b , n ) % x : state , c : centres , b : width , n : nombre of used rbfs phi_vec= [ ] ; phi_sum=0 ; for i=1 : n % loop for to calculate the vector phi phi_i = exp(-1/2*(x - c(i,:)')'*b^(-1)*(x - c(i , : ) ' ) ) ; % gaussian function phi_vec=[phi_vec;phi_i ] ; % not normalized phi vector phi_sum = phi_sum+phi_i ; % sum for normalisation end phi = phi_vec / phi_sum ; % normalized phi vector % after tuning the learning rate for actor and critic alpha_a and alpha_c % every step the following updates shall be carried out : % % generally % value function v = theta_o'*rbf(x , c , b , n ) % policy pi= theta_v'*rbf(x , c , b , n ) % determine u(k ) with exploration term u(k)=theta_v'*rbf(x , c , b , n)+delta_u(k-1 ) % % aplly u(k ) and gain x(k+1 ) [ t , y ] = ode45(@(t , y ) pendulum(y , u(k)),tspan , x(k ) ' ) ; : x(k+1,1)= wraptopi(x(k+1,1 ) ) ; % wrpping to pi % determine temporal difference error delta(k)=r(k)+gamma*theta_o'*rbf(x(k),c , b , n)-theta_o'*rbf(x(k-1),c , b , n ) ; % eligibility trace z = lamda*gamma*z+rbf(x , c , b , n ) ; % critic update theta_o = theta_o+alpha_c*delta(k)*z ; % actor update theta_v = theta_v+alpha_a*delta(k)*delta_u(k- 1)*rbf(x , c , b , n ) ;",22209,22209,2019-04-11T15:26:48.753,2019-04-11T15:26:48.753,"actor - critic algorithm using gaussian radial basis function , local linear regression and shallow neural network",neural-networks reinforcement-learning linear-regression actor-critic,0,3,0
3412,11771,1,,2019-04-11T10:58:59.290,0,53,"i am modelling a process with 4 input parameters x1 x2 x3 x4 . the output of the process is 2 variable y1 y2 that varies with length and time . i also have data from experiments basically recording the trends in the two output variables as i vary my input variable . so far i have only seen neural network examples which will take input x1 x2 x3 x4 t ( t is time ) and predict y1 y2 at said time t ( no consideration of location ) . i would however like to also like to see variation with length as well at a given time t [ y1a y1b ... y1z ; y2a y2b ... y2z ] where ( a , b ... z ) are location points at an incremental distance dh from the start . any help is appreciated . tia",23883,23883,2019-04-11T11:24:33.623,2019-05-15T03:03:06.547,can i use neural networks for a problem ( in description ) ?,neural-networks,1,2,
3413,11773,1,,2019-04-11T13:49:55.527,0,27,i am training a convlstm with a dropout layer ( with prob 0.5 ) . if i train over more than 5 epochs i notice that the network starts to overfit : my validation set loss becomes stationary while the train loss keeps going down with every epoch . and if i train for 20 or more epochs the gap between the validation and train loss is quite substantial . at the same time precision - recall curve becomes much more stable ( i.e. monotonic ) if i train with a large number of epochs ( e.g. 20 ) . why is that ? is this behaviour a common occurrence ?,11417,,,2019-04-11T13:49:55.527,why does precision - recall curve become more stable when neural net begins to overfit ?,neural-networks overfitting performance,0,5,
3414,11774,1,,2019-04-11T14:13:56.783,1,45,"i am in education and i 'm wondering about the use of chatbot - like tools to facilitate automated discussions among students . the chatbot domain of knowledge would be purposely restricted to a specific unit of learning . the chatbot must have flexible knowledge representation because discussing the causes of wars in one grade will be a different discussion than discussing the causes of wars in an older grade . i 'm wondering how to construct the knowledge base of facts and question prompts to ask students ( to keep the inquiry moving forward ) . i also wonder about the chatbot gradually getting "" smarter "" as the discussion progresses . for example , i do n't want the chatbot to give too most useful information too soon and shortcircuit the learning process . said differently , i think the chatbot should respond with more sophisticated information as the discussion progresses . i also think the prompting questions would be different / deeper as the discussion progresses . my question is more of a request for feedback on : the potential of the idea ? is the scope for a proof of concept deliverable within my abilities or should i seek additional resources ? what back - end tool should i use ? i 'm most concerned about finding a visual knowledge representation tool . is there prior product work done in this area ? is there prior research done in this area ? thanks in advance for your thoughts ! p.s . i am reviewing this question thread -- > build conversational ai",23886,1671,2019-04-11T21:04:42.793,2019-04-11T21:04:42.793,chatbot applications [ academia ],natural-language-processing ai-basics chat-bots knowledge-representation reference-request,0,1,
3415,11776,1,,2019-04-11T23:18:09.190,3,29,"i 'm trying to make deep q - learning agent from https://keon.io/deep-q-learning my environment looks like this : https://imgur.com/a/onbictv as you can see my agent is a circle and there is one gray track with orange lines ( reward gates ) . the bolder line is an active gate . the orange line from the circle pointing to his direction . the agent has constant velocity and it can turn left / right 10 degrees or do nothing on the next image are agents sensors https://imgur.com/a/qj7kesi they are rotating with the agent . the states are a distance from the agent to active gate and lengths of sensors . in total there are 1 + 7 states and it is q - learning neural net input dimension . actions are turn left , turn right and do nothing . reward function returns 25 when the agent intersects reward gate ; 125 when agent intersects the last gate ; -5 if agent intersects track border if none of this , reward function compare the distance from the agent to the active gate for current state and next state : if current state distance > next state distance : return 0.1 else return -0.1 also , dqnagent has negative , positive and neutral memory . if reward is -5 , ( state , action , reward , next_state , done ) go to the negative memory , if reward is > = 25 , to positive else to neutral that is because when i 'm forming minibatch for training , i 'm taking 20 random samples from neutral memory , 6 from positive and 6 from negative . every time when agent intersects track border or when he is stuck for more than 30 seconds , i 'm doing training ( replay ) and agent starts from the beginning . this is my model model = sequential ( ) model.add(dense(64 , input_dim = self.state_size , activation='relu ' , kernel_initializer = variancescaling(scale=2.0 ) ) ) model.add(dense(32 , activation='relu',kernel_initializer = variancescaling(scale=2.0 ) ) ) model.add(dense(self.action_size , activation='linear ' ) ) model.compile(loss=self._huber_loss , optimizer = adam(lr = self.learning_rate ) ) return model i tried different kinds of model , a different number of neurons per layer , other activation and loss functions , dropout , batch normalization , and this model works the best for now i tried different reward values also , i tried to use static sensors ( they are not rotating with the agent ) https://imgur.com/a/8edtqif ( green lines on the photo ) sometimes my agent manages to intersect a few gates before hits the border . rarely he manages to traverse half of the track and once , with this settings , he traversed two laps before he stuck . more often , he is only rotating in one place . i think that the problem lays in state representation or reward function . any suggestions would be appreciated",23894,,,2019-04-11T23:18:09.190,deep q - learning agent poor performing actions . need help optimizing,reinforcement-learning python q-learning keras deep-rl,0,0,
3416,11777,1,,2019-04-11T23:36:53.777,0,22,i am implementing a cnn from scratch and it sometimes gave me numpy overflow error . i then debug the network and found that the gradients of the first 2 layers is around 500 to -500 . the gradients of the last 4 layers are around 10 to -10 . is it normal that the gradients are so big ? my library is in my github repo : https://github.com/clementcsmh/neural-network-libary i think it is a exploding gradient problem . did i implemented a wrong back prop or it is only the model 's problem ? my model architecture ( all conv net have no activation and stride of 1 and kernel size of 3 ) conv conv relu conv conv relu flatten dense 100 relu dense 10 relu is there a problem in my network architecture ? i was training with mnist data with adam optimizer .,23713,,,2019-06-01T08:35:41.467,my own cnn model have dw of 500 and have numpy runtime error,convolutional-neural-networks gradient-descent,0,1,
3417,11780,1,,2019-04-12T03:02:26.847,1,28,"i have some ecological data on the confirmed presence of a certain animal . i have data on the : date relevant metadata about the site simple metrics on the animal a complete weather record for the site . i 'm assuming the presence of this animal is driven by weather events , and that the metadata about the site may be an important factor for patterns in the data . what my data ends up looking like is this . # date of observation date&lt;-as.posixct(c(""2015 - 01 - 01"",""2015 - 01 - 11"",""2015 - 01 - 19"",""2015 - 02 - 04"",""2015 - 02 - 12"",""2015 - 02 - 23"",""2015 - 04 - 01"",""2015 - 04 - 10"",""2015 - 04 - 16"",""2015 - 04 - 20 "" ) ) # data about animal size&lt;-c(1,1,1,2,2,3,4,1,2,5 ) color&lt;-c(""b"",""b"",""r"",""r"",""r"",""r"",""b"",""b"",""b"",""y "" ) length&lt;-c(1,10,12,4,5,2,1,2,7,12 ) # weather data airtempdayof&lt;-c(20,40,20,23,24,25,24,25,25,22 ) windspeeddayof&lt;-c(2,3,2,3,4,3,2,3,4,5 ) airtempdaybefore&lt;-c(21,40,22,23,24,24,24,27,25,22 ) windspeeddaybefore&lt;-c(2,5,2,6,4,3,6,3,2,5 ) airtemp2daybefore&lt;-c(21,45,22,23,34,24,24,23,25,23 ) windspeed2daybefore&lt;-c(8,5,3,6,4,7,6,3,2,6 ) # metadata about site type&lt;-c(""forest"",""forest"",""forest"",""forest"",""forest"",""beach""""beach""""beach"",""swamp"",""swamp "" ) population&lt;-c(20,30,31,23,32,43,23,43,23,33 ) use&lt;-c(""industrial"",""commercial"",""industrial"",""commercial"",""industrial"",""commercial"",""industrial"",""commercial"",""industrial"",""commercial "" , ) df&lt;-data.frame(date , size , color , length , airtempdayof , windspeeddayof , airtempdaybefore , windspeeddaybefore , airtemp2daybefore , windspeed2daybefore ) what i do n't have is absence data , so i ca n't make any assumptions about when an organism was not at the site . i 'd like to look for patterns in weather that my be driving the arrival of this organism , but all i have is data on when the organism was spotted . is it possible to apply some sort of machine learning to look for patterns that may be driving the arrival of this animal ? if i do n't have absence data , i 'm assuming i ca nt . i 've looked into pseudo - absence models , but i do n't know how they might apply here . if i ca n't use machine learning to look at drivers for the presence of these animals , is it possible to use ml to look at possible weather patterns that may be associated with some of the metadata about the site ? for example , weather patterns that may be associated with forrest vs beach habitats ? i usually use r for my stats , so any answers including r packages would be helpful . also , note that this is just an example dataset above . i do n't expect to find any patterns in the above data , and my actual dataset is much larger . but any code developed for the above data should be applicable",23896,23896,2019-04-12T12:26:55.153,2019-05-12T14:01:30.347,machine learning to find drivers of an event with presence - only data ( no absence ),neural-networks machine-learning r,1,2,
3418,11781,1,,2019-04-12T05:02:52.887,3,44,"projected bellman error has shown to be stable with linear function approximation . the technique is not at all new . i can only wonder why this technique is not adopted to use with non - linear function approximation ( e.g. dqn ) ? instead , a less theoretical justified target network is used . i could come up with two possible explanations : it does n't readily apply to non - linear function approximation case ( some work needed ) it does n't yield a good solution . this is the case for true bellman error but i 'm not sure about the projected one .",9793,2444,2019-05-10T14:48:20.443,2019-05-10T14:48:20.443,why do n't people use projected bellman error with deep neural networks ?,reinforcement-learning dqn deep-rl function-approximation,0,4,
3419,11783,1,11784,2019-04-12T08:16:52.143,1,56,the -return is defined as $ $ g_t^\lambda = ( 1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}g_{t : t+n}$$ where $ $ g_{t : t+n } = r_{t+1}+\gamma r_{t+2}+\dots + \gamma^{n-1}r_{t+n } + \gamma^n\hat{v}(s_{t+n})$$ is the $ n$ -step return from time $ t$ . how can we use this definition to rewrite $ g_t^\lambda$ recursively ?,22916,22916,2019-04-12T08:28:55.680,2019-04-14T07:44:15.380,how can the -return be defined recursively ?,reinforcement-learning rl-an-introduction return eligibility-traces,1,2,
3420,11785,1,,2019-04-12T08:37:06.037,0,53,"i have searched on how google or any map provider calculates distance between two coordinates . the closest i could find is haversine formula . if i draw a straight line between two points , then haversine formula can be helpful . but since no one will travel straight and typically move through the streets , i want to know if there are any methods to calculate turn by turn points and see how to find multiple ways to travel to the destination from the source . right now my idea is have the two coordinates within a map window . make an algorithm detect the white lines ( path ) in the window . make it understand how they are connected . feed it to an algorithm to solve the travelling salesman problem to find the best path between them . but these things see very memory and process intensive . even with the knowledge that google has the powerhouse to process , to serve so many directions and distance matrix in fractions of seconds in amazing . i want to know if there are different approaches to this ?",9170,2193,2019-04-12T10:14:55.897,2019-05-15T18:29:03.030,how do map providers like google calculate the distance between two coordinates and find turn by turn directions ?,reinforcement-learning object-recognition,2,0,1
3421,11787,1,11791,2019-04-12T11:42:10.900,3,42,"i am trying to understand how alpha zero works , but there is one point that i have problems understanding , even after reading several different explanations . as i understand it ( see for example https://applied-data.science/static/main/res/alpha_go_zero_cheat_sheet.png ) , alpha zero does not perform rollouts . so instead of finishing a game , it stops when it hits an unknown state , uses the neural network to compute probabilities for different actions as well as the value of this state ( "" probability of winning "" ) , and then propagates the new value up the tree . the reasoning is that this is much cheaper , since actually completing the game would take more time then just letting the neural network guess the value of a state . however , this requires that the neural network is decent at predicting the value of a state . but in the beginning of training , obviously it will be bad at this . moreover , since the monte carlo tree search stops as soon as it hits a new state , and the number of different game states is very large , it seems to me that the simulation will rarely manage to complete a game . and for sure , the neural network can not improve unless it actually completes a significant number of games , because that is only real feedback that tells the agent if it is doing good or bad moves . what am i missing here ? the only plausible explanation i can come up with is : if the neural network would be essentially random in the beginning , well then for sure the large number of game states would prevent the tree search from ever finishing if it restarts as soon as it hits a previously unknown game state , so this can not be the case . so perhaps , maybe even if the neural network is bad in the beginning , it will not be very "" random "" , but still be quite biased towards some paths . this would mean that the search would be biased to some smaller set of states among the vast number of different game states , and thus it would tend to take the same path more than once and be able to complete some games and get feedback . is this "" resolution "" correct ? one problem i have though with the above "" resolution "" , is that according to the algorithm , it should favor exploration in the beginning , so it seems that in the beginning it will be biased towards choosing previously not taken actions . this makes it even more seem like the tree search will never be able to complete a game and thus the neural net would not learn .",23910,,,2019-04-12T19:07:00.153,how can alpha zero learn if the tree search stops and restarts before finishing a game ?,deep-learning reinforcement-learning monte-carlo-tree-search alphazero,1,0,
3422,11789,1,,2019-04-12T13:38:21.127,1,21,"heterogeneity : based on the heterogeneity of agents mas can be divided into two categories namely : homogeneous and heterogeneous . homogeneous mas include agents that all have the same characteristics and functionalities , while heterogeneous mas include agents with diverse features . as i read in this paper that these methods can deal with the heterogeneity of agents mas : the dueling double deep q - network ( dddqn ) and independent deep q - network ( idqn ) : first approach to address heterogeneous multi - agent learning in urban traffic control . deep q - network ( dqn ) : to handle heterogeneity , each agent has different experience replay memory and different network policy . the asynchronous advantage actor - critic ( a3c ) algorithm is used to learn optimal policy for each agent , which can be extended to multiple heterogeneous agents . so , can someone tell me what is the best method to deal with heterogeneous multi - agent system mas ?",21181,,,2019-04-12T13:38:21.127,what is the best method to deal with heterogeneous multi agent system mas ?,deep-learning multi-agent-systems,0,0,
3423,11792,1,,2019-04-12T17:06:47.410,1,33,"i 've been trying to read sutton & amp ; barto book chapter 5.1 , but i 'm still a bit confused about the procedure of using monte carlo policy evaluation ( p.92 ) , and now i just ca nt proceed anymore coding a python solution , because i feel like i do n't fully understand how the algorithm works , so that the pseudocode example in the book does n't seem to make much sense to me anymore . ( the orange part ) i 've done the chapter 4 examples with the algorithms coded already , so i 'm not totally unfamiliar with these , but somehow i must have misunderstood the monte carlo prediction algorithm from chapter 5 . my setting is a 4x4 gridworld where reward is always -1 . policy is currently equiprobable randomwalk . if an action would take the newstate ( s ' ) into outside the grid , then you simply stay in place , but action will have been taken , and reward will have been rewarded . discount rate will be 1.0 ( no discounting ) . terminal states should be two of them , ( 0,0 ) and ( 3,3 ) at the corners . on page 92 it shows the algorithm pseudocode and i feel as though i coded my episode generating function correctly thusfar . i have it such that , the results are that i always start in the same starting state ( 1,1 ) coords in the gridworld . currently , i have it so that if you started always in state ( 1,1 ) , then a possible randomly generate episode could be as follows ( in this case also optimal walk ) . note that i currently have the episodes in form of list of tuple ( s , a , r ) . where s will also be a tuple ( row , column ) , but a = string such as "" u "" for up , and r is reward always -1 . so that a possible episode could be like : [ ( ( 1,1 ) , "" u "" , -1 ) , ( ( 0,1 ) , "" l "" , -1 ) ] so that the terminal state is always excluded , so that the last state in episode will be the state immediately close to terminal state . just like the pseudocode describes that you should exclude the terminal state s_t . but , the random episode could have been one where there are repeating states such as [ ( ( 1,1 ) , "" u "" , -1 ) , ( ( 0,1 ) , "" u "" , -1 ) , ( ( 0,1 ) , "" u "" , -1 ) , ( ( 0,1 ) , "" l "" , -1 ) ] i made the loop for each step of episode , such as follows : once you have the episodelist of tuples , iterate for each tuple , in reversed order . i think this should give the correct amount of iterations there ... g can be updated as described in pseudocode . currently the returns(s_t ) datastructure that i have , will be a dictionary where the keys are state tuples ( row , col ) , and the values are empty lists in the beginning . i have a feeling that i 'm calculating the average into v(s_t ) incorrectly because i origianlly thought that you could even omit the v(s_t ) step totally from the algorithm , and only afterwards compute for a separate 2d array v[r , c ] for each state get the sum of the appropriate list elements ( accessed from the dict ) , and divide that sum by the amount of episodes that you ran ? ? ? but i do n't suddently know how to implement the first visit check in the algorithm . like , i literally do n't understand what it is actually checking for . and furthermore i do n't understand how the empirical mean is now supposed to be calculates in the monte carlo algorithm where there is the v(s_t ) = average ( returns(s_t ) ) i will also post my python code thusfar . import numpy as np import numpy.linalg as la import random # your code rows_count = 4 columns_count = 4 v = np.zeros((rows_count , columns_count ) ) reward = -1 # probably not needed directions = [ ' up ' , ' right ' , ' down ' , ' left ' ] # probably not needed maxiters = 10000 eps = 0.0000001 k = 0 # "" memory counter "" of iterations inside the for loop , note that for loop i - variable is regular loop variable rows = 4 cols = 4 # stepsmatrix = np.zeros((rows_count , columns_count ) ) def isterminal(r , c ) : # helper function to check if terminal state or regular state global rows_count , columns_count if r = = 0 and c = = 0 : # i m a bit too lazy to check otherwise the iteration boundaries return true # so that this helper function is a quick way to exclude computations if r = = rows_count-1 and c = = columns_count-1 : return true return false def getvalue(row , col ) : # helper func , get state value global v if row = = -1 : row = 0 # if you bump into wall , you bounce back elif row = = 4 : row = 3 if col = = -1 : col = 0 elif col = = 4 : col =3 return v[row , col ] def getstate(row , col ) : if row = = -1 : row = 0 # helper func for the exercise:1 elif row = = 4 : row = 3 if col = = -1 : col = 0 elif col = = 4 : col =3 return row , col def makeepisode(r , c ) : # helper func for the exercise:1 # # return the count of steps ? ? # by definition , you should always start from non - terminal state , so # by minimum , you need at least one action to get to terminal state statewasterm = false stepstaken = 0 curr = r curc = c while not statewasterm : act = random.randint(0,3 ) if act = = 0 : # # up curr-=1 elif act = = 1 : # # right curc+=1 elif act = = 2 : # # down curr+=1 else:##left curc-=1 stepstaken + = 1 curr , curc = getstate(curr , curc ) statewasterm = isterminal(curr , curc ) return stepstaken v = np.zeros((rows_count , columns_count ) ) episodecount = 100 reward = -1 y = 1.0 # the gamma discount rate # use dictionary where key is statetuple , # and value is statereturnslist # after algorithm for monte carlo policy eval is done , # we can update the dict into good format for printing # and use numpy matrix returnsdict= { } for r in range(4 ) : for c in range(4 ) : returnsdict[(r , c)]= [ ] # "" "" "" first - visit montecarlo episode generation returns the episodelist "" "" "" def firstmcepisode(r , c ) : global reward statewasterm = false stepstaken = 0 curr = r curc = c episodelist= [ ] while not statewasterm : act = random.randint(0,3 ) if act = = 0 : # # up r-=1 act=""u "" elif act = = 1 : # # right c+=1 act=""r "" elif act = = 2 : # # down r+=1 act=""d "" else:##left c-=1 act=""l "" stepstaken + = 1 r , c = getstate(r , c ) statewasterm = isterminal(r , c ) episodelist.append ( ( ( curr , curc ) , act , reward ) ) if not statewasterm : curr = r curc = c return episodelist kakka=0 # for debug breakpoints only ! # first - visit monte carlo with fixed starting state in the s(1,1 ) state for n in range(1 , episodecount+1 ) : eplist = firstmcepisode(1,1 ) g = 0 for t in reversed ( range ( len(eplist ) ) ): g = y*g + reward # note ! reward is always same -1 s_t = eplist[t][0 ] # get the state only , from tuple willappend = true for j in range(t-1 ) : tmp = eplist[j][0 ] if ( tmp = = s_t ): willappend = false break if(willappend ) : returnsdict[s_t].append(g ) t_r = s_t[0 ] # temprow from s_t t_c = s_t[1 ] # tempcol from s_t v[t_r , t_c ] = sum ( returnsdict[s_t ] ) / n kakka = 3 # for debug breakpoints only ! print(v )",23915,,,2019-04-12T17:06:47.410,difficulty understanding monte carlo policy evaluation ( state - value ) for gridworld,reinforcement-learning monte-carlo,0,5,
3424,11793,1,,2019-04-12T20:53:48.807,3,43,"what is the meaning of $ v(d , g)$ ? how do we get these expectation parts ? i was trying to understand it following this article : understanding generative adversarial networks ( d.seita ) , but , after many tries , i still ca n't understand how he got from to .",23918,2444,2019-04-15T09:58:33.980,2019-04-15T09:58:33.980,understanding notation of goodfellow 's gan objective function,machine-learning generative-model generative-adversarial-networks notation,0,4,1
3425,11799,1,11819,2019-04-13T15:19:42.393,3,31,"in decision tree or random forest , each tree has a collection of decision nodes ( in which each node has a threshold value ) and a class labels ( or regression values ) . i know that threshold values are used for comparison with a corresponding feature value . as far as i know , the comparison is performed either "" & lt ; "" , "" > "" or "" = = "" predicate . anything else for the functions taking threshold value and a feature value as inputs ? ?",23924,,,2019-04-15T00:58:28.497,what are possible functions assigned on decision nodes for decision tree prediction ?,decision-tree,1,0,
3426,11800,1,,2019-04-13T16:57:34.497,1,17,"i am attempting to implement an agent that learns to play in the pong environment , the environment was created in pygame and i return the pixel data and score at each frame . i use a cnn to take a stack of the last 4 frames as input and predicts the best action to take , i also make use of training on a minibatch of experiences from an experience replay at each timestep . i have seen an implementation where the game returned a reward of 10 for each time the bot returns the ball and -10 for each time the bot misses the ball . my question is whether it would be better to reward the bot significantly for managing to get the ball passed the opponent , ending the episode . i was thinking of rewarding 10 for winning the episode , -10 for missing the ball and 5 for returning the ball . please let me know if my approach is sensible , has any glaring problems or if i need to provide more information . thank you !",18208,,,2019-04-13T16:57:34.497,deciding the rewards for different actions in pong for a dqn agent,reinforcement-learning dqn rewards reward-clipping,0,2,
3427,11801,1,,2019-04-13T18:17:07.947,1,9,"facing with a multi - class classification task , my question is : are roc and precision - recall ( one - vs - all - rest ) curves useful to evaluate and visualize the performance of a model ? or confusion matrix , precision , recall , f - score ( micro and macro ) are enough ? what do you think about ?",20780,,,2019-04-13T18:17:07.947,evaluation metrics multi - class classification ( roc- pr curves ),machine-learning classification,0,0,
3428,11802,1,,2019-04-13T18:23:28.700,5,93,"image recognition , in the context of machine vision , is the ability of software to identify objects , places , people , writing and actions in images . computers can use machine vision technologies in combination with a camera and artificial intelligence software to achieve image recognition . so , why is image recognition a key function of ai ?",23529,2444,2019-04-13T20:10:00.227,2019-05-22T15:59:59.063,why is image recognition a key function of ai ?,image-recognition philosophy artificial-life,2,2,
3429,11803,1,11805,2019-04-13T18:28:22.733,8,106,"i came across an article , the bitter truth , via the two minute papers youtube channel . rich sutton says ... one thing that should be learned from the bitter lesson is the great power of general purpose methods , of methods that continue to scale with increased computation even as the available computation becomes very great . the two methods that seem to scale arbitrarily in this way are search and learning . what is the difference between search and learning here ? my understanding is that learning is a form of search-- where we iteratively search for some representation of data that minimizes a loss function in the context of deep learning .",22866,2444,2019-04-13T20:10:16.173,2019-05-26T04:16:46.237,what is the difference between search and learning ?,deep-learning philosophy search,2,0,4
3430,11809,1,11821,2019-04-14T13:04:26.620,7,98,"given a large problem , value iteration and other table based approaches seem to require too many iterations before they start to converge . are there other reinforcement learning approaches that better scale to large problems and minimize the amount of iterations in general ?",23288,2444,2019-04-14T18:38:52.930,2019-04-19T01:55:44.767,are there reinforcement learning algorithms that scale to large problems ?,reinforcement-learning,1,0,2
3431,11812,1,11908,2019-04-14T15:01:29.503,2,81,"i 'm building a generative adversarial network that generates images based on an input image . from the literature i 've read on gans , it seems that the generator takes in a random variable and uses it to generate an image . if i were to have the generator receive an input image , would it no longer be a gan ? would the discriminator be extraneous ?",23941,23941,2019-04-14T15:07:20.647,2019-04-21T13:19:57.953,how important is it that the generator of a generative adversarial network does n't take in information about input classes ?,keras generative-model generative-adversarial-networks,2,0,1
3432,11813,1,11814,2019-04-14T18:47:58.480,1,76,"i need to understand the logic of below fol statement . can someone help ? $ $ \forall x \exists y \forall z ( z \neq y \iff f(x ) \neq z ) $ $ does this imply that x , y and z can not be same or f(x ) has no value ?",22322,2444,2019-04-14T18:50:14.480,2019-04-14T19:57:34.443,meaning of the statement,ai-basics logic,1,1,
3433,11815,1,,2019-04-14T22:12:47.033,0,11,"i have a dataset where i 'm trying to gauge the importance of certain drivers ( x and y ) over various time periods . i 'd like to look at the importance of certain ranges of times , which will overlap with each other . i 'm assuming that there may be a time window that is more important than others . as an example , given a range of 15 days before an event , i weighted the 5 - 8th days , so i 'm assuming the 8th day mean will have the highest importance score . but given that there is a lot of overlap between the windows , is this a legitimate way to characterize the data ? as an example dataset ( using the randomforest library in r ) , my dataset would look like this . library(randomforest ) # results can be either big or small result&lt;-as.factor(c(""big "" , "" small "" , "" big "" , "" small "" ) ) # event 1 x1&lt;-c(1,2,1,1,20,30,42,20,3,4,5,6,5,2,2 ) y1&lt;-c(1,3,1,1,20,30,40,21,3,4,4,6,5,3,2 ) # event 2 x2&lt;-c(1,1,1,1,2,3,4,2,3,4,3,3,5,3,2 ) y2&lt;-c(1,1,2,1,2,3,4,2,3,4,5,6,5,2,2 ) # event 3 x3&lt;-c(2,2,1,1,20,33,40,20,3,4,5,6,3,3,2 ) y3&lt;-c(1,0,1,1,22,30,40,20,3,4,5,4,5,3,2 ) # event 4 x4&lt;-c(1,3,1,2,2,3,4,2,3,4,5,6,3,3,2 ) y4&lt;-c(2,1,1,3,2,3,4,2,3,4,5,6,5,2,2 ) # calculate 2,4,8,12,15 day means for each event meanday2x1&lt;-mean(x1[1:2 ] ) meanday2y1&lt;-mean(y1[1:2 ] ) meanday4x1&lt;-mean(x1[1:4 ] ) meanday4y1&lt;-mean(y1[1:4 ] ) meanday8x1&lt;-mean(x1[1:8 ] ) meanday8y1&lt;-mean(y1[1:8 ] ) meanday12x1&lt;-mean(x1[1:12 ] ) meanday12y1&lt;-mean(y1[1:12 ] ) meanday15x1&lt;-mean(x1[1:15 ] ) meanday15y1&lt;-mean(y1[1:15 ] ) meanday2x2&lt;-mean(x2[1:2 ] ) meanday2y2&lt;-mean(y2[1:2 ] ) meanday4x2&lt;-mean(x2[1:4 ] ) meanday4y2&lt;-mean(y2[1:4 ] ) meanday8x2&lt;-mean(x2[1:8 ] ) meanday8y2&lt;-mean(y2[1:8 ] ) meanday12x2&lt;-mean(x2[1:12 ] ) meanday12y2&lt;-mean(y2[1:12 ] ) meanday15x2&lt;-mean(x2[1:15 ] ) meanday15y2&lt;-mean(y2[1:15 ] ) meanday2x3&lt;-mean(x3[1:2 ] ) meanday2y3&lt;-mean(y3[1:2 ] ) meanday4x3&lt;-mean(x3[1:4 ] ) meanday4y3&lt;-mean(y3[1:4 ] ) meanday8x3&lt;-mean(x3[1:8 ] ) meanday8y3&lt;-mean(y3[1:8 ] ) meanday12x3&lt;-mean(x3[1:12 ] ) meanday12y3&lt;-mean(y3[1:12 ] ) meanday15x3&lt;-mean(x3[1:15 ] ) meanday15y3&lt;-mean(y3[1:15 ] ) meanday2x4&lt;-mean(x4[1:2 ] ) meanday2y4&lt;-mean(y4[1:2 ] ) meanday4x4&lt;-mean(x4[1:4 ] ) meanday4y4&lt;-mean(y4[1:4 ] ) meanday8x4&lt;-mean(x4[1:8 ] ) meanday8y4&lt;-mean(y4[1:8 ] ) meanday12x4&lt;-mean(x4[1:12 ] ) meanday12y4&lt;-mean(y4[1:12 ] ) meanday15x4&lt;-mean(x4[1:15 ] ) meanday15y4&lt;-mean(y4[1:15 ] ) df1&lt;-data.frame(meanday2x1,meanday2y1,meanday4x1,meanday4y1,meanday8x1,meanday8y1,meanday12x1,meanday12y1,meanday15x1,meanday15y1 ) colnames(df1)&lt;-c(""meanday2x"",""meanday2y"",""meanday4x"",""meanday4y"",""meanday8x"",""meanday8y"",""meanday12x"",""meanday12y"",""meanday15x"",""meanday15y "" ) df2&lt;-data.frame(meanday2x2,meanday2y2,meanday4x2,meanday4y2,meanday8x2,meanday8y2,meanday12x2,meanday12y2,meanday15x2,meanday15y2 ) colnames(df2)&lt;-c(""meanday2x"",""meanday2y"",""meanday4x"",""meanday4y"",""meanday8x"",""meanday8y"",""meanday12x"",""meanday12y"",""meanday15x"",""meanday15y "" ) df3&lt;-data.frame(meanday2x3,meanday2y3,meanday4x3,meanday4y3,meanday8x3,meanday8y3,meanday12x3,meanday12y3,meanday15x3,meanday15y3 ) colnames(df3)&lt;-c(""meanday2x"",""meanday2y"",""meanday4x"",""meanday4y"",""meanday8x"",""meanday8y"",""meanday12x"",""meanday12y"",""meanday15x"",""meanday15y "" ) df4&lt;-data.frame ( meanday2x4,meanday2y4,meanday4x4,meanday4y4,meanday8x4,meanday8y4,meanday12x4,meanday12y4,meanday15x4,meanday15y4 ) colnames(df4)&lt;-c(""meanday2x"",""meanday2y"",""meanday4x"",""meanday4y"",""meanday8x"",""meanday8y"",""meanday12x"",""meanday12y"",""meanday15x"",""meanday15y "" ) df&lt;-rbind(df1,df2 ) df&lt;-rbind(df , df3 ) df&lt;-rbind(df , df4 ) # 1 single data frame df$result&lt;-result rf & lt;- randomforest(result~. , importance = true , data = df , ntrees=20 ) # nodesize=5 rf rf$importance rfimp & lt;- importance(rf , type = 1 ) rfimp varimpplot(rf ) given the above data and results , is it possible to use the importance values provided from this model when there is such overlap between the time periods ? or is it more proper to break up the data into distinct ( non overlapping ) time periods ?",23896,,,2019-04-14T22:12:47.033,how to gauge importance in random forest when there is overlap between variables,machine-learning decision-tree r,0,0,
3434,11816,1,11818,2019-04-14T22:13:13.363,4,229,"what loss function is most appropriate when training a model with target values that are probabilities ? for example , i have a 3-output model . i want to train it with a feature vector $ x=[x_1 , x_2 , \dots , x_n]$ and a target $ y=[0.2 , 0.3 , 0.5]$ . it seems like something like cross - entropy does n't make sense here since it assumes that a single target is the correct label . would something like mse ( after applying softmax ) make sense , or is there a better loss function ?",17681,2444,2019-04-15T10:11:24.777,2019-04-15T10:11:24.777,what loss function to use when labels are probabilities ?,neural-networks machine-learning loss-functions probability-distribution,1,0,1
3435,11817,1,11837,2019-04-14T22:19:49.867,2,29,"the proof of the consistency of the per - decision importance sampling estimator assumes the independence of $ $ see the proof of theorem 1 in "" eligibility traces for off - policy policy evaluation "" . the result is also stated in equation ( 5.14 ) of sutton and barto 's rl book . i 'm guessing that this is itself a consequence of an assumption of independence between $ $ i do n't understand how this assumption can be justified . consider the extreme case of a nearly deterministic policy and deterministic mdp dynamics . it would seem to me that the two values above are then surely not independent . am i missing something ?",22916,,,2019-04-15T20:12:17.390,are successive actions independent ?,reinforcement-learning markov-decision-process importance-sampling,1,0,
3436,11822,1,,2019-04-15T07:47:20.967,1,30,"i 'm wondering if ai now can help us abstract summary or general idea of long article , for example novel or historical stories , or abstract most important keyword from sentence ; would you please tell me if any of this kind of project is done ? i wish i can improve my reading speed and effectiveness with ai help .",23855,,,2019-05-15T09:01:50.320,can ai help summarize article or abstract sentence keyword ?,natural-language-processing artificial-neuron,1,0,
3437,11824,1,,2019-04-15T08:33:47.020,0,23,"is there an ai application that can produce syntactically ( and semantically ) correct sentences given a bag of words ? for example , suppose i am given the words "" cat "" , "" fish "" , and "" lake "" , then one possible sentence could be "" cat eats fish by the lake "" .",23855,2444,2019-04-15T09:45:42.450,2019-04-15T09:45:42.450,how do i create syntactically correct sentences given several words ?,machine-learning natural-language-processing,0,3,
3438,11825,1,,2019-04-15T09:42:09.253,1,45,"in natural language processing , we can convert words to vectors ( or word embeddings ) . in this vector space , we can measure the similarity between these word embeddings . how can we create a vector space where word spelling and pronunciation can be easily compared ? for example , "" apple "" and "" ape "" , "" start "" and "" startle "" are very similar , so they should also be similar in this new vector space . i am eventually looking for a library that can do this out of the box . i would like to avoid implementing this myself .",23855,2444,2019-04-16T22:18:48.397,2019-04-18T14:17:47.110,how can we create a vector space where word spelling and pronunciation can be easily compared ?,natural-language-processing word-embedding,1,0,
3439,11833,1,,2019-04-15T14:35:28.560,2,49,"do we have cross - language vector space for word embedding ? when measure similarity for apple / pomme / mela / lacus/苹果/りんご , they should be the same if would be great if there 's available internet service of neuron network which already be trained by multiple language",23855,2444,2019-04-16T22:22:42.363,2019-04-18T15:00:27.223,do we have cross - language vector space for word embedding ?,natural-language-processing word-embedding,2,0,
3440,11835,1,,2019-04-15T16:47:17.947,2,50,"i am solving a classification problem with cnn . the number of classes is 5 . how can i decide the number of neurons in the fc layer before the softmax layer ? is it $ n * 5 $ , where $ n$ is the number of classes ? is there any documentation for deciding the number of neurons in the fc layer ( before softmax layer )",23734,23734,2019-04-16T00:47:44.650,2019-04-16T09:33:57.877,how do i choose the number of neurons in the fully - connected layer before the softmax layer ?,neural-networks deep-learning convolutional-neural-networks ai-design,1,0,
3441,11841,1,,2019-04-15T22:49:53.753,0,11,would you suggest to train my own fast - text embedding using the gensim library despite i have 1800 sentences and 2k vocabulary length ? do n't you think there are too few words ? or is there not a minimum threshold on which to train ?,20780,2444,2019-04-16T22:15:23.193,2019-04-16T22:15:23.193,embedding gensim fast - text,machine-learning classification word2vec word-embedding,0,0,
3442,11842,1,,2019-04-16T00:07:03.750,3,44,"i am trying to deploy a machine learning solution online into an application for a client . one thing they requested is that the solution must be able to learn online because the problem may be non - stationary and they want the solution to track the non - stationarity of the problem . i thought about this problem a lot , would the following work ? set the learning rate ( step - size parameter ) for the neural network at a low fixed value so that the most recent training step is weighted more . update the model only once per day , in a mini - batch fashion . the mini - batch will contain data from the day , mixed with data from the original data set to prevent catastrophic interference . by using a mini batch update , i am not prone to biasing my model to the latest examples , and completely forgetting the training examples from months ago . would this set - up be "" stable "" for online / incremental machine learning ? also , should i set up the update step so it samples data from all distributions of my predicted variable uniformly so it gets an "" even "" update ( i.e. , does not overfit to the most probabilistic predicted value ) ?",17706,2444,2019-04-16T10:01:06.413,2019-04-16T10:01:06.413,what are stable ways of doing online machine learning ?,neural-networks machine-learning incremental-learning online-learning,0,9,1
3443,11844,1,,2019-04-16T02:10:20.603,0,18,"so i have a rom of a hacked super mario game ( it has 2 players : mario and luigi ) . feeding in the raw pixel data of this results in very poor rewards . i was wondering if there was a way to transform this rom into tile representation . basically what this means , is that each sprite is converted into a single digit . the background is 0 , the coins and powerups are 1 , enemies 2 , and so on . this has already been done for instances of super mario , but i was wondering how i could apply this to a new rom with the same sprites . how do i create a tile representation for a hacked version of super mario ? an example of tile representation can be found here : https://www.youtube.com/watch?v=qv6uvoq0f44 so far , i have used openai retro to run mario .",23119,,,2019-04-16T02:10:20.603,turn a nes rom into object / tile representation,deep-learning reinforcement-learning neat open-ai,0,0,
3444,11845,1,11848,2019-04-16T06:50:50.500,1,39,"why does informed search more efficiently finds a solution than uniformed search ? if there was an example , it would be easy to understand .",23299,2444,2019-04-16T09:36:30.830,2019-04-16T14:05:51.040,why is informed search more efficient than uniformed search ?,ai-basics search difference efficiency relation,1,5,
3445,11847,1,11850,2019-04-16T12:35:58.513,0,48,how can the a * algorithm be optimized ? any references that shows the optimization of a * algorithm are also appreciated .,23299,2444,2019-04-16T15:33:27.223,2019-04-16T15:33:27.223,how can the a * algorithm be optimized ?,algorithm optimization search a-star,2,4,
3446,11853,1,,2019-04-16T20:23:38.827,0,5,"i 'm aware of the services from microsoft and aws , that may be able to used for such an application . please let me know how these fare , and if other similar services exist elsewhere : https://azure.microsoft.com/en-us/pricing/details/cognitive-services/text-analytics/ https://aws.amazon.com/comprehend/",23988,,,2019-04-16T20:23:38.827,what services are available for interpreting batches of text in order to determine their topic and/or summary ?,machine-learning natural-language-processing natural-language text-summarization sentiment-analysis,0,0,
3447,11866,1,,2019-04-17T04:38:33.790,2,64,"in section 3.2.1 of attention is all you need the claim is made that : dot - product attention is identical to our algorithm , except for the scaling factor of . additive attention computes the compatibility function using a feed - forward network with a single hidden layer . while the two are similar in theoretical complexity , dot - product attention is much faster and more space - efficient in practice , since it can be implemented using highly optimized matrix multiplication code . it does not make sense why dot product attention would be faster . additive attention is nearly identical computation wise ; the main difference is $ q + k$ instead of $ q k^t$ in dot product attention . $ q k^t$ requires at least as many addition operations as $ q + k$ , so how can it possibly be faster ?",21158,2444,2019-04-17T09:17:41.703,2019-04-18T04:00:34.207,why is dot product attention faster than additive attention ?,neural-networks machine-learning deep-learning attention,1,0,
3448,11870,1,11873,2019-04-17T17:35:34.047,3,39,"i have a cnn with the regression task of a single scalar . i was wondering if an additional task of reconstructing the image ( used for learning visual concepts ) , seen in a deepmind presentation with the loss and re - parametrization trick of variational autoencoder , might help the principal task of regression . so you can imagine some convolutions with the role of feature extraction with some output x ( let 's say a vector of 256 values ) , that x goes into the vae which computes z and then the reconstructed image . and then the original regression task would take either x or z in order to compute that scalar value . has anyone tried such an approach , is it worth the work ? thank you",21918,9947,2019-04-17T21:48:15.307,2019-04-19T13:39:02.323,variational autoencoder task for better feature extraction,neural-networks convolutional-neural-networks unsupervised-learning autoencoders supervised-learning,1,0,
3449,11871,1,,2019-04-17T18:15:29.557,1,26,"i have trained a modified vgg classification cnn , with random initialized weights ; therefor the validation accuracy was not high enough for me to accept ( around 66 % ) . now using the weights resulted from training the network , how can i use those weights in training the network again to improve accuracy ? ( e.g. using previous training weights with different learning rate , or increase epochs , .. )",23268,,,2019-04-18T05:47:54.210,how to benefit from previous training weights in training again to increase accuracy ?,convolutional-neural-networks training keras,1,0,
3450,11878,1,11879,2019-04-18T08:16:04.020,2,40,"i 'm working on implementing a q - learning algorithm for a 2 player board game . i encountered what i think may be a problem . when it comes time to update the q value with the bellman equation ( above ) , the last part states that for the maximum expected reward , one must find the highest q value in the new state reached , s ' , after making action a . however , it seems like the i never have q values for state s ' . i suspect s ' can only be reached from p2 making a move . it may be impossible for this state to be reached as a result of an action from p1 . therefore , the board state s ' is never evaluated by p2 , thus its q values are never being computed . i will try to paint a picture of what i mean . assume p1 is a random player , and p2 is the learning agent . p1 makes a random move , resulting in state s . p2 evaluates board s , finds the best action and takes it , resulting in state s ' . in the process of updating the q value for the pair ( s , a ) , it finds maxq'(s ' , a ) = 0 , since the state has n't been encountered yet . from s ' , p1 again makes a random move . as you can see , state s ' is never encountered by p2 , since it is a board state that appears only as a result of p2 making a move . thus the last part of the equation will always result in 0 - current q value . am i seeing this correctly ? does this affect the learning process ? any input would be appreciated . thanks .",24018,,,2019-04-18T09:26:30.120,maximum q value for new state in q - learning never exists,q-learning,1,0,
3451,11880,1,,2019-04-18T11:59:14.253,0,17,"i am trying to train an ann to control a 7 degrees - of - freedom arm . it should reach a target avoiding a single obstacle . given my modelling of the situation , my input layer is composed of 12 nodes : 5 nodes for the 5 joint states 3 nodes for the cartesian coordinates of the target 3 nodes for the cartesian coordinates of the obstacle 1 node for the radius of the obstacle ( it 's a spherical object ) . i have already tried training the ann with dqn . i want to try neuroevolution ( neat in particular ) and see how the results compare . i am using neat - python . as seen in this paper , this sould be feasable . i am having trouble choosing the best fitness function and also some other hyperparameters , namely the population size . i am also puzzled by the extremely long training time for a single generation , but that 's another story . so , the fitness function . i have tried to replicate what i have done with dqn , so , basically , my function evaluates a genome ( so , an ann ) as follows ( pseudocode ) : counter = 0 reapeat for num_of_episodes times : generate a random target generate an obstacle which lies about halfway from the end - effector to the target repeat for timeout times : use the ann to decide the next_action and execute it if obstacle_reached or target_reached , stop counter + = 0.3 * relative_distance + 0.2 * relative_path + 0.5 * didnthitobstacle ( ) fitness = counter / num_of_episodes so , humanly speaking , for each ann we try to execute num_of_episodes time s ( how many times should be enough ? 100 times seems ok but it get 's really slow ) a scenario . in this scenario , we use the ann to search for the target , and if we reach it or the obstacle , we stop . now for each one of this scenarios , we should "" rank "" how well the ann performed ( see the counter + = ... part ) . but how should i do this ? my idea ( stolen from the paper above ) was to compute something like this : 0.3 * relative_distance + 0.2 * relative_path + 0.5 * didnthitobstacle ( ) so basically we see how much we are closer to the target compared to when we started , how "" short "" the path was ( compared to the ideal straight line startpoint - to - target ) , and whether we did or did not hit the target . does this function make sense ? my concern is mainly about how we deal with the obstacle : 50 % of the fitness . is it correct ? i am asking this because i am receiving poor results . another problem that i have is population size . ofc , the bigger the better , but this thing takes a lot to train . how big is ok , in your experience ? thanks for the help .",23527,,,2019-04-18T11:59:14.253,using neuroevolution to train a 7 dof arm : how do i choose the correct fitness function and hyperparameters ?,evolutionary-algorithms neat artificial-neuron robotics,0,9,
3452,11881,1,,2019-04-18T12:21:29.327,0,16,"i am designing a simple multi - agent system in which the agents ' action space is represented as a markov chain , defining how likely an agent is to perform an action and to switch between those actions . my concern is that agents would switch between actions / tasks too frequently instead of sticking to a selected action for some time . i guess this is a common problem in multi - agent systems . could someone please suggest where i could learn more about this and how to resolve it ? thx , manuel",18266,22916,2019-04-18T19:20:12.770,2019-04-18T19:20:12.770,how to manage ' action churn rates ' in multi - agent systems ?,multi-agent-systems planning,0,1,
3453,11883,1,,2019-04-18T17:29:34.560,0,39,"i want to train an ai to detect playing cards . for that reason i bought many different decks , scanned and labeled them . next up would be to create training data with an augmentation library . i found two examples of how other people did that . the problem is that i want my ai to be able to detect multiple cards of the same class in one picture . in the examples they put a label on the top left corner of a card . this makes sense since it is a very good indicator of what class the card is in . unfortunately every card has two of these labels . i think their solution returns "" detected "" if any instance of a given label is found . but many cards with the same label could be in the same picture , and i am not sure if i can detect the quantity of the class with this solution . do you think it is feasible to mark the whole card , prepare the data accordingly and train an ai that detects the count as well ?",24028,22916,2019-04-18T20:24:45.283,2019-05-20T16:01:08.793,detecting playing cards with a neural network,image-recognition classification getting-started,1,0,
3454,11884,1,11887,2019-04-18T20:24:54.033,1,17,i quite often find projects using pre - trained model and using them as a starting point for their new model that learns something novel from thier dataset or on - live learning process - e.g. using a webcam or live audio . is this quite usual and recommended to speed up training a model ? for example using a model trained on imagenet as a first layer to your model that will categorise faces specifically .,11893,,,2019-04-18T22:57:22.557,is it mostly the case to train with available models,training models,1,0,
3455,11885,1,,2019-04-18T20:58:27.530,0,40,"i 've been trying to implement policy improvement for q(s , a ) function as per sutton&amp;barto reinforcement learning book . the original algorithm with first - visit montecarlo is pictured below . i remember the book earlier mentioning that the every - visit variation simply omits the first - visit check "" unless the pair blah , blah ... "" but otherwise the algorithms should be the same ( ? ? ? ) initial policy in the very first iteration ( first episode ) , should be equiprobable randomwalk . in general terms , if action takes you outside the border of the gridworld ( 4x4 ) , then you simply bounce back into where you started from , but reward will have been given , and action will have been taken . i have verified that my code gets stuck ( sometimes ) in the episode generation portion of my code in foreverloop for some reason , early on in the code ( iterations amount in the outerloop is small ) . even though , i thought i followed the pseudocode rather well , but it 's really annoying that sometimes the code gets stuck in foreverloop . the reason must be that for some reason my code updates from the equiprobable randomwalk policy = > into deterministic policy but in a wrong way , such that it can create foreverloops in the episode generation after the first episode has ran ( it must have ran the entire first episode with equiprobable randomwalk ) . the below picture shows that if you get into any state in the marked box , you can not get out from there and get stuck in episode generation ... if the random generator seed it lucky , then it can usually get "" over the hump "" and proceed to make the required number of iterations in the outerloop , and output an optimal policy for the gridworld . here is the python code ( from my experience i ran it in debugger and had to restart it a couple of times , but usually it will show quite fast that it gets stuck into the episode generation , and can not proceed in the iterations in outerloop . import numpy as np import numpy.linalg as la import random from datetime import datetime random.seed(datetime.now ( ) ) rows_count = 4 columns_count = 4 def isterminal(r , c ) : # helper function to check if terminal state or regular state global rows_count , columns_count if r = = 0 and c = = 0 : # i m a bit too lazy to check otherwise the iteration boundaries return true # so that this helper function is a quick way to exclude computations if r = = rows_count - 1 and c = = columns_count - 1 : return true return false maxiters = 100000 reward = -1 actions = [ "" u "" , "" r "" , "" d "" , "" l "" ] v = np.zeros((rows_count , columns_count ) ) returnsdict= { } qdict= { } actdict={0:""u"",1:""r"",2:""d"",3:""l "" } policies = np.array ( [ [ ' t','a','a','a ' ] , [ ' a','a','a','a ' ] , [ ' a','a','a','a ' ] , [ ' a','a','a','t ' ] ] ) "" "" "" returnsdict , for each state - action pair , maintain ( mean , visitedcount ) "" "" "" for r in range(rows_count ) : for c in range(columns_count ) : if not isterminal(r , c ) : for act in actions : returnsdict [ ( ( r , c ) , act ) ] = [ 0 , 0 ] # # maintain mean , and visitedcount for each state - action pair "" "" "" qfunc , we maintain the action - value for each state - action pair "" "" "" for r in range(rows_count ) : for c in range(columns_count ) : if not isterminal(r , c ) : for act in actions : qdict [ ( ( r , c ) , act ) ] = -9999 # # maintain q function value for each state - action pair def getvalue(row , col ) : # helper func , get state value global v if row = = -1 : row = 0 # if you bump into wall , you bounce back elif row = = 4 : row = 3 if col = = -1 : col = 0 elif col = = 4 : col = 3 return v[row , col ] def getrandomstartstate ( ) : illegalstate = true while illegalstate : r = random.randint(0 , 3 ) c = random.randint(0 , 3 ) if ( r = = 0 and c = = 0 ) or ( r = = 3 and c = = 3 ) : illegalstate = true else : illegalstate = false return r , c def getstate(row , col ) : if row = = -1 : row = 0 # helper func for the exercise:1 elif row = = 4 : row = 3 if col = = -1 : col = 0 elif col = = 4 : col = 3 return row , col def getrandomaction ( ) : global actdict return actdict[random.randint(0 , 3 ) ] def getmeanfromreturns(oldmean , n , curval ) : newmean = 0 if n = = 0 : raise exception('exception , incrementalmeanfunc , n should not be less than 1 ' ) elif n = = 1 : return curval elif n & gt;= 2 : newmean = ( float ) ( oldmean + ( 1.0 / n ) * ( curval - oldmean ) ) return newmean "" "" "" get the best action returns string action parameter is state tuple ( r , c ) "" "" "" def getargmaxactq(s_t ) : global qdict qvallist = [ ] salist = [ ] "" "" "" for example get together s1a1 , s1a2 , s1a3 , s1a4 find which is the maxvalue , and get the action which caused it "" "" "" sa1 = ( s_t , "" u "" ) sa2 = ( s_t , "" r "" ) sa3 = ( s_t , "" d "" ) sa4 = ( s_t , "" l "" ) salist.append(sa1 ) salist.append(sa2 ) salist.append(sa3 ) salist.append(sa4 ) q1 = qdict[sa1 ] q2 = qdict[sa2 ] q3 = qdict[sa3 ] q4 = qdict[sa4 ] qvallist.append(q1 ) qvallist.append(q2 ) qvallist.append(q3 ) qvallist.append(q4 ) maxq = max(qvallist ) ind_maxq = qvallist.index(maxq ) # gets the maxq value and the index which caused it "" "" "" when we have index of maxqval , then we know which sa - pair gave that maxqval = & gt ; we can access that action from the correct sa - pair "" "" "" argmaxact = salist[ind_maxq][1 ] return argmaxact "" "" "" qepisode generation func returns episodelist parameters are starting state , starting action "" "" "" def qepisode(r , c , act ) : global reward global policies "" "" "" note ! r , c will both be local variables inside this func they denote the nextstate ( s ' ) in this func "" "" "" statewasterm = false stepstaken = 0 curr = r curc = c episodelist = [ ( ( r , c ) , act , reward ) ] # add the starting ( s , a ) immediately if act = = "" u "" : # # up r -= 1 elif act = = "" r "" : # # right c + = 1 elif act = = "" d "" : # # down r + = 1 else : # # left c -= 1 stepstaken + = 1 r , c = getstate(r , c ) # # check status of the newstate ( s ' ) statewasterm = isterminal(r , c ) # # if status was terminal stop iteration , else keep going into loop if not statewasterm : curr = r curc = c while not statewasterm : if policies[curr , curc ] = = "" a "" : act = getrandomaction ( ) # # "" "" "" get the random action from policy "" "" "" else : act = policies[curr , curc ] # # "" "" "" get the deterministic action from policy "" "" "" if act = = "" u "" : # # up r -= 1 elif act = = "" r "" : # # right c + = 1 elif act = = "" d "" : # # down r + = 1 else : # # left c -= 1 stepstaken + = 1 r , c = getstate(r , c ) statewasterm = isterminal(r , c ) episodelist.append ( ( ( curr , curc ) , act , reward ) ) if not statewasterm : curr = r curc = c return episodelist print(""montecarlo program starting ... \n "" ) "" "" "" monte carlo q - function , exploring starts , every - visit , estimating pi ~~ pi * "" "" "" for iteration in range(1 , maxiters+1 ) : # # for all episodes print(""curiter = = "" , iteration ) print(""\n "" ) if iteration % 20 = = 0 : # # get random seed periodically to improve randomness performance random.seed(datetime.now ( ) ) for r in range(4 ) : for c in range(4 ) : if not isterminal(r , c ) : startr = r startc = c startact = getrandomaction ( ) # # startr , startc = getrandomstartstate ( ) # # get random starting - state , and starting action equiprobably # # startact = getrandomaction ( ) sequence = qepisode(startr , startc , startact ) # # generate q - sequence following policy pi , until terminal - state ( excluding terminal ) g = 0 for t in reversed(range(len(sequence ) ) ) : # # iterate through the timesteps in reversed order s_t = sequence[t][0 ] # # use temp variables as helpers a_t = sequence[t][1 ] r_t = sequence[t][2 ] g + = r_t # # increment g with reward , note ! the gamma discount factor = = 1.0 visitedcount = returnsdict[s_t , a_t][1 ] # # use temp visitedcount visitedcount + = 1 if visitedcount = = 1 : # # special case in iterative mean algorithm , the first visit to any state - action pair curmean = 9999 curmean = getmeanfromreturns(curmean , visitedcount , g ) returnsdict[s_t , a_t][0 ] = curmean # # update mean returnsdict[s_t , a_t][1 ] = visitedcount # # update visitedcount else : curmean = returnsdict[s_t , a_t][0 ] # # get temp mean from returnsdict curmean = getmeanfromreturns(curmean , visitedcount , g ) # # get the new temp mean iteratively returnsdict[s_t , a_t][1 ] = visitedcount # # update visitedcount returnsdict[s_t , a_t][0 ] = curmean # # update mean qdict[s_t , a_t ] = returnsdict[s_t , a_t][0 ] # # update the qfunction with the new mean value tempr = s_t[0 ] # # temp variables simply to disassemble the tuple into row , col tempc = s_t[1 ] policies[tempr , tempc ] = getargmaxactq(s_t ) # # update policy based on argmax_a[q(s_t ) ] print(""optimal policy with monte - carlo , every visit was \n "" ) print(""\n "" ) print(policies ) here is the updated "" hacky fix "" code that seems to get the algorithm "" over the hump "" without getting stuck into foreverloop with deterministic policy . my teacher had recommended that you do n't need to update policy at every step in this kind of monte carlo , so you could have made the policy updates at periodic intervals using python 's modulo operator on the iterationscount or something . also , i had the bright idea that the sutton&amp;barto book described that all state - action pairs must be visited , very large amount of times , for the exploring starts pre - condition of the algorithm to be fulfilled . so , i then decided to enforce the algorithm to have run at least once for all state - action pairs so you start from each state - action pair deterministically one - by - one ( for each episode actually ) . this would still be run with the old randomwalk policy in this early exploration phase , where we are gathering data into the returnsdict , and qdict , but not yet improving deterministic policy . import numpy as np import numpy.linalg as la import random from datetime import datetime random.seed(datetime.now ( ) ) rows_count = 4 columns_count = 4 def isterminal(r , c ) : # helper function to check if terminal state or regular state global rows_count , columns_count if r = = 0 and c = = 0 : # i m a bit too lazy to check otherwise the iteration boundaries return true # so that this helper function is a quick way to exclude computations if r = = rows_count - 1 and c = = columns_count - 1 : return true return false "" "" "" note about maxiters ! ! ! the monte - carlo every visit algorithm implements total amount of iterations with formula totaliters = maxiters * nonterminalstates * possibleactions totaliters = 5000 * 14 * 4 totaliters = 280000 in other words , there will be 5k iterations per each state - action pair in other words there will be an early exploration phase where policy willnot be updated , but the gridworld will be explored with randomwalk policy , gathering qfunc information , and returndict information . in early phase there will be about 27 iterations for each state - action pair during , non - policy - updating exploration ( maxiters * explorationfactor ) / ( stateactionpairs ) = 7500 * 0.2 /56 after that early exploring with randomwalk , then we act greedily w.r.t . the q - function , for the rest of the iterations to get the optimal deterministic policy "" "" "" maxiters = 7500 explorationfactor = 0.2 # # explore that percentage of the first maxiters rounds , try to increase it , if you get stuck in foreverloop , in qepisode function reward = -1 actions = [ "" u "" , "" r "" , "" d "" , "" l "" ] v = np.zeros((rows_count , columns_count ) ) returnsdict= { } qdict= { } actdict={0:""u"",1:""r"",2:""d"",3:""l "" } policies = np.array ( [ [ ' t','a','a','a ' ] , [ ' a','a','a','a ' ] , [ ' a','a','a','a ' ] , [ ' a','a','a','t ' ] ] ) "" "" "" returnsdict , for each state - action pair , maintain ( mean , visitedcount ) "" "" "" for r in range(rows_count ) : for c in range(columns_count ) : if not isterminal(r , c ) : for act in actions : returnsdict [ ( ( r , c ) , act ) ] = [ 0 , 0 ] # # maintain mean , and visitedcount for each state - action pair "" "" "" qfunc , we maintain the action - value for each state - action pair "" "" "" for r in range(rows_count ) : for c in range(columns_count ) : if not isterminal(r , c ) : for act in actions : qdict [ ( ( r , c ) , act ) ] = -9999 # # maintain q function value for each state - action pair def getvalue(row , col ) : # helper func , get state value global v if row = = -1 : row = 0 # if you bump into wall , you bounce back elif row = = 4 : row = 3 if col = = -1 : col = 0 elif col = = 4 : col = 3 return v[row , col ] def getrandomstartstate ( ) : illegalstate = true while illegalstate : r = random.randint(0 , 3 ) c = random.randint(0 , 3 ) if ( r = = 0 and c = = 0 ) or ( r = = 3 and c = = 3 ) : illegalstate = true else : illegalstate = false return r , c def getstate(row , col ) : if row = = -1 : row = 0 # helper func for the exercise:1 elif row = = 4 : row = 3 if col = = -1 : col = 0 elif col = = 4 : col = 3 return row , col def getrandomaction ( ) : global actdict return actdict[random.randint(0 , 3 ) ] def getmeanfromreturns(oldmean , n , curval ) : newmean = 0 if n = = 0 : raise exception('exception , incrementalmeanfunc , n should not be less than 1\n ' ) elif n = = 1 : return curval elif n & gt;= 2 : newmean = ( float ) ( oldmean + ( 1.0 / n ) * ( curval - oldmean ) ) return newmean "" "" "" get the best action returns string action parameter is state tuple ( r , c ) "" "" "" def getargmaxactq(s_t ) : global qdict qvallist = [ ] salist = [ ] "" "" "" for example get together s1a1 , s1a2 , s1a3 , s1a4 find which is the maxvalue , and get the action which caused it "" "" "" sa1 = ( s_t , "" u "" ) sa2 = ( s_t , "" r "" ) sa3 = ( s_t , "" d "" ) sa4 = ( s_t , "" l "" ) salist.append(sa1 ) salist.append(sa2 ) salist.append(sa3 ) salist.append(sa4 ) q1 = qdict[sa1 ] q2 = qdict[sa2 ] q3 = qdict[sa3 ] q4 = qdict[sa4 ] qvallist.append(q1 ) qvallist.append(q2 ) qvallist.append(q3 ) qvallist.append(q4 ) maxq = max(qvallist ) ind_maxq = qvallist.index(maxq ) # gets the maxq value and the index which caused it "" "" "" when we have index of maxqval , then we know which sa - pair gave that maxqval = & gt ; we can access that action from the correct sa - pair "" "" "" argmaxact = salist[ind_maxq][1 ] return argmaxact "" "" "" qepisode generation func returns episodelist parameters are starting state , starting action "" "" "" def qepisode(r , c , act ) : "" "" "" ideally , we should not get stuck in the gridworld ... but , but sometiems when policy transitions from the first episode 's policy = = randomwalk , then , on second episode sometimes we get stuck in foreverloop in episode generation usually the only choice then seems to restart the entire policy into randomwalk ? ? ? "" "" "" global reward global policies "" "" "" note ! r , c will both be local variables inside this func they denote the nextstate ( s ' ) in this func "" "" "" stepstaken = 0 curr = r curc = c episodelist = [ ( ( r , c ) , act , reward ) ] # add the starting ( s , a ) immediately if act = = "" u "" : # # up r -= 1 elif act = = "" r "" : # # right c + = 1 elif act = = "" d "" : # # down r + = 1 elif act = = "" l "" : # # left c -= 1 stepstaken + = 1 r , c = getstate(r , c ) # # check status of the newstate ( s ' ) statewasterm = isterminal(r , c ) # # if status was terminal stop iteration , else keep going into loop if not statewasterm : curr = r curc = c while not statewasterm : if policies[curr , curc ] = = "" a "" : act = getrandomaction ( ) # # "" "" "" get the random action from policy "" "" "" else : act = policies[curr , curc ] # # "" "" "" get the deterministic action from policy "" "" "" if act = = "" u "" : # # up r -= 1 elif act = = "" r "" : # # right c + = 1 elif act = = "" d "" : # # down r + = 1 else : # # left c -= 1 stepstaken + = 1 r , c = getstate(r , c ) statewasterm = isterminal(r , c ) episodelist.append ( ( ( curr , curc ) , act , reward ) ) if not statewasterm : curr = r curc = c if stepstaken & gt;= 100000 : raise exception(""exception raised , because program got stuck in mc qepisode generation ... \n "" ) return episodelist print(""montecarlo program starting ... \n "" ) "" "" "" monte carlo q - function , exploring starts , every - visit , estimating pi ~~ pi * "" "" "" "" "" "" it appears that the qfunction apparently can be unreliable in the early episodes rounds , so we can avoid getting stuck in foreverloop because of unreliable early episodes , but ... we got ta delay updating the policy , until we have explored enough for a little bit ... so our qfunction has reliable info inside of it , to base the decision on , later ... "" "" "" q_function_is_reliable = false # # variable shows if we are currently updating the policy , or just improving q - function and exploring for iteration in range(1 , maxiters+1 ) : # # for all episodes print(""curiter = = "" , iteration , "" , qfunctionisreliable = = "" , q_function_is_reliable ) print(""\n "" ) if iteration % 20 = = 0 : # # get random seed periodically to improve randomness performance random.seed(datetime.now ( ) ) for r in range(4 ) : # # for every non - terminal - state for c in range(4 ) : if not isterminal(r , c ) : startr = r startc = c for act in actions : # # for every action possible startact = act sequence = qepisode(startr , startc , startact ) # # generate q - sequence following policy pi , until terminal - state ( excluding terminal ) g = 0 for t in reversed(range(len(sequence ) ) ) : # # iterate through the timesteps in reversed order s_t = sequence[t][0 ] # # use temp variables as helpers a_t = sequence[t][1 ] r_t = sequence[t][2 ] g + = r_t # # increment g with reward , gamma discount factor is zero visitedcount = returnsdict[s_t , a_t][1 ] visitedcount + = 1 # # if ( s_t , a_t , -1 ) not in sequence[:t ] : # # this is how you could have done the first - visit mc , but we do every - visit now ... if visitedcount = = 1 : # # special case in iterative mean algorithm , the first visit to any state - action pair curmean = 9999 curmean = getmeanfromreturns(curmean , visitedcount , g ) returnsdict[s_t , a_t][0 ] = curmean # # update mean returnsdict[s_t , a_t][1 ] = visitedcount # # update visitedcount else : curmean = returnsdict[s_t , a_t][0 ] # # get temp mean from returnsdict curmean = getmeanfromreturns(curmean , visitedcount , g ) # # get the new temp mean iteratively returnsdict[s_t , a_t][1 ] = visitedcount # # update visitedcount returnsdict[s_t , a_t][0 ] = curmean # # update mean qdict[s_t , a_t ] = returnsdict[s_t , a_t][0 ] # # update the qfunction with the new mean value tempr = s_t[0 ] # # temp variables simply to disassemble the tuple into row , col tempc = s_t[1 ] if iteration & gt;= round(maxiters * explorationfactor ) : # # only start updating policy when we have reliable estimates for qfunction , that is when iteration & gt ; maxiter/10 q_function_is_reliable = true policies[tempr , tempc ] = getargmaxactq(s_t ) # # update policy based on argmax_a[q(s_t ) ] print(""optimal policy with monte - carlo , every visit was \n "" ) print(""\n "" ) print(policies )",23915,23915,2019-04-19T09:42:33.403,2019-04-19T09:42:33.403,"monte - carlo , every - visit gridworld , exploring starts , python code gets stuck in foreverloop in episode generation",python monte-carlo,1,6,
3456,11888,1,,2019-04-19T00:22:01.707,2,48,"at the appendix a of paper "" near - optimal representation learning for hierarchical reinforcement learning "" , the authors express the -discounted state visitation frequency $ d$ of policy as $ $ d=(1-\gamma)a_\pi(i-\gamma^cp_\pi^c)^{-1}\mu\tag 1 $ $ i 've simplifed the notation for easy reading , hoping it does not introduce any error . in the above definition , $ p_\pi^c$ the $ c$ -step transition matrix under the policy , i.e. , $ p_{\pi}^c = p_\pi(s_{c}|s_0)$ , a dirac distribution centered at start state $ s_0 $ and $ $ a_\pi = i+\sum_{k=1}^{c-1}\gamma^kp_\pi^k\tag 2 $ $ they further give the every- $ c$ -steps -discounted state frequency of as $ $ d^c_\pi=(1-\gamma^c)(i-\gamma^cp_\pi^c)^{-1}\mu\tag 3 $ $ to my best knowledge , $ a_\pi$ seems to be the unnormalized -discounted state frequency , but i can not really make sense of the rest . i 'm hoping that someone can shed some light on these definitions . update thank @philip raeisghasem for pointing out the paper cpo . here 's what i 've gotten from that . applying the sum of the geometric series to eq . $ ( 2)$ , we have $ $ a={(i-\gamma^cp_\pi^c)(i-\gamma p_\pi)^{-1}}\tag4 $ $ plugging eq . $ ( 4)$ back into eq . $ ( 1)$ , we get the same result as eq . $ ( 18)$ in the cpo paper : $ $ d=(1-\gamma)(i-\gamma p_\pi)^{-1}\mu\tag 5 $ $ where $ ( 1-\gamma)$ normalizes all weights introduced by so that they are summed to one . however , i 'm still confused . here are the questions i have i can make sense of eq . $ ( 5)$ in infinite horizon . but i do not understand why we have it in the hierarchical policy . to my best knowledge , policies here are low - level , which means they are only valid in a short horizon ( $ c$ steps , for example ) . computing state frequency in infinite horizon here seems confusing . what should i make of $ d_\pi^c$ defined in eq . $ ( 3)$ , originally from eqs . $ ( 26)$ and $ ( 27)$ in the paper ? the authors define them as every- $ c$ -steps -discounted state frequencies of policy . but i do not see why it is the case . to me , they are more like the consequence of eq . $ ( 30)$ in the paper . sorry if anyone feels that this update makes this question too broad . this is kept since i 'm not so sure whether i can get a satisfactory answer without these questions . any partial answer will be sincerely appreciated . thanks in advance .",8689,8689,2019-04-24T00:10:40.357,2019-04-24T00:10:40.357,intuition behind -discounted state frequency,reinforcement-learning,0,4,
3457,11889,1,,2019-04-19T04:37:06.437,1,60,"i have looked at the documentation for the neat python api found here , but it shows calculus like this : the error for each genome is $ 1-\sum_i(e_i - a_i)^2 $ i have n't learned calculus at the moment . so , can someone please explain what the calculation means ?",24036,1671,2019-04-19T19:31:29.750,2019-05-20T12:01:45.323,what does the formula $ 1-\sum_i(e_i - a_i)^2 $ mean in this neat python api ?,ai-basics math neat notation,2,6,
3458,11894,1,,2019-04-19T16:47:09.540,0,40,"i am trying classify cifar10 . the cnn that i generated over fits when the accuracy reaches ~77 % . the code and the plot is given below . i tried dropout , batch normalization and l2 regularization . but the accuracy does not go beyond ~77 . how can i identify the areas to be corrected to reduce over fitting ? convolutional_model = sequential ( ) # 32 convolutional_model.add(conv2d(32 , ( 3 , 3 ) , activation='relu ' , input_shape=(32 , 32 , 3 ) , kernel_regularizer = regularizers.l2(.0002 ) ) ) convolutional_model.add(conv2d(64 , ( 3 , 3 ) , activation='relu ' , kernel_regularizer = regularizers.l2(.0002 ) ) ) convolutional_model.add(maxpooling2d(pool_size=(2 , 2 ) , strides=(2 , 2 ) ) ) # 64 convolutional_model.add(conv2d(64 , ( 3 , 3 ) , activation='relu ' , kernel_regularizer = regularizers.l2(.0002 ) ) ) convolutional_model.add(conv2d(128 , ( 3 , 3 ) , activation='relu ' , padding='same ' , kernel_regularizer = regularizers.l2(.0002 ) ) ) convolutional_model.add(maxpooling2d(pool_size=(2 , 2 ) , strides=(2 , 2 ) ) ) convolutional_model.add(flatten ( ) ) convolutional_model.add(dropout(0.5 ) ) convolutional_model.add(dense(128 , activation='relu ' ) ) convolutional_model.add(dense(10 , activation='softmax ' ) ) print(convolutional_model.summary ( ) ) convolutional_model.compile(optimizer='adam ' , loss='categorical_crossentropy ' , metrics=['accuracy ' ] ) es = earlystopping(monitor='val_loss ' , mode='min ' , verbose=2 , patience=8 ) history = convolutional_model.fit(x_train_part , y_train_part , epochs=200 , verbose=2,validation_data=(x_train_validate , y_train_validate ) , callbacks=[es ] ) scores = convolutional_model.evaluate(x_test , y_test , verbose=2 )",23734,23734,2019-04-20T01:38:04.987,2019-04-20T10:01:54.327,how to identify the areas to reduce over fitting ?,deep-learning convolutional-neural-networks overfitting,1,3,
3459,11895,1,,2019-04-19T17:05:52.470,2,66,"th bayes ' rule specifies how an agent should update its belief in a proposition based on a new piece of evidence . suppose an agent has a current belief in proposition $ h$ based on evidence $ k$ already observed , given by $ p(h \mid k)$ , and subsequently observes $ e$ . in artificial intelligence , where can we use the bayes ' rule ? i am unable to understand this concept .",23529,2444,2019-04-19T18:21:14.213,2019-04-20T16:42:08.487,where can we use the bayes ' theorem in artificial intelligence ?,applications bayes-theorem,2,0,
3460,11900,1,11947,2019-04-20T01:13:46.130,1,18,"the bert paper https://arxiv.org/pdf/1810.04805.pdf section 4.2 covers the squad training . so from my understanding , there are two extra parameters trained , they are two vectors with the same dimension as the hidden size , so the same dimensions as the contextualized embeddings in bert . they are s ( for start ) and e ( for end ) . for each , a softmax is taken with s and each of the final contextualized embeddings to get a score for the correct start position . and the same thing is done for e and the correct end position . i get up to this part . but i am having trouble figuring out how the did the labeling and final loss calculations , which is described in this paragraph "" and the maximum scoring span is used as the prediction . the training objective is the loglikelihood of the correct start and end positions . "" what do they mean by "" maximum scoring span is used as the prediction "" ? furthermore , how does that play into "" the training objective is the loglikelihood of the correct start and end positions "" ? from this source : https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ it says the log - likelihood is only applied to the correct classes . so the we are only calculating the softmax for the correct positions only , not any of the in correct positions . if this interpretation is correct , then the loss will be loss = -log ( softmax(s*t(predictedstart ) / sum(s*ti ) ) -log ( softmax(e*t(predictedend ) / sum(s*ti ) )",18358,,,2019-04-22T21:33:07.277,having trouble figuring out how loss was calculated for squad task in bert paper,natural-language-processing loss-functions,1,0,
3461,11903,1,,2019-04-20T10:19:55.007,0,26,"i am trying to develop a time series model using autoregression . the data set is like as follows index maxima 0 0.743 1 0.837 2 0.838 4 0.896 5 1.014 6 1.003 7 1.01 8 1.101 9 1.097 the maxima point is given is the largest points on each curve . basically , i have to perform multi - step forecasting ( at least 9 steps ahead ) . i 've done it using the recursive approach . but the accuracy of the prediction getting worse as it reaches the end . predicted result python code using the ar model from stats model # fit model for max value model = ar(data ) model_fit = model.fit ( ) yhat_max = model_fit.predict(len(data),len(data ] ) ) for obtaining an accurate prediction , what changes should be done in the approach ? or do i have to change the model ? any kind of help is appreciated .",24006,,,2019-04-20T11:09:43.587,auto - regression - reduce error in prediction,python linear-regression statistical-ai,1,1,
3462,11906,1,,2019-04-20T11:08:42.683,1,32,"consider a feed - forward neural network with one hidden layer . how are the weights between the input and hidden layer updated , after the weights between the hidden layer and output layer are updated ?",24062,2444,2019-04-20T20:04:46.410,2019-04-20T20:04:46.410,how are the weights between the input and hidden layer updated in a 3 layer neural network ?,neural-networks machine-learning backpropagation,0,2,
3463,11910,1,,2019-04-20T18:50:50.313,0,22,"i 'm trying to create a service which compares 2 human poses as a part of my main application . i managed to plot skeletons on the images using openpose . a detailed explanation of my question : i have 2 images of 2 different people . i ran those images through openpose to generate the output with the skeleton on it . now i want to compare the two images . i want to know what basis i can choose for comparing the two images . by comparing , i mean figures so that i can calculate a threshold value . also , if this can somehow be placed on a coordinate plane and i could get to know the coordinate units of the different body parts to draw out the differences , that would be great . i also want to calculate a score based on the similarity between the two poses but first i need to know the basis of comparison and then this can be figured out eventually .",24067,,,2019-04-20T18:50:50.313,human pose comparison,machine-learning image-recognition python javascript,0,1,
3464,11912,1,,2019-04-20T20:36:44.257,1,34,"say i want to train a neural network with 10 classes as outputs and use categorical_cross_entropy as a loss function in keras . this will try to fit the training data as best as possible irregardless of the outcome ( i.e. value ) . if i want to take value into account , i have to use something like a policy gradient rl algorithm . how do i formulate the loss of policy gradient algorithm in this case ? the standard categorical cross entropy loss function is as follows where y _ = true value , and y = predicted value : loss = -mean ( y _ * log(y ) ) i am thinking to just multiply the true value by the reward and still use the categorical cross entropy of keras i.e. y _ = y _ * reward loss = -mean ( y _ * log(y ) ) is my interpretation correct ?",20456,24073,2019-04-21T09:19:18.293,2019-04-21T09:19:18.293,policy gradient loss for neural network training,reinforcement-learning keras policy-gradients reinforce,0,5,
3465,11914,1,11920,2019-04-21T08:20:59.010,2,25,let 's say you have an input which can take one of 10 different unique values . how would you encode it ? have input length 10 and one - hot encode it . have 1 input but normalise the value between the input range . would the end result be the same ?,20352,2444,2019-04-21T13:02:49.870,2019-04-21T13:08:38.553,how should i encode a categorical input ?,neural-networks machine-learning ai-design,1,0,1
3466,11915,1,,2019-04-21T10:58:38.687,2,15,"in the control theory , a forward model describes predicted behaviors of a system . a forward model of a car physics can calculate the position of the car with x / y value if the steering wheel is put in the middle , left or right . the outcome prediction differs , if the speed is low , middle or fast . a naive approach to create such action models is with ordinary differential equations which are forming a mathematical state space . a more elaborated technique is to use machine learning for generating the forward model only with data in the loop . the literature has introduced the term "" action model learning "" for the regression of a forward model . most papers have symbolic tasks in mind which results into a pddl action model , but in theory it 's possible to create numerical values as well with the technique . the problem is , that possible machine learning algorithm are endless . i 've found in the literature action learning frameworks which are working with decision trees , recurrent neural networks , inductive logic programming and max - sat solvers . are there are some examples from real life robotics challenges available ( like the following problem or robocup soccer ) , in which someone has used action model learning with one of the cited algorithm and has recognized if it 's working or not ? which mistakes could be made using such ml techniques , or which situations the forward model ca n't learned ( e.g. using a decision tree , because e.g. the state space is too large ) ?",11571,2444,2019-04-21T12:49:51.997,2019-04-21T12:49:51.997,is action model learning with machine learning techniques feasible ?,machine-learning robotics action-model-learning control-theory,0,0,
3467,11921,1,,2019-04-21T14:24:27.937,2,11,"i am working on a cnn model for image classification . currently a single aws instance ( p2xlarge ) used for model training . i would like to have one more aws instance and distribute the load across these 2 aws instances ( p2xlarge ) . is there a way i can create cluster between 2 aws instances so that i can train the model in parallel ? kindly direct me to the right stackexchange site if this is not the right one . thank you , kk",17831,17831,2019-04-23T06:11:11.310,2019-04-23T06:11:11.310,cluster 2 aws instances for deep learning model training,deep-learning gpu,0,0,
3468,11923,1,,2019-04-21T17:59:01.177,0,19,"can an ai be created to work on itself independently without human assistance from the time it is created ? can it access all the data it wants to from the internet as well as the dark web ? can it have an influence over human minds like controlling the way they think , their thoughts , etc ?",24081,,,2019-04-21T17:59:01.177,can an ai be created to work on itself independently without human assistance from the time it is created ?,ai-design,0,1,
3469,11924,1,11996,2019-04-21T18:02:13.173,0,36,"i understand that gamma is an important factor in determining the rewards for a deep q agent , however during testing of my network i am noticing that the agent is outputting more actions to "" do nothing "" as it learns more about it 's given data set ( financial stock data ) . i have tried tweaking the gamma at different levels ranging from 0 - 1 and everywhere in between , however as the agent continues to learn , the times between actions is getting longer , and longer . this behaviour is undesirable , and it is preferable that the agent be making more often , short - term actions even if they result in reduced reward . does anyone have any tips on how to achieve this ? would a minus gamma have adverse effects on the network ? tldr : time between actions becoming increasingly long over time , would prefer an agent that makes many actions over long - term ones .",20893,20893,2019-04-24T15:07:14.617,2019-04-24T15:07:14.617,encourage deep q to seek short - term reward,deep-learning q-learning dqn rewards,1,6,
3470,11926,1,,2019-04-21T18:24:21.503,0,6,"i need to train an lstm on some light curves , in order to find a signal ( there are 2 classes signal and background ) . however the signal ( data points corresponding to signal ) is around 100 times less frequent than the background so i have a huge class imbalance and in the end all points are labelled as background . i tried to use focal loss , but it does n't help . is there a way to make it work ?",23871,,,2019-04-21T18:24:21.503,training lstm with class imbalance,lstm,0,0,
3471,11927,1,11932,2019-04-21T18:32:46.213,3,70,"nowadays , robots or artificial agents often only perform the specific task they have been programmed to do . will we be able to build an artificial intelligence that feels empathy , that understands the emotions and feelings of humans , and , based on that , act accordingly ?",23538,4302,2019-05-18T22:07:38.403,2019-05-18T22:07:38.403,will we be able to build an artificial intelligence that feels empathy ?,ai-design philosophy emotional-intelligence,1,2,1
3472,11929,1,,2019-04-21T19:23:33.580,4,58,"reading sutton and barto , i see the following in describing policy gradients : how is the gradient calculated with respect to an action ( taken at time t ) ? i 've read implementations of the algorithm , but conceptually i 'm not sure i understand how the gradient is computed , since we need some loss function to compute the gradient . i 've seen a good pytorch article , but i still do n't understand the meaning of this gradient conceptually , and i do n't know what i 'm looking to implement . any intuition that you could provide would be helpful .",16343,22916,2019-04-22T03:51:44.270,2019-04-22T05:17:31.570,how is the policy gradient calculated in reinforce ?,reinforcement-learning policy-gradients rl-an-introduction notation reinforce,1,8,1
3473,11930,1,,2019-04-21T22:40:10.007,0,24,"i am currently studying myself with a subject "" representational(expressive power ) of neural network "" and trying to intentionally fully overfit the neural network which means that at least the model has a power to construct a mapping perfectly in training data input / output . my current data for this experiment is mnist , and i am trying to use autoencoder / decoder structure to check whether i could intentionally overfit the neural network with which network structure . i usually interested in which combination of latent dimension size and how many relu 's are the best combination to enlarge the expressive power of neural network , which means that the combination minimally achieve the training loss(in this case i 'd used binary cross entropy between x and recon_x ) problem is , i did not succeeded to intentionally overfit(loss is almost closely 0 ) . i had tried several deep / shallow fcn with different latent dimension , my best minimal loss achievement is 55 , which looks too much compared to 0 . import torch import torch.nn as nn from utils import idx2onehot class ae(nn.module ) : def _ _ init__(self , encoder_layer_sizes , latent_size , decoder_layer_sizes , conditional = false , num_labels=0 ) : super().__init _ _ ( ) if conditional : assert num_labels & gt ; 0 assert type(encoder_layer_sizes ) = = list assert type(latent_size ) = = int assert type(decoder_layer_sizes ) = = list self.latent_size = latent_size self.encoder = encoder ( encoder_layer_sizes , latent_size , conditional , num_labels ) self.decoder = decoder ( decoder_layer_sizes , latent_size , conditional , num_labels ) def forward(self , x , c = none ) : if x.dim ( ) & gt ; 2 : x = x.view(-1 , 28 * 28 ) z = self.encoder(x , c ) recon_x = self.decoder(z , c ) return recon_x , z def inference(self , device , n=1 , c = none ) : batch_size = n z = torch.randn([batch_size , self.latent_size]).to(device ) recon_x = self.decoder(z , c ) return recon_x class encoder(nn.module ) : def _ _ init__(self , layer_sizes , latent_size , conditional , num_labels ) : super().__init _ _ ( ) self.conditional = conditional if self.conditional : layer_sizes[0 ] + = num_labels self.mlp = nn.sequential ( ) for i , ( in_size , out_size ) in enumerate(zip(layer_sizes[:-1 ] , layer_sizes[1 : ] ) ) : print(i , "" : "" , in_size , out_size ) self.mlp.add_module(name=""l{:d}"".format(i ) , module = nn.linear(in_size , out_size ) ) if i ! = len(layer_sizes ) : print(""relu added @ encoder "" ) self.mlp.add_module(name=""a{:d}"".format(i ) , module = nn.relu ( ) ) # self.mlp.add_module(name=""bn{:d}"".format(i ) , # module = nn.batchnorm1d(out_size ) ) self.linear = nn.linear(layer_sizes[-1 ] , latent_size ) def forward(self , x , c = none ) : if self.conditional : c = idx2onehot(c , n=10 ) x = torch.cat((x , c ) , dim=-1 ) x = self.mlp(x ) z = self.linear(x ) return z class decoder(nn.module ) : def _ _ init__(self , layer_sizes , latent_size , conditional , num_labels ) : super().__init _ _ ( ) self.mlp = nn.sequential ( ) self.conditional = conditional if self.conditional : input_size = latent_size + num_labels else : input_size = latent_size for i , ( in_size , out_size ) in enumerate ( zip([input_size]+layer_sizes[:-1 ] , layer_sizes ) ) : print(i , "" : "" , in_size , out_size ) self.mlp.add_module ( name=""l{:d}"".format(i ) , module = nn.linear(in_size , out_size ) ) if i+1 & lt ; len(layer_sizes ) : if i ! = 0 : print(""relu added @ decoder "" ) self.mlp.add_module(name=""a{:d}"".format(i ) , module = nn.relu ( ) ) # self.mlp.add_module(name=""bn{:d}"".format(i ) , # module = nn.batchnorm1d(out_size ) ) else : print(""sig step "" ) self.mlp.add_module(name=""sigmoid "" , module = nn.sigmoid ( ) ) def forward(self , z , c ) : if self.conditional : c = idx2onehot(c , n=10 ) z = torch.cat((z , c ) , dim=-1 ) x = self.mlp(z ) return x this is the model code that i had used , and if i put [ 784 , 256 , 256 ] to variable "" layer_sizes "" the model generates encoder decoder symmetrically with given input / output dimensional linear transformation with relu between it . i had experimented lots of "" layer_sizes "" and i attach the logs of it for references . # # goal of the project the project goal is about the way to determine the ` optimal number of latent dimension ` . first , the project introduces the linearity and non - linearity and postulates the assumption that linearity corresponds to ` one ` dimension . then , this linearity could be split into ` two ` non - overlapping dimension by one relu based non - linearity . therefore , this project shows that the determination of optimal number of latent dimension preliminarily ` not depend on the data distribution itself ` , but depends on ` the network structure ` , more specifically , depends on the ` total number of dimension that the model about to express ` . the paper will call this total number of dimension that the model about to express as * * model dimension**. after the model dimension being set , one can train the network and check whether it 's possible to over - fit the network with the data given . if the data points over - fit in some point of train epochs , this network can be thought as "" enough to express the data distribution "" . however , if not over - fit , one can consider to enlarge the * * model dimension * * and re - try the over - fit process . # # to - do define the over - fit . the classification threshold of over - fit depends on the experiment . - in which epoch of training process one should determine over - fit ? # # caution it 's better to use whole data when to determine the "" model dimension "" since it 's about how much non - linearity is required for the collected or targeted data domain . # # convergence determination metric when the epochavgloss doe not change more than 1 % for 5 epochs from the first epoch , we determine the training loss being converged # # experiment workflow # # # # # exp_1 : 1 relu applied to 256 dimension . ( then linear transformation to latentdim ) by the assumption , the * * model dimension * * is 512(256 * 2 ) . thus , we verify the assumption by 1 ) check the sequential decrease of loss at certain train epoch while sequentially increase the latentdim with ` 1 * ( mlp + relu ) + latentdim 1 ` epoch 09/10 batch 0937/937 , loss 165.5437 with ` 1 * ( mlp + relu ) + latentdim 2 ` epoch 09/10 batch 0937/937 , loss 150.2990 with ` 1 * ( mlp + relu ) + latentdim 3 ` epoch 09/10 batch 0937/937 , loss 133.2206 with ` 1 * ( mlp + relu ) + latentdim 4 ` epoch 09/10 batch 0937/937 , loss 138.1151 with ` 1 * ( mlp + relu ) + latentdim 8 ` epoch 09/10 batch 0937/937 , loss 110.9839 with ` 1 * ( mlp + relu ) + latentdim 16 ` epoch 09/10 batch 0937/937 , loss 89.6707 with ` 1 * ( mlp + relu ) + latentdim 32 ` epoch 09/10 batch 0937/937 , loss 72.5663 with ` 1 * ( mlp + relu ) + latentdim 64 ` epoch 09/10 batch 0937/937 , loss 54.2545 & gt ; ... since the model converges at latentdim 64 with loss 52 , we shrink down the relu_inputdim to 32 ( go to exp3 ) with ` 1 * ( mlp + relu ) + latentdim 128 ` epoch 09/10 batch 0937/937 , loss 54.3565 with ` 1 * ( mlp + relu ) + latentdim 256 ` epoch 09/10 batch 0937/937 , loss 52.3050 & gt ; ... must keep decreasing . write the code to automatically does this job with ` 1 * ( mlp + relu ) + latentdim 512 ` epoch 09/10 batch 0937/937 , loss 53.2412 & gt ; ... check whether at any latentdim & gt ; 512 , no decrease of loss at fixed train epoch . with ` 1 * ( mlp + relu ) + latentdim 1024 ` epoch 09/10 batch 0937/937 , loss 54.3255 & gt ; as you see , with the expansion of latentdim ` doubled ` , still the lossatfixedstep is not decreased , which means model dimension already being saturated . # # # # exp_2 : now introduce the twice more model dimension by relu with ` 2 * ( mlp + relu ) + latentdim 1024 ` & gt ; epoch 09/10 batch 0937/937 , loss 57.9039 ( without bias .. the sequential relu does n't work ) # # # exp_3 : shrink down relu inputdim to 32 maintaining latentdim 64 # # # summary of algorithm if convgeloss ! = 0 : if modeldim & gt ; latentdim : enlarge latentdim if modeldim = & lt ; latentdim : increase # relu * modeldim = 2 * num_relus to verify this , @ exp latentdim 64 , convergeloss 80 , layersize [ 784 , 32 ] , if one increase the latentdim , convergeloss should not be below 80 let 's check ! @ exp latentdim 128 , convergeloss 80 , layersize [ 784 , 32 ] , convergeloss 80 now , let 's add stack the double relu layers , [ 784 , 32 , 32 ] , which is assumably represents 128 dimension @ exp latentdim 128 , convergeloss 80 , layersize [ 784 , 32 , 32 ] , convergeloss 80 ( still same ) as you see , without enlarge of foremost dimension , the deeper relu does not work . this is reference from raghu(2017 ) now make it wide , such as [ 784 , 64 ] , @ exp_1555829642 latentdim 128 , convergeloss 80 , layersize [ 784 , 64 ] , the convergeloss 65 & lt ; 80 moreover , make it more wide , such as [ 784 , 128 ] , @ exp_1555829642 latentdim 128 , convergeloss 55 , layersize [ 784 , 128 ] , the convergeloss 55 & lt ; 80 moreover , make it more wide , such as [ 784 , 256 ] , @ exp_1555832143 latentdim 128 , convergeloss 55 , layersize [ 784 , 256 ] , the convergeloss 55 = 55 the problem is , latentdim . make sure the latentdim is sufficient @ exp_1555832638 latentdim 256 , convergeloss 55 , layersize [ 784 , 256 ] , the convergeloss 55 = 55 = = = & gt ; question ! how to determine latentdim with less effort not getting through this cumbersome experimental step ? the problem is , latentdim . make sure the latentdim is sufficient @ exp_1555832638 latentdim 128 , convergeloss 65 , layersize [ 784 , 256 , 256 ] , the convergeloss 65 & gt ; 55 the problem is , latentdim . make sure the latentdim is sufficient @ exp_1555832638 latentdim 256 , convergeloss 65 , layersize [ 784 , 256 , 256 ] , the convergeloss 68 & gt ; 55 the problem is , latentdim . make sure the latentdim is sufficient @ exp_1555832638 latentdim 64 , convergeloss 65 , layersize [ 784 , 256 , 256 ] , the convergeloss 68 & gt ; 55 the problem is , latentdim . make sure the latentdim is sufficient @ exp_1555832638 latentdim 128 , convergeloss 60 , layersize [ 784 , 256 , 128 ] , the convergeloss 60 & gt ; 55 the problem is , latentdim . make sure the latentdim is sufficient @ exp_1555834546 latentdim 64 , convergeloss 65 , layersize [ 784 , 256 , 256 ] , the convergeloss 55 = 55 = = = = = & gt ; decrease the latentdim makes the model to learn better ( q1 ) the problem is , latentdim . make sure the latentdim is sufficient @ exp_1555834546 latentdim 32 , convergeloss 65 , layersize [ 784 , 256 , 256 ] , the convergeloss 60 & gt ; 55 if one check the currently get 55 , listed as : [ 784 , 128 ] , ld 128 [ 784 , 128 ] , ld 256 [ 784 , 256 , 256 ] , ld 64 @ 1555843696 , ld64 [ 784 , 128 , 128 ] convergeloss 60&gt;55 @ 1555844254 , ld128 [ 784 , 128 , 128 ] convergeloss 64&gt;55 @ 1555844254 , ld32 [ 784 , 128 , 128 ] convergeloss 66&gt;55 do nt know why , but if the network is deeper , too many latent space decrease the learning efficiency ( q1 ) the problem is , latentdim . make sure the latentdim is sufficient @ exp_1555832638 latentdim 32 , convergeloss 65 , layersize [ 784 , 256 , 256 ] , the convergeloss 55 = 55 maybe , if the modeldim is too big and latentdim is too small , as seen in exp [ 784 , 32 , 32 ] , training might be not working . thus , we have leverage up the latentdim at the same setting from 128 to 256 @ exp_1555830495 convergeloss 80 ( still same )",24085,,,2019-04-21T22:40:10.007,has anyone succeeded to ( intentionally ) overfit the neural network with mnist ?,autoencoders relu,0,1,
3474,11933,1,,2019-04-22T07:05:48.920,1,31,"i have two questions : 1 ) i have been reading an article on alphago and one sentence confused me a little bit , because i 'm not sure what it exactly means . the article says : alphago zero only uses the black and white stones from the go board as its input , whereas previous versions of alphago included a small number of hand - engineered features . source : https://deepmind.com/blog/alphago-zero-learning-scratch/ so what exactly is the input here ? what do they mean by "" just white and black stones as input "" ? what kind of information is the neural network using ? the position of the stones ? 2 ) what type of neural network is alphago zero using ?",24093,22916,2019-04-22T08:19:00.963,2019-04-22T08:20:16.013,alphago neural network inputs,neural-networks deep-learning reinforcement-learning architecture alphago-zero,1,1,
3475,11936,1,,2019-04-22T09:00:45.757,3,34,"i am trying to use deep - q - learning to learn an ann which controls a 7-dof robotic arm . the robotic arm must avoid an obstacle and reach a target . i have implemented a number of state - of - art techinques to try to improve the ann performance . such techniques are : per , double dqn , adaptive discount factor , sparse reward . i have also tried dueling dqn but it performed poorly . i have also tried a number of ann architectures and it looks like that 2 hidden layers with 128 neurons is the best one so far . my input layer is 12 neurons , the output 10 neurons . however , as you can see from the image down here , at a certain point the dqn stops learning and gets stuck at around 80 % of success rate . i do n't understand why it gets stuck , because in my opinion we could reach an higher success rate , 90 % at least , but i just ca n't get out of that "" local minimum "" . so , my question is : what are some techniques i can try to unstuck a dqn from something that looks like a local minimum ? figure : note : the success rate in this picture is computed as the number of successes in the last 100 runs .",23527,23527,2019-04-22T09:34:43.903,2019-04-22T09:34:43.903,dqn agent not learning anymore - what can i do to fix this ?,deep-learning dqn deep-rl,0,5,
3476,11937,1,12040,2019-04-22T09:09:17.073,0,42,"i wrote a convolutional neural network for the mnist dataset with numpy from scratch . i am currently trying to understand every part and calculation . but one thing i noticed was the "" just positive "" derivative of the relu function . my network structure is the following : ( input 28x28 ) conv layer ( filter count = 6 , filter size = 3x3 , stride = 1 ) max pool layer ( size 2x2 ) with relu conv layer ( filter count = 6 , filter size = 3x3 , stride = 1 ) max pool layer ( size 2x2 ) with relu dense ( 128 ) dense ( 10 ) i noticed , when looking at the gradients , that the relu derivative is always ( as it should be ) positive . but is it right that the filter weights are always decreasing their weights ? or is there any way they can increase their weight ? whenever i look at any of the filter 's values , they decreased after training . is that correct ? by the way , i am using stochastic gradient descent with a fixed learning rate for training .",24096,24096,2019-04-23T01:35:17.927,2019-04-28T13:13:01.497,how should the values of the filters of a cnn change ?,convolutional-neural-networks backpropagation relu convolution,2,2,
3477,11941,1,,2019-04-22T12:00:35.690,2,21,"i was trying to implement a dqn without experience reply memory , and the agent is not learning anything at all . i know from readings that experience reply is used for stabilizing gradients . but how important is experience reply in dqn and similar rl algorithms ? if the model needs to learn from memory , why do n't we use a recurrent network , which has inbuilt memory to it ? what is the advantage of experience reply over a recurrent memory ?",39,2444,2019-04-22T12:05:29.007,2019-04-22T12:05:29.007,why experience reply memory in dqn instead of a rnn memory ?,reinforcement-learning dqn deep-rl long-short-term-memory experience-replay,0,1,
3478,11942,1,11946,2019-04-22T13:05:10.057,2,52,"i was wondering whether there is an ai system which could be used to resolve the class clashes problem which mostly happens in universities . in almost every university students face this problem , where two or more courses that many students want to take together get scheduled at the same time . does anyone know about a system which resolves this issue or someone who works on this problem ?",24095,16909,2019-04-22T18:37:16.287,2019-04-22T18:37:16.287,is there any ai system for finding the best way to schedule university classes ?,gofai planning classical-ai scheduling,1,1,
3479,11943,1,,2019-04-22T14:16:13.477,0,23,lets consider knowledge graph and operations on it . there are notions of neural embedding and neural coding for it . what is the relation between neural embedding and neural code ? is neural coding a biological counterpart of neural embedding ( which is an ai notion ) ?,8332,2444,2019-04-22T14:25:13.873,2019-04-22T14:25:13.873,what is the relation between neural embedding and neural code ?,neural-networks word-embedding relation,0,1,
3480,11944,1,,2019-04-22T17:09:39.753,1,70,"the theory and development of computer systems able to perform tasks normally requiring human intelligence , such as visual perception , speech recognition , decision - making , and translation between languages . can artificial intelligence create the next wonder material ?",23529,1671,2019-05-22T21:13:36.260,2019-05-29T20:28:21.570,can artificial intelligence create the next wonder material ?,materials-science,3,1,
3481,11945,1,11952,2019-04-22T18:22:41.017,0,44,"there are several science fiction movies where the robots rebel against their creators : for example , the terminator 's series or i robot . in the future , is it possible that robots will rebel against their human creators ( like in the mentioned movies ) ?",24095,1671,2019-04-22T21:04:13.617,2019-04-23T00:51:28.480,will robots rebel against their human creators ?,control-problem mythology-of-ai neo-luddism ai-takeover,1,4,
3482,11949,1,,2019-04-22T20:45:37.747,1,20,"say i have a standard image classification problem ( ie : cnn is shown a single image and predicts a single classification for it ) . if i were to use bounding boxes to surround the target image ( ie : convert this into an object detection problem ) , would this increase classification accuracy purely through the use of the bounding box ? i 'm curious if the neural network can be "" assisted "" by us when we show it bounding boxes as opposed to just showing it the entire image and letting it figure it all out by itself .",6328,,,2019-05-23T04:06:26.337,do bounding boxes increase accuracy in and of themselves ?,convolutional-neural-networks,1,0,
3483,11953,1,,2019-04-23T01:57:24.070,8,177,"it seems that deep neural networks and other neural network based models are dominating many current areas like computer vision , object classification , reinforcement learning , etc . are there domains where svms ( or other models ) are still producing state - of - the - art results ?",22525,2444,2019-04-23T14:08:00.547,2019-04-30T02:28:33.210,what are the domains where svms are still state - of - the - art ?,machine-learning svm state-of-the-art,4,0,3
3484,11956,1,11991,2019-04-23T04:12:28.513,1,80,how do i choose the search algorithm for a particular task ? which criteria should i take into account ?,23299,2444,2019-04-24T20:08:11.357,2019-04-24T20:08:11.357,how do i choose the search algorithm for a particular task ?,algorithm search,1,0,
3485,11957,1,,2019-04-23T05:30:13.557,1,19,"i 've been working on hinton 's matrix capsule networks for several months . i searched each corner of the internet . but i could n't find anyone that can reproduce hinton 's matrix capsule network . can anyone get the reported accuracy on smallnorb and cifar10 dataset ? ps : i know hinton 's another paper on capsule networks "" dynamic routing between capsules "" is reproducible . please do not confuse the two papers . best",24110,,,2019-04-23T05:30:13.557,is anyone able to reproduce hinton 's matrix capsule networks ?,deep-learning,0,2,
3486,11958,1,,2019-04-23T05:35:54.237,0,11,"i am working on a time series forecasting problem and i am in the process of choosing the optimum network structure . currently i have a 200 cell lstm layer fully connected to 100 neurons in an adjacent layer , which in turn to a 24 neuron output layer . my prediction problem is time series forecast of ghi values in a 24 hour window using previous 24 hours as input . i would like to know whether making the second layer as lstm cells will make any difference ?",24109,,,2019-04-23T05:35:54.237,benefits in using multiple lstm layers ?,training lstm forecasting,0,0,1
3487,11960,1,,2019-04-23T06:27:32.470,1,43,"i am using openai gym framework for reinforcement learning where i am trying solve classic control problem of balancing an inverted pendulum , which is similar to the "" pendulum - v0 "" with some changes in the dynamical equations and introducing some new parameters . my environment look as follows : https://github.com/shritej24c/q_pendulum/blob/master/gym_invpendulum/envs/inv_pendulum.py or import gym import numpy as np from gym import error , spaces , utils from gym.utils import seeding from os import path import random class invpendulumenv(gym.env ) : metadata = { ' render.modes ' : [ ' human ' , ' rgb_array ' ] , ' video.frames_per_second ' : 30 } def _ _ init__(self ) : self.max_theta = np.pi / 8 # rad self.max_thetadot = 0.5 # rad / sec self.max_torque = 300 # n - m self.dt = 0.01 self.viewer = none bounds = np.array([self.max_theta , self.max_thetadot ] ) self.action_space = spaces.box(low=-self.max_torque , high = self.max_torque , shape=(1 , ) , dtype = np.float32 ) self.observation_space = spaces.box(low=-bounds,high=bounds , dtype = np.float32 ) self.seed ( ) def seed(self , seed = none ) : _ , seed = seeding.np_random(seed ) return [ seed ] def step(self , tor ) : # print(tor , "" action provided for the next timestep "" ) th , thdot = self.state # print(""theta "" , "" thetadot "" , th , thdot,'\n ' ) tor_prev = self.action # action at time t-1 # print(""previous timestep torque "" , tor_prev ) g = 9.8 # acceleration due to gravity m = 65 # mass l = 1.1 # length dt = self.dt # time step a = 0.83 # filtering factor b = 0.8 # damping constant k = 8 # stiffness constant c = np.sqrt(40 ) # noise amplitude rmax = 1 tor_con = np.clip(tor , -self.max_torque , self.max_torque)[0 ] + c*np.random.normal(0 , 1 , 1)[0 ] # torque applied by the controller with additive white gaussian noise # print(tor_con,""torque by controller \n "" ) tor_t = a * tor_con + ( 1 - a)*tor_prev # torque at time t with filtering # print(tor_t , "" torque at time t\n "" ) i = m * ( l * * 2 ) # moment of inertia newthdot = thdot + ( tor_t + m * g * l * np.sin(th ) - b * thdot - k * thdot ) / i * dt # dynamical equation solved by euler method # print(newthdot , "" newthetadot "" ) newth = th + newthdot * dt newthdot = np.clip(newthdot , -self.max_thetadot , self.max_thetadot ) # clipping the value of angular velocity # print(""new thetadot and theta "" , newthdot , newth ) self.state = np.array([newth , newthdot ] ) self.action = tor_t done = bool(newth & gt ; np.pi/8 or newth & lt ; -np.pi/8 ) reward = rmax*np.exp(-(newth/(self.max_theta/5))**2 - ( newthdot/(self.max_thetadot/5))**2 ) return self.state , reward , done , { } def reset(self ) : init_th = ( ( random.random ( ) - 0.5 ) * 2 ) * 5 init_thr = init_th * np.pi / 180 init_thdotr = ( ( random.random ( ) - 0.5 ) * 2 ) * 0.0625 self.state = np.array([init_thr , init_thdotr ] ) # print(self.state , "" initial state "" ) self.action = 0 return self.state def render(self , mode='human ' ) : if self.viewer is none : from gym.envs.classic_control import rendering self.viewer = rendering.viewer(500 , 500 ) self.viewer.set_bounds(-2.2 , 2.2 , -2.2 , 2.2 ) surface = rendering.line(start=(-1.2 , -0.05 ) , end=(1.2 , -0.05 ) ) self.viewer.add_geom(surface ) bob = rendering.make_circle(0.15 , filled = true ) bob.set_color(.8 , .3 , .2 ) attributes = rendering.transform(translation=(0.0 , 1.0 ) ) bob.add_attr(attributes ) rod = rendering.filledpolygon([(-0.025 , 0 ) , ( -0.025 , 1.0 - 0.15 ) , ( 0.025 , 1.0 - 0.15 ) , ( 0.025 , 0 ) ] ) rod.set_color(0.2 , 0.2 , 0.7 ) pendulum = rendering.compound([bob , rod ] ) pendulum.set_color(0.4 , 0.5 , 1 ) translate = rendering.transform(translation=(0.0 , -0.05 ) ) pendulum.add_attr(translate ) self.pole_transform = rendering.transform ( ) pendulum.add_attr(self.pole_transform ) self.viewer.add_geom(pendulum ) axle_fill = rendering.make_circle(radius=.1 , res=30 , filled = true ) axle_fill.set_color(1 , 1 , 1 ) axle = rendering.make_circle(radius=0.1 , res=30 , filled = false ) semi = rendering.transform(translation=(0.0 , -0.05 ) ) axle_fill.add_attr(semi ) axle.add_attr(semi ) axle.set_color(0 , 0 , 0 ) self.viewer.add_geom(axle_fill ) self.viewer.add_geom(axle ) pivot = rendering.make_circle(0.02 , filled = true ) self.viewer.add_geom(pivot ) hide = rendering.filledpolygon([(-2.2 , -0.07 ) , ( -2.2 , -2.2 ) , ( 2.2 , -2.2 ) , ( 2.2 , -0.07 ) ] ) hide.set_color(1 , 1 , 1 ) self.viewer.add_geom(hide ) fname = path.join(path.dirname(__file__ ) , "" clockwise.png "" ) self.img = rendering.image(fname , 0.5 , 0.5 ) self.imgtrans = rendering.transform ( ) self.img.add_attr(self.imgtrans ) self.viewer.add_onetime(self.img ) self.pole_transform.set_rotation(self.state[0 ] ) if self.action ! = 0 : self.imgtrans.scale = ( -self.action / 8 , np.abs(self.action ) / 8) return self.viewer.render(return_rgb_array=mode = = ' rgb_array ' ) def close(self ) : if self.viewer : self.viewer.close ( ) self.viewer = none i have used ddpg algorithm to train the agent in the above environment almost similar to existing code i.e. https://github.com/keras-rl/keras-rl/blob/master/examples/ddpg_pendulum.py whereas my code is as follows https://github.com/shritej24c/q_pendulum/blob/master/gym_invpendulum/envs/ddpg_pendulum.py i am not able to get expected results and i ca n't seem to understand what is the problem . hypertuning the parameters seems to be the solution but still where to start from ....",22023,,,2019-04-23T06:27:32.470,difficulty in balancing pendulum using deep reinforcement learning algorithm,reinforcement-learning open-ai environment gym,0,7,0
3488,11962,1,11963,2019-04-23T07:04:04.147,-1,30,"i am working on cnn . i have saved images in drive to do image augmentation in keras , i have used method ( .flow_from_directory(directory ) ) since it require directory path . i have mounted drive and give path to the function but it 's not working ps : beginner in colab and cloud",15368,,,2019-04-23T20:39:03.990,use image data from drive to colab for image augmentation,deep-learning convolutional-neural-networks image-generation,1,0,
3489,11964,1,11967,2019-04-23T09:18:11.477,1,29,could we even use reinforcement learning with big datasets ? or in rl does the agent built its own dataset ?,24003,1847,2019-04-23T10:16:52.177,2019-04-23T10:30:15.710,is there any example of using q - learning with big data ?,reinforcement-learning,1,0,
3490,11966,1,11972,2019-04-23T10:11:10.767,1,36,could changing the order of convolution layers in a cnn improve accuracy or training time ?,24003,2444,2019-04-23T13:29:48.050,2019-04-23T15:18:40.130,does changing the order of the convolution layers in a cnn have any impact ?,neural-networks machine-learning deep-learning convolutional-neural-networks,1,0,
3491,11971,1,11989,2019-04-23T15:07:09.970,1,37,"raul rojas ' neural networks a systematic introduction , section 8.2.1 calculates the variance of the output of a hidden neuron . raul rojas says that "" for binary vectors we have $ e[x_i^2 ] = \frac{1}{3}$ "" where $ x_i$ is the input value transported through each edge to a node . i do n't quite get how he reaches this result . thank you for your time :)",14892,,,2019-04-24T11:18:10.333,binary vector expected value,neural-networks probability-distribution,1,1,
3492,11975,1,,2019-04-23T15:53:56.627,1,12,"i am following this link : weakly - supervised - object - localization to create heatmap of the region in an image where the cnn looks to identify the class . as per the above mentioned repository , following steps are done : step 1 : trained a model on my custom dataset of 3 classes def creat_model ( ) : inputs = input(shape=(299,299,3 ) ) # notic : preprocess is different in each model resize = lambda(resize,(299,299,3))(inputs ) # normal = lambda(preprocess_input,(256,256,3))(resize ) base_model = inceptionv3(weights='imagenet ' , include_top = false ) conv = base_model(resize ) # conv = base_model(resize ) gav = globalaveragepooling2d()(conv ) outputs = dense(3,activation='softmax')(gav ) model = model(inputs , outputs ) model.compile(optimizer='sgd ' , loss='categorical_crossentropy ' , metrics=['accuracy ' ] ) return model def training(model ) : earlystopping = earlystopping ( ) modelchenkpoint = modelcheckpoint('model_best_only',save_best_only = true ) model.fit(trainx,trainy , batch_size=32 , nb_epoch=1 , validation_data=(testx , testy ) , callbacks=[earlystopping , modelchenkpoint ] ) model.save('model_save_file.h5 ' ) step 2 : get convolutional feature map using def get_conv(model , test_imgs ) : # using inceptionv3 's output # base_model = model(model.input , model.get_output_at.output ) inputs = input(shape=(299,299,3 ) ) resize = lambda(resize,(299,299,3))(inputs ) inception_v3 = model.get_layer('inception_v3 ' ) outputs = inception_v3(resize ) new_model = model(inputs , outputs ) # new_model.save('conv.npy ' ) print('loading the conv_features of test_images ....... ' ) conv_features = new_model.predict(test_imgs ) print('loading the conv_features done ! ! ! ' ) print(conv_features ) return conv_features step 3 : saved weights.npy , conv_features.npy and predict_label.npy as numpy arrays using conv_features = get_conv(model , testx ) np.save('conv_features ' , conv_features ) print('predict the labels of test_images ....... ' ) predict_label = model.predict(testx ) np.save('predict_label ' , predict_label ) print('extraction the weight between gav and dense(2048x10 ) ....... ' ) w = model.get_weights()[-2 ] np.save('weight ' , w ) now i want to use this information and test this on a random input image and get the result as shown in the above github page . the code shown in the github page for doing this ( i assume ) is : import numpy as np import cv2 import tensorflow as tf w = np.zeros((10000,2048),dtype='float32 ' ) w = np.load('weight.npy ' ) conv = np.load('conv_features.npy ' ) predict = np.load('predict_label.npy ' ) predict_label = ( np.argmax(predict,axis=1)).astype('uint8 ' ) # this w meas the coefficients of linear combination:10000x2048 w = np.transpose(w)[predict_label , : ] w = np.expand_dims(w,axis=2 ) # w and conv are too large temp1 = w[0:10 ] temp2 = conv[0:10 ] del w del conv sess = tf.session ( ) weight = tf.placeholder(dtype='float32',shape=temp1.shape,name='weight ' ) features = tf.placeholder(dtype='float32',shape=temp2.shape,name='features ' ) resize_features = tf.image.resize_bicubic(features,size=(299,299 ) ) resize_features = tf.reshape(resize_features,[-1,299*299,2048 ] ) # hotmap = tf.batch_matmul(resize_features,weight ) hotmap = tf.matmul(resize_features,weight ) hotmap = tf.reshape(hotmap,[-1,299,299 ] ) result_map = sess.run(hotmap,feed_dict={weight:temp1,features:temp2 } ) could anyone please help me to use this code and show the result on an input image as shown in the github repository ? or any relevant information regarding this ?",24123,,,2019-04-23T15:53:56.627,simple weakly supervised object localizetion using keras . how to visualize the results ?,machine-learning deep-learning convolutional-neural-networks keras,0,0,
3493,11976,1,,2019-04-23T16:50:10.383,3,169,what is an artificial neural network in artificial intelligence ? it is apparently used to find patterns in data and it is loosely inspired by human neural networks .,23529,1671,2019-05-29T21:19:57.020,2019-05-29T21:19:57.020,what is an artificial neural network ?,neural-networks machine-learning ai-basics terminology definitions,3,3,
3494,11977,1,,2019-04-23T17:01:42.867,0,27,"i 'm trying to figure out what 's the best way to get a trained model that does facial detection to run on a camera that 's connected to a computer and perform inference . since i do n't need the device to train the model , is there a low cost solution that can still do at least 10fps on a 1280x720 image ? is there any device that can run the model and have camera and computer all built in a small device ? if not , any suggestion on what 's the best way to put one together and deploy it in production environment ? thanks !",11493,,,2019-05-31T10:51:08.797,edge ai device to run inference,ai-design hardware ai-development,1,0,
3495,11979,1,,2019-04-23T19:12:53.727,6,174,"gradient descent works on the equation of mean squared error , which is an equation of a parabola $ y = x^2 $ . we often say that weight adjustment in a neural network by gradient descent algorithm can hit a local minima and get stuck in there . how is local minima on the equation of a parabola possible , where the slope is always parabolic ?",11789,2444,2019-04-25T21:09:05.203,2019-04-29T18:43:48.860,how is local minima possible in gradient descent ?,neural-networks machine-learning math gradient-descent,3,1,
3496,11980,1,,2019-04-23T19:31:01.550,1,27,"note : i am not asking for general advantages of neuroevolution over standard approaches ( e.g. : architecture search , parallelization ) , i am asking for examples of tasks in which , currently , neuroevolved networks outperform anns trained with gradient - based techniques . of course , this is not opinion based , as i am asking for examples based on facts .",23527,23527,2019-04-23T20:55:14.617,2019-04-23T20:55:14.617,"what are some examples of tasks in which , currently , neuroevolution outperforms gradient - based approaches ?",neural-networks neat,0,2,
3497,11985,1,12002,2019-04-24T03:42:24.370,2,32,"let 's say i 've trained a cnn that is predicting / inferring live samples that it has n't seen before . in the event the network makes a correct prediction , would including this as a new sample in its training set increase the model accuracy even further when re - training the network ? i 'm unsure about this since it seems as though the network has already learnt the necessary features for making the correct prediction , so adding it as a new training sample might be redundant . on the other hand it might also reinforce to the network that it 's on the right track , perhaps giving it further confidence to generalize with whatever features its learnt in regards to that class , that it might be able to apply to the same class in other images it might otherwise make an incorrect prediction with ? the reason i 'm thinking of this is that manually labeling each image is a time - consuming process , however if a simple "" correct / incorrect "" popup box was presented after the network made a live prediction , then it 's simply a matter of clicking a single button to generate a new labelled training sample , which would be a far easier labeling task . so how useful would it be to do something like this ?",6328,,,2019-04-24T19:26:43.057,does reinforcing correct predictions increase model accuracy further ?,neural-networks,1,2,
3498,11986,1,,2019-04-24T08:31:55.223,1,13,"we have various types of data features with different temporal scale . for example , some of them describe the state per second while others may describe the state per day or per month from another aspect . the former features are dense on the time scale and latter features are sparse . simply concatenate them into one feature vector seems not proper . is there any typical method in machine learning can handle with problem ?",20587,,,2019-04-24T08:31:55.223,how to combine features with different temporal scale in machine learning,machine-learning feature-selection,0,0,
3499,11987,1,11997,2019-04-24T09:08:25.843,1,36,"i am now working on training an alphazero player for a board game . the implementation of board game is mine , mcts for alphazero was taken elsewhere . due to complexity of the game , it takes a much longer time to self - play than to train . as you know , alphazero has 2 heads : value and policy . in my loss logging i see that with time , the value loss is decreasing pretty significantly . however , the policy loss only demonstrates fluctuation around its initial values . maybe someone here has run into similar problems ? i would like to know if its my implementation problem ( but then the value loss is decreasing ) or just a matter of not enough data . also , perhaps importantly , the game has ~17k theoretically possible moves , but only 80 at max are legal at any single state ( think chess - a lot of possibles but very few are actually legal at any given time ) . also , if mcts has 20 simulations , then the improved probabilities vector ( against which we train our policy loss ) will have at most 20 non - zero entries . my idea was that it might be hard for the network to learn such sparse vectors . thank you for any ideas !",21278,,,2019-04-24T15:56:48.053,alphazero policy head loss not decreasing,neural-networks reinforcement-learning loss-functions alphago alphazero,1,1,
3500,11988,1,,2019-04-24T11:07:44.487,2,33,"as far as i know : in pddl an environment is designed as well as the initial state described . when we describe the target state the solver creates some sort of a graph . how is the graph built and what are the keys ( keywords ) in pddl referring to ? i know that there are many flavours of pddl , but let 's go with a stard or the most common version of pddl .",19413,19413,2019-04-25T08:00:37.027,2019-04-25T08:00:37.027,how does a pddl - solver find a solution for a given problem ?,planning,1,1,
3501,11992,1,12123,2019-04-24T14:15:02.803,1,45,"following the dqn algorithm with experience replay : we calculate the $ loss=(q(s , a)-(r+q(s+1,a)))^2 $ . assume i have positive but changing rewards . meaning , $ r&gt;0 $ . thus , since the rewards are positive , by calculating the loss , i notice that almost always $ q(s)&lt ; q(s+1)+r$ . therefore , the network learns to always increase the q function , and eventually the q function is higher in same states in later learning steps . how can i stabilize the learning process ?",25141,,,2019-05-03T09:37:05.753,how to stop dqn q function from increasing during learning ?,reinforcement-learning q-learning dqn loss-functions value-function,3,1,
3502,11994,1,,2019-04-24T14:53:35.293,0,28,"i have a big amount of light curves ( image below ) and i am trying to label the points as signal or background ( the signal appears usually periodically , several times , for a given light curve ) . however , the data is not labeled . i tried labeling it by hand and using a bi - directional lstm succeeds in labeling the data points properly . however , there are thousands of light curves and labeling all of them would take very long . is there any good unsupervised approach to do this ( unsupervised lstm maybe , but any other method that might work on time series would do just fine ) ? thank you !",23871,23871,2019-04-24T21:30:29.097,2019-04-24T21:30:29.097,unsupervised lstm,lstm unsupervised-learning,0,3,
3503,11998,1,,2019-04-24T16:11:42.350,0,32,"i 'm currently building an agent that learns to play kalah through reinforcement learning . i 've gotten quite far along . with an of 0 , meaning no exploration and only exploitation , it is able to win 80 % against a random agent and over half against a minimax agent . however , a question still bugs me . i 'm not sure how to set up my -greedy policy . this is because the agent only starts converging after millions of games , and that training is done by batches , i.e. i do 100 000 games , save my model in a file and exit the program . i 've gotten along so far by leaving my at a steady 0.2 . however , i was wondering if there was something i could use that would make this process more efficient , maybe decaying my at the rate with which it accumulates knowledge or something to that effect . any input on this is appreciated ! thanks !",24018,23527,2019-04-24T16:35:49.927,2019-04-24T16:35:49.927,-greedy policies for huge state space,reinforcement-learning,0,1,
3504,11999,1,,2019-04-24T18:15:44.087,7,49,"semantics means the meaning and interpretation of words , signs , and sentence structure . semantics largely determine our reading comprehension , how we understand others , and even what decisions we make as a result of our interpretations . semantics can also refer to the branch of study within linguistics that deals with language and how we understand meaning . this has been a particularly interesting field for philosophers as they debate the essence of meaning , how we build meaning , how we share meaning with others , and how meaning changes over time.so actually in artificial intelligence what do semantic analysis used for ?",23529,1671,2019-04-24T19:41:04.240,2019-04-24T22:24:46.210,"in artificial intelligence , what are semantic analyses used for ?",semantics artificial-life,2,0,2
3505,12001,1,,2019-04-24T19:09:25.247,1,15,"having two point clouds , the second being a transformation of the first , how could i utilize a neural network in order to solve the pose ( transformation in terms of x , y , z , rx , ry , rz ) of the second point cloud ? since the point clouds can be rather large ( ~200,000 points ) , i think it 'd be best to first select regions from each point cloud and see if their geometries are similar ( still researching the optimal method for this ) . if they are not similar , i 'd choose two new points . if they are similar , i 'd use those two regions when implementing the neural network to discern the pose . my preliminary research has led me to believe that a siamese neural network may work in this scenario but i 'm not sure if there are better alternatives . one of my goals is to accomplish this without relying on iterative closest point . any and all insight is appreciated . thanks .",25147,,,2019-04-24T19:09:25.247,point cloud alignment using a neural network ?,neural-networks convolutional-neural-networks,0,0,
3506,12005,1,,2019-04-25T02:24:42.983,1,34,what is the goal of a constraint solver ? how are constraints propagated in a constraint satisfaction search ? any references are also appreciated .,23299,2444,2019-04-25T09:48:20.413,2019-04-25T09:48:20.413,understanding constraint search,constraint-satisfaction-problems,1,0,1
3507,12007,1,12009,2019-04-25T12:28:33.710,1,38,"i 've been unsure about a principle of q - learning , i was hoping someone could clear it up . when a new state is encountered , and thus there are no existing q values , and that the algorithm decides to exploit , and not explore , how is the move chosen , since all the values are 0 ? is it chosen randomly ? this intuitively would make sense , since after this , the state - move pair would have a value and thus the matrix would get filled up throughout the iterations . but i just want to make sure i understand this correctly ... thanks",24018,,,2019-04-25T15:06:15.077,picking a random move in exploitation in q - learning,reinforcement-learning q-learning,1,0,
3508,12008,1,,2019-04-25T12:56:02.830,1,27,"i 'm building an actor - critic reinforcment learning algorithm to solve environments . i want to use a single encoder to find representation of my environment . when i share the encoder with the actor and the critic , my network is n't learning anything : class encoder(nn.module ) : def _ _ init__(self , state_dim ) : super(encoder , self).__init _ _ ( ) self.l1 = nn.linear(state_dim , 512 ) def forward(self , state ) : a = f.relu(self.l1(state ) ) return a class actor(nn.module ) : def _ _ init__(self , state_dim , action_dim , max_action ) : super(actor , self).__init _ _ ( ) self.l1 = nn.linear(state_dim , 128 ) self.l3 = nn.linear(128 , action_dim ) self.max_action = max_action def forward(self , state ) : a = f.relu(self.l1(state ) ) # a = f.relu(self.l2(a ) ) a = torch.tanh(self.l3(a ) ) * self.max_action return a class critic(nn.module ) : def _ _ init__(self , state_dim , action_dim ) : super(critic , self).__init _ _ ( ) self.l1 = nn.linear(state_dim + action_dim , 128 ) self.l3 = nn.linear(128 , 1 ) def forward(self , state , action ) : state_action = torch.cat([state , action ] , 1 ) q = f.relu(self.l1(state_action ) ) # q = f.relu(self.l2(q ) ) q = self.l3(q ) return q however , when i use different encoder for the actor and different for the critic , it learn properly . class actor(nn.module ) : def _ _ init__(self , state_dim , action_dim , max_action ) : super(actor , self).__init _ _ ( ) self.l1 = nn.linear(state_dim , 400 ) self.l2 = nn.linear(400 , 300 ) self.l3 = nn.linear(300 , action_dim ) self.max_action = max_action def forward(self , state ) : a = f.relu(self.l1(state ) ) a = f.relu(self.l2(a ) ) a = torch.tanh(self.l3(a ) ) * self.max_action return a class critic(nn.module ) : def _ _ init__(self , state_dim , action_dim ) : super(critic , self).__init _ _ ( ) self.l1 = nn.linear(state_dim + action_dim , 400 ) self.l2 = nn.linear(400 , 300 ) self.l3 = nn.linear(300 , 1 ) def forward(self , state , action ) : state_action = torch.cat([state , action ] , 1 ) q = f.relu(self.l1(state_action ) ) q = f.relu(self.l2(q ) ) q = self.l3(q ) return q i m pretty sure its becuase of the optimizer . in the shared encoder code , i define it as foolow : self.actor_optimizer = optim.adam(list(self.actor.parameters())+ list(self.encoder.parameters ( ) ) ) self.critic_optimizer = optim.adam(list(self.critic.parameters ( ) ) ) + list(self.encoder.parameters ( ) ) ) in the seperate encoder , its just : self.actor_optimizer = optim.adam((self.actor.parameters ( ) ) ) self.critic_optimizer = optim.adam((self.critic.parameters ( ) ) ) two optimizers must be becuase of the actor critic algorithm , in which the loss of the actor is the value . how can i combine two optimizers to optimize correctly the encoder ?",25141,,,2019-04-25T12:56:02.830,how to properly optimize shared network between actor and critic ?,neural-networks reinforcement-learning optimization actor-critic pytorch,0,14,0
3509,12010,1,,2019-04-25T15:39:38.677,0,47,"i am trying to understand this post , but i get confused by the definitions and the differences . what 's definition of equivariant ? if i remove all the pooling layers from a cnn , will it make the network to detect features in pixel resolution ? for example , detecting the local maximum of a pixel . for example , can a cnn be designed to return true for the following case ? and false for the shifted window : in the second case it returns false because the 3x3 submatrix is n't centered ( yellow dash line ) around the local maximum . will an architecture that is from keras.layers import dense , conv2d , flatten model = sequential ( ) model.add(conv2d(128 , kernel_size=2 , activation=’relu ’ , padding='same ' , input_shape=(3,3,1 ) ) ) model.add(conv2d(64 , kernel_size=2 , activation=’relu ’ , padding='same ' ) ) model.add(flatten ( ) ) model.add(dense(10 , activation=’softmax ’ ) ) be able to differentiate between the tiling of the larger grayscale image ?",12975,12975,2019-04-28T07:50:33.303,2019-04-28T07:50:33.303,how can i suppress a cnn ’s translation invariant or translation equivariant ?,machine-learning convolutional-neural-networks keras,1,1,
3510,12011,1,,2019-04-25T16:43:43.963,1,60,"is there a hebb neural network ? what kind of functions can it implement ? or , are there multiple "" hebb networks "" , that is , neural networks that learn in a hebbian fashion ?",23299,2444,2019-04-28T12:28:38.207,2019-04-28T12:57:24.533,is there a hebb neural network ?,neural-networks machine-learning terminology difference hebbian-learning,1,3,
3511,12013,1,,2019-04-25T17:31:23.693,0,17,"the gym core file simply defines action as in step(action ) as an "" object "" . i 've searched for an action object in the code without success . can it be any python object ?",25161,,,2019-04-25T17:31:23.693,can an openai gym action be any object ?,open-ai,0,1,
3512,12014,1,,2019-04-25T23:33:47.507,0,10,"in the original faster r - cnn paper , the authors parameterized the box coordinates for regression under rpn . below is the snippet of how they computed it : & nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp ; & nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp ; & nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp ; though this confuses me . they set the number of anchor boxes at 256 . though the number of ground - truth boxes ( i 'll call $ n^*$ ) for each image is highly variable and is never equal to 256 , unless by some coincidence . same can be said for the number of predicted boxes ( $ n$ ) . so how do they compute these numbers ? would there be $ ( 4)(256)n$ predicted parameters and $ ( 4)(256)n^*$ truth parameters ? in the case that $ n \neq n^*$ , which is most definitely possible especially in early epochs , the loss can not be computed as the lengths do not match . what am i misunderstanding here ? there must be something inferred from the article because the authors never touch on it again .",22563,22563,2019-04-25T23:39:52.553,2019-04-25T23:39:52.553,parameterized coordinates in region proposal networks ( rpns ) for faster r - cnn,neural-networks machine-learning convolutional-neural-networks object-recognition,0,0,2
3513,12015,1,,2019-04-26T05:19:34.923,0,13,"i am working on image classification problem where i have to identify whether given image is original one or disguised.during data analysis i found that data is evenly distributed amongst class / target but when i saw the distribution for each of the features they have either skewed or bimodal distribution . when i trained my model with both before transformation and after ( box - cox ) accuracy does not improve , it is stuck at 50 % only . i am not getting where i am going wrong or else does this uneven distribution of features does not affect neural network ? please suggest ........",25183,,,2019-04-26T05:19:34.923,does skewed distribution for features in dataset impact neural network accuracy ?,neural-networks deep-learning mlp,0,0,
3514,12017,1,12018,2019-04-26T09:04:08.863,1,30,"i have data with about 100 numerical features and a multi - labelling that encodes ownership of a certain product ( i.e. my labels are of the form $ [ x_i , i=1 , \dots , n]$ where n is the number of products and $ x_i$ is either 0 or 1 ) . my neural network approach to this currently looks like this model = sequential ( ) model.add(dense(1024 , activation='relu ' , input_dim = x_train.shape[1 ] ) ) model.add(dense(1024 , activation='relu ' ) ) model.add(dense(1024 , activation='relu ' ) ) model.add(dense(1024 , activation='relu ' ) ) model.add(dense(y_train.shape[1 ] , activation='softmax ' ) ) in keras ( i.e. a couple of dense layers with relu activation , then an output layer with softmax ) . now my question is : will the network consider labels of the other products when considering a probability to assign to the label of one product ? i would like that to happen , but i ca n't quite grasp whether it does ( my suspicion is no ) . i 'm new to multilabel classification and relatively new to nn in general , so i hope this is n't too inept a question .",25190,16565,2019-05-20T08:46:03.147,2019-05-20T08:46:03.147,multi - label classification ( e.g. as used in keras ) using dnn,neural-networks classification keras,1,0,
3515,12019,1,12025,2019-04-26T10:20:29.403,3,36,"i have a genetic algorithm that maximizes a fitness function with two variables f(x , y ) . i have been running the algorithm with various parameters in mutation and crossover probability ( 0.1 , 0.2 , ... ) since i do nt have much theoretical knowledge of ga , how could i proceed in order to find the optimal values for mutation and crossover probability , and if necessary the optimal population size ?",25194,10135,2019-05-05T12:46:52.967,2019-05-05T12:46:52.967,how to find optimal mutation probability and crossover probability ?,algorithm genetic-algorithms mutation,1,1,
3516,12020,1,,2019-04-26T12:36:13.383,1,32,"i have an idea for a new type of ai for two - player games with alternating turns , like chess , checkers , connect four , and so on . a little background : traditionally engines for such games have used the minimax algorithm combined with a heuristic function when a certain depth has been reached to find the best moves . in recent days engines using reinforcement learning , etc ( like alphazero in chess ) have increased popularity , and become as strong as or stronger than the traditional minimax engines . my approach is to combine these ideas , to some level . a minimax tree with alpha - beta pruning will be used , but instead of considering every move in a position , these moves will be evaluated with a neural net or some other machine learning method , and the moves which seem least promising will not be considered further . the more interesting moves are expanded like in traditional minimax algorithms , and the same evaluation are again done for these nodes ' children . the pros and cons are pretty obvious : by decreasing the breadth ( number of moves in a position ) , the computation time will be reduced , which again can increase the search depth . the downside is that good moves may not be considered , if the machine learning method used to evaluate moves are not good enough . one could of course hope that the position evaluation itself ( from the neural net , etc ) is good enough to pick the best move , so that no minimax is needed . however , combining the two approaches will hopefully make better results . a big motivation for this approach is that it resembles how humans act when playing games like chess . one tends to use intuition ( which will be what the neural net represents in this approach ) to find moves which looks interesting . then one will look more thoroughly at these interesting moves by calculating moves ahead . however , one does not do this for all moves , only those which seem interesting . the idea is that a computer engine can play well by using the same approach , but can of course calculate much faster than a human . to illustrate the performance gain : the size of a minimax tree is about b^d , where b is the average number of moves possible in each position , and d is the search depth . if the neural net can reduce the size of considered moves b to half , the new complexity will be ( b/2)^d . if d is 20 , that means reducing the computation time by approx . 1 million . my questions are : does anyone see any obvious flaws about this idea , which i might have missed ? has it been attempted before ? i have looked a bit around for information about this , but have n't found anything . please give me some references if you know any articles about this . do you think the performance of such a system could compete with those of pure minimax or those using deep reinforcement learning ? exactly how the neural net will be trained , i have not determined yet , but there should be several options possible .",17488,,,2019-04-26T12:36:13.383,minimax combined with machine learning to determine if a path should be explored,neural-networks game-ai minimax chess alpha-beta-pruning,0,1,1
3517,12021,1,12033,2019-04-26T13:30:05.243,4,85,"i want to create a simple game which basically consists of 2d circles shooting smaller circles at each other ( to make hitbox detection easier for the start ) . my goal is to create an ai which adapts its own behaviour to the player‘s . for that , i want to use a nn as brain . every frame , the nn is fed with the same inputs as the player and his output is compared to the players output . ( outputs in this case are pressed keys like the up - arrow ) as inputs , i want to use a couple different important factors : for example , the direction of the enemy player as number from 0 to 1 i also want to input the direction , size and speed of enemy ’s and own projectiles and this is where my problem lies . if there was only one bullet per player , it would be easy but i want the number of bullets to be variable so the number of input neurons would have to be variable . my approaches : 1 ) use a big amount of neurons and set unused ones to 0 ( not elegant at all ) 2 ) instead of specific values , just use all the pixels ‘ rgb values as inputs ( would limit the game as colours would deliver all the information ) ( + factors like speed and direction would probably not have any impact ) is there a more promising approach to this problem ? i hope you can give me some inspiration . also , is there a difference in ranging input values between 0/1 or -1/1 ? thank you in advance , mo edit : in case there aren‘t enough questions for you , is there a way to make the nn remember things ? for example , if i added a mechanic to the game which involves holding a key , i would add an input neuron which inputs 1 if the certain key is pressed and 0 if it isn‘t but i doubt that would work .",25201,25201,2019-04-26T13:44:05.560,2019-04-28T15:22:34.793,neural network with varying inputs ( for a game ai ),neural-networks game-ai neurons java,2,0,1
3518,12023,1,,2019-04-26T13:58:43.330,0,30,which libraries can be used for image caption generation ?,24076,2444,2019-04-26T16:23:45.617,2019-04-26T16:23:45.617,which libraries can be used for image caption generation ?,machine-learning image-recognition generative-model,0,4,
3519,12024,1,,2019-04-26T14:50:28.840,1,32,"suppose we have a data set $ x$ that is split as $ x_{\text{train}}$ , $ x_{\text{val}}$ and $ x_{\text{test}}$ and the outcome variable is binary . let 's say we train three different models ( logistic regression , random forest , and a support vector machine ) using $ x_{\text{train}}$ . we then get predictions for $ x_{\text{val}}$ using each of the three models . in stacking , is it correct to say that we train a logistic regression model on a data set of dimension $ |x_{\text{val}}| \times 3 $ with the predicted values and actual values of the validation set ? this logistic regression model is then used to predict outcomes for data in $ x_{\text{test}}$ ?",25207,2444,2019-04-28T12:22:41.437,2019-05-28T15:03:45.403,do we train a logistic regression model using a dataset that is 3 times bigger than the validation dataset ?,machine-learning datasets,1,2,
3520,12026,1,,2019-04-27T05:28:22.510,1,20,i need help with expectiminimax problem : start a game . the first player flips a coin . the second player flips a coin . the first player decides if he wants to flip another coin . the second player decides if he wants to flip another coin . game over . the winner is the player who has earned more points . the points are computed as follows : if the player flips ones and got the head - 1 point . if the player flips ones and got the tail - 2 points . if the player flips twice and got 2 heads - 4 points . if the player flips twice and got 2 tails - 4 points . if the player flips twice and got one head and one tail - 0 points . task : draw the expectiminimax tree and write the value of each node . did i draw the tree properly ?,25217,3771,2019-04-30T01:56:46.523,2019-04-30T01:56:46.523,expectiminimax tree,ai-basics minimax,0,0,2
3521,12029,1,,2019-04-27T11:35:27.807,4,134,"the ai must predict the next number in a given sequence of incremental integers ( with no obvious pattern ) using python but so far i do n't get the intended result ! i tried changing the learning rate and iterations but so far no luck ! example sequence : [ 1 , 3 , 7 , 8 , 21 , 49 , 76 , 224 ] expected result : 467 result found : 2,795.5 cost : 504579.43 this is what i 've done so far : import numpy as np # init sequence data = \ [ [ 0 , 1.0 ] , [ 1 , 3.0 ] , [ 2 , 7.0 ] , [ 3 , 8.0 ] , [ 4 , 21.0 ] , [ 5 , 49.0 ] , [ 6 , 76.0 ] , [ 7 , 224.0 ] ] x = np.matrix(data ) [ : , 0 ] y = np.matrix(data ) [ : , 1 ] def j(x , y , theta ) : theta = np.matrix(theta).t m = len(y ) predictions = x * theta sqerror = np.power((predictions-y ) , [ 2 ] ) return 1/(2*m ) * sum(sqerror ) datax = np.matrix(data ) [ : , 0:1 ] x = np.ones((len(datax ) , 2 ) ) x [ : , 1 : ] = datax # gradient descent function def gradient(x , y , alpha , theta , iters ) : j_history = np.zeros(iters ) m = len(y ) theta = np.matrix(theta).t for i in range(iters ) : h0 = x * theta delta = ( 1 / m ) * ( x.t * h0 - x.t * y ) theta = theta - alpha * delta j_history[i ] = j(x , y , theta.t ) return j_history , theta print('\n'+40*'= ' ) # theta initialization theta = np.matrix([np.random.random ( ) , np.random.random ( ) ] ) # learning rate alpha = 0.02 # iterations iters = 1000000 print('\n== model summary = = \nlearning rate : { } \niterations : { } \ninitial theta : { } \ninitial j : { : .2f}\n ' .format(alpha , iters , theta , j(x , y , theta).item ( ) ) ) print('training model ... ' ) # train model and find optimal theta value j_history , theta_min = gradient(x , y , alpha , theta , iters ) print('done , model is trained ' ) print('\nmodelled prediction function is:\ny = { : .2f } * x + { : .2f } ' .format(theta_min[1].item ( ) , theta_min[0].item ( ) ) ) print('cost is : { : .2f}'.format(j(x , y , theta_min.t).item ( ) ) ) # calculate the predicted profit def predict(pop ) : return [ 1 , pop ] * theta_min # now p = len(data ) print('\n'+40*'= ' ) print('initial sequence was:\n ' , * np.array(data ) [ : , 1 ] ) print('\nnext numbers should be : { : , .1f } ' .format(predict(p).item ( ) ) ) update another method i tried but still giving wrong results import numpy as np from sklearn import datasets , linear_model # define the problem problem = [ 1 , 3 , 7 , 8 , 21 , 49 , 76 , 224 ] # create x and y for the problem x = [ ] y = [ ] for ( xi , yi ) in enumerate(problem ) : x.append([xi ] ) y.append(yi ) x = np.array(x ) y = np.array(y ) # create linear regression object regr = linear_model.linearregression ( ) regr.fit(x , y ) # create the testing set x_test = [ [ i ] for i in range(len(x ) , 3 + len(x ) ) ] # the coefficients print('coefficients : \n ' , regr.coef_ ) # the mean squared error print(""mean squared error : % .2f "" % np.mean((regr.predict(x ) - y ) * * 2 ) ) # explained variance score : 1 is perfect prediction print('variance score : % .2f ' % regr.score(x , y ) ) # do predictions y_predicted = regr.predict(x_test ) print(""next few numbers in the series are "" ) for pred in y_predicted : print(pred )",3894,3894,2019-04-27T21:37:38.573,2019-05-09T10:43:17.573,use machine learning / artificial intelligence to predict next number ( n+1 ) in a given sequence of random increasing integers,neural-networks machine-learning training python,2,5,
3522,12034,1,12036,2019-04-28T03:47:35.933,3,1235,"these guys here : https://www.patreon.com/aiangel are saying that they 've created a ai who can chat and stream . as the so - called administrator "" rogue "" said : this chat / streamer bot are no fake . also , there 's more about the dynamics of this chat / streamer bot on youtube : https://www.youtube.com/watch?v=wyfwjhqhlgo&amp;t=463s https://www.youtube.com/watch?v=gtvivssqlhe considering that videos i realy think that this bot is totaly fake . i mean , i think that even the most advanced ai bot do not get even close to a real conversation like this one . now , of course that you can say that this is a artistic project or something , but the people behind all of this are on patreon , and the people who are paying to these guys possibly are getting totally fooled , which is a serious thing when we 're talking about real money . so , is aiangel a real bot ? ( with this question i 'm spreading this possible fake to community )",25232,1671,2019-04-29T18:16:15.597,2019-05-06T09:57:07.157,"is "" aiangel "" ( patreon ) a fake ?",chat-bots turing-test ai-hoaxes,2,2,1
3523,12037,1,,2019-04-28T10:41:00.447,3,23,"anyone knows a resources ( papers , articles and especially repositories ) regarding controlling multiple units with rl . the controlled units should not be fixed , for example in real time strategy the agent builds various units ( workers , soldiers ... ) and later controls them . during the game various units could die and new ones are built . i think good contemporary example is alphastar , while openai five controls just a single agent . this might be incorrect since i 've never played those games .",25236,,,2019-04-28T10:41:00.447,code examples of controlling multiple units with rl,reinforcement-learning,0,0,
3524,12042,1,12047,2019-04-28T16:55:21.243,11,1858,"surprisingly this was n't asked before - at least i did n't find anything besides some vaguely related questions . so , what is a recurrent neural network , and what are their advantages over regular nns ?",23527,,,2019-05-25T18:04:03.570,what is a recurrent neural network ?,recurrent-neural-networks,2,2,6
3525,12044,1,,2019-04-29T02:51:14.563,3,26,"i am looking for good introductory and advanced books in ai , especially unsupervised learning . i have already read books like probabilistic graphical models from d. kholler and pattern recognition and machine learning from c. m. bishop . i am also very familiar with the ph.d . thesis of k. p. murphy , on dynamic bayesian networks . i have read all of the above mostly for the probability aspects , not really the applications to ai and ml . i would like to know what are the good ref . for unsupervised learning , that focuses on practical exercises and examples instead of deep and abstract concepts . thanks for your information .",25262,,,2019-04-29T02:51:14.563,references and books for unsupervised learning,unsupervised-learning reference-request,0,1,2
3526,12046,1,12060,2019-04-29T07:03:44.927,1,58,"i recently watched a youtube video ( sorry , ca n't remember the link ) where ( a very talented ) someone created what they called a "" static ai "" . somewhere in the video they said something along the lines of : "" this is a static ai , it 's very simple and not dynamic at all "" what does this mean ? what 's the difference between a static ai and a dynamic ai ?",25274,2444,2019-04-29T14:28:16.990,2019-04-30T02:04:48.647,what 's the difference between a static ai and a dynamic ai ?,ai-basics terminology,1,1,1
3527,12048,1,,2019-04-29T08:07:22.677,4,42,"in section 4.3 of paper learning by playing - solving sparse reward tasks from scratch , the authors define retrace as $ $ q^{ret}=\sum_{j = i}^\infty\left(\gamma^{j - i}\prod_{k = i}^jc_k\right)[r(s_j , a_j)+\delta_q(s_i , s_j)],\\ \delta_q(s_i , s_j)=\mathbb e_{\pi_{\theta'}(a|s)}[q^\pi(s_i,\cdot;\phi')]-q^\pi(s_j , a_j;\phi')\\ c_k=\min\left(1,{\pi_{\theta'}(a_k|s_k)\over b(a_k|s_k)}\right ) $ $ where i omit for simplicity . i 'm quite confused about the definition of $ q^{ret}$ , which seems not consistent with retrace define in safe and efﬁcient off - policy reinforcement learning : $ $ \mathcal rq(x , a):=q(x , a)+\mathbb e_\mu[\sum_{t\ge0}\gamma^t\left(\prod_{s=1}^tc_s\right)(r_t+\gamma\mathbb e_\pi q(x_{t+1},\cdot)-q(x_t , a_t ) ] $ $ what should i make of $ q^{ret}$ in the first paper ?",8689,2444,2019-05-08T14:07:47.040,2019-05-08T14:07:47.040,inconsistent definitions of the retrace,reinforcement-learning q-learning,0,0,
3528,12049,1,,2019-04-29T12:02:38.847,0,14,"i need to train an autoencoder in keras with the jpg images i took myself . model = sequential ( ) # 1st convolution layer model.add(conv2d(16 , ( 3 , 3 ) , padding='same ' , data_format='channels_first ' , input_shape=(3,224,224 ) ) ) model.add(activation('relu ' ) ) model.add(maxpooling2d(pool_size=(2,2 ) , padding='same ' ) ) # 2nd convolution layer model.add(conv2d(2,(3 , 3 ) , padding='same ' ) ) model.add(activation('relu ' ) ) model.add(maxpooling2d(pool_size=(2,2 ) , padding='same ' ) ) # ------------------------- # 3rd convolution layer model.add(conv2d(2,(3 , 3 ) , padding='same ' ) ) model.add(activation('relu ' ) ) model.add(upsampling2d((2 , 2 ) ) ) # 4rd convolution layer model.add(conv2d(16,(3 , 3 ) , padding='same ' ) ) model.add(activation('relu ' ) ) model.add(upsampling2d((2 , 2 ) ) ) # ------------------------- model.add(conv2d(1,(3 , 3 ) , padding='same ' ) ) model.add(activation('sigmoid ' ) ) model.summary ( ) which generates a model as : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ layer ( type ) output shape param # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = conv2d_1 ( conv2d ) ( none , 16 , 224 , 224 ) 448 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_1 ( activation ) ( none , 16 , 224 , 224 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ max_pooling2d_1 ( maxpooling2 ( none , 16 , 112 , 112 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ conv2d_2 ( conv2d ) ( none , 2 , 112 , 112 ) 290 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_2 ( activation ) ( none , 2 , 112 , 112 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ max_pooling2d_2 ( maxpooling2 ( none , 2 , 56 , 56 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ conv2d_3 ( conv2d ) ( none , 2 , 56 , 56 ) 38 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_3 ( activation ) ( none , 2 , 56 , 56 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ up_sampling2d_1 ( upsampling2 ( none , 2 , 112 , 112 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ conv2d_4 ( conv2d ) ( none , 16 , 112 , 112 ) 304 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_4 ( activation ) ( none , 16 , 112 , 112 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ up_sampling2d_2 ( upsampling2 ( none , 16 , 224 , 224 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ conv2d_5 ( conv2d ) ( none , 1 , 224 , 224 ) 145 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ activation_5 ( activation ) ( none , 1 , 224 , 224 ) 0 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = total params : 1,225 trainable params : 1,225 non - trainable params : 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i compile and train as : model.compile(optimizer='adadelta ' , loss='binary_crossentropy ' ) batch_size = 16 # 16 train_datagen = imagedatagenerator(rescale=1./255 ) train_generator = train_datagen.flow_from_directory ( ' cropped/ ' , target_size=(224 , 224 ) , batch_size = batch_size , class_mode='categorical ' ) test_datagen = imagedatagenerator(rescale=1./255 ) validation_generator = test_datagen.flow_from_directory ( ' cropped/ ' , target_size=(224 , 224 ) , batch_size = batch_size , class_mode='categorical ' ) model.fit_generator ( train_generator , steps_per_epoch=1000 , epochs=20 , validation_data = validation_generator , validation_steps=1000 ) i end up with the error message : valueerror : error when checking target : expected activation_5 to have 4 dimensions , but got array with shape ( 16 , 2 ) should i use conv2d instead ?",25211,,,2019-04-29T12:40:57.330,autoencoder for color images in keras backed by mxnet,keras autoencoders,1,0,
3529,12052,1,,2019-04-29T13:20:11.437,0,17,"i 'm doing a model to detect duplicate in my database ( there is a lot of features that can be different but mean same object in the end ) so i have my feature vector for my duplicate dataset which contains score , distance and relation between 2 identical object that i labelised such as ( 0 , 0 , 123 , 14000 , 5 , 10 , 0 , 0 , -1 ) since duplicate are a rare event i was wondering what size of dataset should i use for non duplicate features , since i want my model to learn about the disparity of the multiple features i have i though i should have like 10 times the number of example of non - duplicate and in my model change the weight of duplicate class by multiplying by 10 . is that a good thing to do or is it better to take 50%/50 % of duplicate / non duplicate features for my model ? also , should i apply filter to chose my non - duplicate dataset in order to have like close object on some features but different on other . or should i take them randomly among all the data i have ?",23107,,,2019-04-29T13:20:11.437,size of dataset for feature vector with rare event,machine-learning datasets,0,0,
3530,12053,1,,2019-04-29T15:30:20.570,3,54,"i have recently started working on a control problem using a deep q network as proposed by deepmind ( https://arxiv.org/abs/1312.5602 ) . initially , i implemented it without experience replay . the results were very satisfying , although after implementing er , the results i got were relatively bad . thus i started experimenting with batch size and memory capacity . ( 1 ) i noticed that if i set batch size = 1 and memory capacity = 1 i.e. the same as doing normal online learning as previously , the results are then ( almost ) the same as initially . ( 2 ) if i increased capacity and batch size e.g. capacity = 2000 and batch size = 128 , the q values for all actions tend to converge to very similar negative values . a small negative reward -1 is received for every state transition except of the desired state which receives +10 reward . my gamma is 0.7 . every state is discrete and the environment can transition to a number of x states after action a , with every state in x having a significant probability . receiving a positive reward is very rare as getting to a desired state can take a long time . thus , when sampling 128 experiences if ' lucky ' only a small amount of experiences may have a positive reward . since , when doing mini - batch training we average the loss over all the samples and then update the dqn i was wondering whether generally the positive rewards can become meaningless as they are ' dominated ' by the negative ones . which means that this would result in a very slower convergence to actual values ? and also justifies the the convergence to similar negative values as in ( 2 ) ? is this something expected ? i am looking to implement . prioritised er as a potential solution to this , but is there something wrong inn the above logic ? i hope this does makes sense . please forgive me if i make a wrong assumption above as i am new to the field . edit : the problem seemed to be that indeed finding rewards very rarely would result in sampling almost never , especially at the begging of training , which in turn resulted in very slow convergence to the actual q values . the problem was successfully solved using prioritised er -but i believe any form of careful stratified sampling would result in good results",25288,25288,2019-05-01T20:05:52.423,2019-05-31T21:03:15.380,experience replay not always giving better results,reinforcement-learning q-learning dqn experience-replay,1,7,
3531,12059,1,,2019-04-30T01:15:00.733,1,17,"there are multiple ways to implement parallelism in reinforcement learning . one is to use parallel workers running in their own environments to collect data in parallel , instead of using replay memory buffers ( this is how a3c works for example ) . however , there are methods like ppo that use batch training on purpose . how is parallelism usually implemented for algorithms that still use batch training ? are gradients accumulated over parallel workers and the combined ? is there another way ? what are the benefits of doing parallelism one way over another ?",25294,,,2019-04-30T01:15:00.733,how is parallelism implemented in rl algorithms like ppo ?,reinforcement-learning,0,1,
3532,12061,1,,2019-04-30T08:06:27.763,1,66,"a fundamental principle in science is to discuss controversial topics on a content level but avoid to attack certain persons because of their world - understanding . as a consequence it 's not recommended to criticize fuzzy logic by questioning the reputation of key figures in that domain and ask if they are scientists at all . the only way to argue against fuzzy logic is by the theory itself , which means that we have to analyze the cons of uncertainty and meta - mathematics , independently who has formulated it first . in general , fuzzy logic is a textcorpus written down by a community who beliefs that this is a great idea . and by sure , they are wrong , because it contradicts fundamental principles in nature . one thing can be either small or tall . a bird can either fly or not . if the bird is in the air , he is flying and if he is on the ground , then he is walking . this kind of distinction is natural . what fuzzy logic is trying to imagine is the opposite . it does n't make sense to explain how fuzzy logic sees animals , because there is only one kind of physics available and either somebody has understand the principle or he deserves to be called a bad student . a possible argumentation against fuzzy logic is to show that it was n't invented as a mathematical theory but as a non - scientific concept . quote : “ fuzzy set theory [ ... ] has its roots in the social nature of human understanding . fuzzy sets approach directly relates to practical applications of [ ... ] creative misunderstanding ” dimitrov , vladimir . "" fuzzy logic in service to a better world : the social dimensions of fuzzy sets . "" imac / ieee cscc’90 conference , athens,(greece ) . 1999 . pdf so the papers says basically , that fuzzy logic is the opposite of a mathematical understanding of a world . are more examples available which disprove fuzzy logic ?",11571,,,2019-04-30T13:07:44.893,how to disprove fuzzy logic ?,fuzzy-logic,1,1,
3533,12062,1,,2019-04-30T08:53:34.283,0,16,"i 'd like to measure the difference between 2 grid - worlds to determine the generalization capacity of my agent using tabular q - learning . example ( openai frozen lake ) : sfff fhfh fffh hffg and : sffg fhfh fffh hffh are not very different but the tabular policy that i found on the first environement will completely fail on the new environment . a correct distance should to measure the policy on the first environment and compute the norm between this one and the optimal policy ( not found with rl ) of the new environment . is it accurate ? i think this is a bit strange because measure the difference between 2 policies is the intrinsic answer .. how can i measure accurately 2 environments , not forgetting the transition matrix of the environment ?",23838,23838,2019-04-30T09:09:03.100,2019-04-30T09:09:03.100,measure grid - world environments difference for reinforcement learning,reinforcement-learning q-learning policy,0,0,
3534,12065,1,12067,2019-04-30T14:26:46.283,4,915,"what are the areas that belong to the reinforcement learning ? td(0 ) , q - learning and sarsa are all temporal - difference algorithms , which belong to the reinforcement learning area , but is there more to it ? is dynamic programming policy iteration and value iteration considered as part of reinforcement learning ? or are these just basis for the temporal difference algorithms which are the only rl algorithms ?",24054,2444,2019-04-30T14:58:37.233,2019-04-30T16:06:16.180,what algorithms are considered reinforcement learning algorithms ?,reinforcement-learning terminology definitions,2,0,
3535,12066,1,12102,2019-04-30T14:51:25.057,3,65,"raul rojas ' neural networks a systematic introduction , section 8.2.1 calculates the standard deviation of the output of a hidden neuron . from : $ $ \sigma^2 = \sum^n_{i=0}e[w_i^2]e[x_i^2 ] $ $ when i try what i get is ( with $ e[x_i^2 ] = \frac{1}{3}$ and $ w_i \in [ -\alpha , \alpha]$ ): $ $ \sigma^2 = \sum^n_{i=0}e[w_i^2]e[x_i^2 ] = [ n\frac{(\alpha-(-\alpha))^2}{12}][n\frac{1}{3}]=n^2\alpha^2\frac{1}{9 } $ $ $ $ \sigma = \sqrt{n^2\alpha^2\frac{1}{9}}=n\alpha\frac{1}{3 } $ $ but raul rojas concludes : $ $ \sigma = \frac{1}{3}\sqrt{n}\alpha $ $ am i missing some implicance of the law of large numbers use for the input to the node ? thank you for your time :)",14892,1671,2019-05-02T00:18:40.127,2019-05-18T07:53:32.840,standard deviation of the total input to a neuron,neural-networks math probability-distribution,1,0,
3536,12068,1,,2019-04-30T15:36:39.950,3,35,"previously i have learned that the softmax as the output layer coupled with the log - likelihood cost function ( the same as the the nll_loss in pytorch ) can solve the learning slowdown problem . however , while i am learning the pytorch mnist tutorial , i 'm confused that why the combination of the log_softmax as the output layer and the nll_loss ( the negative log likelihood loss ) as the loss function was used ( l26 and l34 ) . i found that when log_softmax + nll_loss was used , the test accuracy was 99 % , while when softmax + nll_loss was used , the test accuracy was 97 % . i 'm confused that what 's the advantage of log_softmax over softmax ? how can we explain the performance gap between them ? is log_softmax + nll_loss always better than softmax + nll_loss ?",25305,,,2019-05-31T03:06:25.897,what 's the advantage of log_softmax over softmax ?,loss-functions activation-function,1,0,1
3537,12069,1,,2019-04-30T15:40:31.133,1,26,"it is mentioned by fu 2019 that overfitting might have a negative effect on training dqn . they showed that with either early stopping or experience replay this effect could be reduced . the first is reducing overfitting , the latter is increasing data . it does n't only have negative effects on the returns though , my test shows that it has a negative effect on value errors as well ( diff . between predict v and ground truth v ) . i observed frequently with limited data that the training diverged almost 100 % of the time ( on small nets ) . since increasing the amount of data could reduce the chance of divergence , i think this is an effect from overfitting . overfitting should mean low training loss , however , my observation is that there is a strong correlation between td loss and value error . that is if i see a jump in td loss , i could expect to see a jump in value error around that moment . or it is not overfitting because it is not really fit ( i.e. high loss ) but over - optimization that is for sure . now the question is why ? there are two points : if it is overfitting , overfitting should have a positive effect because remembering values for all training states correctly is hardly a bad thing . ( in fact , my training data is a superset of my testing data , so remembering should be fine . ) if it does n't fit , this begs a question what over - optimization really does . it does n't seem to fit , but it does have a negative effect . how could that be ?",9793,,,2019-04-30T15:40:31.133,why overfitting is bad in dqn ?,reinforcement-learning deep-rl,0,4,0
3538,12071,1,,2019-04-30T16:25:06.407,3,33,"in most of rl algorithms i saw , there is a coefficient that reduces actions exploration over time , to help convergence . but in actor - critic , or other algorithms ( a3c , ddpg , ... ) used in continuous action spaces , the different implementation i saw ( mainly using ornstein uhlenbeck process ) is correlated over time , but not decreased . the action noises are clipped into a range of [ -1 , 1 ] and are added to policies that are between [ -1 , 1 ] too . so , i do n't understand how it could work in environments with hard - to - obtain rewards . any thought about this ?",23818,,,2019-04-30T16:25:06.407,should noise ( such as ou ) be decreased over time in actor / critic algorithms ?,deep-learning reinforcement-learning actor-critic ddpg,0,3,
3539,12072,1,12079,2019-04-30T17:32:28.253,1,31,one way to speed up a neural network is to prune the network and reducing number of neurons in each layer . what are the other methods to speed up inference ?,25307,,,2019-05-08T18:04:22.470,what are the various methods for speeding up neural network for inference ?,neural-networks deep-learning convolutional-neural-networks,1,0,
3540,12077,1,,2019-04-30T18:57:47.973,-1,90,"you probably get this question a lot "" will ai be the end of the human race ? "" . however , my concern is not that ai will "" take over "" or their goal misalign with ours . instead , i fear that ai and machines will become so good at doing everything better than humans that we find ourselves not having anything to do . humans need things to do to stay mentally healthy and i fear that all that will be left is just consumption of what the machines and ai create . will this make us humans feel like our existence is meaningless and eventually every human becomes depressed and humanity goes extinct through mass suicides or something similar . can this happen , is it feasible ? if ai and machines get to that point , could humanity suffer mass mental illness or will there still be stuff for us to do ?",25308,1847,2019-05-01T06:20:06.267,2019-05-09T19:24:33.293,will ai cause mass mental illness and end humanity ?,philosophy healthcare,2,1,
3541,12081,1,,2019-05-01T03:53:26.863,0,7,"i have googled for a long time for rotated object detection datasets . most of papers focused on rotated object detection using dota , hrsc2016 or coco text detection dataset . some researcher also collect their own datasets but almost all of their theme is areal object detection . is there any others dataset focus on rotated object detection ?",25318,,,2019-05-01T03:53:26.863,is there any other rotated object detection datasets ?,computer-vision object-recognition,0,0,
3542,12083,1,12088,2019-05-01T08:09:32.053,1,49,"can a normal neural network work as good as a convolutional network ? if yes , how much more time and neurons would it need compared to a cnn ?",19783,,,2019-05-01T11:40:03.137,is it possible for a nn to reach the same results as cnns ?,neural-networks deep-learning convolutional-neural-networks,2,0,
3543,12085,1,,2019-05-01T08:27:56.603,0,19,"without considering lyrics , there are two type of songs : songs that have a distinct tune which you can easily remember , and songs that are the opposite . can we design a network to identify the songs that have a distinct tune ? it 's very hard to find good music , i ca n't get a song that has a distinct tune for even 100 + songs which consumes a lot of time .",25322,,,2019-05-01T08:27:56.603,is it possible to use ai to find music that have a distinct tune ?,machine-learning,0,2,1
3544,12086,1,12090,2019-05-01T08:39:45.717,0,24,"if i 'd previously trained a model and saved the whole training session ( using tensorflow tf.train.saver.save ( ) ) , and then later restored the session ( using tensorflow tf.train.saver.restore ( ) ) but then wanted to train using different hyperparameters ( batch / mini - batch and learning rate ) , then i assume the optimizer would need to be reset . is that the only part of the previous session that would need to reset ?",20352,,,2019-05-01T12:04:36.490,"changing the batch , mini - batch and learning rate for a previously - trained model",ai-design tensorflow,1,0,
3545,12092,1,12093,2019-05-01T13:14:22.413,2,21,"i 've never worked with very large models that require weeks or months of training , but in such a situation , what happens if you want to add extra features inputs , do you need to re - train the entire model again from scratch , or how is it handled ? e.g. if tesla added an extra sensor to its cars , would it need to re - train its network again from scratch to include this extra sensor input ?",20352,,,2019-05-01T14:46:54.053,adding input features - is complete re - training required ?,ai-design training,1,0,1
3546,12095,1,,2019-05-01T16:29:02.923,4,47,"it this podcast between oriol vinyals and lex friedman : https://youtu.be/kedt2or9xlo?t=1769 , at 29:29 , oriol vinyals refers to a paper : if you look at research in computer vision where it makes a lot of sense to treat images as two dimensional arrays ... there is actually a very nice paper from facebook . i forgot who the authors are but i think [ it 's ] part of kaiming he 's group . and what they do is they take an image , which is a 2d signal , and they actually take pixel by pixel , and scramble the image , as if it was a just a list of pixels , crucially they encode the position of the pixels with the xy coordinates . and this is a new architecture which we incidentally also use in starcraft 2 called the transformer , which is a very popular paper from last year which yielded very nice results in machine translation . do you know which paper he is referring to ? i 'm guessing maybe he is talking about non - local neural networks , but i 'm probably guessing wrong . edit : after reviewing the recent publications of kaiming he ( http://kaiminghe.com/ ) , maybe i 'm guessing right . any thoughts ?",1741,1741,2019-05-01T16:42:22.740,2019-05-13T17:27:44.853,name of paper for encoding / representing xy coordinates in deep learning,neural-networks deep-learning computer-vision papers,1,2,1
3547,12097,1,12130,2019-05-02T01:23:50.440,3,13,"in this blog toward the end , the author writes the following : for the sake of my question , let ’s assume that a terminal state gives a reward of +1 for a win and -1 for a loss . when the author says “ for any two consecutive nodes this perspective is opposite , ” does that mean that if $ q_i$ is positive ( for example , 4 ) for player a at a given node , the same node will have the negative of that value for player b ( -4 in my hypothetical ) ? do i need to compute two statistics to store the node value ( one for each player ) or can i simply store statistics for one player and flip the sign at every consecutive node ?",16343,16343,2019-05-02T01:51:21.120,2019-05-04T13:38:26.523,how does monte carlo tree search uct exploitation value change based on perspective ?,monte-carlo-tree-search implementation,1,0,
3548,12098,1,,2019-05-02T02:14:29.780,1,12,"recently openai removes their board game environments . ( it may be possible to install an older version to get access to them , but i have n’t downgraded ) . is there a list of repositories or resources of board game or similar environments that can be used to practice rl implementations ? things like checkers , chess , backgammon , or even grid world and mazes would be excellent . a running list might be useful for many in this community .",16343,,,2019-05-02T02:14:29.780,what are a list of board game environments for rl practice ?,reinforcement-learning deep-rl environment open-source,0,1,
3549,12099,1,,2019-05-02T03:08:37.743,2,21,"in the constraint propagation in csp , it is often stated that pre - processing can solve the whole problem , so no search is required at all . and the key idea is local consistency . what does this actually mean ?",23299,2444,2019-05-02T13:00:02.243,2019-05-02T13:00:02.243,what is local consistency in constraint satisfaction problems ?,terminology definitions constraint-satisfaction-problems,0,1,
3550,12100,1,,2019-05-02T04:35:40.697,2,42,"is it possible to use t - sne , pca or umap to find separating hyperplane ? assume we have data points in high dimensional space and we want to phase separate it into two sets of points ? is there a way to use t - sne , pca or umap for such a task ?",12975,12975,2019-05-16T02:40:10.097,2019-05-16T02:40:10.097,"using umap , pca or t - sne to find the separating hyperplane ?",machine-learning deep-learning,1,0,
3551,12101,1,,2019-05-02T07:39:49.367,1,30,"i want to formulate the following sentence using fol ; i want also to know whether there any contradictions in it , or if it is consistent . assuming human relations are binary : all human relations are utilitarian ( human relations are utilitarian only when people in those relations are selfish and calculative ) . i am a human . people are humans . unselfish people are nice . therefore , i am nice .",22322,23527,2019-05-02T08:46:52.580,2019-05-02T08:46:52.580,formulation of a sentence using fol,logic knowledge-representation,0,5,
3552,12103,1,,2019-05-02T10:25:44.270,1,33,"i have skimmed through a bunch of deep learning books , but i have not yet understood whether we must use the experience replay buffer with the a3c algorithm . the approached i used is the following : i have some threaded agents that play their own copy of an enviroment ; they all use a shared nn to predict ' best ' move given a current state ; at the end of each episode , they push the episode ( in the form of a list of steps ) in a shared memory a separated thread reads from the shared memory and executes the train step on the shared nn , training it episode after episode . is this an appropriate approach ? more specifically , do i need to sample data to train the nn from the shared memory ? or should i push in the shared memory each step , just when it 's done by a single agent ? in this last case , i wonder how i could calculate discounted rewards . i 'm afraid that with my current approach i 'm doing nothing more that presenting n episodes to the nn , with the hope that each agent explores the enviroment differently from other agents ( so that nn is presented a richer variety of states ) .",25352,2444,2019-05-02T13:09:43.720,2019-05-02T13:09:43.720,do we need to use the experience replay buffer with the a3c algorithm ?,reinforcement-learning ai-design deep-rl actor-critic experience-replay,0,5,
3553,12104,1,,2019-05-02T10:52:38.540,-1,23,"in a 8-puzzle states are divided into two adjacent sets , such that any state is reachable from any other state in the same set , while no state is reachable from any state in the other set . we need a procedure to decide which set a given state is in , and explain why this is useful for generating random states . please help .",22322,,,2019-05-02T10:52:38.540,8 puzzle problem,ai-design ai-basics,0,1,
3554,12110,1,,2019-05-02T15:31:08.017,2,29,"in the trust region policy optimization ( trpo ) paper , on page 10 , it is stated an informal overview is as follows . our proof relies on the notion of coupling , where we jointly define the policies and so that they choose the same action with high probability $ ( 1−\alpha)$ . surrogate loss $ l_π(\hat\pi)$ accounts for the the advantage of the first time that it disagrees with , but not subsequent disagreements . hence , the error in $ l_\pi$ is due to two or more disagreements between and , hence , we get an $ o(\alpha^2)$ correction term , where is the probability of disagreement . i do n't see how this holds ? in what way does $ l_\pi$ account for one disagreement ? surely when disagrees with you will have different trajectories under expectation for each so then $ l_\pi$ immediately is different from ? thanks for any insight . i understand the proof given , but i wanted to try and capture this intuition .",25153,2444,2019-05-02T15:58:11.030,2019-05-02T15:58:11.030,how does the trpo surrogate loss account for the error in the policy ?,reinforcement-learning deep-rl policy-gradients trpo,0,0,2
3555,12114,1,,2019-05-02T18:44:59.150,1,12,"today , ai is mainly driven by own - profit - oriented companies ( e.g. facebook , amazon , google ) . admittedly , there 's a lot of ai in the health sector ( even in the public health sector ) and there 's a lot of ai in the sustainability sector & ndash ; but also mostly driven by obviously own - profit - oriented companies ( e.g. tesla , uber , google ) . on the other side , one often hears from hard - core economists that centrally planned (= public - profit - oriented ) economies ( or economic principles ) are "" the work of the devil "" - and that they failed all over history ( sometimes for understandable reasons ) . but intelligently planning global economic processes and applying these plans with the help of state - of - the - art ai - given the huge amounts of really big data available , and given the argument that globalization is finally for the benefit of all - would seem to be a rewarding endeavour , at least for parts of the ai community . why is n't this endeavour undertaken more decidedly ? ( or is it ? ) where do i find approaches to apply ai to global economic processes ? ( not only describing and understanding but mainly planning and executing ? )",25362,,,2019-05-02T18:44:59.150,artificial intelligence in the public and global sector,applications ethics game-theory problem-solving social,0,0,
3556,12115,1,12119,2019-05-02T19:49:34.240,1,25,"often in nlp project the data points contain both text and float embeddings , and it 's very tricky to deal with . csvs take up a ton of memory and are slow to load . but most the other data formats seem to be meant for either pure text or pure numerical data . there are those that can handle data with the dual data types , but those are generally not flexible for wrangling . for example , for pickle you have to load the entire thing into memory if you want to wrangle anything . you can just append directly to the disk like you can with hdf5 , which can be very helpful for huge datasets which can not be all loaded into memory ? also , any alternatives to pandas for wrangling huge datasets ? sometimes you ca n't load all the data into pandas without causing a memory crash .",18358,,,2019-05-03T06:44:32.197,what data formats / pipelining are best to store and wrangle data which contains both text and float vectors ?,natural-language-processing datasets data-science structured-data data-mining,1,0,
3557,12116,1,,2019-05-02T21:15:42.313,5,70,"this might be a trivial question but i could n't find any reliable answers on the internet . almost all the neural network architectures for self - driving cars that i have seen on the internet have a feedforward network , previous frames will not help in making the current decision . i have read somewhere that tesla uses two last frames captured to make a decision , even then 2 frames will not be that useful in this case . this might not be very helpful when predicting things ie .. lane cut - ins , as the system needs to observe the vehicle ( that is going to cut in ) behavior such as turn indicator , vehicle veering towards center lane over time in order to predict . can someone explain if this is the way production self - driving card such as tesla work ? or is it something like the below ? or are they using something like many to one recurrent net where inputs are cnn vectors of previous few frames and output is the control ?",25366,,,2019-05-11T15:49:40.887,are self - driving cars using single frame or multiple frame to make decision ?,self-driving autonomous-vehicles,2,0,1
3558,12118,1,,2019-05-03T03:15:23.973,0,27,"it seems like doing this would lead to much needed nonlinearity , otherwise we 're just doing linear transformations . this observation applies to transformer , additive attention , etc",21158,,,2019-05-04T21:55:26.710,why do n't people use nonlinear activation functions after projecting query key value in attention ?,neural-networks,0,2,0
3559,12120,1,,2019-05-03T06:54:34.240,0,12,"how can i know if it is possible to logically infer both sentences from the given database ? ( using first order logic ) my basic assumptions are that(because of dealing with ai ) : closed world assumptions , domain closure and unique names assumptions . the database i have : every professor counsels at least one student every student has a counselor , who is a professor eric is a professor counseling meeting occur at the campus rachel is a student every counselor meets with all of his counselees(the ones he counsels ) ( every counselor meets with all of his counselees , and if he meets with someone he must be his counselee ) where : s(x ) - x is a student p(x ) - x is a professor c(x , y ) - x counsels y m(x , y ) - x meets with y o(x , y ) - x occurs at y h(x , y ) - x has y p - a professor s - a student c - a counselor m - a meeting h - a counselee i - a counseling u - a campus what i do n't understand is how is it possible to infer : 1)eric counsels rachel 2)eric was at the campus very much struggling with it and spent so much time without being able to infer neither of those",25371,,,2019-05-03T06:54:34.240,logically infering using resolutions,logic knowledge-representation,0,0,
3560,12124,1,,2019-05-03T13:25:58.730,0,18,"i am following this repository [ deep - diver / cifar10-img - classification - tensorflow][1]. to make a neural network for cifar-10 dataset . import pickle import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import labelbinarizer def batch_features_labels(features , labels , batch_size ) : "" "" "" split features and labels into batches "" "" "" for start in range(0 , len(features ) , batch_size ) : end = min(start + batch_size , len(features ) ) yield features[start : end ] , labels[start : end ] def display_image_predictions(features , labels , predictions , top_n_predictions ) : n_classes = 10 label_names = load_label_names ( ) label_binarizer = labelbinarizer ( ) label_binarizer.fit(range(n_classes ) ) label_ids = label_binarizer.inverse_transform(np.array(labels ) ) fig , axies = plt.subplots(nrows=top_n_predictions , ncols=2 , figsize=(20 , 10 ) ) fig.tight_layout ( ) fig.suptitle('softmax predictions ' , fontsize=20 , y=1.1 ) n_predictions = 3 margin = 0.05 ind = np.arange(n_predictions ) width = ( 1 . - 2 . * margin ) / n_predictions for image_i , ( feature , label_id , pred_indicies , pred_values ) in enumerate(zip(features , label_ids , predictions.indices , predictions.values ) ) : if ( image_i & lt ; top_n_predictions ) : pred_names = [ label_names[pred_i ] for pred_i in pred_indicies ] correct_name = label_names[label_id ] axies[image_i][0].imshow((feature*255).astype(np.int32 , copy = false ) ) axies[image_i][0].set_title(correct_name ) axies[image_i][0].set_axis_off ( ) axies[image_i][1].barh(ind + margin , pred_values[:3 ] , width ) axies[image_i][1].set_yticks(ind + margin ) axies[image_i][1].set_yticklabels(pred_names[::-1 ] ) axies[image_i][1].set_xticks([0 , 0.5 , 1.0 ] ) % matplotlib inline % config inlinebackend.figure_format = ' retina ' import tensorflow as tf import pickle import random save_model_path = ' final_model ' batch_size = 64 n_samples = 10 top_n_predictions = 5 def test_model ( ) : test_features , test_labels = pickle.load(open('preprocess_training.p ' , mode='rb ' ) ) loaded_graph = tf.graph ( ) with tf.session(graph=loaded_graph ) as sess : # load model loader = tf.train.import_meta_graph(save_model_path + ' .meta ' ) loader.restore(sess , save_model_path ) # get tensors from loaded model loaded_x = loaded_graph.get_tensor_by_name('input_x:0 ' ) loaded_y = loaded_graph.get_tensor_by_name('output_y:0 ' ) loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0 ' ) loaded_logits = loaded_graph.get_tensor_by_name('logits:0 ' ) loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0 ' ) # get accuracy in batches for memory limitations test_batch_acc_total = 0 test_batch_count = 0 for train_feature_batch , train_label_batch in batch_features_labels(test_features , test_labels , batch_size ) : test_batch_acc_total + = sess.run ( loaded_acc , feed_dict={loaded_x : train_feature_batch , loaded_y : train_label_batch , loaded_keep_prob : 1.0 } ) test_batch_count + = 1 print('testing accuracy : { } \n'.format(test_batch_acc_total / test_batch_count ) ) # print random samples random_test_features , random_test_labels = tuple(zip(*random.sample(list(zip(test_features , test_labels ) ) , n_samples ) ) ) random_test_predictions = sess.run ( tf.nn.top_k(tf.nn.softmax(loaded_logits ) , top_n_predictions ) , feed_dict={loaded_x : random_test_features , loaded_y : random_test_labels , loaded_keep_prob : 1.0 } ) display_image_predictions(random_test_features , random_test_labels , random_test_predictions , top_n_predictions ) test_model ( ) training part went well but while testing the model i am getting following error --------------------------------------------------------------------------- keyerror traceback ( most recent call last ) & lt;ipython - input-3-dfa44b9162e3&gt ; in & lt;module&gt ; ( ) 90 91 ---&gt ; 92 test_model ( ) & lt;ipython - input-3-dfa44b9162e3&gt ; in test_model ( ) 67 loaded_y = loaded_graph.get_tensor_by_name('output_y:0 ' ) 68 loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0 ' ) ---&gt ; 69 loaded_logits = loaded_graph.get_tensor_by_name('logits:0 ' ) 70 loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0 ' ) 71 /home / sherry/.local / lib / python3.6 / site - packages / tensorflow / python / framework / ops.py in get_tensor_by_name(self , name ) 3652 raise typeerror(""tensor names are strings ( or similar ) , not % s. "" % 3653 type(name).__name _ _ ) -&gt ; 3654 return self.as_graph_element(name , allow_tensor = true , allow_operation = false ) 3655 3656 def _ get_tensor_by_tf_output(self , tf_output ) : /home / sherry/.local / lib / python3.6 / site - packages / tensorflow / python / framework / ops.py in as_graph_element(self , obj , allow_tensor , allow_operation ) 3476 3477 with self._lock : -&gt ; 3478 return self._as_graph_element_locked(obj , allow_tensor , allow_operation ) 3479 3480 def _ as_graph_element_locked(self , obj , allow_tensor , allow_operation ) : /home / sherry/.local / lib / python3.6 / site - packages / tensorflow / python / framework / ops.py in _ as_graph_element_locked(self , obj , allow_tensor , allow_operation ) 3518 raise keyerror(""the name % s refers to a tensor which does not "" 3519 "" exist . the operation , % s , does not exist in the "" -&gt ; 3520 "" graph . "" % ( repr(name ) , repr(op_name ) ) ) 3521 try : 3522 return op.outputs[out_n ] keyerror : "" the name ' logits:0 ' refers to a tensor which does not exist . the operation , ' logits ' , does not exist in the graph . "" [ 1 ] : https://github.com/deep-diver/cifar10-img-classification-tensorflow/blob/master/cifar10_image_classification.ipynb ` ` `",23922,,,2019-05-03T13:25:58.730,error loading tensorflow model,neural-networks python tensorflow models,0,1,
3561,12125,1,,2019-05-03T14:11:27.203,2,25,"i trained a neural network which makes a regression to a poincarè disk model with radius $ r = 1 $ . i want to optimize using the hyperbolic distance $ $ \operatorname{arcosh } \left ( 1 + \frac{2|pq|^2|r|^2}{(|r|^2 - |op|^2)(|r|^2 - |oq|^2 ) } \right ) $ $ where $ |op|$ and $ |oq|$ are the distances of $ p$ and respectively $ q$ to the centre of the disk , $ |pq|$ the distance between $ p$ and $ q$ , $ |r|$ the radius of the boundary circle of the disk and is the inverse hyperbolic function of hyperbolic cosine . but there is a problem in the poincarè disk model with $ r = 1 $ , the distance is defined only for vectors which have norm less than $ 1 $ . a neural network does not know this rule , so it can predict vectors with norm greater than $ 1 $ . so , i tried to use the distance defined in a space with $ r = 2 $ , and it works very well for the learning task , but i 'm doubtful because the distance does n't scale in a linear way . will there be unwanted effects , in your opinion ?",25377,2444,2019-05-03T21:02:19.750,2019-05-03T21:02:19.750,should i use the hyperbolic distance loss in the case of poincarè disk model ?,machine-learning math loss-functions geometric-deep-learning,0,4,
3562,12126,1,,2019-05-03T22:17:52.547,0,18,"suppose i have a lot of scans of hardcopy documents , in the form of jpegs . some of them are potentially scans of driver 's licenses or identification cards . i wonder what would be a good way to identify those scans that contain driver 's license / id cards . one thought i had was training a model or use an existing pretrained model that can detect faces . however , if the data set i have has a lot of scans of photos of people , it would cause false positives . so i am not sure how i might approach this problem . any thought would be much appreciated !",22719,,,2019-05-05T17:33:44.780,how to identify whether images contain driver 's licenses or i d cards,image-recognition classification facial-recognition,1,0,
3563,12127,1,,2019-05-04T07:27:45.790,4,34,"i 'm trying to develop skill to deal with very small amount of labeled samples ( 250 labeled/20000 total , 200 features ) practicing on kaggle "" do n't overfit "" dataset ( traget_practice have provided all 20,000 targets ) . i 've read a ton of papers and articles on this topic , but nothing what i tried was n't improved simple regularized svm result ( best acc 75 / auc 85 ) or any other algorithm result ( lr , k - nn , naivebayes , rf , mlp ) . i believe that result can be better ( on leaderboard they go even over auc 95 ) what i 've tried without success : remove outliers i 've tried to remove 5%-10 % outliers with ellipticenvelope and with isolationforest . feature selection i 've tried rfe ( with or without cv ) + l1 / l2 regularised logisticregression , and selectkbest ( with chi2 ) . semi - supervised techniques i 've tried co - training with different combinations of two complementary algorithms and : 100 - 100 : splitted features . also i 've tried labelspreading , but i do n't know how to provide most uncertain samples ( i tried predictions from another algorithms but there was a lot of mislabeled samples and without any success ) . ensembling classifiers stackingclassifier with all possible combinations of algorithms and this also does n't improve result ( best is same as svm acc75 / auc 85 ) . can anyone give me advice what i 'm doing wrong or what else to try ?",25393,,,2019-05-04T07:27:45.790,how to deal with small amount of labeled samples ?,machine-learning data-science,0,2,
3564,12128,1,,2019-05-04T08:44:31.557,1,14,"if we have a neural network that learns the generative model for p(a , b , c ) and now , we want to learn the generative model for p(a , b , c , d ) is there any theory that says learning p(a , b , c ) and then composing it with p(d | a , b , c ) is faster than learning p(a , b , c , d ) from scratch ?",21158,10135,2019-05-04T15:17:42.783,2019-05-04T15:17:42.783,theoretical grounding for ease of training with a prior,neural-networks theory,0,2,
3565,12129,1,,2019-05-04T09:08:23.283,1,17,"i was learning about gan when the term "" label smoothing "" appears . from the video tutorial that i watch , they use the term "" label smoothing "" to change the binary labels when calculating the loss of discriminator network . instead of using 0 or 1 label they use 0 or 0.9 labels . what is the main purpose of this label smoothing ? i 've skimmed through the original paper , there is a lot of maths , that honestly , i got difficulties in understanding it . but i notice this paragraph in there : we propose a mechanism for encouraging the model to be less confident . while this may not be desired if the goal is to maximize the log - likelihood of training labels , it does regularize the model and makes it more adaptable and it gives me another question : why "" this may not be desired if the goal is to maximize the log - likelihood of training labels "" ? what do they mean with "" adaptable "" ?",16565,,,2019-05-04T09:08:23.283,what is the intuition behind the label smoothing ?,generative-adversarial-networks regularization,0,0,
3566,12133,1,,2019-05-04T16:03:30.890,1,32,"i 've heard the expression "" gaussian kernel "" in several contexts ( e.g. in the kernel trick used in svm ) . a gaussian kernel usually refers to a gaussian function ( that is , a function similar to the probability density function of a gaussian distribution ) that is used to measure the similarity between two vectors ( or numbers ) . why is this gaussian function called a "" kernel "" ? why not just calling it a gaussian ( similarity ) function ? does it have something to do with the kernel of a linear transformation ?",2444,2444,2019-05-04T16:08:34.100,2019-05-04T16:08:34.100,"why do we use the word "" kernel "" in the expression "" gaussian kernel "" ?",machine-learning terminology math history svm,0,1,1
3567,12135,1,,2019-05-04T23:40:15.413,4,39,"suppose i have an mdp $ ( s , a , p , r)$ where the $ p(s_j|s_i , a_i)$ is uniform , i.e given an state $ s_i$ and an action $ a_i$ all states $ s_j$ are equally probable . now i want to find an optimal policy for this mdp . can i just apply the usual methods like policy gradients , actor - critic to find the optimal policy for this mdp ? or is there something i should be worried about ? at least , in theory , it should n't make any difference . but i 'm wondering are there any practical considerations i should be worried about ? should the discount factor , in this case , be high ? the reward function here depends both on states and actions and is not uniformly random .",25403,1847,2019-05-05T16:55:39.960,2019-05-05T17:05:59.460,reinforcement learning with uniformly random dynamics,reinforcement-learning markov-decision-process,2,3,
3568,12136,1,,2019-05-05T00:11:04.380,1,10,"i am trying to figure out how als works when minimizing the following formula : : i would like to know how does alternating least squares work in this case . how does it minimize the equation on the picture ? the idea of the whole equation that needs to be minimized , i think , it is like when we do a simple linear regression and we have to fit the line . am i right ? in the lineal regression we do $ ( y - \hat y)^2 $ . in the case of the paper we do $ ( r_{ui } - \mu -b_{i}-b_{u})^2 + \lambda( ... )$ just in case i leave the link : paper",25405,25405,2019-05-05T06:58:08.657,2019-05-05T06:58:08.657,estimating baselines using als,machine-learning optimization linear-algebra recommender-system,0,2,
3569,12138,1,,2019-05-05T04:42:43.137,0,24,"i was wondering if the following ‘ shared actor - critic ’ principal using local rules has been established ? .. take an actor network , which can form abstract ( ie hidden layer ) representations using a threshold based 3 factor local learning rule ( see below ) . note ; the rule is something like associative reward - penalty ( arp ) reinforce algorithm . add a single critic neuron , connected to the hidden layer of the actor network , using a value based 3 factor local learning rule ( see below ) . so the principal being that the actor rule is effective at forming abstract representations , but is poor at forming a value mapping . conversely the critic rule is effective at forming a value mapping , but poor at forming abstract representations - but this is ok because it piggybacks those of the actor . is this principal established in actor - critic models ? references please if yes are there any limitations / issue ? how biologically plausible is the architecture & amp ; rules ? many thanks actor neuron weight update rule : if pre if post if reward increase + ’ ve else increase -’ve else if reward increase -’ve else increase + ’ ve else no change critic neuron weight update rule : if pre ( reward * discount ) - post activation else no change",22305,,,2019-05-05T04:42:43.137,shared actor - critic using only local rules,neural-networks reinforcement-learning,0,0,
3570,12139,1,,2019-05-05T05:32:55.163,0,18,why is deep learning more boarder than machine learning ?,23299,23299,2019-05-05T06:47:13.603,2019-05-05T06:47:13.603,machine learning vs deep learning,machine-learning deep-learning,0,0,
3571,12142,1,12147,2019-05-05T08:45:40.797,11,628,"with the increasing complexity of recaptcha , i wondered about the existence of some problem , that only a human will ever be able to solve ( or that ai wo n't be able to solve as long as it does n't reproduce exactly the human brain ) . for instance , the distorted text used to be only possible to solve by humans . although ... the computer now got the [ distorted text ] test right 99.8 % , even in the most challenging situations . it seems also obvious that the distorted text ca n't be used for real human detection anymore . i 'd also like to know whether an algorithm can be employed for the creation of such a problem ( as for the distorted text ) , or if the originality of a human brain is necessarily needed .",25411,2444,2019-05-05T13:09:20.900,2019-05-05T18:44:56.283,problems that only humans will ever be able to solve,agi problem-solving intelligence-testing ai-complete,2,0,2
3572,12144,1,,2019-05-05T10:05:47.557,3,29,"the prioritized experience replay paper gives two different ways of sampling from the replay buffer . one , called "" proportional prioritization "" , assigns each transition a priority proportional to its td - error . $ $ p_i = |\delta_i|+\epsilon$$ the other , called "" rank - based prioritization "" , assigns each transition a priority inversely proportional to its rank . $ $ p_i = 1/\text{rank}(i)$$ where is the rank of transition $ i$ when the replay buffer is sorted according to $ |\delta_i|$ . the paper goes on to show that the two methods give similar performance for certain problems . are there times when i should choose one sampling method over the other ?",22916,22916,2019-05-05T10:22:17.427,2019-05-05T10:22:17.427,which kind of prioritized experience replay should i use ?,deep-learning reinforcement-learning dqn experience-replay,1,0,
3573,12146,1,,2019-05-05T11:47:46.943,0,56,what can be the examples of rote learning in artificial intelligence to understand it more easily ? any references are appreciated .,23299,2444,2019-05-05T12:17:33.657,2019-05-19T01:04:59.093,examples of rote learning in ai,machine-learning,2,0,
3574,12150,1,,2019-05-05T15:09:38.190,2,46,"given the chinese room argument , and given the development in chatbots and machine learning , is n't turing test superseded by some other way of evaluating ai 's inteligence ? would an positive result of a turing test provide any value , besides telling that a machine is good at conversations ( but possibly nothing more ) ?",25414,,,2019-05-05T16:36:47.390,"is the turing test still relevant , as of 2019 ?",turing-test,1,0,
3575,12154,1,12306,2019-05-05T18:08:03.360,5,69,"in my understanding , the mind arises from a physical system , the brain . i see that there is a big research under the topic of simulating physical systems efficiently ( especially in quantum computing ) . hence , in theory , we could achieve agi by simulating the physical brain . is there any research i should look into regarding this topic ? i would like to hear if it is possible , why , why not , what are the limitaions , how far we are from achieving this , and anything else , really . also , i would like to read something about my first assumption ( "" the mind arises from a physical system , the brain "" ) . i have searched ai.se but i 've found only related questions , so i do n't think this is duplicate . for reference : why are we asking , “ how can we simulate the brain ? ” how powerful a computer is required to simulate the human brain ? are there any artificial neuromorphic systems which can mimic the brain ? is it possible to build human - brain - level artificial intelligence based on neuromorphic chips and neural networks ? ( this one is pretty similar but i think it 's slighly different , and it does n't answer my question ) . note : i am not asking for the possibility now , but in general , so telling me that "" we do n't know the brain enough "" is not on topic .",23527,23527,2019-05-05T18:16:01.870,2019-05-14T15:59:22.350,"is there any paper , article or book that analyzes the feasibility of acheiving agi through brain - simulation ?",agi human-like brain human-inspired,2,0,3
3576,12157,1,,2019-05-05T20:25:17.603,2,8,"in section 3 of this paper the author outlines how garb was adapted to reduce the variance in updating parameters to an internal reward function estimator . i have read it a number of times and understand it up through the end of the explanation of garb . the author then goes on to explain how they use backprop to implement this procedure , which is the point at which i stop understanding . is there an open source implementation available to look at ? i ca n’t figure out if $ g_t$ is actually computed and used or not ? and not certain how the internal reward gradient is calculated . any insight you can provide would be helpful . edit : after reading it several times and several related papers , i think i have more understanding but not quite there . so my main questions are : 1 ) are we keeping a full eligibility trace vector with the dimensionality of the vector equal to the number of parameters in ( all nn params ) ? 2 ) do we use the gradient calculation via backdrop at every step to calculate $ g_t$ ? 3 ) do we have to maintain $ 3 * \theta$ parameters , one for theta , one for $ e$ and one for $ g_t$ ? 3 ) what then is the procedure at terminal $ g_t$ to update the parameters ? a simple element wise matrix operation ? 4 ) how often to update the parameters of ?",16343,16343,2019-05-06T14:38:07.353,2019-05-06T14:38:07.353,how is garb implemented in pgrd - dl to calculate gradients w.r.t . internal rewards ?,neural-networks reinforcement-learning monte-carlo-tree-search rewards monte-carlo,0,2,
3577,12158,1,,2019-05-05T21:15:03.413,0,15,"i 'm currently using google speech to text api to transcribe audio in real time ( police scanner audio dispatches ) . the audio quality is n't great and i 've been putting in key words to try to help train it . is there a way to either use google , amazon aws , ibm watson , to create a model based on past audio dispatches where i can manually type in what was said to help train it ? it seems like putting in key words wo n't really cut it . any other suggestions to help make it more accurate ?",25432,,,2019-05-05T21:15:03.413,speech to text models,machine-learning audio-processing,0,0,
3578,12159,1,,2019-05-06T04:37:58.027,1,35,"background : i have seen lots of people asking whether multiplication and pseudo - random sequences can be approximated by a nn without providing whether the inputs and outputs are bounded or not , and people have answered it ( lot of upvotes ) based on conventional nn knowledge . without taking into consideration the aforementioned fact . tl;dr how good / is it possible by a neural network to approximate an unbounded function provided it is trained on a subset of the number line and the test inputs are significantly outside the subset ? can a neural network do regression for an unbounded function ? to me it is impossible if the output function is sigmoid , since the best approximation ( basis of all signal decomposition and reconstruction schemes ) of a function the fourier series ( ) demands dirichlet 's condition to be satisfied , one of which more or less states that the value should be absolutely integrable ( ) . the sigmoid can be more or less thought in terms of a sinusoidal function as its value is bounded like a sinusoid . now , if the output function used is relu then the output is unbounded . but still it is just some linear combination of weights gone through some non - linear functions ( in the previous layers which at best might be linearly unbounded if previous layers are relu ) . so one can assume , that even though the neural net can approximate an unbounded linear function , can it approximate an unbounded polynomial function or an exponential function ? although the regression problem might seem more suitable to fourier transform analogy than fourier series , i have used the fs analogy based on the fact that ft output is is continuous function as opposed to fs ( in nn regression we are adding outputs of several nodes , similar to what we do in fs where $ number_{nodes } & lt;&lt ; \infty$ .",9947,9947,2019-05-10T15:38:27.227,2019-05-10T15:38:27.227,neural networks vs unbounded outputs,neural-networks fourier-approximation,0,8,
3579,12160,1,,2019-05-06T06:32:31.823,0,32,"while i 've been able to solve mountaincar - v0 using deep q learning , no matter what i try i ca n't solve this enviroment using policy - gradient approaches . as far as i learnt searching the web , this is a really hard enviroment to solve , mainly because the agent is given a reward only when it reaches the goal , which is a rare event . i tried to apply the so called "" reward engineering "" , more or less substituting the reward given by the enviroment with a reward based upon the "" energy "" of the whole system ( kinetic plus potential energy ) , but despite this no luck . i ask you : is correct to assume that mountaincar - v0 is beyond the current state of the art a3c algorithm , so that it requires some human intervention to suggest the agent the policy to follow , for example adopting reward engineering ? could anyone provide any hint about which reward function could be used , provided that reward engineering is actually needed ? thanks for your help .",25352,,,2019-05-06T06:32:31.823,a3c fails to solve mountaincar - v0 enviroment ( implementation by openai gym ),reinforcement-learning,0,0,
3580,12161,1,12180,2019-05-06T08:03:54.360,2,68,"there are different kinds of machine learning algorithms , both univariate and multivariate , that are used for time series forecasting : for example arima , var or ar . why is it harder ( compared to classical models like arima ) to achieve good results using neural network based algorithms ( like ann and rnn ) for multi step time series forecasting ?",24006,2444,2019-05-07T13:32:59.060,2019-05-07T13:50:51.917,why is it harder to achieve good results using neural network based algorithms for multi step time series forecasting ?,deep-learning prediction comparison forecasting time-series,2,2,
3581,12163,1,,2019-05-06T08:08:29.923,0,20,"i read several papers and articles where it is suggested that transposed convolution with 2 strides is better than upsampling then convolution . however implementing such model with the transposed convolution resulted in heavy checkboard effect , where the whole generated image is just a pattern of squares and no learning takes place . how to properly implement it without totally messing up the generation ? with the upsampling+convolution i got okay result but i want to improve my model . i am trying to generate images based on the celeba dataset . i use keras with tf and i used the following code : model.add(conv2dtranspose(256 , 5 , 2 , padding='same ' ) ) model.add(leakyrelu(alpha=0.2 ) ) model.add(batchnormalization(momentum=0.9 ) ) model.add(conv2dtranspose(128 , 5 , 2 , padding='same ' ) ) model.add(leakyrelu(alpha=0.2 ) ) model.add(batchnormalization(momentum=0.9 ) ) model.add(conv2dtranspose(64 , 5 , 2 , padding='same ' ) ) model.add(leakyrelu(alpha=0.2 ) ) model.add(batchnormalization(momentum=0.9 ) ) here i try to turn a 4x4 image into a 32x32 . later it will be turned into a 64x64 image with 1 or 3 channels depending on the image . however i get the following pattern always . some tweaking usually leads to some other pattern but it does not really change : checkboard effect thank you for your answers in advance",25438,,,2019-05-06T08:08:29.923,transposed convolution as upsampling in dcgan,deep-learning computer-vision generative-adversarial-networks,0,0,
3582,12167,1,,2019-05-06T13:34:27.300,0,11,i have a python notebook in ibm watson studio for a time series forecasting purpose . it takes two input data max min and it returns two lists of data having 10 steps ahead prediction of both max and min . i want to make it available for an external application . i have searched for the solution like api creation but did n't get any source . can anyone suggest me the proper way or helpful resources to do it ?,24006,,,2019-05-06T13:34:27.300,deploy notebook in ibm watson,machine-learning python cloud-services,0,0,
3583,12168,1,12175,2019-05-06T14:27:50.723,2,40,"so , i have a dataset which has around 1388 unique products and i have to do unsupervised learning on them in order to find anomalies ( high / low peaks ) . the data below just represents one product . the contextid is the product number , and the stepid indicates different stages in the making of the product . contextid backsgas_flow_sccm stepid time_ms 427 7290057 1.7578125 1 09:20:15.273 428 7290057 1.7578125 1 09:20:15.513 429 7290057 1.953125 2 09:20:15.744 430 7290057 1.85546875 2 09:20:16.814 431 7290057 1.7578125 2 09:20:17.833 432 7290057 1.7578125 2 09:20:18.852 433 7290057 1.7578125 2 09:20:19.872 434 7290057 1.7578125 2 09:20:20.892 435 7290057 1.7578125 2 09:20:22.42 436 7290057 16.9921875 5 09:20:23.82 437 7290057 46.19140625 5 09:20:24.102 438 7290057 46.19140625 5 09:20:25.122 439 7290057 46.6796875 5 09:20:26.142 440 7290057 46.6796875 5 09:20:27.162 441 7290057 46.6796875 5 09:20:28.181 442 7290057 46.6796875 5 09:20:29.232 443 7290057 46.6796875 5 09:20:30.361 444 7290057 46.6796875 5 09:20:31.381 445 7290057 46.6796875 5 09:20:32.401 446 7290057 46.6796875 5 09:20:33.431 447 7290057 46.6796875 5 09:20:34.545 448 7290057 46.6796875 5 09:20:34.761 449 7290057 46.6796875 5 09:20:34.972 450 7290057 46.6796875 5 09:20:36.50 451 7290057 46.6796875 5 09:20:37.120 452 7290057 46.6796875 7 09:20:38.171 453 7290057 46.6796875 7 09:20:39.261 454 7290057 46.6796875 7 09:20:40.280 455 7290057 46.6796875 12 09:20:41.429 456 7290057 46.6796875 12 09:20:42.449 457 7290057 46.6796875 12 09:20:43.469 458 7290057 46.6796875 12 09:20:44.499 459 7290057 46.6796875 12 09:20:45.559 460 7290057 46.6796875 12 09:20:45.689 461 7290057 47.16796875 12 09:20:46.710 462 7290057 46.6796875 12 09:20:47.749 463 7290057 46.6796875 15 09:20:48.868 464 7290057 46.6796875 15 09:20:49.889 465 7290057 46.6796875 16 09:20:50.910 466 7290057 46.6796875 16 09:20:51.938 467 7290057 24.21875 19 09:20:52.999 468 7290057 38.76953125 19 09:20:54.27 469 7290057 80.46875 19 09:20:55.68 470 7290057 72.75390625 19 09:20:56.128 471 7290057 59.5703125 19 09:20:57.247 472 7290057 63.671875 19 09:20:58.278 473 7290057 70.5078125 19 09:20:59.308 474 7290057 71.875 19 09:21:00.337 475 7290057 69.82421875 19 09:21:01.358 476 7290057 69.23828125 19 09:21:02.408 477 7290057 69.23828125 19 09:21:03.548 478 7290057 72.4609375 19 09:21:04.597 479 7290057 73.4375 19 09:21:05.615 480 7290057 73.4375 19 09:21:06.647 481 7290057 73.4375 19 09:21:07.675 482 7290057 73.4375 19 09:21:08.697 483 7290057 73.4375 19 09:21:09.727 484 7290057 74.21875 19 09:21:10.796 485 7290057 75.1953125 19 09:21:11.827 486 7290057 75.1953125 19 09:21:12.846 487 7290057 75.1953125 19 09:21:13.865 488 7290057 75.1953125 19 09:21:14.886 489 7290057 75.1953125 19 09:21:15.907 490 7290057 75.9765625 19 09:21:16.936 491 7290057 75.9765625 19 09:21:17.975 492 7290057 75.9765625 19 09:21:18.997 493 7290057 75.9765625 19 09:21:20.27 494 7290057 75.9765625 19 09:21:21.55 495 7290057 75.9765625 19 09:21:22.75 496 7290057 75.9765625 19 09:21:23.95 497 7290057 76.85546875 19 09:21:24.204 498 7290057 76.85546875 19 09:21:25.225 499 7290057 76.85546875 19 09:21:25.957 500 7290057 76.85546875 19 09:21:26.984 501 7290057 75.9765625 19 09:21:27.995 502 7290057 75.9765625 19 09:21:29.2 503 7290057 76.7578125 19 09:21:30.13 504 7290057 76.7578125 19 09:21:31.33 505 7290057 76.7578125 19 09:21:32.59 506 7290057 76.7578125 19 09:21:33.142 507 7290057 76.7578125 19 09:21:34.153 508 7290057 75.87890625 19 09:21:34.986 509 7290057 75.87890625 19 09:21:35.131 510 7290057 75.87890625 19 09:21:35.272 511 7290057 75.87890625 19 09:21:35.451 512 7290057 76.7578125 19 09:21:36.524 513 7290057 76.7578125 19 09:21:37.651 514 7290057 76.7578125 19 09:21:38.695 515 7290057 76.7578125 19 09:21:39.724 516 7290057 76.7578125 19 09:21:40.760 517 7290057 76.7578125 19 09:21:41.783 518 7290057 76.7578125 19 09:21:42.802 519 7290057 76.7578125 19 09:21:43.822 520 7290057 76.7578125 19 09:21:44.862 521 7290057 76.7578125 19 09:21:45.884 522 7290057 76.7578125 19 09:21:46.912 523 7290057 76.7578125 19 09:21:47.933 524 7290057 76.7578125 19 09:21:48.952 525 7290057 76.7578125 19 09:21:49.972 526 7290057 76.7578125 19 09:21:51.72 527 7290057 77.5390625 19 09:21:52.290 528 7290057 77.5390625 19 09:21:52.92 529 7290057 77.5390625 19 09:21:53.361 530 7290057 77.5390625 19 09:21:54.435 531 7290057 76.66015625 19 09:21:55.602 532 7290057 76.66015625 19 09:21:56.621 533 7290057 72.94921875 22 09:21:57.652 534 7290057 3.90625 24 09:21:58.749 535 7290057 2.5390625 24 09:21:59.801 536 7290057 2.1484375 24 09:22:00.882 537 7290057 2.05078125 24 09:22:01.259 538 7290057 2.1484375 24 09:22:01.53 539 7290057 1.953125 24 09:22:02.281 540 7290057 1.953125 24 09:22:03.311 541 7290057 2.1484375 24 09:22:04.331 542 7290057 2.1484375 24 09:22:05.351 543 7290057 1.953125 24 09:22:06.432 544 7290057 1.85546875 24 09:22:07.519 545 7290057 1.7578125 24 09:22:08.549 546 7290057 1.85546875 24 09:22:09.710 547 7290057 1.7578125 24 09:22:10.738 548 7290057 1.85546875 24 09:22:11.798 549 7290057 1.953125 24 09:22:12.820 550 7290057 1.85546875 1 09:22:13.610 551 7290057 1.85546875 1 09:22:14.629 552 7290057 1.953125 1 09:22:15.649 553 7290057 1.85546875 2 09:22:16.679 554 7290057 1.85546875 2 09:22:17.709 555 7290057 1.85546875 2 09:22:18.729 556 7290057 1.953125 2 09:22:19.748 557 7290057 1.85546875 2 09:22:20.768 558 7290057 1.7578125 3 09:22:21.788 559 7290057 1.7578125 3 09:22:22.808 560 7290057 1.85546875 3 09:22:23.829 561 7290057 1.953125 3 09:22:24.848 562 7290057 1.85546875 3 09:22:25.898 563 7290057 1.953125 3 09:22:27.39 564 7290057 1.953125 3 09:22:28.66 565 7290057 1.7578125 3 09:22:29.87 566 7290057 1.85546875 3 09:22:30.108 567 7290057 1.7578125 3 09:22:31.129 568 7290057 1.953125 3 09:22:32.147 569 7290057 1.85546875 3 09:22:33.187 i use the following code to plot a graph . code : lineplot = x.loc[x['contextid ' ] = = 7290057 ] x_axis = lineplot.values[:,3 ] y_axis = lineplot.values[:,1 ] plt.figure(1 ) plt.plot(x_axis , y_axis ) and the graph : in this graph , the peaks ( marked in red circles ) are the anomalies which need to be detected . and when i have a graph like this : no anomalies must be caught since there are no undesirable peaks . i tried using oneclasssvm , but i am somehow not satisfied with the results . i would like to know which unsupervised learning algorithm can be used for such a task at hand . thanks",23380,,,2019-05-07T08:41:09.563,which unsupervised learning algorithm can be used for peaks detection ?,machine-learning deep-learning unsupervised-learning anomaly-detection,1,0,
3584,12169,1,,2019-05-06T17:07:15.197,2,14,i would like to train a neural network with the openai gym taxi and see how it would react on a new map . is it possible to insert a new map in the openai gym taxi v.2 ?,25448,16565,2019-05-07T08:26:26.807,2019-05-07T08:26:26.807,is it possible to insert a new map in the openai gym taxi v.2 ?,open-ai academia,0,0,
3585,12171,1,,2019-05-06T19:20:08.353,2,121,"i am building a video analytics program for counting moving things in a video . i am detecting bicycles and nothing else . i run object detection using the ssd mobile - net model in all the frames and store the bounding box coordinates ( x , y , w , h ) of each detection to a csv file . so for a video , i have a csv file of one row each for a frame and each row has multiple detections of d1 , d2 , d3 , .. , dn . each detection has the bounding box coordinates as values . d1 is x , y , w , h . based on the x , y values of each detection , i am trying to find the direction of the bicycles and if the bicycle crosses the whole frame to do a up / down count . how do i count / track(don't want to use classic tracking algos ) these bounding boxes moving in the video ? i see lstm / rnn coming up in my search results when i search for video analytics . being a noob , i am not able to find any tutorial that suits my needs . i would like to check if my approach towards the problem is correct . i do n't want to use the classical tracking solutions for two reasons i feel the tracking and counting conditions that i program in python is always leaky / fails in certain conditions , hence i want to see how ai manages to count the objects . the video stream i am using has heavy distortion on the objects that i track , hence the shape and size of the object changes drastically within 10s/20s of frames . any help or suggestion towards other better approaches is much appreciated . edit 1 : the area of view under the camera is fixed . and we expect the bicycles to move from one entry side . lets assume that the view and entry / exit is like shown in this video https://www.youtube.com/watch?v=tw7pl3bszr4",22093,22093,2019-05-12T18:32:18.570,2019-05-13T23:20:58.130,object in / out counting using cnn+rnn,computer-vision lstm,1,2,
3586,12172,1,,2019-05-06T20:48:25.060,5,56,"i 'm interested in the industrial use of gdl ( see https://arxiv.org/abs/1611.08097 ) . is it used in industry ? that is , does any company have access to non - euclidean data and process it directly instead of converting it to a more standard format ?",22365,2444,2019-05-06T20:54:37.843,2019-05-16T15:39:40.437,do you know any examples of geometric deep learning used in industry ?,applications geometric-deep-learning real-world,1,1,1
3587,12173,1,,2019-05-07T05:11:43.027,1,20,"i have searched but found that some similarity measures are for continuous data and some are for categorical data . but i want to know the similarity measures which are use for both data , continuous and categorical ?",23501,,,2019-05-07T05:21:52.430,what are the similarity measure use for both continuous and categorical data ?,machine-learning search,1,0,
3588,12176,1,,2019-05-07T09:00:29.800,0,15,i want to make 8 fold cross validation from the dataset . the dataset is the musical onsets annotated which has txt format at each songs . how to make the another folder like called splits ( the name of folder ) which is contains 8 fold of songs . each fold will be written as a txt format . i need some code references to make 8 fold like that . do i need use sklearn kfold ? and how to save each song like stated in the third picture ? thank you .,18885,,,2019-05-07T09:00:29.800,n fold cross validation,machine-learning,0,2,1
3589,12178,1,,2019-05-07T11:43:35.537,0,32,"i am currently training a cnn model by using cifar10 images ( 50000 for training , another 10000 for validation ) . i plot training loss , validation loss and accuracy against training iteration : i am not sure when should i stop the training , should i stop it at around iteration 1250 , since overfitting happens beyond that point ? or should i stop it at around iteration 5000 , since i can get the maximum accuracy ? why can the overfitted model have a higher accuracy ?",21213,2444,2019-05-08T13:16:25.177,2019-05-08T13:16:25.177,why could an overfitted cnn model have a higher validation accuracy ?,machine-learning convolutional-neural-networks training overfitting,1,0,
3590,12181,1,,2019-05-07T13:21:10.413,1,26,"he starts his paper from 1987 [ 1 ] with a reference to the prospector expert system which was using bayes rules to handle uncertain knowledge . then he explains the idea behind probabilistic logic which is used for determine the value of true / false statements in a semantic tree . the probability values are displayed in a 3d box graphically to make the concept clearer for the newbies . he calls the concept quote : “ straightforward generalization of the ordinary true - false semantics ” but something is wrong with the paper . approximately in the middle of the text , i missed the point why such an alternative logic concept is useful . is n't it enough to deal with normal yes / no statements which can stored in a turing machine much easier ? why do we need complicated 3d boxes with a probability landscape ? [ 1 ] nilsson , nils j. "" probabilistic logic . "" artificial intelligence 28.1 ( 1986 ) : 71 - 87 .",11571,,,2019-05-07T19:45:17.417,was nils nilsson wrong with probabilistic knowledge representation ?,knowledge-representation probabilistic,2,3,
3591,12182,1,12198,2019-05-07T13:26:01.993,1,30,"what is a temporal feature , what features make something temporal in nature ? is this problem agnostic ? how does it change from different fields of study ?",22093,2444,2019-05-08T00:02:24.320,2019-05-08T00:07:31.807,what is a temporal feature ?,machine-learning definitions signal-processing feature,1,0,
3592,12183,1,,2019-05-07T13:41:03.393,2,78,"i hope this question is ok here , but since i have found a tag which deals with these issues ( profession ) , i 'll ask away . i also hope this may be useful to other people with similar doubts , since i am failing to find valuable information on this topic online . i am interested in the theoretical side of cs , such as computability , logic , complexity theory and formal methods . at the same time , i am deeply fascinated by artificial intelligence and the questions it poses to our understanding of the notion of intelligence and what does it mean to be a human being . in general , is ai a more "" applied""/engineeristic field , or are there theoretical aspects to research in ? in short : if i prefer formal / theoretical compsci , is ai a bad career choice ? ( note : i am asking this because i am a cs undergrad considering getting into a ai msc ) .",23527,23527,2019-05-07T13:51:38.157,2019-05-08T04:32:16.963,"if i am interested in theoretical computer science , is ai a bad choice ?",profession,2,8,
3593,12187,1,12192,2019-05-07T14:53:05.343,0,34,"how fast does monte carlo tree search converge ? is there proof that it does converge ? how does it compare to temporal difference learning in terms of convergence speed ( assuming the evaluation step is a bit slow ) ? is there a way to exploit the information gathered during the simulation phase to accelerate mcts ? sorry if too many questions , if you have to choose one , please choose the last question : p. thanks .",23866,,,2019-05-07T19:38:23.603,monte carlo tree search convergence,reinforcement-learning monte-carlo-tree-search temporal-difference,1,0,
3594,12189,1,,2019-05-07T15:50:03.360,0,65,"is reinforcement learning problem adaptable to the setting when there is only one - final - reward . i am aware about problems with sparse and delayed rewards , but what about the only one reward and quite long path ?",8332,2444,2019-05-07T16:04:38.000,2019-05-07T20:23:20.010,can reinforcement learning be used for tasks where only one final reward is received ?,reinforcement-learning,1,0,
3595,12191,1,,2019-05-07T17:23:47.053,2,10,"apologies for the lengthy title . my question is about the weight update rule for logistic regression using stochastic gradient descent . i have just started experimenting on logistic regression . i came across two weight update expressions and did not know which one is more accurate and why they are different . the first method : source : ( book ) artificial intelligence : a modern approach by norvig , russell on page 726 - 727 : using the l2 loss function : where g stands for the logistic function g ' stands for g 's derivative w stands for weight hw(x ) represents the logistic regression hypothesis the other method : source ( paper authored by charles elkan ) : logistic regression and stochastic gradient training . can be found here",25463,,,2019-05-07T17:23:47.053,what is the right formula for weight update rule in logistic regression using stochastic gradient descent,machine-learning ai-basics gradient-descent logistic-regression,0,2,
3596,12194,1,,2019-05-07T20:47:41.957,0,18,"i need to fix this python code to my project there is the code : https://www.alexkras.com/transcribing-audio-file-to-text-with-google-cloud-speech-api-and-python/ here the error : traceback ( most recent call last ) : file "" source_file.py "" , line 1 , in import speech_recognition as sr importerror : no module named speech_recognition someone can help me ?",25473,,,2019-05-08T13:44:36.800,google speech to text api,machine-learning python ai-development,1,1,
3597,12206,1,12207,2019-05-08T15:05:25.370,0,251,"in sutton 's rl : an introduction 2nd edition it says the following(page 203 ) : state aggregation is a simple form of generalizing function approximation in which states are grouped together , with one estimated value ( one component of the weight vector w ) for each group . the value of a state is estimated as its group ’s component , and when the state is updated , that component alone is updated . state aggregation is a special case of sgd ( 9.7 ) in which the gradient , rv̂(s t , w t ) , is 1 for s t ’s group ’s component and 0 for the other components . and follows up with a theoretical example . my question is , imagining my original state space is [ 1,100000 ] , why ca n't i just say that the new state space is [ 1 , 1000 ] where each of these numbers corresponds to an interval : so 1 to [ 1,100 ] , 2 to [ 101,200 ] , 3 to [ 201,300 ] , and so on , and then just apply the normal td[0 ] formula , instead of using the weights ? my main problem with their approach is the last sentence : in which the gradient , rv̂(s t , w t ) , is 1 for st ’s group ’s component and 0 for the other components . if v(s , w ) is the linear combination of a feature vector and the weights ( w ) , how does the gradient of that function can be 1 for a state and 0 for others ? there are not as many w as states or groups of states . e.g. : if my feature vector is 5 numbers between 0 and 100 . for example ( 55,23,11,44,99 ) for a specific state , how do you choose a specific group of states for state aggregation ? maybe what i 'm not understanding is the feature vector . if we have a state space that is[1,10000 ] as in the random walk , what can be the feature vector ? does it have the same size as the number of groups after state aggregation ?",24054,24054,2019-05-08T15:55:05.107,2019-05-08T16:10:16.210,state aggregation methods,reinforcement-learning function-approximation,1,0,
3598,12209,1,12211,2019-05-08T16:39:42.997,1,28,"background i am working on a robotic arm controlled by a dqn + a python script i wrote . the dqn receives the 5 joint states , the coordinates of a target , the coordinates of the obstacle and outputs the best action to take ( in terms of joint rotations ) . the python script checks if the action suggested by the dqn is safe . if it is , it performs it . otherwise , it performs the second highest - ranking action from the dqn ; and so on . if no action is possible , collision : we fail . during training , this python functionality was n't present : the arm learned how to behave without anything else to correct his behaviour . with this addition on the already - trained network , the performance raised from 78 to 95 % . now my advisor ( bachelor 's thesis ) asked me to leave the external controller on during training to check whether this improves learning . question here 's what happens during training ; at each step : the ann selects an action if it is legal , the python script executes it , otherwise it chooses another one . now ... on which action should i perform backprop ? the one proposed by the arm or the one which was really executed ? ( so , which action should i perform backprop on ? ) i am really confused . on the one hand , the arm did choose an action so my idea was that we should , in fact , learn on that action . on the other hand , during the exploration phase ( greedy ) , we backprop on the action which was randomly selected and executed , with no interest on what was the output of the arm . so , it would be rational too , in this case , to perform backprop on the action really executed ; so the one chosen by the script . what is the right thing to do here ? . ( bonus question : is it reasonable to train with this functionality on ? would n't it be better to train the network by itself , and then later , enhance its performance with this functionality ? )",23527,23527,2019-05-08T16:47:10.813,2019-05-08T18:19:09.347,dqn agent helped by a controller : on which action should i perform backprop ?,reinforcement-learning,1,0,
3599,12210,1,,2019-05-08T16:58:41.877,1,12,"so when using semi - gradient td(0 ) you need to convert your state representation into a feature vector that represents the state and as far as i know , should not be correlated . is the input on the ann of a dqn the same ? should it be a feature vector that represents the state ? what considerations should one have when creating such vector ?",24054,,,2019-05-08T16:58:41.877,dqn ann input vs linear function approximator feature vector,reinforcement-learning dqn,0,0,
3600,12213,1,,2019-05-08T20:50:07.520,1,17,"i have some variable length input vectors for my own use case of a ' stylistic transfer'-esque process , and i am wondering if anyone knows of a way to engineer an input that maps to a 0 element in embedding space . this would be an element that simply holds space but would be readily overlaid with vector addition of another embedded input . my rationale is that i could pad the inputs with these zero elements to mask what i do n't care about and have a semantically meaningful vector addition in the embedding space . i wonder if i could permute some training examples with a chosen value which all map to the same output and this would allow a neural net to learn such a feature .",12849,,,2019-05-08T20:50:07.520,creating a zero element in embedding space,neural-networks deep-learning word-embedding,0,0,
3601,12214,1,,2019-05-08T22:06:36.610,1,28,"i am making a school project where i should develop any kind of game where i can have one reactive agent and one agent based on machine learning competing with each other . my game consists of a salesmen problem . basically , i have 3 types of entities , consumers , salesmen , and hotspots . the consumers are represented by the person with a green background . there are 8 of them . they basically move around the whole game using random walks and they tend o aggregate on the hotspots ( the orange icon with the router in it ) . the salesmen are represented by the person with the dark grey background . one of them is controlled by a reactive agent that has in it some rules that i programmed and the other one is controlled by my dqn model . the salesmen have 5 available actions , move up , right , down , left or sell . when they choose to sell the simulation will try to sell to the closest consumer in a predetermined max range . if no consumers exist in that range or if the consumer rejects to buy then the sell fails . i started training a deep q network that i built using tensorflow . as input features , i am giving the agent current position , the position of each consumer and a boolean saying if the consumer was recently asked to buy or not ( consumers that were asked to buy something will reject future offers for a determined amount of time with 100 % probability ) . for the output layer , i have 5 nodes , one for each available action . here is a screenshot of the game : the red number on the right - bottom corner of each agent represents their total utility . i decided to give the agents the following rewards : sell_successed_reward = 3 - the agents receives 3 points for each success sell . sell_failed_reward = -0.010 - the agent loses 0.01 points for each failed sell moving_reward = -0.001 - the agent loses 0.001 points for each move not_moving_reward = -0.0125 - the agent loses 0.0125 points for standing in the same position ( ie . not moving or trying to move against a wall ) i started training my agent but i seems to do not learn anything ! i left it training for around 3 hours and i could not see any improvement . i tried different activation functions , batch sizes , exploration rates etc but no noticiable effect . my question is : can a dqn learn in this type of enviroment where there are a lot of random walks ? if yes what could be my problem ? not enought training time ? bad input features ? bad implementation ? here are the files with my implementation of dqn : agent : https://github.com/daniel3303/aasma-project/blob/master/src/agent/deeplearningagent/deeplearningagent.py training : https://github.com/daniel3303/aasma-project/blob/master/train_dqn.py thanks .",21688,,,2019-05-08T22:06:36.610,dqn not able to learn in a game where other agents perform random walks,neural-networks machine-learning deep-learning reinforcement-learning tensorflow,0,2,
3602,12215,1,,2019-05-09T00:33:42.617,1,92,how can i develop a prediction algorithm in the case of games of chance ? suppose there is a 50:50 chance of winning . is there way of creating a prediction algorithm ?,17164,1671,2019-05-15T21:10:23.737,2019-05-15T21:10:23.737,how can i develop a prediction algorithm for a game of chance ?,machine-learning reinforcement-learning prediction probabilistic games-of-chance,3,7,
3603,12216,1,,2019-05-09T02:44:54.127,2,74,"i have found various references describing naive bayes and they all demonstrated that it used mle for the calculation . however , this is my understanding : $ p(y = c|x)$ $ p(x|y = c)p(y = c)$ with $ c$ is the class the model may classify $ y$ as . and that 's all , we can infer $ p(x|y = c)$ and $ p(c)$ from the data . i do n't see where the mle shows its role .",25509,16708,2019-05-09T17:38:25.150,2019-05-16T11:54:03.210,what is the relationship between mle and naive bayes,naive-bayes maximum-likelihood conditional-probability,1,0,
3604,12217,1,12218,2019-05-09T02:59:36.227,1,37,"i would provide a sound signal of about 2 - 3 seconds to my neural network . i have trained my network with a single word , like if i speak "" hello "" the network may tell if "" hello "" is spoken or not , but some other word like "" world "" is spoken , it will say "" hello "" is not spoken . i just want classification of sound if its a specific command or word . what is the best way to do this , i am not a that much advanced in dnn , i only know about nn and cnn , i want to know if there is some research paper or tutorial , or need some explanation about the work .",25510,,,2019-05-09T04:52:33.373,how to do speech recognition on a single word,neural-networks deep-learning voice-recognition,1,0,
3605,12222,1,,2019-05-09T11:16:16.647,0,19,"i 'm kind of stuck , and instead of trying to randomly shoot the net with my ideas maybe i can consult it with you ( one epoch takes 7h , so i cant't test my random ideas ) . here 's the crime scene : my objective is to train a vgg - family net on specific custom moderately - large dataset ( 4.3 mln images , 7205 classes ) . since 1 epoch takes 7h to calculate ( on whole dataset ) , i 've tuned the hyperparameters on 300 classes ( approx . 200 000 images ) . the net gets about 50 % top-1 accuracy after 40 epochs , which is ok for me ( learning curve attached ) , and also does pretty good on 1000 classes ( on the pic ) . now i 'm prepared for the big heist and i 'm training the net on whole dataset . but the net does n't really learn anything , with the accuracy oscillating around random , even after 44 epochs ( yep , 13 days of training ) . specs : batch size : 64 learning rate : log decay from 0.01 class weightening to prevent overrepresented classes to mess with weights net trained from scratch the net architecture and training is exactly the same ( except ofc last fc layer which size i 've changed from 300 to 7205 ) . do you have any ideas on this ? to small fc layers ? wrong learning rate ? what am i missing ?",25520,,,2019-05-09T11:16:16.647,net stops to learn when i increase number of classes,neural-networks convolutional-neural-networks image-recognition,0,0,
3606,12223,1,,2019-05-09T11:18:29.680,0,5,"i want to discretize a continuous attribute , i applied a lot of methods but it 's clear that i 'm losing information at each time , until i found the fdic method wich seems great but i could n't impelement it . this is the paper 's name : dynamic data discretization technique based on frequency and k - nearest neighbour algorithm . this is the explanation part from the paper : it begins with sorting each numerical attribute according to its value . the following is performed : 1 . compute frequency distribution of pattern and put into its own k interval ; 2.calculate the w intervals width ( length ) on each pattern observed by finding the distance between two k interval patterns ; 3 . compute means of w intervals on the distribution of k intervals ; 4 . automatic min threshold is computed based on means distribution",25521,,,2019-05-09T11:18:29.680,is there anyone who tried to discretize a continuous feature using the fdic method(frequency dynamic interval class ) ? ( seems a great method ),machine-learning time-series,0,0,
3607,12225,1,,2019-05-09T12:03:54.643,-2,13,"num_epochs = 2 # we iterate 20 times over the entire training set kernel_size = 3 # we will use 3 * 3 kernel throughput pool_size = 2 # we wil 2 * 2 pooling throughput conv_depth_1 = 32 # we will initially have 32 krnels per conv . layer conv_depth_2 = 64 # switching to 64 after the first pooling layer drop_prob_1 = 0.25 # dropout after first pooling with probability 0.25 drop_prob_3 = 0.5 # dropout after first pooling with probability 0.25 hidden_size = 512 # the fc layer will have 512 neurons ( x_train , y_train),(x_test , y_test)= cifar10.load_data ( ) # fetch cifar data print(x_train[1 ] ) num_train , depth , height , width = x_train.shape # there are 50000 training example num_test = x_test.shape[0 ] # there are 10000 test examples i. cifar -10 num_classes = np.unique(y_train).shape[0 ] # there are 10 image classes x_train = x_train.astype('float32 ' ) x_test = x_test.astype('float32 ' ) y_train = np_utils.to_categorical(y_train , num_classes ) # one - hot encode the labels classes to catogories y_test = np_utils.to_categorical(y_test , num_classes ) # one - hot encode the labels classes to catogories inp = input(shape=(depth , height , width ) ) # n.b depth goes first in keras # conv[32 ] -&gt ; conv[32 ] -&gt ; pool ( with dropout on the pooling layers ) conv_1 = convolution2d(conv_depth_1,kernel_size , kernel_size , border_mode='same ' ) conv_2 = convolution2d(conv_depth_1,kernel_size , kernel_size , border_mode='same ' ) pool_1 = maxpooling2d(pool_size=(2,2))(conv_2 ) drop_1 = dropout(drop_prob_1)(pool_1 ) i am facing this error please help me to sort this . thank you valueerror : layer max_pooling2d_15 was called with an input that is n't a symbolic tensor . received type : & lt;class ' keras.layers.convolutional.conv2d'&gt;. full input : [ & lt;keras.layers.convolutional.conv2d object at 0xb4e073c88&gt;]. all inputs to the layer should be tensors .",25523,,,2019-05-09T12:03:54.643,valueerror : layer max_pooling2d_15 was called with an input that is n't a symbolic tensor,convolutional-neural-networks,0,1,
3608,12226,1,,2019-05-09T13:02:53.130,0,42,"i 'm trying to train a ppo agent and my average rewards graph looks like this . could this indicate that it 's stuck at a local maximum ? do i need to promote exploring by increasing the entropy or does this look like a bug with my implementation ? also , my action space is continuous . thanks ! hyperparameters : learning rate = 0.01 entropy coefficient = 0.01 value function loss coefficient = 0.5 gamma / discount factor = 0.995 minibatch size = 512 epochs = 3 clip epsilon = 0.1",23494,,,2019-05-09T13:02:53.130,reinforcement learning with ppo : rewards decreasing,reinforcement-learning policy-gradients proximal-policy-optimization,0,5,
3609,12231,1,,2019-05-09T20:36:31.373,0,25,"i am working on a self driving car powered by a raspberry pi . my first step is to use ppo to teach it to not run into walls . but i am having trouble getting it to work . i want to allow the car to have action of [ right , left , speed_up , speed_down ] the state i send in is the distance from each of 3 sensors and the speed . all normalized to 0 - 1 . the reward is based on the distance from the 3 sensors with a bonus for going faster . -1 reward for speed = = 0 , and -500 if you run into a wall . this always gets the agent just sitting still moving in circles , collecting -1 rewards . what am i doing wrong ? how would you shape your rewards ? any advice ?",25535,2444,2019-05-09T21:15:59.163,2019-05-09T21:15:59.163,how do i define the reward function in the case of self - driving raspberry pi car ?,reinforcement-learning rewards proximal-policy-optimization,0,0,1
3610,12233,1,,2019-05-10T03:59:18.300,1,23,"i 'm a grad student from ee . so , basically , there 's an electrical circuit that is supposed to output "" 0 "" or "" 1 "" by exactly 50 to 50 chance . it generates a number of big arrays of 0s and 1s , each of which amounts to more than 4,000 of the numbers . but because these arrays are physically generated in a fab , i assume it might develop some dependencies among numbers and some output could be predicted by more than 50 % chance . for example , due to some variations in the process , "" 1 "" can be more likely to come than "" 0 "" after a sequence of "" 001100 "" . then let 's say i make a simple deep neural network which takes 7 inputs and gives 1 output . i simply slice my array by 8 numbers , 7 of which are given to the input and the last one is used as a label ( the true answer ) . i train my simple dnn using all these sliced numbers and it will learn some sequences . finally , i apply my nn to a test set , and if it predicts the next number with an accuracy of more than 50 % , that proves my assumption , and if it does n't that is also good for me because it says my circuits are good . would it work ?",25540,9947,2019-05-10T05:12:31.787,2019-05-10T05:12:31.787,would this nn for my chip outputs work ?,neural-networks deep-learning classification,0,7,
3611,12234,1,12235,2019-05-10T04:41:27.073,2,24,"i have seen in several jupyter notebooks people initializing the nn weights using : np.random.randn(d , m ) / np.sqrt(d ) other times they just do : np.random.randn(d , m ) what is the advantage of dividing the gaussian distribution by the squared root of the number of neurons in the layer ? thanks",21688,,,2019-05-10T04:53:38.247,why is it so common to initialize weights with a guassian distribution divided by the square root of number of neurons in a layer ?,neural-networks,1,0,
3612,12236,1,,2019-05-10T07:06:40.253,2,35,"i have a large data set that does n't fit in memory and would have to use something like keras 's model.fit_generator if i would like to train the model on all of the available data . the problem is that my data load time is larger than a single epoch and i would hate to incur that data load cost for each epoch . the alternative approach that yields some value is to load as much data as possible , train the model for a few hundred epochs , then load the next portion of the data and reiterate for the same amount of epochs . and repeat this until all my data is "" seen "" by the model . intuitively i understand that this is sub - optimal as the model will tend to optimize for the latest portion of the data and "" forget "" the previous data but i would like a more in - depth explanation of the downsides of that method and if there are any ways to overcome them .",25542,25542,2019-05-11T06:42:35.433,2019-05-11T06:42:35.433,difference between retraining on different portions of data and training initially on larger data set,training datasets keras,0,8,
3613,12237,1,,2019-05-10T08:34:47.450,-1,30,"i 'm using yolo in ubuntu now . but it is the first time that i 'm participating in an ai project . since the performance of my laptop is not good , i want to train image files on my pc and move back the deep learning dataset to my labtop . i 've never used ubuntu before and i have no idea how to do this .",25543,16229,2019-05-29T17:56:00.133,2019-05-29T17:56:00.133,how can i move a deep learning dataset trained on my desktop to a labtop ?,deep-learning,0,1,
3614,12239,1,,2019-05-10T12:15:31.887,0,34,"the universal approximation theorem states that a feed - forward neural network with a single hidden layer containing a finite number of neurons can approximate a wide variety of interesting ( continuous ) functions ( provided a few assumptions on the activation function are met ) . is there any other machine learning model ( apart from any neural network model ) that has been proved to be an universal function approximator ( and that is potentially comparable to neural networks , in terms of usefulness and applicability ) ?",2444,2444,2019-05-10T13:55:14.367,2019-05-10T13:55:14.367,which machine learning models are universal function approximators ?,neural-networks machine-learning math reference-request function-approximation,1,3,
3615,12240,1,,2019-05-10T13:22:04.883,0,9,"i am reading the paper convolutional sequence to sequence learning by facebook ai researchers and having trouble to understand how the dimensions of convolutional filters work here . please take a look at the relevant part of the paper below . let 's say the input to the kernel x is k*d ( say k=5 words of d=300 embedding dimenisonality ) . therefore the input is 5 * 300 . in a computer vision task a kernel would slide over parts of the image , in nlp you usually see kernel taking up the whole width of the input matrix . so i would expect kernel to be m*d ( e.g. 3 * 300 - slide over 3 words and look at their whole embeddiings ) . however , the kernel here is of dimensionality 2d x kd which in our hypothetical example would be 600 * 1500 . i do n't understand how this massive kernel would slide over an input that is by far lower dimensional ( 5 * 300 ) . in computer vision you could zero - pad the input , but here zero - padding would basically turn the input matrix into mostly zeros with only a handful of meaningful numbers . thanks for shedding some light on it !",21278,,,2019-05-10T14:00:36.113,convolutional sequence to sequence learning kernel parameters,convolutional-neural-networks natural-language-processing machine-translation,1,0,
3616,12243,1,,2019-05-10T16:06:26.733,0,39,"i 'm currently looking into the context of adding an lstm to my ppo pytorch implementation . my plan is to add one lstm layer right after the last convolutional layer . i 'm wondering now whether it is sufficient to implement the model and change the way experiences are sampled for updating the policy . so the experiences would be separated into sequences and these sequences would be randomly sampled to update the policy . is there more to implementing an lstm in this context ? did i miss something ? this is the model that i came up with import numpy as np import torch from torch import nn from torch.distributions import categorical from torch.nn import functional as f class model(nn.module ) : def _ _ init__(self ) : super().__init _ _ ( ) # 1st convolutional layer # input : 5x84x84 # output : 32x20x20 self.conv1 = nn.conv2d(in_channels=5 , out_channels=32 , kernel_size=8 , stride=4 , padding=0 ) nn.init.orthogonal_(self.conv1.weight , np.sqrt(2 ) ) # 2nd convolutional layer # input : 32x20x20 # output : 64x9x9 self.conv2 = nn.conv2d(in_channels=32 , out_channels=64 , kernel_size=4 , stride=2 , padding=0 ) nn.init.orthogonal_(self.conv2.weight , np.sqrt(2 ) ) # lstm layer hidden size = 512 self.lstm = nn.lstmcell((9 * 9 * 64 ) + 2 , 512 ) self.lstm.bias_ih.data.fill_(0 ) self.lstm.bias_hh.data.fill_(0 ) # policy layer self.pi_logits = nn.linear(in_features=512 , out_features=8 ) nn.init.orthogonal_(self.pi_logits.weight , np.sqrt(0.01 ) ) # value function layer self.value = nn.linear(in_features=512 , out_features=1 ) nn.init.orthogonal_(self.value.weight , 1 ) def forward(self , vis_obs : np.ndarray , vec_obs : np.ndarray , hx , cx ) : x : torch.tensor # process observation through the convolutional layers x = f.relu(self.conv1(vis_obs ) ) x = f.relu(self.conv2(x ) ) # flatten the output of the convolutional layers x = x.view(-1 , 9 * 9 * 64 ) # add vector observation to flattened layer x = torch.cat((x , vec_obs ) , 1 ) # feed the hidden layer hx , cx = self.lstm(x , ( hx , cx ) ) x = hx # policy and value output layers pi = categorical(logits = self.pi_logits(x ) ) value = self.value(x).reshape(-1 ) return pi , value , hx , cx this is what i 'm currently trying : add hx and cx to the experiences that are sampled by the agents initialize hx and cx with zeros on episode done , set hx and cx back to zeros for updating the policy : train sampled experiences in shuffeled mini - batches",22467,22467,2019-05-13T08:55:37.147,2019-05-13T08:55:37.147,details of implementing an lstm in reinforcement learning,reinforcement-learning lstm,0,0,
3617,12244,1,,2019-05-10T17:58:28.847,0,16,"i searched through the internet but could n't find a reliable article that answers this question . can we use autoencoders for unsupervised cnn feature learning of unlabeled images like the below and use the encoder part of the auto - encoder for transfer learning of few labeled images from the dataset ? as shown below . i believe this will reduce the labeling work and increase the accuracy of a model . however , i have concerns like the more cost in computing , failing to learn all required features , etc .. please let me know if any employed this method in large scale learning such as image - net . ps : pardon if it is trivial or vague as i am new to the field of ai and computer vision .",25366,,,2019-05-10T17:58:28.847,can we use autoencoders for unsupervised cnn feature learning ?,convolutional-neural-networks computer-vision autoencoders feature-extraction,0,2,
3618,12245,1,,2019-05-10T19:30:49.353,1,17,"i am currently working on learning the features provided by a pre - trained network for image retrieval . currently i take the features provided by the pre - trained network , use global max pooling to essentially provide me with a vector and then use fully connected layers to learn the feature vector . this has provided good results , although prone to over - fitting , particularly without dropout . is it possible / would it be beneficial to use a 1d convolutional layer instead of the fully connected layers to learn the features ? bearing in mind this is essentially still image data that has just been transformed . model.add(globalaveragepooling2d(input_shape=input_shape ) ) model.add(dense(256 , activation=""relu "" ) ) model.add(dropout(0.2 ) ) model.add(dense(256 , activation=""relu "" ) ) i 'm not sure how to try this practically in keras as 1d convolutional layers only seem to accept a 3 dimensional input tensor . any suggestions welcome !",18577,,,2019-05-10T19:30:49.353,learning features from a pre - trained network,convolutional-neural-networks feature-extraction,0,0,
3619,12246,1,,2019-05-10T19:54:24.343,1,103,"besides all the fashion about machine learning , data analysis and reinforcement learning , what is going on in the expert systems field and symbolic ai ? there are plenty of domains where machine learning ca n't be used for various reasons ( absence of data , highly critical missions , need of reproducibility ... ) . how to deal with learning in systems in which operators have today a preponderant role and where their knowledge is considered as not questionable face to knowledge acquired by any learning system ? in such a domain , traditional embedded systems designs are still in use , with real - time embedded c , sometimes assembly and vhdl ( which is considered as recent technology which need to proove itself ) , on quite old fully qualified soc . combining this technologies to all previously cited constraints ( data , safety , operator knowledge ) and we obtain the perfect candidate for expert systems , knowledge based systems and rule - based / inference engines . where data is human - readable / revisable without launching new learning & amp ; validation operations . on the other hand ml systems are considered as good candidates for specific tasks as signal processing , shape recognition , ... when data are ( sometimes ) plenty . the more it goes , the more we see deep learning algorithms trying to solve decisions problems by learning from low - fidelity simulations and one billion attempts . the level of expectations of ml systems seems to me hard to council in highly constrained industrial applications where learn new use - case might be sometimes impossible ( e.g. product delivered to client and kept confidential ) . more precisely , how can you build intelligent systems in such industrial domain without being considered as an outdated engineer proposing good - old - fashion - ai instead of following the ml / data - oriented trend ? expert systems and knowledge based systems seem to me sometimes to be the only solution for the cited specific context . what about today applications where critical decisions are taken ? which kind of inference engines are used ( banks , automotive , aeronautics ) ? do we call them expert systems ? and from your point of view , is gofai still have a progression margin to make today 's systems smarter ?",25555,25555,2019-05-11T15:05:05.600,2019-05-15T02:31:05.323,are expert systems dead ?,machine-learning expert-system,3,4,
3620,12247,1,12249,2019-05-10T23:43:24.807,0,35,"can you describe this system in more detail ? i understand that the environment sends a signal indicating whether or not the action taken by the agent was ' good ' or not , but it seems too simple . basically , can you detail the nitty - gritty workings of this system ? i dunno , i may just be overthinking things . sorry if this question was too broad .",25560,,,2019-05-11T00:45:40.447,what is the reward system of reinforcement learning ?,reinforcement-learning rewards,1,0,
3621,12250,1,,2019-05-11T05:42:13.527,1,51,"the demand is to locate the invoice within a camera captured image about that invoice . the invoice is always a white paper with printed black or blue characters , tables and red stamps . sometimes the background behind the invoice is dark(about 60 % of all the samples ) , but others are not . sometimes there is shadow on it . the question is how to detect the vertices , edges and corners of the invoice in this image ? what algorithms should be applied ? the android application camscanner seems to have this function but not is effective every time . so does the cidetector interface on ios . what algorithms does camscanner or ios use(traditional or deep ) ?",14948,14948,2019-05-14T02:04:15.950,2019-05-14T02:04:15.950,how to locate the invoice within a camera captured image ?,image-recognition,0,5,1
3622,12251,1,12253,2019-05-11T06:38:06.143,2,46,"i came across this formula in sutton and barto : rl an intro ( 2nd edition ) equation number 4.7 ( page number 78 ) . if and are deterministic policies and $ q_\pi(s , \pi'(s ) ) \geq v_\pi(s)$ then the policy is as good or better than . note on convention : as per the convention of the book goes , i think they are using rewards for state to action to state transition sequence rather than state to action transition . my questions are : why are they comparing state value function to action value function ? is n't it obvious the above equation might hold true ( provided we select the best action among the possible actions ) even for the policy since then the equation will change to $ q_\pi(s , \pi(s ) ) \geq v_\pi(s)$ and we know $ v_\pi(s ) = \sum_{a \in \mathcal{a}(s ) } \pi(a|s)q_\pi(s , a)$ ? what is the inconsistency here ?",9947,2444,2019-05-11T10:58:02.860,2019-05-11T10:58:02.860,possible inconsistency in the policy improvement equation,reinforcement-learning rl-an-introduction policy,1,2,
3623,12252,1,,2019-05-11T08:21:42.707,0,10,"i have a network which has a input size of ( 28x28x1 ) and since i 'm using ( 3x3 convolution ) so the receptive field is ( 3x3 ) . before going further i will show the code snippet from keras.layers import activation , maxpooling2d model = sequential ( ) # adding sequential layer model.add(convolution2d(32 , 3 , 3 , activation='relu ' , input_shape=(28,28,1 ) ) ) # receptive field = ( 3x3 ) input channel dimension = ( 28x28x1 ) model.add(convolution2d(64 , 3 , 3 , activation='relu ' ) ) # receptive field = ( 5x5 ) input channel dimension = ( 26x26x32 ) model.add(convolution2d(128 , 3 , 3 , activation='relu ' ) ) # receptive field = ( 7x7 ) input channel dimension = ( 24x24x64 ) model.add(maxpooling2d(pool_size=(2 , 2 ) ) ) # receptive field = ( 7x7 ) input channel dimension = ( 22x22x128 ) model.add(convolution2d(256 , 3 , 3 , activation='relu ' ) ) # receptive field = ( 9x9 ) input channel dimension = ( 11x11x128 ) model.add(convolution2d(512 , 3 , 3 , activation='relu ' ) ) # receptive field = ( 11x11 ) input channel dimension = ( 9x9x256 ) model.add(convolution2d(1024 , 3 , 3 , activation='relu ' ) ) # receptive field = ( 13x13 ) input channel dimension = ( 7x7x512 ) model.add(convolution2d(2048 , 3 , 3 , activation='relu ' ) ) # receptive field = ( 15x15 ) input channel dimension = ( 5x5x1024 ) model.add(convolution2d(10 , 3 , 3 , activation='relu ' ) ) # receptive field = ( 17x17 ) input channel dimension = ( 3x3x2048 ) model.add(flatten ( ) ) # receptive field = ( 17x17 ) input channel dimension = ( 1x1x10 ) model.add(activation('softmax ' ) ) # adding activation layer model.summary ( ) # will give the whole summary of the network in the network before the flattening of the network the receptive field = ( 17x17 ) and the input channel dimension= ( 1x1x10 ) . my question is will the network perform worse when the receptive field becomes larger than the input image(input channel dimension ) ?",25563,,,2019-05-11T08:21:42.707,bigger receptive field,convolutional-neural-networks computer-vision,0,0,1
3624,12255,1,,2019-05-11T11:11:43.090,5,63,many examples work with a table based method for q - llearning . this may be suitable for discrete state(observation ) or actions like a robot in a grid world but is there a way to use q - learning for continous spaces like the control of a pendulum ?,19413,,,2019-05-11T13:17:59.623,is q - learning suitable for continous ( state or action ) spaces ?,reinforcement-learning q-learning,2,0,1
3625,12262,1,,2019-05-11T17:43:44.460,0,28,"from what i 've seen , neural networks take a set of atomic inputs . i want an input to be a variable array , i.e. a group of people ( with unique ids ) . if i did n't care about their i d , i could simply feed a count of people as an input - but i do . my thought are to have some form of forward - feeding , and iterate through each person , using their i d as an input to a node representing "" insert entered room "" does this make sense ? the same i d would be used in different nodes at different points . i.e to a "" user left room "" or "" user sat down "" also , how would one model two users interacting , atomically for inputs ? i 'm guess use 2 inputs representing "" interacter a "" and "" interacter b """,25574,,,2019-05-11T17:43:44.460,how would you feed a neural network a variable sized array as an input ?,neural-networks convolutional-neural-networks,0,7,
3626,12263,1,,2019-05-11T19:48:00.137,2,29,"i am wondering how am i supposed to train a model using actor / critic algorithms in environments with opponents . i tried the followings ( using a3c and ddpg ) : play against random player . i had rather good results , but not as good as expected since most interesting states can not be reached with a random opponent . play against list of specific ais . results were excellent against those ais , but very bad with never seen opponents play against itself . seemed the best to me , but i could not get any convergence due to non - stationary environment . any thought or advice about this would be very welcome .",23818,,,2019-05-16T18:41:42.183,training actor - critic algorithms in games with opponents,neural-networks deep-learning reinforcement-learning ddpg,1,2,
3627,12264,1,,2019-05-12T00:15:10.860,3,62,"i looked for existing posts on stack exchange , which kind of answer the questions about the reward system and reward function , but not specifically what i want to ask here , which is how do you actually decide what reward value to give for each action in a given state for an environment ? is this purely experimental and down to the programmer of the environment . so it 's a heuristic approach of simply trying different reward values and see how the learning process shapes up ? of course i understand that the reward values have to make sense , and not just put completely random values , i.e. if the agent makes mistakes then deduct points ... etc . so am i right in saying it 's just about trying different reward values for actions encoded in the environment and see how it affects the learning ?",25360,25360,2019-05-14T02:55:48.447,2019-05-15T08:24:48.120,deciding on a reward per each action in a given state ( q - learning ),reinforcement-learning q-learning dqn,2,1,
3628,12265,1,,2019-05-12T00:19:31.450,-3,31,"i have created a simple feed forward neural network library in java - and i need a benchmark to compare and troubleshoot my library . computer specs : amd ryzen 7 2700x eight - core processor ram 16.0 gb windows 10 os note that i am not using a gpu . please list the following specs : computer specs ? gpu or cpu ( cpu is proffered but gpu is good info ) number of inputs 784 ( this is fixed ) for each layer : how many nodes ? what activation function ? output layer : how many nodes ? ( 10 if classification or 1 as regression ) what activation function ? what loss function ? what gradient descent algorithm ( i.e. : vanilla ) what batch size ? how many epochs ? ( not iterations ) and finally , what is the training time and accuracy ? thank you so much edit just to give an idea of what i am dealing with . i created a network with 784 input nodes 784 in hidden layer 0 256 in hidden layer 1 128 in hidden layer 2 1 output nodes mini - batch size 5 16 threads for backprop and it has been training for ~8 hours and has only completed 694 iterations - that is not even 20 % of one epoch . how is this done in minutes as i 've seen some claims ?",12498,12498,2019-05-12T23:49:48.747,2019-05-12T23:49:48.747,how long does it take to train mnist data set with a simple neural network,neural-networks machine-learning deep-learning datasets,0,3,
3629,12266,1,,2019-05-12T00:23:20.273,1,31,"what 's the differences between semi - supervised learning and self - supervised visual representation learning , and how they are connected ?",12975,12975,2019-05-12T07:32:55.153,2019-05-12T11:27:43.500,what is the relation between semi - supervised and self - supervised visual representation learning ?,machine-learning difference supervised-learning self-supervised-learning semi-supervised-learning,1,0,1
3630,12268,1,,2019-05-12T02:41:42.447,1,32,"i have the following code ( below ) , where an agent uses q - learning ( rl ) to play a simple game . what appears to be questionable for me in that code is the fixed learning rate . when it 's set low , it 's always favouring the old q - value over the learnt / new q - value ( which is the case in this code example ) and vice - versa when it 's set high . my thinking was should n't the learning rate be dynamic , i.e. it should start high because at the beginning we do n't have any values in the q - table and the agent is simply choosing the best actions it encounters . so we should be favouring the new q - values over the existing ones ( in the q - table , which there 's no values , just zeros at the start ) . over time ( say every n number of episodes ) , ideally we decrease the learning rate to reflect that overtime the values in the q - table are getting more and more accurate ( with the help of the bellman equation to update the values in the q - table ) . so lowering the learning rate will start to favour the existing value in the q - table over the new ones . i 'm not sure if my logic has gaps and flaws , but i 'm putting it out there in the community to get feedback from experienced / experts opinions . just to make things easier , the line to refer to , in the code below ( for updating the q - value using the learning rate ) is under the comment : # update q - table for q(s , a ) with learning rate import numpy as np import gym import random import time from ipython.display import clear_output env = gym.make(""frozenlake-v0 "" ) action_space_size = env.action_space.n state_space_size = env.observation_space.n q_table = np.zeros((state_space_size , action_space_size ) ) num_episodes = 10000 max_steps_per_episode = 100 learning_rate = 0.1 discount_rate = 0.99 exploration_rate = 1 max_exploration_rate = 1 min_exploration_rate = 0.01 exploration_decay_rate = 0.001 rewards_all_episodes = [ ] for episode in range(num_episodes ) : # initialize new episode params state = env.reset ( ) done = false rewards_current_episode = 0 for step in range(max_steps_per_episode ) : # exploration - exploitation trade - off exploration_rate_threshold = random.uniform(0 , 1 ) if exploration_rate_threshold & gt ; exploration_rate : action = np.argmax(q_table[state , : ] ) else : action = env.action_space.sample ( ) new_state , reward , done , info = env.step(action ) # update q - table for q(s , a ) with learning rate q_table[state , action ] = q_table[state , action ] * ( 1 - learning_rate ) + \ learning_rate * ( reward + discount_rate * np.max(q_table[new_state , : ] ) ) state = new_state rewards_current_episode + = reward if done = = true : break # exploration rate decay exploration_rate = min_exploration_rate + \ ( max_exploration_rate - min_exploration_rate ) * np.exp(-exploration_decay_rate*episode ) rewards_all_episodes.append(rewards_current_episode ) # calculate and print the average rewards per thousand episodes rewards_per_thousands_episodes = np.array_split(np.array(rewards_all_episodes ) , num_episodes/1000 ) count = 1000 print ( "" * * * * * * * average reward per thousands episodes * * * * * * * * * * * * "" ) for r in rewards_per_thousands_episodes : print(count , "" : "" , str(sum(r/1000 ) ) ) count + = 1000 # print updated q - table print(""\n\n * * * * * * * * * q - table * * * * * * * * * * * * * \n "" ) print(q_table )",25360,,,2019-05-12T08:42:44.197,static or dynamic learning rate ( q - learning ),reinforcement-learning python q-learning,1,1,
3631,12270,1,12272,2019-05-12T11:15:54.263,3,89,"consider the gridworld problem in rl . formally , policy in rl is defined as . if we are solving gridworld by policy iteration then the following pseudocode is used : now the question is in the policy improvement step i can have 2 interpretations of the second step in the for loop : in this step we check which action ( say going right for a particular state ) has the highest reward and assign going right a probability of 1 and rest actions a probability of 0 . thus in pe step we will always go right for all iterations for that state even if it might not be the most rewarding function after certain iterations . we keep the policy improvement step in mind and while doing the pe step we update sthe $ v(s)$ based on the action giving highest reward ( say for 1st iteration $ k=0 $ going right gives highest reward we update based on that while for $ k=1 $ we see going left gives highest reward and update our value based on that likewise . thus action changes depending on maximum reward . for me the second interpretation is very similar to value iteration . so which one is the correct interpretation of a policy iteration ?",9947,9947,2019-05-12T15:42:54.547,2019-05-12T16:19:28.950,what is policy iteration in rl ?,reinforcement-learning policy,1,13,1
3632,12271,1,,2019-05-12T13:54:47.517,3,82,"i was trying to figure out how to create a solver to the puzzle of putting 11 pieces in a board ( 8 x 8) . i created the game in http://www.xams.com.br/quebra . it is possible to turn the piece 90 degrees each time counterclockwise ( girar ) and mirror it vertically ( inverter ) , and so the piece can assume 8 forms . when clicking in solver button ( resolver ) , it tries to put pieces randomly in board in brutal force method ( it spends a lot of time ) . using this method , i was not able to achieve the result . i would like to try something smarter than this and having a machine learning algorithm for this would be great . i don´t know how to formulate the problem . how would you start this please ? edit : i am still building the page and there is so much to improve . you need to click the center of 3x3 matrix where you want to put the piece . edit2 :",16837,2444,2019-05-12T23:44:37.587,2019-05-20T00:18:34.183,how do i solve the problem of positioning 11 pieces into a 8x8 puzzle ?,machine-learning ai-design game-ai combinatorial-games,1,4,
3633,12273,1,,2019-05-12T18:42:16.040,1,31,"i 'm building a web application that collects schema.org data from different webshops as amazon , shopify , etc . it collects data every 6h and shows the current and lowest price . it is used for monitoring products and buying at the lowest price . my goal is to recognize products from different shops as the same product . every shop has its own title for the same product . example : google pixel 2 64 gb clearly white ( unlocked ) smartphone google pixel 2 gsm / cdma google unlocked ( clearly white , 64 gb , us warranty ) problems : do n't have a lot of data ( only products chosen by the user ) needs to support every new product that app does n't have data history",25589,16565,2019-05-13T08:37:34.733,2019-05-13T08:37:34.733,string matching algorithm for product recognition,machine-learning datasets similarity,0,3,
3634,12274,1,,2019-05-12T18:50:32.373,0,40,"in reinforcement learning , there are the concepts of stochastic ( or probabilistic ) and deterministic policies . what is the difference between them ?",2444,2444,2019-05-12T18:59:23.210,2019-05-12T19:49:58.913,what is the difference between a stochastic and a deterministic policy ?,reinforcement-learning difference policy deterministic-policy stochastic-policy,1,1,
3635,12276,1,,2019-05-12T19:02:49.683,2,18,"the q - learning does not guarantee convergence for continuous state space problems ( why doesn&#39;t q - learning converge when using function approximation ? ) . in that case , is there an algorithm which can guarantee convergence ? i am looking at model - based rl specifically ilqr but all the solutions i find are for the continuous action space problem .",19541,2444,2019-05-12T19:07:42.540,2019-05-12T19:07:42.540,are there reinforcement learning algorithms that ensure convergence for continuous state space problems ?,reinforcement-learning convergence,0,0,
3636,12277,1,,2019-05-12T19:13:15.043,0,30,"i m trying to make a dark image brighter using cnn - unet arcitecture . when i train the network i get the following results : when i cut the features in half for pruning , and do full train again , i get the following resuls : there are those bubble artifacts near the light . why they are happening ? 1)how can i locate the problametic neurons that cause it ? 2)can i train the network(change the loss or anything ) to not generate those made up bubbles ?",25141,25141,2019-05-14T14:00:54.833,2019-05-14T14:00:54.833,artifacts after pruning unet cnn,deep-learning convolutional-neural-networks tensorflow loss-functions,1,0,
3637,12279,1,,2019-05-13T05:02:36.080,1,28,"i am training an agent to play a simple game using double deep q learning . however , the variance in agent performance is very high , even for agents trained with same model parameters . for example , i can train agent a and agent b using the exact parameters and agent a beats b 800 to 200 . i think i understand why this is happening , when training starts the model is initialized with different weights , and this leads the model to find different local max / min . the above makes it difficult to find optimal parameters . what are the strategies to reduce this variance ? what parameters should i look at tweaking ? more details about the environment : this is a two player game ( zombie dice ) ; however , in my implementation so far the agents are learning to maximize expected score on their turn , so the actions and score of the opponent is ignored . the variance is higher when i am using purely greedy strategy with no exploitation at all . though it exists in both cases . i would say roughly 2/3 wins for stronger side with greedy and 3/5 with exploration out of 1000 matches . the environment is stochastic ; i have not done many assessment runs maybe 20 or 30 , it is mostly eyeballing , but the differences are fairly large ; therefore , i am confident that this is not due to chance . i tested the models against themselves , and i get scores very close to 50/50 . however , two different models trained with same parameters give results very different from 50/50 . i tested this with models trained with different types of parameters and it is generally the same problem .",25248,1847,2019-05-14T06:51:59.043,2019-05-14T06:51:59.043,high variance in performance of q - learning agents trained with same parameters,deep-learning reinforcement-learning q-learning,0,3,
3638,12280,1,,2019-05-13T06:44:42.127,0,31,gpus are known to help with when the input is ' wider ' i.e. images that have hundreds of thousands of parameters . but what if we have a less than 1000 parameter state space input that is very deep - let 's say a 50 layer mlp . do gpus help in this case ? what 's happening under the hood ?,21158,,,2019-05-13T06:44:42.127,do gpus help with ' depth ' of a network ?,neural-networks gpu,0,1,
3639,12283,1,,2019-05-13T13:43:31.060,1,65,"i am designing a neural network using deep q - learning , which teaches an agent how to play snake ( the classic nokia game from the 90'ies ) . the goal of the game is to navigate the snake on a playing field ( 2d ) , and to eat a randomly placed fruit . as the snake eats the fruit , it grows in length . the game ends if the snake hits the game border , or it self , so as the number of fruits consumed increases , so does the difficulty of navigating without hitting something . i have trained the snake game on a 10x10 playing field using the following inputs : x direction of the snake y direction of the snake the fraction of playing field occupied by the snake itself a bool which says if the fruit is in front of the snake ( from it 's current direction ) a bool which says if the fruit is to the left of the snake ( from it 's current direction ) a bool which says if collision ( game over ) in front of the snake is possible a bool which says if collision to the right is possible a bool which says if collision to the left is possible with this choice of inputs i am getting the snake agent to work reasonably well , and score until it plateaus out . i have examined the cases where the snake dies , and it all happens when it has no way of escaping , for example , it turns around and blocks its own path , until it finally has no where to go . this is more likely to happen as the snake increases in lenght . i was thinking on how i could improve this performance . it seems to me , that the reason the snake can make a self - blocking turn is because of the inputs . since the path it takes is clear , there 's no information , that the next path is not , or that continuing further down will eventually lead to game over . if the snake agent was aware of all obstacles in each step , i.e. the entire state - space , then i could imagine that would help train the network towards finding the optimal path without ending up blocking itself . since i have made the snake game myself , i can return to the agent a matrix , or an unfolded vector , which contains the inputs for each column / row on the playing field . a blocked cell would be set to 0 , a free cell 1 , the fruit cell has value 0.75 and the snake head ( which moves ) could be assigned value 0.25 . after trying this approach , i have to say i was unsuccesful . the snake ends up just turning in circles , even if i use the same reward system as the 8 input case shown above . i am therefore trying to understand what is happening here . am i missing information ? i would think the full 10x10 state space would give me exactly enough information to lead to the correct evaluation of the next path . i would very much like to hear someone elses input to this approach . thanks a lot",25606,,,2019-05-14T07:47:22.513,choice of inputs features for snake game,neural-networks python q-learning,1,0,
3640,12284,1,,2019-05-13T16:12:59.710,2,51,"monte carlo ( mc ) methods are methods that use some form of randomness or sampling . for example , we can use an mc method to approximate the area of a circle inside a square : we generate random 2d points inside the square and count the number of points inside and outside the circle . in reinforcement learning , an mc method is a method that "" samples "" experience ( in the form of "" returns "" ) from the environment , in order to approximate e.g. a value function . a temporal - difference algorithm , like $ q$ -learning , also performs some form of sampling : it chooses an action using a stochastic policy ( e.g. -greedy ) and observes a reward and the next state . so , could n't $ q$ -learning also be considered a mc method ? can an mc method be model - based ?",2444,2444,2019-05-13T16:20:35.523,2019-05-13T18:10:49.223,what is the relation between monte carlo and model - free algorithms ?,reinforcement-learning monte-carlo temporal-difference relation model-free,1,0,1
3641,12285,1,,2019-05-13T16:50:39.160,0,26,"i know case - based reasoning has four stages : retrieve , retain , re - use and revise . used for solving new problems by adapting solutions that were used to solve old problems , like car issues . the advantages of it are that solutions are proposed quickly and there 's no need to start from scratch , but is there a real - world problem where using this would not be suitable ? just trying to better my understanding of it .",25609,2444,2019-05-13T17:41:08.847,2019-05-13T17:41:08.847,are there real - world problems where case - based reasoning is not suitable ?,reinforcement-learning,0,6,
3642,12286,1,,2019-05-13T17:17:53.707,1,21,"consider a problem where we have a finite number of states and actions . given a state and an action , we can easily know the reward and the next state ( deterministic ) . the state space is really large and has no final state . there was a paper that for a problem of this type used td(0 ) by filling the value table and chose its actions by : $ $ i 've read somewhere that is ok to use prediction algorithms when the model is well - described with the objective of choosing best actions and not only evaluating the policy . what is the purpose , advantages and disadvantages of using td prediction here instead of a td control algorithms ( and just saving the $ q(s , a)$ table ) ? if it was about space , you still have to store a table with all the rewards for each state - action pair , right ? i 'm not sure if i was able to explain myself very well as i was trying to keep it short , if some clarification is needed please tell me .",24054,2444,2019-05-13T17:38:29.977,2019-05-13T17:38:29.977,why should we use td prediction as opposed to td control algorithms ?,reinforcement-learning temporal-difference,0,2,
3643,12289,1,,2019-05-13T20:13:47.060,2,59,"our customer runs a tour agency . he has an excel spreadsheet containing the following information for people that have contacted them : customer name , country , tour duration ( requested by customer ) , tour date , number of people in the tour ( usually from 1 to 3 ) , price given to the customer , answer : accepted / rejected ( indicates if customer accepted or rejected the price given by the tour agency ) . my customer wants a predictor or tool that can let him enter the details given by future customers , e.g : number of participants , tour duration , country ( not sure if necessary ? ) and the system will return the best price to charge the customer ( so he wo n't reject the proposal but pay the maximum possible ) . another option would be that the tour agency owner will enter the price and the system will answer "" customer will accept that price "" or "" customer will reject that price "" . is this even possible ? i think it may be done using neural networks trained with the previous answers from customers that the tour agency owner has in his excel spreadsheet ?",25614,,,2019-05-14T13:35:41.070,predict best price using neural network ?,neural-networks deep-learning prediction,3,0,
3644,12294,1,,2019-05-14T02:56:59.147,0,25,"i 'm looking for some general advice here before i dive in . i 'm interested in creating a new environment for openai gym to provide some slightly more challenging continuous control problems than the ones currently in their set of classic control environments . the intended users of this new environment are just me and members of a meetup group i am in but obviously i want everyone to be able to access , install and potentially modify the same environment . what 's the easiest way to do this ? can i simply import and sub - class the openai gym.env module similar to the way cartpole.py does . or do i need to create a whole new pip package as this article suggests ? also , before i invest a lot of time on this , has anyone already created a cart - pole system environment where the goal is to stabilize the full state ( not just the vertical position of the pendulum ) ? ( i tried googling and could n't find any variants on the original cart - pole - v1 but i 'm suspicious as i ca n't be the first person to make modified versions of some of these classic control environments ) . thanks , ( i realize this question is a bit open - ended ) but hoping for some good advice that will point me in the right direction .",25618,25618,2019-05-14T03:02:50.083,2019-05-14T16:37:21.317,advice on creating a new environment using openai gym,python open-ai environment,1,0,
3645,12295,1,,2019-05-14T06:16:39.420,1,14,"i am trying to estimate the real world distance ( in metres ) between two points in a perspective image using an uncalibrated camera . however , the dimensions of an object in the image are known . i thought of using pixel per metre , but , being a perspective image , that did not seem like a viable approach . i believe i need the camera matrix from the image ( maybe using the known object ) and compute the 3d coordinates of the point , then simply compute the euclidean distance between the points . if that is the right approach , how may i do it ? if there are any alternatives to this approach , kindly let me know .",25623,2444,2019-05-14T18:23:35.450,2019-05-14T18:23:35.450,estimate distance between points in perspective image,machine-learning computer-vision,0,0,1
3646,12301,1,,2019-05-14T11:42:22.680,0,13,"i am trying to create a dialogue policy model on dstc data . this model takes in a state of the conversation and outputs an act the machine must take . i am creating this model using reinforcement learning . i have used this dqn agent and a user simulator to train the model with few modifications from the original code . i am manually giving the rewards for each turn through long credit assignment with -1 at each turn and at end , ( 10 - turns ) for success and -2*(max_turns ) for failure . max_turns is kept at 20 turns for each conversation . i have already trained over 1000 conversations yet the model does not seem to learn a bit . i wanted to know what common problems might be causing it to fail at all conversations . am i training it wrong ? is my reward system incorrect ? or are 1000 conversations just not enough ? any help , tips that leads me in correct direction will be appreciated .",19244,,,2019-05-14T11:42:22.680,problems while training a dqn agent on dstc dataset,reinforcement-learning q-learning chat-bots rewards on-policy,0,0,
3647,12302,1,,2019-05-14T12:27:46.610,0,32,"progressive software development includes agile methodologies , such as scrum , test driven design , user stories , lean development , and continuous integration . business driven software development and business driven it management are being pushed as the answer to business issues with it departments . because of the pressures for reducing time to market and the increasing need for continuous delivery , quality control often suffers . one of the goals of ai has been to automate the coding of software , yet the use of current ai in software development seems somewhat stalled . most companies have a plethora of processes that , if made to produce a structured audit trail , would make for great training data . although ai techniques are used in the software developed and some of the ide products use ai internally , the writing of code is still manual and , having seen much of the code written in many corporations , generally poorly written in many ways . variable names are not systematically chosen or too abbreviated to be clear . design is often either not easily extensible , over - complicated , or unlike common design patterns and lacking comments to illuminate its approach . unit testing is often nonexistent , below 25 % in code coverage , or shut off because there are many failures and a lack of commitment to fix them . documentation is poor or nonexistent and the diagrams that may exist are difficult to read , when informative graphically messy , and when not messy , not informative . there 's no particular reason why development processes and methodologies could n't benefit from ai technology . in the resulting software , benefits are achieved using drools to execute rules in scala and java and genetic algorithms have been used to generate hypotheses to test as rules . recurrent networks have been used to prioritize items in workflow queues , so lstm and gru networks . how can these technologies built into the software systems that humans write and the tools they use to write them be used to assist with the design , implementation , and testing of the code . what avenues toward the automation of typical it department software development have opened because of the last thirty years of ai development ? what specifically is the low hanging fruit toward which research investments should be made ? response to comments one of the challenges with having an ai forum as a sibling to stack overflow is the dissimilarity of the nature of the two topics . ai is as boundless as the concept of intelligence , except that it must be somewhere applied to human inventions , such as computers , circuits , or robots . furthermore it often requires a few paragraphs of background to shift the mode of thinking to a new one necessary to understand a post . there is often no way to remove something one sees as irrelevant or ethereal without abandoning part of the readership that ca n't shift their thinking without the prologue . ... do you mean the ai technologies ? if yes , you described some of the use cases of ai in software systems . yes . the ones mentioned . ides use context based searches as the developer types to try to guess what the developer is trying to program and assist in finding the exact spelling of classes , methods , functions , and other mnemonics . under the hood there is some history based prioritization leaning toward what google and yahoo does with user and ip profiles and with cookies to better guess ahead what might be typed . software that developers write might use drools . genetic algorithms have been used to add new rules . there may be other ai technologies not mentioned , so as to keep the question brief enough or because i do n't know of them yet . what 's your question then ? it is specified at the end . adding the context to the front and back of the question , we have , "" [ given this condition of software development , ] what specifically is the low hanging fruit toward which research investments should be made [ to use ai to a greater degree to move toward automated software development ] ? "" can you be less metaphorical ? i 'm not sure where a metaphor is clouding the question . perhaps others see the point of the question and will answer it . we 'll see . is your question simply : "" which type of ai applications have been developed in software engineering ? "" that is not at all the question asked . similarly , the question is not about how to use ai in ides . it is how ai can next be used to further the automation of software development itself , not the product of software development . this is not a particularly new question . the question of what is the next step in the automation of software development has been around since before fortran and cobol . it is just a new question to this ai stack exchange and likely an important addition .",4302,4302,2019-05-15T01:40:37.833,2019-05-15T01:40:37.833,ai use in the automation of software development ?,ai-design automation quality-control software-development,0,3,
3648,12303,1,,2019-05-14T14:07:44.433,2,11,"i 'm building cnn network of image to image . after training , i have some bad results in part of the image . i would like to find the neurons that most influenced those pixels and do retraining only for them . i have seen some previous works about visualizing networks , like here : https://github.com/utkuozbulak/pytorch-cnn-visualizations but they are only finding activations maps or visualizing with softmax in the output layer . how can i do that for image to image ?",25144,,,2019-05-14T14:07:44.433,how to debug and find neurons that most influenced a pixel in the output image ?,deep-learning convolutional-neural-networks image-generation,0,0,
3649,12307,1,,2019-05-14T16:28:21.080,2,51,"i understand the way we build a model but all of the online courses i 've found end with this -- i ca n’t find any course explaining the process utilising the model to address the problem . how do i use the model to address the real world problems ? example : i have a model to predict the kb number for the given incident . how do i integrate and use this model ? is there any course out there that explains the whole process from data collection , model building and utilising the model to solve the real world problems ?",25628,1671,2019-05-15T18:35:03.507,2019-05-15T18:35:03.507,next step after building the model in machine learning ?,machine-learning ai-basics getting-started models real-world,1,1,
3650,12310,1,12323,2019-05-14T20:50:07.443,0,18,"i am looking at lstm example here however , i am not sure how to modify the setup if i have forecast available ( assuming perfect forecast ) for temp : temperature and pres : pressure at time t. i.e. pollution_t = fn(temp_t , pres_t , temp_t_1 , pres_t_1 , ... , othervariables and lagged values ) _ t represents the perfect forecast available _ t_1 represents the variable values at previous time steps basically i am looking for something of the following setup :",25634,25634,2019-05-14T21:19:59.223,2019-05-15T15:40:35.550,modifying lstm to include forecast,lstm,1,0,
3651,12311,1,,2019-05-14T22:06:54.437,1,27,"i 've got a lego mindstorms ev3 with ev3dev and ev3-python installed . i wanted to try out artificial intelligence with the robot , and my first project is going to be to get the robot to try and recognize some images ( using convolutions ) and do an action related to the image it has seen . however , i ca n't find a way to use tensorflow ( or any ai module for that matter ) on an ev3 . does anyone know how to incorporate tensorflow or any other modules into the ev3 ? help would be gladly appreciated .",25639,2444,2019-05-14T22:09:58.027,2019-05-15T01:13:05.493,how do i integrate tensorflow with ev3dev ?,python tensorflow robotics,1,0,
3652,12313,1,,2019-05-15T00:27:48.573,3,41,"i am trying to understand why attention models are different than just using neural networks . essentially the optimization of weights or using gates for protecting and controlling cell state ( in recurrent networks ) , should eventually lead to the network focusing on certain parts of the input / source . so what is attention mechanism really adding to the network ? a potential answer in the case of encoder - decoder rnns is : the most important distinguishing feature of this approach from the basic encoder – decoder is that it does not attempt to encode a whole input sentence into a single fixed - length vector . instead , it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation . this frees a neural translation model from having to squash all the information of a source sentence , regardless of its length , into a fixed - length vector . we show this allows a model to cope better with long sentences . - neural machine translation by jointly learning to align and translate which made sense and the paper says that it worked better for nmt . a previous study indicated that breaking down the sentence into phrases could lead to better results : in this paper , we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model . once each segment has been independently translated by the neural machine translation model , the translated clauses are concatenated to form a final translation . empirical results show a significant improvement in translation quality for long sentences . - overcoming the curse of sentence length for neural machine translation using automatic segmentation which paved the way for further research resulting in attention models . i was also going through an article on attention is not quite all you need where it said something similar : an lstm has to learn to sequentially retain past values together in a single internal state across multiple rnn iterations , whereas attention can recall past sequence values at any point in a single forward pass . and a more curated blog on the family of attention mechanism gives insight on how different ways have been formulated for implementing the concept : attention ? attention ! specifically , i want to know how attention mechanism is formulated for this task ( aforementioned ) or in general . a detailed mathematical insight would be helpful , probably somewhat on these lines : understanding attention in nn mathematically",4573,,,2019-05-15T03:44:55.880,a mathematical explanation of attention mechanism,neural-networks machine-learning deep-learning recurrent-neural-networks attention,1,0,
3653,12315,1,,2019-05-15T03:11:44.070,2,87,"dataset description i am working on famous abide autism datasets . the dataset is very big in a sense that it has more than 1000 subjects containing half of them as autisitic and other half as healthy controls.the dataset is taken from 17 sites across the word and each site used a varying time dimension when recording the subjects fmri . my question i want to use this dataset for classification task but only issue is time varying subjects as features set are fixed to 200 so you can say that i have subjects dimensions like 150 x200 , 75 x 200 , 300 x 200 ... so on . so what are advanced ai or deep learning techniques that i can use to fix this time dimension for every subjects or can anybody suggest some deep learning framework or model that i could use to fix these varying time dimensions across subjects ? my effort approach 1 i have applied pca to the time dimension and fixed them to 50 and tried other numbers also but it did not produce good accuracy for classification approach 2 i also tried to use only specific time points from every subject like take only 40 time points from every subject to fix the dimension but again it did not work as definitely filtering some time series data on every subject would loose crucial information .",25642,25642,2019-05-15T03:20:45.860,2019-05-20T12:48:49.527,how to fix time dimension in time varying data - sets using deep learning model for classification ?,deep-learning datasets ai-community,2,0,
3654,12317,1,,2019-05-15T04:44:48.280,0,21,"i 'm a high school senior who is very new to making neural networks . i 've been using the iris flower dataset ( https://www.kaggle.com/arshid/iris-flower-dataset ) to build my neural network . my model gets above 90 % accuracy for both the training and the testing data , however when i made my classifier using the weights and biased terms from the model , the classifier always classifies the data as "" iris - virginica "" . i am not sure what the problem is , and any help would be appreciated . it should be noted that i want to make the neural network myself using feed forward , backpropagation , gradient descent , etc . i do not want to use an existing classifier from a well - known library ( e , g kneighbors from sklearn ) . my code is below . # full code # importing necessary libraries import numpy as np import pandas as pd from sklearn.model_selection import train_test_split df = pd.read_csv(""iris.csv "" ) # loading the data into python df.head()#checking the contents to manipulate # preprocessing the data ( making the features and the targets ) x = np.asmatrix(np.copy(df))[:,:5 ] # getting all the columns of the feature data x = np.delete(x , 0 , axis = 1 ) # dropping column of index 0 because it is "" i d "" . nameoftargets = df.species.unique ( ) # getting the unique values of the target column for one hot encoding y_data = [ ] # empty list that will eventually become target data for i in df.iloc[:,5 ] : for j in range(nameoftargets.shape[0 ] ) : # for j from 0 to n , where n is the number of items in nameoftargets if i = = nameoftargets[j ] : y_data.append(j ) # the index number of the item in nameoftargets is how they will be represented # in the target data . i.e if the value of the target is equal to the first # item of nameoftargets , the value is represented by the item 's index ( 0 ) . n = len(y_data ) # getting the number of items in the dataset y = np.zeros(n*nameoftargets.shape[0]).reshape(n,nameoftargets.shape[0 ] ) # making the target matrix . the number of rows = number of subjects , number of columns = number of unique targets for i in range(n ) : # one hot encoding . after the loop finishes , y will be the final target matrix . t = y_data[i ] y[i , t ] = 1 # standardizing values in the feature matrix x for i in range(x.shape[1 ] ) : x[:,i ] = ( x[:,i].astype(float ) - np.mean(x[:,i].astype(float)))/np.std(x[:,i].astype(float ) ) x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42 ) # splitting the data into testing data and training data # deep learning np.random.seed(1 ) # making sure the weights are the same every time the cell is rerun ( still random ) n , d = x_train.shape # n = num subjects , d = num features m = 100 # num hidden nodes of the first hidden layer k = nameoftargets.shape[0 ] # number of outputs iteration_num = 1000 # number of times gradient descent will be performed a = 0.02 # learning rate # creating the weights w = np.random.randn(d*m).reshape(d,m ) v = np.random.randn(m*k).reshape(m,k ) # creating the biased terms b = np.random.randn(m).reshape(1,m ) b_ones = np.ones(n).reshape(n,1 ) b = np.dot(b_ones,b ) c = np.random.randn(k).reshape(1,k ) c_ones = np.ones(n).reshape(n,1 ) c = np.dot(c_ones , c ) for j in range(iteration_num ) : # back propagation # feed forward z = np.dot(x_train,w ) + b z = 1/(1 + np.exp(-z.astype(float ) ) ) predictions = np.exp(np.dot(z,v ) + c ) # softmax for i in range(predictions.shape[0 ] ) : predictions[i , : ] = predictions[i,:]/np.sum(predictions[i , : ] ) # gradient descent dv = np.dot(z.t,(y_train - predictions ) ) dz = np.dot(np.dot(np.dot((y_train - predictions ) , v.t).t , z),(1-z.t ) ) # m x n matrix dw = np.dot(x_train.t,dz.t ) db = np.dot(np.dot(np.dot((y_train - predictions ) , v.t).t , z ) , ( 1-z.t)).t.sum(axis = 0 ) dc = ( y_train - predictions).sum(axis = 0 ) w + = a*dw.astype(float ) v + = a*dv.astype(float ) b + = a*db.astype(float ) c + = a*dc.astype(float ) if j%100 = = 0 : # every 100 iterations , print out the cost and accuracy total = -np.dot(y_train.t , np.log(predictions ) ) cost = total.sum ( ) accuracy = np.mean(np.round(predictions ) = = y_train ) print(cost , accuracy ) print ( "" "") print(""final cost and accuracy of training data : "") print(cost , accuracy ) # applying the model to the test data . the x_test data must be put through the softmax function and compared to y_test # feed forward z = np.dot(x_test,w ) + b[0 ] z = 1/(1 + np.exp(-z.astype(float ) ) ) test_predictions = np.exp(np.dot(z,v ) + c[0 ] ) # softmax for i in range(test_predictions.shape[0 ] ) : test_predictions[i , : ] = test_predictions[i,:]/np.sum(test_predictions[i , : ] ) test_acc = np.mean(np.round(test_predictions ) = = y_test ) test_total = -np.dot(y_test.t , np.log(test_predictions ) ) test_cost = total.sum ( ) print ( "" "") print(""cost and accuracy of testing data : "") print(test_cost , test_acc ) def classify(slen , swid , plen , pwid ) : x = np.array([slen , swid , plen , pwid]).reshape(1,-1 ) # converting to 2d matrix for calculations z = np.dot(x,w ) + b[0 ] z = 1/(1 + np.exp(-z.astype(float ) ) ) test_predictions = np.exp(np.dot(z,v ) + c[0 ] ) # softmax for i in range(test_predictions.shape[0 ] ) : test_predictions[i , : ] = test_predictions[i,:]/np.sum(test_predictions[i , : ] ) test_predictions = np.round(test_predictions)[0 ] # reshaping back to 1d matrix j = np.where(test_predictions = = 1)[0][0 ] return nameoftargets[j ] print(classify(4.7 , 3.2 , 1.3 , 0.2 )",25643,,,2019-05-15T04:44:48.280,"using the iris flower dataset , why does my classifier classify any data inputted as "" iris - virginica "" ?",neural-networks machine-learning classification python,0,1,
3655,12318,1,,2019-05-15T09:32:24.957,1,66,"i setupped a small drone simulator using physx , the time step is at 200 hz , while motors update like regular escs ( at 50 hz ) . i computed the inertia matrix , tweaked a bit mass of components to be real , air drag , gravity etc . after a first partial success in tuning a pid algorithm i got bored to find and hunt perfect values , and started thinking to tune it with a nn , but then i thinked , why using pid at all if nn can find better solutions ? . i started playing with nn , but then i realized i have no traning data . sure i could do nn.train(input,expectedoutput ) ; but what is actually the expected output ? to be useful it has to be the thrust force of propellers tuned to keep input ( position and orientation ) stable in the desired place . but actually i do n't ( want to ) know in advance the thrust that every single propeller has . since it is a simulation it is ok spending some computing time between each simulation step ( eventually i 'll try to optimize it later to fit it in a microcontroller ) .. so , is it possible given a regular nn implementation where i can select number of : input neurons hidden neurons output neurons find a way to train my model live ? i need a way to tell the nn hei this time you performed x keep going or go the opposite way .",1863,,,2019-05-15T16:26:19.913,"drone training , how to train without training data ?",neural-networks training,2,0,
3656,12320,1,,2019-05-15T11:50:38.113,0,5,"video frames super - resolution with deep learning ? i 've been searching for the whole day and could find no papers\projects tackling that problem . for example : suppose i have a series of n frames of some object , how can i use that information to get a better image quality of that object ? and i 'm not talking about single - image super resolution",25412,,,2019-05-15T11:50:38.113,super resolution of an object in a video using adjacent frames,convolutional-neural-networks computer-vision,0,0,
3657,12321,1,12346,2019-05-15T15:15:16.280,4,63,"ok , due to previous question i was pointed to use reinfrocement learning . so far what i understood from random websites is the following : there is a q(s , a ) function involved i can assume my neural network ~ q(s , a ) my simulation has a state ( n input variables ) my actor can perform m possible actions ( m output variables ) at each step of the simulation my actor perform just the action corresponding to the max(outputs ) ( in my case the actions are 1/2/3 % increase or decrease to propellers thrust force . ) from this website i found that at some point i have to : estimate outputs q[t ] ( or so called q - values ) estimate outputs over next state q[t+1 ] let the backpropagation algorithm perform error correction only on the action performed on next state . the last 3 points are not clear at all to me , infact i do n't have yet the next state what i do instead is : estimate previous outputs q[t-1 ] estimate current outputs q[t ] let backpropagation fix the error for max q value only actually for code i use just this library which is simple enough to allow me understand what happens inside : neuralnetwork library initializing the neural network ( with n input neurons , n+m hidden neurons and m output neurons ) is as simple as network network = new neuralnetwork ( n , n+m , m ) ; then i think to understand there is the need for an arbitrary reward function public double r ( ) { double distance = ( currentposition - targetposition).vectormagnitude ( ) ; if(distance&lt;100 ) return 100-distance ; // the nearest the greatest the reward return -1 ; // too far } then what i do is : // init step var previousinputs = readinputs ( ) ; updateinputs ( ) ; var currentinputs = readinputs ( ) ; //estimate previous outputs q[t-1 ] previousoutputs = network.query ( previousinputs ) ; //estimate current outputs q[t ] currentoutputs = network.query ( currentinputs ) ; // compute modified max value int maxindex = 0 ; double maxvalue = double.minvalue ; selectmax ( currentoutputs , out maxvalue , out maxindex ) ; // apply the modified max value to previous outputs previousoutputs[maxindex ] = r ( ) + discountvalue * currentoutputs[maxindex ] ; //let backpropagation fix the error for max q value only network.train ( previousinputs , previousoutputs ) ; // advance simulation by 1 step and see what happens runphysicssimulationstep(1/200.0 ) ; draweverything ( ) ; but it does n't seem to work very nice . i let simulation running for over one hour without success . probably i 'm reading the algorithm in a wrong way .",1863,1863,2019-05-15T16:54:09.290,2019-05-16T11:03:40.107,"q - learning , am i interpreting correctly $ q(s , a ) = r + \gamma \max_{a ' } q(s',a')$ ?",neural-networks reinforcement-learning q-learning,1,0,2
3658,12322,1,,2019-05-15T15:22:40.597,1,32,i found sentiment analysis and emotion recognition as two different categories on paperswithcode.com . should both be the same as my understanding ? if not what 's the difference ?,25658,2444,2019-05-15T22:20:08.487,2019-05-16T08:01:14.560,what is the difference between sentiment analysis and emotion recognition ?,definitions difference emotional-intelligence sentiment-analysis,2,0,
3659,12324,1,,2019-05-15T15:42:09.150,1,44,"what is the point of having multiple lstm units in a single layer ? surely if we have a single unit it should be able to capture ( remember ) all the data anyway and using more units in the same layer would just make the other units learn exactly the same historical features ? i 've even shown myself empirically that using multiple lstms in a single layer improves performance , but in my head it still does n't make sense , because i do n't see what is it that other units are learning that others are n't ? is this sort of similar to how we use multiple filters in a single cnn layer ?",25659,2444,2019-05-15T22:17:06.743,2019-05-16T00:22:53.620,why do we need multiple lstm units in a layer ?,machine-learning deep-learning lstm,1,1,
3660,12327,1,,2019-05-15T18:02:10.820,0,15,"i am trying to implement the dqn algorithm for the task of hvac control . i have the algorithm implemented using pytorch . i know that the hvac simulator is working as it works for other control methods . when i implement dqn , the algorithm does not converge as it should . i have included a graph showing the convergence with a single hidden layer nn with 5 units ( red curve ) and another with 2 hidden layers , each with 32 hidden units ( blue curve ) for one run . are there any simple explanations that would explain why the algorithm is n't converging ? i am currently using a linearly decreasing epsilon : e = e*0.9997 , starting from 0.9 and ending at 0.05 . i 've also included some of the rl code i am using . any help would be appreciated . thanks . ` ` ` class replaymemory(object ) : def _ _ init__(self , capacity ) : self.capacity = capacity self.memory = [ ] self.position = 0 def push(self , * args ) : if len(self.memory ) & lt ; self.capacity : self.memory.append(none ) self.memory[self.position ] = transition(*args ) self.position = ( self.position + 1 ) % self.capacity def sample(self , batch_size ) : return random.sample(self.memory , batch_size ) def _ _ len__(self ) : return len(self.memory ) def _ _ init__(self ) : super(dqn , self).__init _ _ ( ) self.hidden = nn.linear(4 , 32 ) self.hidden1 = nn.linear(32 , 32 ) self.head = nn.linear(32 , 2 ) def forward(self , x ) : x = f.relu(self.hidden(x ) ) x = f.relu(self.hidden1(x ) ) return self.head(x.view(x.size(0 ) , -1 ) ) ` ` ` batch_size = 128 gamma = 0.999 eps_start = 0.9 eps_end = 0.05 eps_decay = 200 target_update = 10 policy_net = dqn().to(device ) target_net = dqn().to(device ) target_net.load_state_dict(policy_net.state_dict ( ) ) target_net.eval ( ) optimizer = optim.rmsprop(policy_net.parameters ( ) ) memory = replaymemory(10000 ) steps_done = 0 global curepisode curepisode=0 ` ` ` def select_action(state ) : global steps_done sample = random.random ( ) global curepisode eps_threshold = eps_start*(0.9997**curepisode ) if(eps_threshold&lt;eps_end ) : eps_threshold = eps_end if sample & gt ; eps_threshold : with torch.no_grad ( ) : return policy_net(state).max(1)[1].view(1 , 1 ) else : return torch.tensor([[random.randrange(2 ) ] ] , device = device , dtype = torch.long ) def optimize_model ( ) : if len(memory ) & lt ; batch_size : return transitions = memory.sample(batch_size ) batch = transition(*zip(*transitions ) ) non_final_mask = torch.tensor(tuple(map(lambda s : s is not none , batch.next_state ) ) , device = device , dtype = torch.uint8 ) non_final_next_states = torch.cat([s for s in batch.next_state if s is not none ] ) state_batch = torch.cat(batch.state ) action_batch = torch.cat(batch.action ) reward_batch = torch.cat(batch.reward ) state_action_values = policy_net(state_batch).gather(1 , action_batch ) next_state_values = torch.zeros(batch_size , device = device ) next_state_values[non_final_mask ] = target_net(non_final_next_states).max(1)[0].detach ( ) expected_state_action_values = ( next_state_values * gamma ) + reward_batch loss = f.smooth_l1_loss(state_action_values , expected_state_action_values.unsqueeze(1 ) ) optimizer.zero_grad ( ) loss.backward ( ) for param in policy_net.parameters ( ) : param.grad.data.clamp_(-1 , 1 ) optimizer.step ( ) runs = 1 for i_runs in range ( runs ) : hvac = hvac ( ) num_episodes = 20000 bestreward = -1.0e20 convergence = list ( ) for i_episode in range(num_episodes ) : curepisode = i_episode print('episode ' , i_episode ) state = hvac.reset ( ) total_reward=0 done = false t=0 while ( not done ) : state = torch.floattensor([state ] , device = device ) action = select_action(state ) actionlist = list ( ) if(int(action)==0 ) : actionlist.append(100 ) actionlist.append(0 ) else : actionlist.append(0 ) actionlist.append(100 ) next_state , reward , done , info = hvac.step(actionlist ) total_reward = total_reward+reward reward = torch.floattensor([reward ] , device = device ) next_state_tensor = torch.floattensor([next_state ] , device = device ) memory.push(state , action , next_state_tensor , reward ) state = next_state if done : next_state = none t = t+1 optimize_model ( ) print('total_reward ' , total_reward ) convergence.append(total_reward ) if(i_episode%5==0 ) : print('episode % s : best reward % s ' % ( str(i_episode),str(bestreward ) ) ) if i_episode % target_update = = 0 : target_net.load_state_dict(policy_net.state_dict ( ) ) ` ` `",25661,,,2019-05-15T18:02:10.820,dqn algorithm not converging for hvac control task,reinforcement-learning dqn convergence,0,0,
3661,12330,1,,2019-05-15T20:40:40.257,4,63,"first i will clarify the context , i have to learn new technologies for my bachelor thesis . i am making a mobile application similar to flappy bird , except it 's voice controlled . the idea is to have this app as a practice tool for people with voice problems ( monotony ) . the bird flies upwards when the user makes a high pitch , and downwards when it 's a low pitch . the app part is pretty much complete . but my project also consists of a website where the vocal coach can follow his user 's progress . i am saving all the user 's game history in a database . more specifically , every time the bird passes through a pipe opening , i save the pitch values that he correctly produced . i also save the pitch value that made him lose the game , along with the pitch value that he was supposed to produce to not lose . having all these data in my database , this is where i 'd like to add ai as a new technology for my project . i thought of a nice feature where it would calculate the user 's strong and weak points . for example it would say that a user is strong at high pitches , but is not good at low pitches . but since i do n't know anything about ai , i 'm not sure to what extent this is possible and have no idea where to start . i would really appreciate if someone could share their knowledge with me by pointing me to existing libraries / frameworks ! i 'd most preferably include the ai code inside my api that is directly connected to my database , so i can directly return the calculated data to my website . my api is a springboot app , so i guess i would need java ai libraries ?",25662,1671,2019-05-16T16:21:28.407,2019-05-16T16:21:28.407,ai method for evaluating user performance based on audio pitch re : public speaking,ai-basics software-evaluation audio-processing,1,0,
3662,12335,1,12336,2019-05-16T00:37:12.737,0,11,"i 'm using 10-fold cross validation on all models . here you can see both plots : since i am using k - fold cross validation , is it okay to name it "" validation error vs training error "" or "" test error vs training error "" would be better ?",25405,2444,2019-05-16T01:03:02.080,2019-05-16T01:03:02.080,"should i call the error "" validation error "" or "" test error "" during cross validation ?",machine-learning terminology overfitting cross-validation,1,0,
3663,12340,1,12362,2019-05-16T05:06:39.407,1,59,"below you have the plots of the training and validation errors for two different models . both plots show the rmse values for the validation dataset versus the number of training epochs . it is observed that models get lower rmse value as training progresses . the model associated with the first plot is performing quite well . the gap is quite narrowed . i think the model associated with this second plot is doing pretty good , but not as well as the other . the gap is much broader . the model of the first plot was trained using a data set containing 1 million of ratings , while the second one used only 100k . i 'm implementing the collaborative filtering ( cf ) algorithm . i am optimising it using sgd . are any of these models overfitting or underfitting ?",25405,25405,2019-05-17T02:03:48.390,2019-05-17T02:03:48.390,which model is better given their training and validation errors ?,machine-learning overfitting recommender-system cross-validation underfitting,1,4,1
3664,12341,1,,2019-05-16T06:16:22.003,3,32,"i have a set of 15 unique playing cards from a deck of 52 playing cards . a given state is represented by the respective card values in the set of 15 cards , where the card value is a prime number associated with that card . for example , ah is represented by 3 . how should i represent a single state for the nn ? should it be a list of the 15 prime numbers representing the list of cards ? i was hoping that i could represent a single state as the sum of each of all 15 prime numbers and then throw that sum through a sigmoid function . my concern , however , is that the nn will lose information if i reduce the dimension of the state to a single attribute ( even if that attribute is unique to that state - the sum of n prime numbers is unique compared to the sum of any other n prime numbers ) . how important is the dimensionality of each state for deep q learning ? i 'd really appreciate even some general direction .",25671,2444,2019-05-16T13:51:35.863,2019-05-16T13:51:35.863,how do i represent a multi - dimensional state using a neural network ?,machine-learning deep-learning q-learning dqn function-approximation,1,5,
3665,12343,1,,2019-05-16T08:51:45.540,2,72,"i have a dataset as follows ( and the table extends to include an extra 146 columns for companies 4 - 149 ) is there an algorithm i could use effectively to find similar patterns in sales from the other companies when compared to my company ? i thought of using k - means clustering , but as i 'm dealing with 150 companies here it would likely become a bit of a mess , and i do n't think linear regression would work here .",25675,2444,2019-05-19T14:49:31.417,2019-05-19T14:49:31.417,is there a machine learning algorithm to find similar sales patterns ?,machine-learning unsupervised-learning linear-regression pattern-recognition k-means,2,1,
3666,12345,1,12369,2019-05-16T09:32:32.857,2,40,"in lenet 5 's first layer , the number of feature maps is equal to the number of kernels . however , the second convolutional layer has a depth different from the 3rd layer . does the filter size dictate the number of feature maps ?",25676,2444,2019-05-17T13:28:21.927,2019-05-17T13:40:47.743,is the number of feature maps equal to the number of kernels in the lenet 5 architecture ?,deep-learning convolutional-neural-networks feature,2,2,
3667,12348,1,,2019-05-16T11:34:49.153,1,43,"at the moment i am working on a vehicle counting & amp ; classification project . for a specific part in the project i need to get back only the completely visible vehicles from my input data ( images ) . i am wondering if this could be done ( more ) automatically in the following way : zoom in such that only approximately one van would be visible divide the vehicles into two categories : truncated and non - truncated train on these two classes after training and testing , use the model to find the completely visible vehicles . so the main question is , is it possible that this would give sufficient results or should i try to find another solution ?",23473,,,2019-05-16T14:54:58.830,divide classes into truncated and non - truncated objects,neural-networks training,2,0,
3668,12351,1,,2019-05-16T13:39:16.707,0,40,"it is very hard to say what cognitive science is.the disagreements about the scope and methods of this new science are , however , a very healthy situation.there is much disagreement about exactly what cognitive science is - and even more about what exactly it studies and what is the correct way to study it . cognitive science is , just , like ai , a cross - disciplinary activity.it involves [ at the very least ] psychologists , neurobiologists , linguists , computer scientists and philosophers . cognitive science also has the best claim to be the science of mind in humans , animals , machines and extra terrestrial aliens[if there is any]. cognitive science has also had a tremendous influence on human psychology.ai has produced technology but the export of ideas to other discipline is , at least , probably more important . sometimes it is observed that people doing cognitive science try to avoid the term artificial intelligence . cognitive science is more about trying to find out how the human intelligence or mind works.and that it would use artificial intelligence to make tests or experiments , to test ideas and so forth . is artificial intelligence a research tool for cognitive science?cognitive science deals with living things , while ai tries to create an intelligence artificially . my question is about , is it all about cognitive science ? most people who use""cognitive science "" are referring more to understanding human intelligence for it 's own sake.ai is more about implementing "" intelligence "" on a computer , where the techniques used may or may not be influenced by research done under rubric of cognitive science . again , my question is about , it has been many years since cognitive science has come closer to ai .these two sciences have differences.why the relationship between ai and cognitive science is much more complicated?how and when the relationship between these two sciences did begin to grow ?",19160,,,2019-05-16T13:39:16.707,what is cognitive science about ? how the relationship between ai and cognitive science did begin to grow ?,ai-basics terminology research cognitive-science cognition,0,3,
3669,12352,1,,2019-05-16T14:15:10.707,-1,13,"i am working through the tutorial here . i just cleaned up the code for myself but other than that code is as in original tutorial . i am using the pollution data from the following link when i try to clear session after each calibration , i get the error : valueerror : tensor tensor(""dense_1 / biasadd:0 "" , shape= ( ? , 1 ) , dtype = float32 ) is not an element of this graph . my code is : import sys from math import sqrt import numpy as np from matplotlib import pyplot import pandas as pd from sklearn.preprocessing import minmaxscaler from sklearn.preprocessing import labelencoder from sklearn.metrics import mean_squared_error from datetime import datetime def series_to_supervised(data , n_in=1 , n_out=1 , dropnan = true ) : n_vars = 1 if type(data ) is list else data.shape[1 ] df = pd.dataframe(data ) cols , names = list ( ) , list ( ) # input sequence ( t - n , ... t-1 ) for i in range(n_in , 0 , -1 ) : cols.append(df.shift(i ) ) names + = [ ( ' var%d(t-%d ) ' % ( j+1 , i ) ) for j in range(n_vars ) ] # forecast sequence ( t , t+1 , ... t+n ) for i in range(0 , n_out ) : cols.append(df.shift(-i ) ) if i = = 0 : names + = [ ( ' var%d(t ) ' % ( j+1 ) ) for j in range(n_vars ) ] else : names + = [ ( ' var%d(t+%d ) ' % ( j+1 , i ) ) for j in range(n_vars ) ] # put it all together agg = pd.concat(cols , axis=1 ) agg.columns = names # drop rows with nan values if dropnan : agg.dropna(inplace=true ) return agg def parsehrtr(x ) : return datetime.strptime(x , ' % y % m % d % h ' ) def fn_trainhere(train_x , train_y , test_x , test_y ) : from keras.models import sequential from keras.layers import dense from keras.layers import lstm from datetime import datetime from keras import backend as k model = sequential ( ) model.add(lstm(50 , input_shape=(train_x.shape[1 ] , train_x.shape[2 ] ) ) ) model.add(dense(1 ) ) model.compile(loss='mae ' , optimizer='adam ' ) model.fit(train_x , train_y , epochs=5 , batch_size=72 , validation_data=(test_x , test_y ) , verbose=2 , shuffle = false ) k.clear_session ( ) return model def fn_predicthere(calibparams , nn , test_x , laghours , numoffeatures , scaler , test_y ) : import keras model = calibparams[nn ] yhat = model.predict(test_x ) test_x = test_x.reshape((test_x.shape[0 ] , laghours * numoffeatures ) ) inv_yhat = np.concatenate((yhat , test_x [ : , -(numoffeatures - 1 ) : ] ) , axis=1 ) inv_yhat = scaler.inverse_transform(inv_yhat ) inv_yhat = inv_yhat [ : , 0 ] test_y = test_y.reshape((len(test_y ) , 1 ) ) inv_y = np.concatenate((test_y , test_x [ : , -(numoffeatures - 1 ) : ] ) , axis=1 ) inv_y = scaler.inverse_transform(inv_y ) inv_y = inv_y [ : , 0 ] return sqrt(mean_squared_error(inv_y , inv_yhat ) ) class lstmoo ( ) : def _ _ init__(self , laghours , numoffeatures , numoftraindays ) : self.laghours , self.numoffeatures , self.numoftraindays = laghours , numoffeatures , numoftraindays self.readdata ( ) self.networkdesign ( ) self.forecast ( ) def readdata(self ) : dataset = pd.read_csv(r'c:\pmidata\raw.csv ' , parse_dates=[['year ' , ' month ' , ' day ' , ' hour ' ] ] , index_col=0 , date_parser = parsehrtr ) dataset.drop('no ' , axis=1 , inplace = true ) dataset.columns = [ ' pollution ' , ' dew ' , ' temp ' , ' press ' , ' wnd_dir ' , ' wnd_spd ' , ' snow ' , ' rain ' ] dataset.index.name = ' date ' dataset['pollution'].fillna(0 , inplace = true ) # drop the first 24 hours dataset = dataset[24 : ] # save to file dataset.to_csv(r'c:\pmidata\pollution.csv ' ) dataset = pd.read_csv(r'c:\pmidata\pollution.csv ' , header=0 , index_col=0 ) values = dataset.values # integer encode direction encoder = labelencoder ( ) values [ : , 4 ] = encoder.fit_transform(values [ : , 4 ] ) values = values.astype('float32 ' ) # normalize features self.scaler = minmaxscaler(feature_range=(0 , 1 ) ) scaled = self.scaler.fit_transform(values ) # frame as supervised learning reframed = series_to_supervised(scaled , self.laghours , 1 ) values = reframed.values n_train_hours = self.numoftraindays * 24 self.train = values[:n_train_hours , :] self.test = values[n_train_hours : , :] def networkdesign(self ) : n_obs = self.laghours * self.numoffeatures train_x , train_y = self.train [ : , : n_obs ] , self.train [ : , -self.numoffeatures ] train_x = train_x.reshape((train_x.shape[0 ] , self.laghours , self.numoffeatures ) ) test_x , self.test_y = self.test [ : , : n_obs ] , self.test [ : , -self.numoffeatures ] self.test_x = test_x.reshape((test_x.shape[0 ] , self.laghours , self.numoffeatures ) ) self.calibparams = { } for nn in range(1 , 3 ) : np.random.seed(nn ) model = fn_trainhere(train_x , train_y , self.test_x , self.test_y ) self.calibparams[nn ] = model def forecast(self ) : for nn in range(1 , 3 ) : rmse = fn_predicthere(self.calibparams , nn , self.test_x , self.laghours , self.numoffeatures , self.scaler , self.test_y ) print('test rmse : % .3f ' % rmse ) if _ _ name _ _ = = ' _ _ main _ _ ' : laghours , numoffeatures , numoftraindays = 3 , 8 , 365 lstmoo(laghours , numoffeatures , numoftraindays ) however , code runs fine when i comment out the line : k.clear_session ( )",25634,,,2019-05-16T19:07:37.673,error using keras clear_session while multiple calibration,tensorflow keras lstm,1,1,
3670,12357,1,,2019-05-16T18:50:27.717,0,12,"so i´m currently implementing my first neural network using grus as a model and keras as an implementation since it´s pretty highlevel . my problem is about the classification of 8 hour long timeseries with 11 different events with 1 second timesteps or to be more specific : sleep recordings . since the gru can´t handle the whole timeseries at once , i split it in 500 timepoint pieces with 50 seconds ( 10 % ) overlap . also since i don´t have much data , after the training / test split i´m oversampling the train data by duplicating the underrepresented classes up to 10 times . so at the end i get a set of ca . 14.000 training snippets and ca 1.800 to test the model . that to the data . next thing is the model , represented with the keras code : verbose , epochs , batch_size = 2 , 50 , 1200 # 1200 batch_size was the maximum the gpu can handle n_timesteps , n_features , n_outputs = trainx.shape[1 ] , trainx.shape[2 ] , trainy.shape[1 ] model = sequential ( ) model.add(gru(128 , input_shape=(n_timesteps , n_features ) , return_sequences = true , dropout=0.7 , kernel_regularizer = regularizers.l2(0.01 ) , activation=""relu "" ) ) model.add ( gru(64 , return_sequences = true , go_backwards = false , dropout=0.7 , kernel_regularizer = regularizers.l2(0.01 ) , activation=""relu "" ) ) model.add ( gru(32 , return_sequences = true , go_backwards = false , dropout=0.7 , kernel_regularizer = regularizers.l2(0.01 ) , activation=""relu "" ) ) model.add ( gru(16 , return_sequences = true , go_backwards = false , dropout=0.7 , kernel_regularizer = regularizers.l2(0.01 ) , activation=""relu "" ) ) model.add(timedistributed(dense(units=16 , activation='relu ' ) ) ) model.add(flatten ( ) ) model.add(dense(n_outputs , activation='softmax ' ) ) # define custom optimizer with configurable learning rate sgd = optimizers.sgd(lr=0.1 , momentum=0.9 , decay=0.0 , nesterov = false ) model.compile(loss='categorical_crossentropy ' , optimizer = sgd , metrics=['accuracy ' ] ) so as a short summary : i am using 4 gru layers with a "" pyramid "" shape , so the network has to be more specific towards the end . also one fully connected layer as some kind of "" adapter "" and one output layer with the size of my features . i am using sgd as an optimizer . the results are always pretty bad , here are the loss and accuracy plots of an example run featuring 25 epochs : despite the huge dropout each stage , it still seems to be overfitting . also as you can see , the accuracy of test and train is not changing after the 2nd epoch . the result is this sad looking confusion - matrix , showing only one class beeing detected by the network , which is not even the one with the highest amount of data in the dataset : [ [ 231 0 0 0 0 0 0 0 ] [ 55 0 0 0 0 0 0 0 ] [ 647 0 0 0 0 0 0 0 ] [ 141 0 0 0 0 0 0 0 ] [ 444 0 0 0 0 0 0 0 ] [ 0 0 0 0 0 0 0 0 ] [ 118 0 0 0 0 0 0 0 ] [ 74 0 0 0 0 0 0 0 ] ] so what are possible approaches in order to : a ) fight the overfitting , b ) just one class beeing detected and c ) fix the stationary accuracy to be growing again ? since i´m pretty new to neural networks i am thankful for your time and effort !",25687,25687,2019-05-16T19:11:06.003,2019-05-16T19:11:06.003,train and test accuracy of gru network not increasing after 2nd epoch,neural-networks recurrent-neural-networks keras loss-functions time-series,1,0,
3671,12360,1,12424,2019-05-16T19:30:30.733,0,42,"understandably rnns are very good at solving problems involving audio , video and text processing due to arbitrary input length of this sort of data . what i do n't understand is why rnns are also superior at predicting time series data and why we use them over simple mlp dnns . say i wanted to predict what the value in the time series is at t+1 . i would take a window of lets say t-50 , t-49 ... t and then feed loads of sampled training data into a network . i could either choose to have a single lstm unit remembering the entire window and basing the predictions on that or i could simply make a 50 neuron wide mlp network . what exactly is it about rnns that makes them better in this scenario or any scenario for that matter ? i understand that the lstm would have substantially less weights ( in this scenario ) and should be less computationally intensive , but apart from that i do n't see any difference in these two methods .",25659,,,2019-05-20T09:53:01.590,time series rnn vs dnn,neural-networks machine-learning deep-learning recurrent-neural-networks lstm,2,0,
3672,12363,1,,2019-05-17T08:02:50.547,0,7,"i 'm trying to annotate images with coco key points for pose estimation using https://github.com/jsbroks/coco-annotator . as described in the installation section i cloned the repo . i installed docker and docker - compose . following this i started the container with $ docker - compose up and it is running . i am now on the website https://annotator.justinbrooks.ca/ , i created one user and datasets but they do not appear in the repo datasets/ folder . i tried to create them manually and to load images in them but they do not appear on the website graphic interface . i tried to scan , reload the webpage , create other datasets but it does not work . the container seems to work properly , it detects when i put an image in the datasets/ folder but it throws some errors . here is the last lines ( i can post the whole log ) : annotator_webclient | [ file watcher ] file /datasets / haricot.jpg for created annotator_webclient | [ file watcher ] adding new file to database : /datasets / image annotator_message_q | 2019 - 05 - 16 13:01:08.841 [ error ] & lt;0.461.0&gt ; closing amqp connection & lt;0.461.0&gt ; ( 172.18.0.4:42614 -&gt ; 172.18.0.2:5672 ) : annotator_message_q | missed heartbeats from client , timeout : 60s am i missing something fundamental or there is a bug ? i 'm using safari on macos and i also tried firefox on ubuntu 18 . i 'm not behind a proxy , but maybe some ports are not open or something like this .",19859,,,2019-05-17T08:02:50.547,ca n't create datasets and load images in coco annotator,algorithm training datasets,0,0,
3673,12365,1,,2019-05-17T12:25:42.863,0,31,"right now i am trying to synthesize training images for a cnn and due to the nature of the application , there is a finite number of sample images to learn from . from other research , i expect to be using about 200,000 training images at a resolution of 1280 * 720 , which with 3 channel at 8 bits will take about 550 gb to save uncompressed . this number can and probably will rise in the future , meaning more memory that i will need to provide . i imagine that there are applications that required even more training data with higher complexity and that there are solutions to handling that such as compression techniques and the like . my question : are there solutions for the memory management beyond compressing the images with jpeg and such besides generating and instantly consuming the pictures without saving them to permanent memory ?",25702,,,2019-05-31T15:10:00.060,how to manage large amounts of image data for training ?,convolutional-neural-networks datasets memory,1,1,
3674,12366,1,12367,2019-05-17T12:26:56.863,0,31,"i 've recently been reading up on cnns and this part of the architecture is really confusing me . assume , i have an input of size [ 32 * 32 * 3 ] and pass it to a convolution layer . now , if my kernel size were to be [ 5 * 5 * 3 ] and the depth of my convolution layer were to be 1 , only one feature map would be produced for the image . here , each neuron would have a 75 weights ( +1 bias ) . if i wanted to calculate multiple feature maps in this layer , say 3 , is each local section ( in this example [ 5 * 5 * 3 ] ) of the image looked on by three different neurons and each of their weights trained individually ? and what would be the output volume of this layer ?",25704,,,2019-05-17T13:48:14.760,convolution layer neurons when extracting multiple feature maps,neural-networks deep-learning convolutional-neural-networks feature,1,5,
3675,12370,1,,2019-05-17T14:53:12.187,1,26,"i am currently working with a small dataset of 20x300 . since i have so few datapoints , i was wondering if i could use an approach similar to leave - one - out cross - validation but for testing . here 's what i was thinking : train / test split the data , with only one data point in the test set . train the model on training data , potentially with grid_search / cross - validation use the best model from step 2 to make a prediction on the one data point and save the prediction in an array repeat the previous steps until all the data points have been in the test set calculate your preferred metric of choice ( accuracy , f1-score , auc , etc ) using these predictions the pros of this approach would be to : you do n't have to split the data into train / test so you can train with more datapoints . the cons would be : this approach suffers from potential ( ? ) data leakage . you are calculating an accuracy metric from a bunch of predictions that potentially came from different models , due to the grid searches , so i 'm not sure how accurate it is going to be . i have tried the standard approaches of train / test splitting but since i need to take out at least 5 points for testing , then i do n't have enough points for training and the roc auc becomes very bad . i would really appreciate some feedback about whether this approach is actually feasibly or not and why .",25708,,,2019-05-18T10:34:12.223,leave one out testing,training cross-validation,1,0,
3676,12388,1,,2019-05-17T17:05:22.153,2,19,"i 'm trying to teach a humanoid agent how to stand up after falling . the episode starts with the agent lying on the floor with its back touching the ground , and its goal is to stand up in the shortest amount of time . but i 'm having trouble in regards to reward shaping . i 've tried multiple different reward functions , but they all end up the same way : the agent quickly learns to sit ( i.e. lifting its torso ) , but then gets stuck on this local optimum forever . any ideas or advice on how to best design a good reward function for this scenario ? a few reward functions i 've tried so far : current_height / goal_height current_height / goal_height - 1 current_height / goal_height - reward_prev_timestep ( current_height / goal_height)^n ( tried multiple different values of n ) ...",25712,,,2019-05-17T17:05:22.153,humanoid agent reward shaping,reinforcement-learning,0,1,
3677,12390,1,,2019-05-17T18:24:30.060,3,52,"i am new in the field of ai . i am working to create the flappy bird using genetic algorithm . after reading and seeing some examples , i saw that most implementations use a neural network + genetic algorithm and after certain generations , you achieve a very good agent that survives very long . i currently struggling to implement the neural network since i have never taken a machine learning course . on many examples that i have read neural networks require training inputs and outputs . for the flappy bird , i ca n't think of output since you do n't really know if the action of flapping will benefit you or not . in the example that i followed , synaptic.js is used and it is pretty straight - forward . however , in python , i ca n't find a simple library that will initialize randomly and adjust the weights and biases depending on the good agents that survive longer . what would be the right way to implement this neural network without having a training dataset ? is there any way to create flappy bird without using neural networks , just genetic algorithm ? the example in javascript that i am referring to : flappy bird using machine learning",25714,,,2019-05-17T18:24:30.060,how to implement a neural network for flappy bird in python ?,neural-networks python genetic-algorithms,0,0,2
3678,12391,1,,2019-05-17T23:24:26.993,-2,28,hello all my name is james pannell and i 'm doing a university research paper on the usage of ai within the healthcare setting to diagnose illnesses and would highly value your opinions on some of the key issues / areas of controversy via the survey link : https://forms.gle/shk5td96hxg7dczk6 if you could please take part this would be excellent and very much appreciated . each answer only needs a sentence or two and you do n't have to know much about ai its mostly opinion - based thanks ! have a great day !,25652,,,2019-05-17T23:24:26.993,could ai be used to diagnose illnesses within the healthcare setting i.e. medical diagnostics ?,ai-basics,0,1,
3679,12396,1,,2019-05-18T08:57:10.483,1,28,"problem : i 've been reading research papers on how to solve a peg solitaire using graph searching , but all the papers kind of assume you know how to do the reduction(polynomial time conversion ) from the peg solitaire to the graph , which i do not , but this is how i assumed it was done . for those of you unfamiliar : https://www.youtube.com/watch?v=bt6gpgvuneq the goal is to only have one peg on the board and you get rid of pegs by jump one peg over another . a peg can only jump if the it 's jumping into an empty space as shown in the picture above . what i 've tried : i had the idea of converting the problem to a tree where each node represents the state after an action is taken and each edge represents the action taken . so the root node would the initial state which is the board shown above then it 's children would be the state of the board after any of the possible legal actions that can be taken . so for example : then the children of each of those nodes would be the possible moves for them and you can find a solution once you 've reached depth 31 in the tree because there are 32 pegs and you win the game where there 's only 1 left . is this the right approach ? it feels a little too abstract because i 'd have to represent the edges as peg moves , but that 's weird cause they 're usually numbers or constraint .",25721,2444,2019-05-18T14:48:49.590,2019-05-18T15:59:15.077,using graph searching to solve peg solitaire ?,game-ai search graphs graph-theory,1,0,
3680,12397,1,,2019-05-18T09:41:11.200,2,49,"i 've been doing some reading about gans , and although i 've seen several excellent examples of implementations , the descriptions of why those patterns were chosen is n't clear to me in many cases . at a very high level , the purpose of the discriminator in a gan is establish a loss function that can be used to train the generator . ie . given the random input to the generator , the discriminator should be able to return a probability of the result being a ' real ' image . if the discriminator is perfect the probability will always be zero , and the loss function will have no gradient . therefore you iterate : generate random samples generate output from the generator evaluate the output using the discriminator train the generator update the discriminator to be more accurate by training it on samples from the real distribution and output from the generator . the problem , and what i do n't understand , is point 5 in the above . why do you use the output of the generator ? i absolutely understand that you need to iterate on the accuracy of the discriminator . to start with it needs to respond with a non - zero value for the effectively random output from the generator , and slowly it needs to converge towards correctly classifying images at ' real ' or ' fake ' . in order to achieve this we iterate , training the generator with images from the real distribution , pushing it towards accepting ' real ' images . ... and with the images from the generator ; but i do n't understand why . effectively , you have a set of real images ( eg . 5000 pictures of faces ) , that represent a sample from the latent space you want the gan to converge on ( eg . all pictures of faces ) . so the argument goes : as the generator is trained iteratively closer and closer to generating images from the latent space , the discriminator is iteratively trained to recognise from the latent space , as though it had a much larger sample size than the 5000 ( or whatever ) sample images you started with . ... ok , but that 's daft . the whole point of dnn 's is that given a sample you can train it to recognise images from the latent space the samples represent . i 've never seen a dnn where the first step was ' augment your samples with extra procedurally generated fake samples ' ; the only reason to do this would be if you can only recognise samples in the input set , ie . your network is over - fitted . so , as a specific example , why ca n't you incrementally train the discriminator on samples of ( ' real ' * epoch / iterations + ' noise ' * 1 - epoch / iterations ) , where ' noise ' is just a random input vector . your discriminator will then necessarily converge towards recognising real images , as well as offering a meaningful gradient to the generator . what benefit does feeding the output of the generator in offer over this ?",25722,25722,2019-05-19T04:26:23.197,2019-05-19T04:26:23.197,why use the output of the generator to train the discriminator in a gan ?,generative-adversarial-networks,1,1,
3681,12402,1,12406,2019-05-18T23:07:43.073,6,60,"for discrete action spaces , what is the purpose of the actor in actor - critic algorithms ? my current understanding is that the critic estimates the future reward given an action , so why not just take the action that maximizes the estimated return ? my initial guess at the answer is the exploration - exploitation problem , but are there other , more important / deeper reasons ? or am i underestimating the importance of exploration vs. exploitation as an issue ? it just seems to me that if you can accurately estimate the value function , then you have solved the rl challenge .",25732,,,2019-05-19T15:28:39.503,purpose of actor in actor - critic algorithm ?,reinforcement-learning actor-critic,1,0,1
3682,12404,1,,2019-05-19T04:27:59.313,2,32,"update on 2019 - 05 - 19 : my question is about teaching ai to solve the problem , not letting ai teach a human developer to solve a problem . original post : i 'm a software developer but very new to ai . today my friend and i talked ( to be more exact , chatted ) about the development of ai . one topic was about implementing the capability of "" given a problem , analyzing the problem and designing a solution "" . since we are both software developers , we used a simple example problem in our discussion to see how ai might possibly find a solution : print the following three lines on the console : * * * * * * * * * my friend and i thought we may use some formal method to describe what we want but not how we implement it . it 's the ai 's job to figure out the solution . then we came to the question i 'm asking here : since my friend and i are both outsiders of ai research , we do n't know if there is any existing research ( we believe such research must have existed somewhere ) that teaches ai to analyze the problem ( which is formally defined ) and design a solution using the given tools . for us human beings , our analysis of the problem and designing might look like the following : let me choose a programming language . for example , c. let me see what tools i have in the chosen programming language . oh , here they are : putchar(ch ) which prints a single character on the console . printf(str ) which prints a string on the console . for - loop ; if - else ; support of subroutines ; etc . i see the result has three lines of characters : line 1 , 2 , and 3 . i see the numbers of ' * ' in the three lines are an arithmetic progression and there is a connection of line number and character number : given the line number i , the character number is 2*i-1 , where i is 1 , 2 , and 3 . this is repetition and i can use a for - loop . each line is the repetition of ' * ' so i may implement a function to do this . void print_line(int n ) { for ( int i = 0 ; i & lt ; n ; i++ ) { putchar ( ' * ' ) ; } putchar('\n ' ) ; } int main(int argc , char * argv [ ] ) { for ( int i = 1 ; i & lt;=3 ; i++ ) { print_line(2 * i - 1 ) ; } return 0 ; } alternatively , i may design a naive solution of using printf ( ) three times and hard - code each string : printf(""*\n "" ) ; printf(""***\n "" ) ; printf(""*****\n "" ) ; we think an ai that can do this may follow a similar analyzing and designing approach as a human developer does . in general , we think : this ai should have a toolbox using which it can solve some problems ( possibly not all problems ) . in my example above , this toolbox may be a programming language and its corresponding library . this ai should have the knowledge about some concepts ( such as console and string in the example above ) and their relationships . this ai should have the knowledge that connects the toolbox and the concepts , so the ai knows how a tool can manipulate the properties of a concept . most importantly , this ai should have the capability of figuring out one or more paths that connect the input to the desired output , using the toolbox . this process , we think , needs the capability of "" analysis "" and "" design "" . excuse us if the description is still vague . my friend and i are both new to ai so , in fact , we do n't even know if "" analysis "" and "" design "" are the proper words to use . we will be glad to clarify if needed . btw , we did some quick search about such ai : bayou by rice university does n't look like understanding the problem , either . deepcoder uses deep learning and i doubt whether it understands the problem , either . the ai - programmer uses genetic algorithms to generate the desired string in brainfuck . but this ai does n't look like understanding the problem . it looks like a trial - and - error with feedback .",4817,4817,2019-05-19T13:25:23.573,2019-05-19T13:25:23.573,"is there research about teaching ai to "" analyze the problem and design a solution "" ?",machine-learning algorithm ai-basics research human-like,1,3,
3683,12408,1,,2019-05-19T13:28:32.363,1,150,"i asked a question relating to tictactoe playing in rl . from the answer it seems to me a lot is dependent on the opponent ( rightly so , if we write down the expectation equations ) . my question is ( in the context of tictactoe or chess ) : how to make the rl player a perfect player ( tictactoe ) or an expert ( chess ) . as far as ttt is concerned when playing against a perfect player an rl will become perfect conditioned on the clause that the opponent is perfect . so will this hold true if the same rl algo , with its learned values are used to play some other lesser perfect players ? the question can be extended to the scenario , can a rl player with pre - trained values ( assume from a perfect or expert opponent ) be used in any scenario with best results ? note : the problem is more severe in chess , since experts will use some kind of opening moves which will not match with say a random player and thus finding values for those states becomes a problem , since we have not encountered it during training time . footnote : any resources on game playing rl is appreciated .",9947,,,2019-05-19T14:37:28.403,adversarial game playing using rl,reinforcement-learning game-ai combinatorial-games,1,0,
3684,12410,1,,2019-05-19T15:54:27.450,0,16,"i 'm having serious issues with the implementation of the lrp algorithm for neural networks in matlab . the challenge is to implement the equations correctly . i 'm trying to implement the deep - taylor version of the lrp . i 'm testing it on a feed - forward full - connected neural network with one hidden layer trained on the mnist dataset . i start initializing the relevances of the last layer with the predictions of the network : r{l } = nn.layers{nn.lastlayer}.output ; then , i need to perform the following equation through the hidden layers ( in my case is just one ) : $ $ r_i^l = \sum_j \frac{a_i^l \cdot w_{ij}^{+}}{\sum_k a_j^{l+1 } \cdot w_{kj}^{+ } } \cdot r_j^{l+1}$$ thanks to a paper , i 've followed some suggestions and correctly implemented that equation as follows : for l = nn.numoflayers-1 : -1 : 1 z = max(nn.layers{l+1}.w , 0 ) * nn.layers{l}.output ; s = r{l+1 } ./ z ; c = ( max(nn.layers{l+1}.w , 0 ) ) ' * s ; r{l } = nn.layers{l}.output . * c ; end i hope the syntax is clear , "" nn "" is the neural network struct and other fields should be intuitive to understand . the elements are columns , it means that outputs $ a$ are $ numberofneurons \times 1 $ . the weights are matrices such that on the rows there are the neurons of the next layer , and on the columns the neurons of the current layer . now , my problem is , how do i compute the relevances for the first layer ? ( input layer ) because , in the input layer i have n't the $ a$ values ( that are the outputs transformed by the activation function ) . so i 've looked also in a paper and found that the following equation can be used to compute the relevances of the first layer ( the layer of interest ) : $ $ r_i^1 = \sum_j \frac{w_{ij}^{2}}{\sum_k w_{kj}^{2 } } \cdot r_j^{2}$$ i 've tried different ways to implement that in matlab , but the results seems wrong . for instance , i 've tried with the following code : for i = 1 : inputsize scalar = 0 ; for j = 1 : nn.layers{1}.layersize norm = 0 ; for k = 1 : inputsize norm = norm + nn.layers{1}.w(j,k)^2 ; end scalar = ( nn.layers{1}.w(j,i)^2 * r{1}(j))/norm ; rfin(i ) = rfin(i ) + scalar ; end end and then return $ r$ . these are the results i should see : this is an example of what i get : do you know what am i doing wrong ? do you have ideas on how to implement correctly in matlab the second equation ? thank you very much .",25751,,,2019-05-19T15:54:27.450,difficulties to implement the layer - wise relevance propagation in matlab,neural-networks backpropagation matlab,0,1,
3685,12411,1,,2019-05-19T19:52:06.153,2,21,"i 've been studying branch and bound 's graph algorithm and i hear it always finds the optimal path because it uses previously found solutions to find others , but i have n't been able to find a proof on why it finds the optimal path(in fact most sites kind of do a bad job generalizing the algorithm itself ) . i was wondering what the proof is that this algorithm always find the optimal path in the case of a graph with 1 or more goal nodes ?",25721,25721,2019-05-19T20:39:13.770,2019-05-19T22:20:44.360,proof branch and bound always finds optimal path in a graph ?,search proofs graphs branch-and-bound,1,8,
3686,12412,1,,2019-05-19T20:10:32.770,2,17,"i appreciate that there are many ways to arrange the memory in a nn , and that the numerical representations may be with bytes to floats depending on an implementation . what is typical amount of memory required for the "" application side "" for the better nn programs such as alpha zero or an automated driving ai ? how much does it matter ?",25756,,,2019-05-19T20:10:32.770,how much physical memory does alpha zero 's neural net require ?,neural-networks memory,0,0,
3687,12414,1,,2019-05-19T21:15:24.323,1,152,"i have a .csv file called ratings.csv with the following structure : userid , movieid , rating 3 , 12 , 5 2 , 7 , 6 the rating scale goes from 0 to 5 stars . i want to be able to plot the sparsity of the matrix like it 's done in the following picture : as you can see , ratings scale goes from 0 to 5 on the right . it is a very well thought plot . i have matlab , python , r etc . could you come up with something and help me ? i ’ve tried hard but i can not find the way to do it .",25405,2444,2019-05-20T14:03:41.343,2019-05-20T14:03:41.343,how do i plot a matrix of ratings ?,machine-learning recommender-system data-visualization,2,0,
3688,12415,1,,2019-05-19T22:26:41.723,3,20,can measure ( computed rigorously or approximately ) of integrated information theory serve as reward for self - evolving / learning reinforcement learning system and hence we let this system to become / evolve into conscious system ?,8332,2444,2019-05-19T22:39:26.757,2019-05-19T22:39:26.757,can measure of integrated information theory serve as reward for reinforcement learning system ?,reinforcement-learning artificial-consciousness integration,0,0,
3689,12418,1,12445,2019-05-20T01:40:49.383,4,107,"i 've been trying to learn backpropagation for cnns . i read several articles like this one and this one . they all say that to compute the gradients for the filters , you just do a convolution with the input volume as input and the error matrix as the kernel . after that , you just subtract the filter weights by the gradients(multiplied by the learning rate ) . i implemented this process but it 's not working . here 's a simple example that i tried : input volume ( randomised ) 1 -1 0 0 1 0 0 -1 1 in this case , we want the filter to only pick up the top left 4 elements . so the target output will be : 1 0(supposed to be -1 , but relu is applied ) 0 1 we know that the desired filter is : 1 0 0 0 but we pretend that we do n't know this . we first randomise a filter : 1 -1 1 1 the output right now is : 3 0 -2 1 apply relu : 3 0 0 1 error ( target - output ) : -2 0 0 0 use error as kernel to compute gradients : -2 2 0 -2 say the learning rate is 0.5 , then the new filter is : 2 -2 1 2 this is still wrong ! it 's not improving at all . if this process is repeated , it wo n't learn the desired filter . so i must have understood the math wrong . so what 's the problem here ?",25745,25745,2019-05-21T06:05:06.607,2019-05-23T01:05:26.910,how are filters weights updated for a cnn ?,deep-learning convolutional-neural-networks backpropagation math,1,0,0
3690,12421,1,12432,2019-05-20T05:58:09.850,5,145,"i am trying to solve for lambda using temporal difference learning i am trying to figure out what lambda i need , to make td(λ)=td(1 ) but i get the incorrect value of lambda . here is how i did : from scipy.optimize import fsolve , leastsq import numpy as np class td_lambda : def _ _ init__(self , probtostate , valueestimates , rewards ) : self.probtostate = probtostate self.valueestimates = valueestimates self.rewards = rewards self.td1 = self.get_vs0(1 ) def get_vs0(self , lambda _ ) : probtostate = self.probtostate valueestimates = self.valueestimates rewards = self.rewards vs = dict(zip(['vs0','vs1','vs2','vs3','vs4','vs5','vs6'],list(valueestimates ) ) ) vs5 = vs['vs5 ' ] + 1*(rewards[6]+1*vs['vs6']-vs['vs5 ' ] ) vs4 = vs['vs4 ' ] + 1*(rewards[5]+lambda_*rewards[6]+lambda_*vs['vs6']+(1-lambda_)*vs['vs5']-vs['vs4 ' ] ) vs3 = vs['vs3 ' ] + 1*(rewards[4]+lambda_*rewards[5]+lambda_**2*rewards[6]+lambda_**2*vs['vs6']+lambda_*(1-lambda_)*vs['vs5']+(1-lambda_)*vs['vs4']-vs['vs3 ' ] ) vs1 = vs['vs1 ' ] + 1*(rewards[2]+lambda_*rewards[4]+lambda_**2*rewards[5]+lambda_**3*rewards[6]+lambda_**3*vs['vs6']+lambda_**2*(1-lambda_)*vs['vs5']+lambda_*(1-lambda_)*vs['vs4']+\ ( 1-lambda_)*vs['vs3']-vs['vs1 ' ] ) vs2 = vs['vs2 ' ] + 1*(rewards[3]+lambda_*rewards[4]+lambda_**2*rewards[5]+lambda_**3*rewards[6]+lambda_**3*vs['vs6']+lambda_**2*(1-lambda_)*vs['vs5']+lambda_*(1-lambda_)*vs['vs4']+\ ( 1-lambda_)*vs['vs3']-vs['vs2 ' ] ) vs0 = vs['vs0 ' ] + probtostate*(rewards[0]+lambda_*rewards[2]+lambda_**2*rewards[4]+lambda_**3*rewards[5]+lambda_**4*rewards[6]+lambda_**4*vs['vs6']+lambda_**3*(1-lambda_)*vs['vs5']+\ + lambda_**2*(1-lambda_)*vs['vs4']+lambda_*(1-lambda_)*vs['vs3']+(1-lambda_)*vs['vs1']-vs['vs0 ' ] ) + \ ( 1-probtostate)*(rewards[1]+lambda_*rewards[3]+lambda_**2*rewards[4]+lambda_**3*rewards[5]+lambda_**4*rewards[6]+lambda_**4*vs['vs6']+lambda_**3*(1-lambda_)*vs['vs5']+\ + lambda_**2*(1-lambda_)*vs['vs4']+lambda_*(1-lambda_)*vs['vs3']+(1-lambda_)*vs['vs2']-vs['vs0 ' ] ) return vs0 def get_lambda(self , x0 = np.linspace(0.1,1,10 ) ) : return fsolve(lambda lambda_:self.get_vs0(lambda_)-self.td1 , x0 ) the expected output is : 0.20550275877409016 but i am getting array([1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] ) i can not understand what am i doing incorrectly . td = td_lambda(probtostate , valueestimates , rewards ) td.get_lambda ( ) # output : array([1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] ) i am just using td(λ ) for state 0 after one iteration . i am not required to see where it converges , so i do n't update the value estimates .",25768,25768,2019-05-23T04:59:19.487,2019-06-01T08:29:59.993,why am i getting the incorrect value of lambda ?,reinforcement-learning python markov-decision-process temporal-difference,2,12,
3691,12423,1,,2019-05-20T09:00:52.617,0,27,"i have a fairly large dataset consisting of different images with and without persons that i want to use for a project . the problem is that i only want the pictures that contain faces , and it is best if there is only a crop of the face . i already looked at facenet and openface , but i thought that there must be a simpler already trained solution just to sort the dataset so i can get started with my own project .",25770,16229,2019-05-28T19:37:24.130,2019-05-28T19:37:24.130,tool to help clean dataset to only contain faces,machine-learning image-recognition datasets getting-started,2,2,1
3692,12426,1,,2019-05-20T10:39:29.067,-1,31,"what should we do we want more interested in ( noisy ) linking words in the sentence than named entities , what should we do ? example : "" what a luckii fish ! ! ! "" this is noisy text individually their words have different meaning but when they occur together their meaning is changed . how to solve this kind of problems",22997,22997,2019-05-21T09:46:22.160,2019-05-21T09:46:22.160,linking words in nlp,deep-learning natural-language-processing,0,2,
3693,12429,1,,2019-05-20T11:10:51.767,1,32,"i 'm studying convolutional neural networks from the following article https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/ . if we take a grayscale image , the value of the pixel will be between 0 and 255 . now , if we apply a filter to our "" new image "" , can we have pixels whose values are not included in this range ? in this case , how can we create the convolved image ?",25772,2444,2019-05-20T14:05:51.713,2019-05-20T14:05:51.713,what are the value of the pixels of the convolved image ?,neural-networks convolutional-neural-networks,1,0,
3694,12433,1,12457,2019-05-20T14:16:37.620,4,74,"in the paper governance by glass - box : implementing transparent moral bounds for ai behaviour , the authors seem to be presenting a black box method of testing . are these ideas really new ? were n't these ideas already proposed in translating values into design requirements ( by ibo van de poel ) ? black - box testing had already been proposed much earlier .",25777,25777,2019-05-22T14:07:07.007,2019-05-22T14:07:07.007,"are the ideas in the paper "" governance by glass - box : implementing transparent moral bounds for ai behaviour "" novel ?",machine-learning ai-design ai-basics ai-community papers,1,0,1
3695,12434,1,,2019-05-20T15:59:51.887,3,36,i have come across something that ibm offers called neural architecture search . you feed it a data set and it outputs an initial neural architecture that you can train . i am wondering if you have any papers on this and if they use heuristics or is this meta machine learning ?,25780,2444,2019-05-20T17:20:08.717,2019-05-20T21:17:26.960,neural architecture search,neural-networks machine-learning neural-architecture-search hyperparameter-optimization,2,0,1
3696,12440,1,,2019-05-20T17:26:16.873,0,58,"i am using keras 2.07 , with python 3.5 , tensorflow 1.3.0 on windows 10 i am testing the architecture used in paper intra prediction using fully connected network for video coding . i hope to use it for my own data . i used test data which i thought would converge very quickly . the learning rate i using in my model is 0.1 . then my val loss not decrease . can someone look or try this code ? am i assuming wrong , coding wrong or impatient ? thanks def multi_input_model ( ) : input1 _ = input(shape=(4,20,1 ) , name='input1 ' ) input2 _ = input(shape=(16,4,1 ) , name='input2 ' ) x1 = flatten()(input1 _ ) x2 = flatten()(input2 _ ) x = concatenate()([x1 , x2 ] ) for i in range(3 ) : # x = dropout(0.3)(x ) x = dense(1024 , kernel_initializer = randomnormal(stddev = std ) , use_bias = true , bias_initializer = ' zeros ' , activation='relu ' ) ( x ) # x = dropout(0.3)(x ) x = dense(64 , kernel_initializer = randomnormal(stddev = std ) , use_bias = true , bias_initializer = ' zeros ' , ) ( x ) output _ = reshape((8,8,1 ) , name='output')(x ) model = model(inputs=[input1 _ , input2 _ ] , outputs=[output _ ] ) model.summary ( ) return model sgd = sgd(lr = 0.1 , momentum=0.9 ) model = multi_input_model ( ) model.compile(optimizer=sgd , loss='mean_squared_error ' , metrics=[mse ] ) model summary : using tensorflow backend . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ layer ( type ) output shape param # connected to = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = input1 ( inputlayer ) ( none , 4 , 20 , 1 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ input2 ( inputlayer ) ( none , 16 , 4 , 1 ) 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ flatten_1 ( flatten ) ( none , 80 ) 0 input1[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ flatten_2 ( flatten ) ( none , 64 ) 0 input2[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ concatenate_1 ( concatenate ) ( none , 144 ) 0 flatten_1[0][0 ] flatten_2[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dense_1 ( dense ) ( none , 1024 ) 148480 concatenate_1[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dense_2 ( dense ) ( none , 1024 ) 1049600 dense_1[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dense_3 ( dense ) ( none , 1024 ) 1049600 dense_2[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ dense_4 ( dense ) ( none , 64 ) 65600 dense_3[0][0 ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ output ( reshape ) ( none , 8 , 8 , 1 ) 0 dense_4[0][0 ] = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = total params : 2,313,280 trainable params : 2,313,280 non - trainable params : 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ training traceback train on 985373 samples , validate on 246344 samples epoch 1/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 7s 7us / step - loss : 0.0054 - mse : 353.9386 - val_loss : 0.0087 - val_mse : 566.5364 lr : 0.08 epoch 2/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0044 - mse : 288.5897 - val_loss : 0.0082 - val_mse : 534.4153 lr : 0.08 epoch 3/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0042 - mse : 270.7345 - val_loss : 0.0080 - val_mse : 517.2601 lr : 0.08 epoch 4/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0040 - mse : 259.6213 - val_loss : 0.0078 - val_mse : 504.8340 lr : 0.08 epoch 5/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0039 - mse : 251.3669 - val_loss : 0.0076 - val_mse : 495.0704 lr : 0.08 epoch 6/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0038 - mse : 244.7449 - val_loss : 0.0075 - val_mse : 486.6413 lr : 0.08 epoch 7/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0037 - mse : 239.2320 - val_loss : 0.0074 - val_mse : 480.2631 lr : 0.08 epoch 8/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0036 - mse : 234.5702 - val_loss : 0.0073 - val_mse : 473.3974 lr : 0.08 epoch 9/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0035 - mse : 230.5504 - val_loss : 0.0072 - val_mse : 468.4981 lr : 0.08 epoch 10/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0035 - mse : 227.0740 - val_loss : 0.0071 - val_mse : 463.1125 lr : 0.08 epoch 11/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0034 - mse : 224.0050 - val_loss : 0.0071 - val_mse : 459.0103 lr : 0.08 epoch 12/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0034 - mse : 221.3146 - val_loss : 0.0070 - val_mse : 455.0988 lr : 0.08 epoch 13/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0034 - mse : 218.9012 - val_loss : 0.0069 - val_mse : 451.7273 lr : 0.08 epoch 14/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0033 - mse : 216.6984 - val_loss : 0.0069 - val_mse : 448.4461 lr : 0.08 epoch 15/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0033 - mse : 214.7047 - val_loss : 0.0069 - val_mse : 446.2414 lr : 0.08 epoch 16/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0033 - mse : 212.8751 - val_loss : 0.0068 - val_mse : 443.6305 lr : 0.08 epoch 17/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0032 - mse : 211.1775 - val_loss : 0.0068 - val_mse : 441.6251 lr : 0.08 epoch 18/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0032 - mse : 209.5941 - val_loss : 0.0068 - val_mse : 439.3194 lr : 0.08 epoch 19/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0032 - mse : 208.1229 - val_loss : 0.0067 - val_mse : 436.9524 lr : 0.08 epoch 20/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0032 - mse : 206.7133 - val_loss : 0.0067 - val_mse : 435.1670 lr : 0.08 epoch 21/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0032 - mse : 205.4114 - val_loss : 0.0067 - val_mse : 432.8971 lr : 0.08 epoch 22/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0031 - mse : 204.1449 - val_loss : 0.0066 - val_mse : 431.2357 lr : 0.08 epoch 23/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0031 - mse : 202.9737 - val_loss : 0.0066 - val_mse : 430.8359 lr : 0.08 epoch 24/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0031 - mse : 201.8537 - val_loss : 0.0066 - val_mse : 429.2760 lr : 0.08 epoch 25/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0031 - mse : 200.7699 - val_loss : 0.0066 - val_mse : 428.2598 lr : 0.08 epoch 26/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0031 - mse : 199.7556 - val_loss : 0.0066 - val_mse : 426.2759 lr : 0.08 epoch 27/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0031 - mse : 198.7569 - val_loss : 0.0065 - val_mse : 425.0138 lr : 0.08 epoch 28/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0030 - mse : 197.8049 - val_loss : 0.0065 - val_mse : 423.5451 lr : 0.08 epoch 29/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0030 - mse : 196.8937 - val_loss : 0.0065 - val_mse : 422.2164 lr : 0.08 epoch 30/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0030 - mse : 196.0350 - val_loss : 0.0065 - val_mse : 421.1507 lr : 0.08 epoch 31/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0030 - mse : 195.1807 - val_loss : 0.0065 - val_mse : 420.2791 lr : 0.08 epoch 32/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0030 - mse : 194.3681 - val_loss : 0.0065 - val_mse : 420.0271 lr : 0.08 epoch 33/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0030 - mse : 193.5861 - val_loss : 0.0064 - val_mse : 418.1573 lr : 0.08 epoch 34/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0030 - mse : 192.8048 - val_loss : 0.0064 - val_mse : 417.7967 lr : 0.08 epoch 35/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0030 - mse : 192.0708 - val_loss : 0.0064 - val_mse : 416.3381 lr : 0.08 epoch 36/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 191.3404 - val_loss : 0.0064 - val_mse : 416.3695 lr : 0.08 epoch 37/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 190.6411 - val_loss : 0.0064 - val_mse : 415.9791 lr : 0.08 epoch 38/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 189.9464 - val_loss : 0.0064 - val_mse : 414.0931 lr : 0.08 epoch 39/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 189.2725 - val_loss : 0.0064 - val_mse : 413.8717 lr : 0.08 epoch 40/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 188.6250 - val_loss : 0.0064 - val_mse : 413.0042 lr : 0.08 epoch 41/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 187.9844 - val_loss : 0.0064 - val_mse : 413.0950 lr : 0.08 epoch 42/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 187.3532 - val_loss : 0.0063 - val_mse : 412.4408 lr : 0.08 epoch 43/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 186.7512 - val_loss : 0.0063 - val_mse : 411.1885 lr : 0.08 epoch 44/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 186.1417 - val_loss : 0.0063 - val_mse : 410.7527 lr : 0.08 epoch 45/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0029 - mse : 185.5806 - val_loss : 0.0063 - val_mse : 409.3184 lr : 0.08 epoch 46/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 185.0026 - val_loss : 0.0063 - val_mse : 410.3592 lr : 0.08 epoch 47/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 184.4485 - val_loss : 0.0063 - val_mse : 409.0613 lr : 0.08 epoch 48/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 183.8902 - val_loss : 0.0063 - val_mse : 409.2569 lr : 0.08 epoch 49/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 183.3491 - val_loss : 0.0063 - val_mse : 408.1287 lr : 0.08 epoch 50/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 182.8373 - val_loss : 0.0063 - val_mse : 407.0794 lr : 0.08 epoch 51/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 182.3068 - val_loss : 0.0063 - val_mse : 407.4011 lr : 0.08 epoch 52/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 181.7958 - val_loss : 0.0062 - val_mse : 405.9963 lr : 0.08 epoch 53/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 181.2954 - val_loss : 0.0062 - val_mse : 406.3628 lr : 0.08 epoch 54/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 180.8150 - val_loss : 0.0062 - val_mse : 405.7962 lr : 0.08 epoch 55/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 180.3107 - val_loss : 0.0062 - val_mse : 405.0873 lr : 0.08 epoch 56/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 179.8412 - val_loss : 0.0062 - val_mse : 405.1387 lr : 0.08 epoch 57/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 179.3717 - val_loss : 0.0062 - val_mse : 404.4157 lr : 0.08 epoch 58/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0028 - mse : 178.9016 - val_loss : 0.0062 - val_mse : 403.5622 lr : 0.08 epoch 59/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 178.4647 - val_loss : 0.0062 - val_mse : 403.3207 lr : 0.08 epoch 60/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 177.9997 - val_loss : 0.0062 - val_mse : 403.5156 lr : 0.08 epoch 61/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 177.5587 - val_loss : 0.0062 - val_mse : 403.2921 lr : 0.08 epoch 62/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 177.1332 - val_loss : 0.0062 - val_mse : 402.8525 lr : 0.08 epoch 63/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 176.6831 - val_loss : 0.0062 - val_mse : 402.3887 lr : 0.08 epoch 64/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 176.2728 - val_loss : 0.0062 - val_mse : 401.6309 lr : 0.08 epoch 65/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 175.8403 - val_loss : 0.0062 - val_mse : 401.4650 lr : 0.08 epoch 66/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 175.4337 - val_loss : 0.0062 - val_mse : 401.6886 lr : 0.08 epoch 67/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 175.0153 - val_loss : 0.0062 - val_mse : 401.1379 lr : 0.08 epoch 68/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 174.6212 - val_loss : 0.0062 - val_mse : 401.4452 lr : 0.08 epoch 69/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 174.2141 - val_loss : 0.0062 - val_mse : 401.5032 lr : 0.08 epoch 70/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 173.8265 - val_loss : 0.0062 - val_mse : 400.2583 lr : 0.08 epoch 71/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 173.4480 - val_loss : 0.0062 - val_mse : 399.9907 lr : 0.08 epoch 72/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 173.0551 - val_loss : 0.0062 - val_mse : 400.5084 lr : 0.08 epoch 73/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0027 - mse : 172.6990 - val_loss : 0.0062 - val_mse : 400.7729 lr : 0.08 epoch 74/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 172.3093 - val_loss : 0.0061 - val_mse : 398.8733 lr : 0.08 epoch 75/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 171.9405 - val_loss : 0.0061 - val_mse : 399.6324 lr : 0.08 epoch 76/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 171.5539 - val_loss : 0.0062 - val_mse : 400.0404 lr : 0.08 epoch 77/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 171.2163 - val_loss : 0.0061 - val_mse : 399.0176 lr : 0.08 epoch 78/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 170.8428 - val_loss : 0.0061 - val_mse : 398.2502 lr : 0.08 epoch 79/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 170.5107 - val_loss : 0.0061 - val_mse : 399.4756 lr : 0.08 epoch 80/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 170.1418 - val_loss : 0.0061 - val_mse : 398.6772 lr : 0.08 epoch 81/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 169.7998 - val_loss : 0.0061 - val_mse : 398.6140 lr : 0.08 epoch 82/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 169.4597 - val_loss : 0.0061 - val_mse : 398.5729 lr : 0.08 epoch 83/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 169.1229 - val_loss : 0.0061 - val_mse : 397.8155 lr : 0.08 epoch 84/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 168.7856 - val_loss : 0.0061 - val_mse : 397.5096 lr : 0.08 epoch 85/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 168.4588 - val_loss : 0.0061 - val_mse : 397.9902 lr : 0.08 epoch 86/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 168.1268 - val_loss : 0.0061 - val_mse : 397.3359 lr : 0.08 epoch 87/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 167.8185 - val_loss : 0.0061 - val_mse : 397.9455 lr : 0.08 epoch 88/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 167.4879 - val_loss : 0.0061 - val_mse : 397.7877 lr : 0.08 epoch 89/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 167.1769 - val_loss : 0.0061 - val_mse : 397.3274 lr : 0.08 epoch 90/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 166.8585 - val_loss : 0.0061 - val_mse : 397.2146 lr : 0.08 epoch 91/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 166.5457 - val_loss : 0.0061 - val_mse : 397.8896 lr : 0.08 epoch 92/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 166.2354 - val_loss : 0.0061 - val_mse : 396.7620 lr : 0.08 epoch 93/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0026 - mse : 165.9282 - val_loss : 0.0061 - val_mse : 397.0715 lr : 0.08 epoch 94/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0025 - mse : 165.6314 - val_loss : 0.0061 - val_mse : 397.2557 lr : 0.08 epoch 95/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0025 - mse : 165.3255 - val_loss : 0.0061 - val_mse : 396.8092 lr : 0.08 epoch 96/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0025 - mse : 165.0327 - val_loss : 0.0061 - val_mse : 396.3043 lr : 0.08 epoch 97/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0025 - mse : 164.7327 - val_loss : 0.0061 - val_mse : 397.1202 lr : 0.08 epoch 98/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0025 - mse : 164.4545 - val_loss : 0.0061 - val_mse : 397.8727 lr : 0.08 epoch 99/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0025 - mse : 164.1428 - val_loss : 0.0061 - val_mse : 397.3806 lr : 0.08 epoch 100/100 985373/985373 [ = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ] - 6s 6us / step - loss : 0.0025 - mse : 163.8922 - val_loss : 0.0061 - val_mse : 396.7912 lr : 0.08",25783,,,2019-05-20T17:26:16.873,train loss does not convergence,python keras,0,3,
3697,12441,1,,2019-05-20T19:17:34.923,4,97,"will the machine become so powerful that it begins to think to a point where it is more capable than the humans ? notice , i said capable , not powerful . no doubt computers are more powerful at giving us answers faster than human brainpower , but are they more capable ? so can artificial intelligence take over the world ? .",23529,1671,2019-05-20T21:26:14.160,2019-05-29T21:19:13.640,can artificial intelligence take over the world ?,superintelligence mythology-of-ai ai-takeover,2,2,
3698,12444,1,,2019-05-20T21:34:18.660,0,42,"first of all i do n't play games at all and i still quite new to deep learning.i was using alex - net(transfer learning actually ) in matlab to classify images in my current laptop(i5 - 3230,without any gpu ) . and it was taking roughly 25 - 30 hours to finish a training and i do n't dare using goolenet . i have to buy a laptop . how fast will be if i use gpu ? and i am not that rich to buy 2080 or 1080ti that easily . actually i am poor and i have struggle financially if the price exceeds $ 1200.i have intention to do research on signal and medical image processing using deep learning in future . will it be enough for me if i buy a laptop with gtx 1060 6 gb or is it essential / overkill to buy one with rtx2060 ? feel free to give any other option that could fill my requirements ... tia ...",25788,,,2019-05-20T21:34:18.660,rtx 2060 vs gtx 1060 6 gb for deep learning,deep-learning convolutional-neural-networks matlab hardware-evaluation gpu,0,4,
3699,12446,1,,2019-05-21T07:07:56.920,1,19,"i am trying to learn utility functions for ships through their ais data . i have a lot of data available and plan on focusing on fishing boats . so far i 've researched a lot of irl algorithms but i 'm not sure if i missed a important one that could be applied . i 've found this paper https://journals.sagepub.com/doi/citedby/10.1177/0278364917722396 , but i 'm not sure if this is really applicable . i would need to transform the ais data as trajectorys , add material from openseamap and plot this to images of trajectories . or did i misunderstand the paper completely ? my other found approach would be selecting features as position , distance to other vessels and others . and then try to apply continous max entropy deep inverse reinforcement learning . is there another approach that may be easier in your eyes ?",25796,,,2019-05-21T07:07:56.920,learning utility function for ais data,reinforcement-learning convolutional-neural-networks utility-based,0,0,
3700,12447,1,,2019-05-21T07:30:15.377,1,23,"i 'm trying to make use of sensor data from voc , humidity , age , sampling rate and use them as nn input data . below are the technical questions i 'm struggling to find out . for each training set , i have 2500 data points for voc and humidity . however , for the age and sampling rate , i have only one for each . i 'm wondering if it would work to just put 5002(=2500(voc)+ 2500(hum)+1(age ) + 1 ( sampling rate ) ) input data points to the layer . * note : sampling rate is in the data set because the voc and hum data have different data points for each training set . i reduced the data points by sampling from the original data points . however , i know for sure that timespan does matter . please help me out ! a good reference is also a huge welcome !",25797,,,2019-05-21T07:30:15.377,how to deal with neural network input data with different length and type,neural-networks machine-learning,0,0,
3701,12448,1,,2019-05-21T08:37:45.133,4,61,"what is a simple turn - based game , that can be used to validate a monte - carlo tree search code and it 's parameters ? before applying it to problems where i do not have a possiblity to validate its moves for correctness , i would like to implement a test case , that makes sure that it behaves as expected , especially when there are some ambiguities between different implementations and papers . i built a connect - four game in which to mcts - ai play against each other and an iterated prisoners dilemma implementation , in which a mcts - ai plays against common strategies like tit - for - tat , but i am still not sure if there is a real good interpretation if the mcts - ai finds the best strategy . another alternative would be a tic - tac - toe game , but mcts will exhaust the whole search space within little steps , so it is hard to tell how the implementation will perform on other problems . in addition , expanding a full game tree does not tell you if any states before the full expansion are following the best mcts strategy . example : you can alternate in the expand step of player 1 's tree between optimize for player 1 and optimize for player 2 , assuming that player 2 will not play the best move for player 1 , but the best move for himself . not doing so would result in an optimistic game tree , that may work in some cases , but probably is not the best choice for many games , while it would be useful for cooperative games . when the game tree is fully expanded , you can find the best move , even when the order of the expand steps was not optimal , so using a game that can be fully expanded is no good test to validate the in - between steps . is there a simple to implement game , that can be used for validation , in which you can reliably check for each move , if the ai did find the expected move ?",25798,25798,2019-05-22T09:55:11.893,2019-05-22T09:55:11.893,what is a simple game for validation of mcts ?,monte-carlo-tree-search,1,9,
3702,12450,1,,2019-05-21T13:19:08.540,3,89,"in the paper nonlinear interference mitigation via deep neural networks , the the following network is illustrated . the network structure is the network parameters are , where $ w_1 $ and $ w_2 $ are linear matrices and is element - wise nonlinear function ( $ i$ is the index of layer ) . where should i add this ? is it possible to learn the parameter ? i do n't think it is the same idea as activation function since it is positioned in the middle of two linear matrices ... or can it be added as embedding layer ?",25806,2444,2019-05-21T19:55:35.587,2019-05-21T20:07:25.053,can you learn parameters in nonlinear function ?,deep-learning deep-network,1,15,0
3703,12451,1,,2019-05-21T14:28:36.720,2,43,is there any conmprehensive paper or site that describes an overview of the known black box testing techniques ?,25777,1671,2019-05-21T18:06:44.967,2019-05-21T18:28:30.243,what are the known ways to test ai black boxes ?,ai-design theory reference-request black-box,1,1,
3704,12453,1,,2019-05-21T18:44:19.703,1,30,"i think reinforcement learning would be a good fit for this problem , but i am not sure of how to deal with a seemingly infinite number of actions . in the beginning of each game ( generic rts game ) , the player places down units anywhere on the map . then as the game progresses , the player can move units around by selecting on them and clicking on a valid location on the map . they must take into consideration things like distance and travel time . an ai agent must do the same . how would i represent these actions ? it ’s not as simple as selecting ‘ up ’ , ‘ down ’ , ‘ right’, ... etc . should the agent just randomly pick locations on the map ? are there any papers or implementations i can look at to help me get started ?",25810,,,2019-05-21T18:44:19.703,action spaces for an rts game,reinforcement-learning algorithm game-ai,0,4,1
3705,12455,1,,2019-05-21T20:39:52.907,0,14,"i am using the following library : https://github.com/vishnugh/evo-neat which seems to be a pretty simple neat - implementation . therefore i am using the following config : package com.evo.neat.com.evo.neat.config ; / * * * created by vishnughosh on 01/03/17 . * / public class neat_config { public static final int inputs = 11 ; public static final int outputs = 2 ; public static final int hidden_nodes = 100 ; public static final int population = 300 ; public static final float compatibility_threshold = float.max_value ; public static final float excess_coefficent = 1 ; public static final float disjoint_coefficent = 1 ; public static final float weight_coefficent = 5 ; public static final float stale_species = 2 ; public static final float steps = 0.1f ; public static final float perturb_chance = 0.9f ; public static final float weight_chance = 0.5f ; public static final float weight_mutation_chance = 0.5f ; public static final float node_mutation_chance = 0.1f ; public static final float connection_mutation_chance = 0.1f ; public static final float bias_connection_mutation_chance = 0.1f ; public static final float disable_mutation_chance = 0.1f ; public static final float enable_mutation_chance = 0.2f ; public static final float crossover_chance = 0.1f ; public static final int stale_pool = 10 ; } however , there are way too much species ( about 60 ) . i do not know how to reduce this number , given the fact that the compatibility_threshold is already maximized . so what 's my fault ? note : i am not using : http://nn.cs.utexas.edu/keyword?stanley:ec02 since this algorithm seems not to work in a changing environment ( where fitness can vary hardly )",19062,1847,2019-05-22T06:02:38.730,2019-05-22T06:02:38.730,how to reduce amount of species in neat ?,evolutionary-algorithms neat,0,0,
3706,12458,1,,2019-05-20T16:02:26.653,1,61,i ’ve been thinking about this for a few days and ca n’t tell if this would feel morally just to an average user,,1671,2019-05-23T20:02:34.730,2019-05-29T10:43:51.900,"would it be ethical to use ai to determine a user ’s gender from the content they upload , without them knowing ?",applications ethics social,3,1,1
3707,12462,1,,2019-05-22T07:36:30.033,1,29,"in the book "" reinforcement learning : an introduction "" ( 2018 ) sutton and barto explain at page 221 a form of tile coding using hashing , to reduce memory consumption . i have two questions about that : how can this approach reduce memory consumption ? is n't it just dependent on the number of tiles ( you have to store one weight for each tile ) . they state that there is only a "" little loss of performance "" . in my understanding , the sense of tile coding ( and coarse coding ) is , that near - by states have many tiles in common and far - away states have only few tilings in common . with tilings "" randomly spread throughout the state space "" this is n't the case . how does this not influence performance ?",21299,,,2019-05-22T07:36:30.033,hashed tile coding vs regular tile coding,reinforcement-learning rl-an-introduction function-approximation,0,2,
3708,12463,1,,2019-05-22T07:49:14.167,0,36,"are there any open sourced algorithms that can take a couple of images as an input and generate a new , similar image based on that input ? or are there any resources where i can learn to create such an algorithm ?",25821,,,2019-05-22T09:07:16.780,algorithm that creates new images based on other images,neural-networks algorithm image-generation art-aesthetics,1,0,
3709,12464,1,,2019-05-22T07:54:13.650,0,14,i want my models to be accessible only by my programs . how do i encrypt and decrypt the model when i run inference on my model ? is there any existing technology that is widely used ?,22093,,,2019-05-22T07:54:13.650,encrypting and decrypting model files,models security,0,1,
3710,12465,1,,2019-05-22T08:29:41.703,0,37,"i 'm currently working on a group project where we need to find a pattern in a given dataset . the dataset is a collection of x , y , z values of a gyroscope from someone who is walking . if you plot these values you 'll get a result like this . and this is how our dataset looks like . we are new to ai and ml so we first did some general research like understanding how matrices work and how to do some basic predictions with frameworks like tensorflow and pytorch . now we want to start on this problem . what we need to do is to find a pattern in the dataset and count how many times this pattern appears in this dataset . we started of with some none ai functions to count , we managed to do that but the way we counted will probably only work on this specific dataset . so that 's why we decided to do this with ai . we would love to hear as many different approaches as possible to count the steps since we are still learning .",25820,,,2019-05-23T00:51:37.040,recognize pattern in dataset,machine-learning datasets pattern-recognition supervised-learning,2,1,
3711,12468,1,,2019-05-22T09:07:58.227,0,9,"in the transformer ( adapted in bert ) , we normalize the attention weights ( dot product of keys and queries ) using a softmax in the scaled dot - product mechanism . it is unclear to me whether this normalization is performed on each row of the weight matrix or on the entire matrix . in the tensorflow tutorial , it is performed on each row ( axis=-1 ) , and in the official tensorflow code , it is performed on the entire matrix ( axis = none ) . the paper does n't give much details . to me , both methods can make sens but they have a strong impact . if on each row , then each value will have a roughly similar norm because its weights some to one . if on the entire matrix , some values might be "" extinguished "" because all of its weights can be very close to zero .",7783,,,2019-05-22T09:07:58.227,about the softmax in the transformer / bert,tensorflow machine-translation,0,1,
3712,12469,1,,2019-05-22T10:20:35.140,0,53,"recently i simulated the gambler 's problem in rl : now , the problem is , the curve does not at all appear the way as given in the book . the "" best policy "" curve appears a lot more undulating than it is shown based on the following factors : sensitivity ( i.e. the threshold for which you decide the state values have converged ) . probability of heads ( expected ) . depending the value of sensitivity it also depends on whether i find the policy by finding the action ( bet ) which cause the maximum return by using $ & gt;$ or by using $ & gt;=$ in the following code i.e : initialize maximum = -inf best_action = none loop over states : loop over actions of the state : if(action_reward&gt;maximum ) : best_action = action also note that if we make the final reward as 101 instead of 100 the curve becomes more uniform . this problem has also been noted in the following thread . so what is the actual intuitive explanation behind such a behaviour of the solution . also here is the thread where this problem is discussed .",9947,9947,2019-05-22T10:37:45.300,2019-05-23T00:53:46.963,the problem with the gambler 's problem in rl,reinforcement-learning probabilistic,1,0,
3713,12470,1,,2019-05-22T11:54:54.033,4,54,"as a layman in ai i want to get an idea how big data players like facebook model individuals ( of which they have so many data ) . there are two scenarios i can imagine : neural networks build clusters of individuals by pure and "" unconscious "" big data analysis ( not knowing , trying to understand and naming the clusters and "" feature neurons "" on intermediate levels of the network ) with the only aim to predict some decisions of the members of these clusters with highest possible accuracy . letting humans analyze the clusters and neurons ( trying to understand what they mean ) they give names to them and possibly add human - defined "" fields "" ( like "" is an honest person "" ) if these were not found automatically , and whose values are then calculated from big data . the second case would result in a specific psychological model of individuals with lots of "" human - understandable "" dimensions . in case there is such a model , i would be very interested to know as much about it as possible . what can be said about this : is there most probably such a model ( that is kept as a secret e.g. by facebook ) ? has somehow tried to guess how it may look like ? are there leaked parts of the model ? my aim is to know and understand by which categories facebook ( as an example ) classifies its users .",25362,25362,2019-05-23T11:34:36.693,2019-05-31T11:03:51.513,psychological models at facebook et al,neural-networks data-science,1,0,
3714,12472,1,12480,2019-05-22T15:24:37.517,10,79,"when designing solutions to problems such as the lunar lander on openaigym , reinforcement learning is a tempting means of giving the agent adequate action control so as to successfully land . but what are the instances in which control system algorithms , such as pid controllers , would do just an adequate job as , if not better than , reinforcement learning ? questions such as this one do a great job at addressing the theory of this question , but do little to address the practical component . as an artificial intelligence engineer , what elements of a problem domain should suggest to me that a pid controller is insufficient to solve a problem , and a reinforcement learning algorithm should instead be used ( or vice versa ) ?",22424,22424,2019-05-22T17:02:39.760,2019-05-22T23:56:44.683,when should i use reinforcement learning vs pid control ?,reinforcement-learning ai-design control-theory,1,4,4
3715,12479,1,,2019-05-22T21:45:21.910,0,9,"how would one go about inputting multiple high dimensionality categorical columns using tensorflow 's embedding feature columns ? does that even make sense to do ? for example : for a car price predictor , the features are : make model trim",25834,,,2019-05-22T21:45:21.910,multiple embedding layers ?,machine-learning tensorflow word-embedding feature,0,0,
3716,12482,1,,2019-05-23T00:57:08.200,0,18,"the difference in clasterisation task is , that the classes of objects are not given , but must be determined from the dataset . usual usecase for nn is like having images of cats and dogs , and the right answers in form like [ 1,0 ] for cat , [ 0 , 1 ] for dog , to backpropagate from those results . if the right answers are unknown , how nn will learn then ? also there might be unknown amount of classes and the output vector could "" grow "" somehow",25836,,,2019-05-23T00:57:08.200,"is there any known visual neural network , capable of image clasterisation ?",neural-networks image-recognition,0,2,
3717,12487,1,,2019-05-23T10:45:07.830,2,33,"there is plenty of literature describing lstms in a lot of detail and how to use them for multi - variate or uni - variate forecasting problems . what i could n't find though , is any papers or discussions describing time series forecasting where we have correlated forecast data . an example is best to describe what i mean . say i wanted to predict number of people at a beach for the next 24 hours and i want to predict this in hourly granularity . this quantity of people would clearly depend on the past quantity of people at the beach as well as the weather . now i can make an lstm architecture of some sort to predict these future quantities based upon what happened in the past quite easily . but what if i now have access to weather forecasts for the next 24 hours too ? ( and historical forecast data too ) . the architecture i came up with looks like this : so i train the left upper branch on forecast data , then train the right upper branch on out - turn data , then freeze their layers to and join them to form the final network in the picture and train that on both forecasts and out - turns . ( when i say train , the output is always the forecast for the next 24 hours ) . this method does in fact have better performance than using forecasts or out - turns alone . i guess my question is , has anyone seen any literature around on this topic and/ or knows a better way to solve these sort of multivariate time series forecasting problems and is my method okay or completely flawed ?",25659,,,2019-05-23T10:45:07.830,lstm architecture,deep-learning lstm time-series,0,0,
3718,12488,1,,2019-05-23T11:27:12.730,2,23,is it possible to recognize the height and width of the sails of different kitesurfers and windsurfers taken from public webcams ? and show these information on video in real time ? or on screenshots ?,25851,,,2019-05-23T11:27:12.730,sails size recognition,machine-learning image-recognition object-recognition,0,2,
3719,12489,1,,2019-05-23T14:42:28.070,0,11,"my tensorflow logging messages shows twice . after some investigation i figured out the cause is tensorflow hub . example : code : import tensorflow as tf import tensorflow_hub tf.logging.set_verbosity(tf.logging.info ) tf.logging.info(""hello test ! "" ) output : info : tensorflow : hello test ! i0523 16:35:51.024926 140735788589952 log.py:13 ] hello test ! desired output : info : tensorflow : hello test ! what i tried : i tried to inverse the order of the imports and i ended up with only the second line of output . this is better but i want to know how to get only the first line of the output ! thanks for your help .",23350,,,2019-05-23T14:42:28.070,tensorflow hub causes tensorflow logging to duplicate !,machine-learning deep-learning tensorflow,0,0,
3720,12490,1,12502,2019-05-23T15:36:42.400,3,27,"can the decoder in a transformer model be parallelized like the encoder ? as far as i understand the encoder has all the tokens in the sequence to compute the self - attention scores . but for a decoder this is not possible ( in both training and testing ) , as self attention is calculated based on previous timestep outputs . even if we consider some technique like teacher forcing , where we are concatenating expected output with obtained , this still has a sequential input from the previous timestep . in this case , apart from the improvement in capturing long - term dependencies , is using a transformer - decoder better than say an lstm when comparing purely on the basis of parallelization ?",25859,,,2019-05-24T10:57:08.330,transformer based decoding,deep-learning lstm sequence-modelling,1,0,
3721,12491,1,,2019-05-23T19:13:16.093,1,31,"i 'm currently in an artificial intelligence class and have been given a challenging homework problem . my linear algebra is n't quite up to scratch , and the problem is based around simplifying the multiplication of some vectors . normally we have a solution model of the form $ y = wx+b $ that can perform linear regression . during a discussion of multilayer perceptrons and neural networks which take the output of a neuron ( function ) and input it into another function , we were given the following question : simplify the following : $ $ { w_3[w_2(w_1 \overrightarrow{x } + b_1 ) + b_2 ] + b_3}$$ i believe the intent of this question is to show that this does not add any complexity to the model , and can be reduced to something similar to this : $ $ \overrightarrow{w}\overrightarrow{x } + \overrightarrow{b } $ $ is this assumption correct ? if so , what initial steps would i take to simplify this . i 'd like to do the work myself so tips or some identity in linear algebra would suffice .",18870,75,2019-05-24T19:14:58.010,2019-05-24T19:14:58.010,simplifying a multiplication of vectors,neural-networks linear-algebra,0,6,
3722,12497,1,,2019-05-24T05:54:37.987,1,75,"i am not sure i understand what is the advantage of using a vae 's over a deterministic auto encoder ? for example , assuming we have just 2 labels , a deterministic auto encoder will always map a given image to the same latent vector . however , one expects that after the training , the 2 classes will form separate clusters in the encoder space . in the case of the vae an image is mapped to an encoding vector probabilistically . however , one still ends up with 2 separate clusters . now , if one passes a new image ( at the test time ) , in both cases the network should be able to place that new image in one of the 2 clusters . how are these 2 clusters created using the vae better than the ones from the deterministic case ?",23871,9947,2019-05-24T06:35:34.587,2019-05-25T20:06:09.987,why are vae 's useful ?,neural-networks autoencoders probabilistic,2,1,1
3723,12499,1,12500,2019-05-24T07:04:47.830,2,165,"i just learned about gan and i 'm a little bit confused about the naming of latent vector . first , in my understanding , a definition of a latent variable is a random variable that ca n't be measured directly ( we needs some calculation from other variables to get its value ) . for example , knowledge is a latent variable . is it correct ? and then , in gan , a latent vector $ z$ is a random variable which is an input of the generator network . i read in some tutorials , it 's generated using only a simple random function : z = np.random.uniform(-1 , 1 , size=(batch_size , z_size ) ) then how are the two things related ? why do n't we use the term "" a vector with random values between -1 and 1 "" when referring $ z$ ( generator 's input ) in gan ?",16565,,,2019-05-24T20:50:20.150,why is it called latent vector ?,terminology generative-adversarial-networks,2,4,1
3724,12506,1,,2019-05-24T12:05:46.863,3,27,"i also asked this question here but i 'm repeating it on this se because i feel it is more relevant . no intention to spam . i am researching into coding a solver for a variant of the sokoban game with multiple agents , restrictions ( eg . colors of stones , goals ) and relaxations ( push and pull possible , etc . ) by researching online i have found classic papers in the field , like the rolling stone paper ( by andreas junghanns and jonathan schaeffer ) and sokoban : enhancing general single - agent search methods using domain knowledge from the same authors . these solutions seem to be outdated and i am currently structuring my solver per the notes of the two most performant solvers : yass and sokolution . from the research , i 've done these two seem to be my best bet . it is apparent that they are not enough by themselves to solve a multi - agent environment . those solvers are made for a single agent . so far , i have failed to find useful multi - agent proposals . in this context , my question is : what can be considered state - of - the - art in order to : coordinate multiple agents with different goals , and plug a solver 's solution in and validate / edit it ? what are some search terms i can use to research this further ? thank you very much",25888,16565,2019-05-25T21:09:31.783,2019-05-25T21:09:31.783,multi agent sokoban search solvers state of the art,research intelligent-agent multi-agent-systems,0,0,
3725,12508,1,,2019-05-24T13:06:30.177,4,38,"if the heuristic $ h$ satisfies the additional condition $ h(x ) ≤ d(x , y ) + h(y)$ for every edge $ ( x , y)$ of the graph ( where d denotes the length of that edge ) , then $ h$ is called monotone , or consistent . in such a case , $ a^*$ can be implemented more efficiently — roughly speaking , no node needs to be processed more than once ( see closed set below)—and $ a^*$ is equivalent to running dijkstra 's algorithm with the reduced cost $ d'(x , y ) = d(x , y ) + h(y ) − h(x).$ can someone intuitively explain why the reduced cost is of this form ? for the complete article .",25729,9947,2019-05-24T13:34:50.117,2019-05-24T13:34:50.117,a * is similar to dijkstra with reduced cost,graph-theory,0,0,1
3726,12509,1,,2019-05-24T15:27:04.913,1,30,i want to be able to improve my lower level device specific programming abilities to assist in future endeavors . examples would be learning to write custom tensorflow operations in c++ optimized to work on gpus . does anyone know good resources to find tutorials showing which packages to use for which devices and etc . ? an approach other than just reading source and replicating until understanding would be nice .,25496,,,2019-05-24T15:27:04.913,any good resources for learning programming gpu level operations ?,c++ gpu,0,1,1
3727,12510,1,,2019-05-24T17:39:06.653,3,40,"no matter what i google or what paper i read , i ca n't find an answer to my question . in a deep convolutional neural network , let 's say alexnet ( krizhevsky , 2012 ) , filter weights are learned by means of back - prop . but how are kernels themselves selected ? i know kernels had been used in image processing long before convnets , hence i 'd imagine there would be a set of filters based on kernels ( e.g. ) that are proven to be effective for edge detection and the likes . reading around the web , i also found something about "" randomly generated kernels "" . does anyone know if and when this practice is adopted ? thanks in advance for any answer !",25893,,,2019-05-24T20:45:13.113,are filter kernels fixed or learned ?,convolutional-neural-networks,2,0,
3728,12513,1,12515,2019-05-24T21:23:13.537,1,54,"if i use a desktop pc with a gpu , how long it might take to train face recognition deep neural network on let 's say dataset of 2.6 million images and 2600 identities ? i guess it should depend on various properties ( e.g. , type of the dnn ) . but i am just looking for a rough estimation . is it a matter of hours / days or years ? thanks !",25902,1671,2019-05-29T20:33:02.010,2019-05-29T20:33:02.010,how long it takes to train face recognition deep neural network ? ( rough estimation ),deep-learning ai-basics deep-network hardware facial-recognition,2,2,
3729,12516,1,12517,2019-05-25T05:08:26.117,8,773,"i 'm trying to learn neural networks by watching this series of videos and implementing a simple neural network in python . here 's one of the things i 'm wondering about : i 'm training the neural network on sample data , and i 've got 1,000 samples . the training consists of gradually changing the weights and biases to make the cost function result in a smaller cost . my question : should i be changing the weights / biases on every single sample before moving on to the next sample , or should i first calculate the desired changes for the entire lot of 1,000 samples , and only then start applying them to the network ?",25904,,,2019-05-27T13:01:40.860,is neural networks training done one - by - one ?,neural-networks python,2,5,1
3730,12523,1,12526,2019-05-25T11:52:33.930,2,27,"i m implementing a neural network framework from scratch in c++ as a learning exercise . there is one concept i do n't see explained anywhere clearly : how do you go from your last convolutional / pooling / something layer which is "" 3 dimensional "" to your first fully connected layer in the network ? many sources say , that you should flatten the data . does this mean that you should just simply create a $ 1d$ vector with a size of $ n*m*d$ ( $ n*m$ is the last conv . layer 's size , and $ d$ is the number of activation maps in that layer ) and put the numbers in it one by one in some arbitrary order ? if this is the case i understand how to propagate further down the line , but how does backprogation work here ? just put the values in reverse order into the activation maps ? i also read , that you can do this "" flattening "" as a tensor contraction . how does that work exactly ?",25910,9947,2019-05-25T13:04:30.233,2019-05-25T13:09:34.400,convolutional layer to fully connected layer implementation,neural-networks convolutional-neural-networks c++,1,1,
3731,12524,1,12525,2019-05-25T12:02:33.703,3,30,"in chapter 5 of deep learning book of ian goodfellow , some notations in the loss function as below make me really confused . i tried to understand $ x , y \sim p_{data}$ means a sample $ ( x , y)$ sampled from original dataset distribution ( or $ y$ is the ground truth label ) . the loss function in formula 5.101 seems to be correct for my understanding . actually , the formula 5.101 is derived from 5.100 by adding the regularization . therefore , the notation $ x , y \sim \hat{p}_{data}$ in formula 5.96 and 5.100 is really confusing to me whether the loss function is defined correctly ( kinda typo error or not ) . if not so , could you help me to refactor the meaning of two notations , are they similar and correct ? many thanks for your help .",25911,2444,2019-05-25T12:24:56.213,2019-05-25T13:00:33.477,being confused of distribution notations in deep learning book,machine-learning deep-learning notation,1,0,1
3732,12527,1,,2019-05-25T13:44:26.573,1,21,"i myself am not new to nlp , but for some reason i am unable to grasp purity of bert . i have seen a ton of blogs , github repos , but none could clarify bert usage to me . it would be helpful if you could provide two things : a clear implementation of bert , preferably in ipython notebook . some papers on bert excluding the original paper by google .",25512,,,2019-05-25T13:44:26.573,bert super easy implementation,natural-language-processing python,0,1,
3733,12528,1,,2019-05-25T14:51:20.180,2,15,"i am new to machine learning and i would like to seek some advice / help for directions on implementing a binary classification for a series of data and tell if it is a straight line or a not ? for example i have the following data e.g. training data : 0,2,4 -- straight line 0,10,5 -- not a straight line 0,99,99 -- not a straight line 0,1,2 -- straight line my validation data would be : 0,60,120 -- straight line 0,120,60 -- not a straight line",25913,,,2019-05-25T14:51:20.180,binary classification for a series of data ( using keras ) to tell if it is a straight line or not a straight line,machine-learning keras,0,0,
3734,12529,1,,2019-05-25T14:53:36.397,0,15,"i 'm working on an advantage a2c implementation , and i just finished creating the value network . i train this network with the standard mse loss of discounted rewards - to - go : $ $ i would like to be able to evaluate and assess the performance and ability of the value network as i train , especially to see how that relates and interacts with the changes and improvements in the policy , however i 'm not sure how to do this . my first instinct was to track the loss after each batch of experiences , but this does n't work . as the policy improves , episodes last longer , and the value loss increases . i understand why this happens , as it is harder to predict the rewards - to - go when the length of the future is more undetermined . to fix this , i tried dividing the loss that i 'm computing by the total number of steps in the batch of episodes . however , the loss is still increasing as the policy improves ( as the episodes get longer ) . why is this still happening ? then , is there anything i can do to get a better assessment of the quality of the value network ?",25732,,,2019-05-25T14:53:36.397,a2c critic loss interpretation,neural-networks deep-learning reinforcement-learning loss-functions actor-critic,0,0,
3735,12531,1,,2019-05-25T18:24:22.527,2,36,"in simple words , what is the prediction accuracy ? what is it based on ? how does it help ? when is it used ?",23529,2444,2019-05-25T19:29:46.883,2019-05-25T19:29:46.883,what is the prediction accuracy ?,machine-learning terminology definitions prediction,1,0,
3736,12534,1,,2019-05-25T23:17:17.500,0,38,"i have been offered a job with a software firm which does high level custom software . i believe i am the first person there with some ai background and that very soon now many of their projects will require some ai . i am not a lead , i do n't think i have the skills , technical or managerial . is it possible for a software development team with one person with some ai background , to learn to implement various ai techniques succesfully ? my own background is that i studied ai but with a focus on cognition ( ie : use ai to better understand cognition , human or otherwise ) . afterwards i learned the craft of software development ( java webapps ) and worked in this field until i found a phd position . during my phd i dabbled a bit with machine learning , academically . soon i focussed again on the more cognition oriented approach , and looked for ways of formalising knowledge from literature so it could be used inside a custom classifier to better analyse a new dataset . the funding ran out before i could finish and i returned to software development . ideally i would love a job where i could join a ai - oriented team , and learn more about production - level ai on the job . the job i have been offered is with a firm that makes custom software . they are starting a new project which should employ ai to help human operators fulfil a complex task . another project uses very costly calculation and one ml component . the data it works on is public and google has announced it will have a go at the data . i expect that when google is done , there will be a scramble to use ml for all the components . in general i think that 5 years from now all their software projects will require some form of ai . if i were running this software company i would go and look for a lead developer who had proven themselves in ai . then i would task this person with interviewing to find 2 ' luitenants ' . what are the chances of a software development team including one member who knows about machine learning in an academic context , getting up - to - speed and delivering production level software incorporating ai ?",25929,7800,2019-05-29T17:51:34.180,2019-05-30T05:09:09.197,"single ai - person in software firm about to adopt ai , good idea ?",machine-learning software-development,1,3,
3737,12537,1,,2019-05-26T05:09:59.803,2,95,"i have a doubt regarding the cross validation approach and train - validation - test approach . i was told that i can split a dataset into 3 parts : train : we train the model . validation : we validate and adjust model parameters . test : never seen before data . we get an unbiased final estimate . so far , we have split into three subsets . until here everything is okay . attached is a picture : then i came across the k - fold cross validation approach and what i do n’t understand is how i can relate the test subset from the above approach . meaning , in 5-fold cross validation we split the data into 5 and in each iteration the non - validation subset is used as the train subset and the validation is used as test set . but , in terms of the above mentioned example , where is the validation part in k - fold cross validation ? we either have validation or test subset . i would like to cite this information from https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7 training dataset training dataset : the sample of data used to fit the model . the actual dataset that we use to train the model ( weights and biases in the case of neural network ) . the model sees and learns from this data . validation dataset validation dataset : the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters . the evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration . the validation set is used to evaluate a given model , but this is for frequent evaluation . we as machine learning engineers use this data to fine - tune the model hyperparameters . hence the model occasionally sees this data , but never does it “ learn ” from this . we(mostly humans , at - least as of 2017 ) use the validation set results and update higher level hyperparameters . so the validation set in a way affects a model , but indirectly . test dataset test dataset : the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset . the test dataset provides the gold standard used to evaluate the model . it is only used once a model is completely trained(using the train and validation sets ) . the test set is generally what is used to evaluate competing models ( for example on many kaggle competitions , the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close , and it is the result of the the model on the test set that decides the winner ) . many a times the validation set is used as the test set , but it is not good practice . the test set is generally well curated . it contains carefully sampled data that spans the various classes that the model would face , when used in the real world . i would like to say this : taking this into account , we still need the test split in order to have a good assement of our model . otherwise we ’re only training and adjusting parameters but never take the model to the battle field thank you ! check this splitting :",25405,25405,2019-05-26T08:03:35.420,2019-05-26T08:03:35.420,understanding k - fold cross validation,neural-networks machine-learning training cross-validation,0,9,
3738,12540,1,,2019-05-26T13:32:12.817,1,17,"copy from my reddit post : ( sorry if this does not fit here , please tell me and i delete it ) help regarding i 'm working on an implementation of ppo , which i plan to use in my ( bachelors ) thesis . to test whether my implementation works , i want to use the lunarlandercontinuous - v2 environment . now my implementation seems to work just fine , but plateues much too early - at an average reward of ~ -1.8 reward per timestep , where the goal should be somewhere around ~ +2,5 reward per timestep . as the implementation generally learns i am somewhat confused , as to why it then pleateus so early . some details regarding my implementation , also here is the github repo : i use parallelized environments via openai 's subproc_vecenv i use the actor critic version of ppo i use generalized advantage estimation as my advantage term i only use finished runs ( every run used in training has reached a terminal state ) even though critic loss in the graphic below looks small it is actually rather large , as the rewards are normalized and therefor the value targets are actually rather small the critic seemingly predicts a value independent of the state it is fed - that is it predicts for every state just the average over all the values . that seems like harsh underfitting , which is weird as the network is already rather large for the problem in my opinion . but this seems to be the most likely cause for the problem in my opinion . edit1 : added image",25945,25945,2019-05-27T21:47:01.047,2019-05-27T21:47:01.047,"implementation of ppo - value loss not converging , return plateauing",reinforcement-learning proximal-policy-optimization advantage-actor-critic,0,0,
3739,12542,1,,2019-05-26T19:07:36.687,3,24,"given enough experiment data on time taken for objects to fall to earth from different heights , one can create various models that will accurately predict the time it will take for an object falling at any height ( in the inner the atmosphere , this is a toy example ) . in this simple example the model is deterministic , it will always produce an output given an input regardless of the amount of data over a small threshold — something akin to newton ’s gravity equation . try modelling something like the stock - market ( the other extreme end of the scale ) and the predictions will never reliably converge on a predictable accurate model . is there any way of knowing whether your domain will yield a deterministic or non - deterministic model or not ?",11893,,,2019-05-26T19:07:36.687,how to know when a environment will yield a deterministic model,models convergence,0,2,
3740,12543,1,,2019-05-26T19:23:58.417,2,45,"in every day life , it seems that we all have various habits and actions that we perform . for example , we wake up and check our email / facebook etc . on our phones . we do n't look at are current state right now , and consider the values of all the possible future trajectories . we basically choose the action that maximizes our "" reward "" at our current state . question . is it practical to randomly initialize actions $ a \in a$ , states $ s \in s$ and a policy and update this according to some algorithm ( e.g. reinforce , exploration , etc . ) to achieve some desired goal in your life ? this could be done , for example , by uniformly sampling a random number in the interval $ ( 0,1)$ and acting according to a policy . for example , suppose your goal is to get married , get a new house etc . what would be appropriate return functions in this case ? return is usually defined as the immediate reward plus the discounted cumulative future rewards . but is this the right definition for practical problems like marriage / dating , buying a house , etc . ? would you define return as $ r_{\text{total } } = r_{\text{marriage}}+r_{\text{buying house}}$ in our example , where each of those individual returns are typical immediate plus discounted rewards and try to maximize that ? or would it be better to maximize $ r_{\text{marriage}}$ and $ r_{\text{buying house}}$ individually ?",25953,25953,2019-05-26T20:19:04.257,2019-05-26T20:19:04.257,reinforcement learning in real life / practical terms,reinforcement-learning,0,2,
3741,12544,1,12545,2019-05-26T22:04:46.983,3,57,"i 'm interested in ant colony optimization algorithms and bee algorithms , but i 'm confused what are the applications of these algorithms can you suggest me some examples of applications can i work on ?",25940,2444,2019-05-26T23:20:11.043,2019-05-26T23:47:52.917,what are the applications of ant colony optimization algorithms ?,applications swarm-intelligence ant-colony,1,1,1
3742,12548,1,12582,2019-05-27T06:28:42.280,1,33,"there is this problem i have encountered , i was trying to classify the pixels from input image into classes , sort of like segmentation , using a encoder - decoder cnn . the “ interested ” pixels usually locate in the top right corner of the input image , but the input images are too big , which i have to slice them in patches , by doing this , each input patch loses its “ which region of the whole picture it ’s from ” information . i 'm using pytorch , i thought of manually add this patch location info into the input , but then it will be convoluted , which does make sense to me since it 's not a part of an image . i 'm new to this , not sure if i 'm thinking the whole thing right , how should i manually add this info into the input correctly or if there is some keywords i can do some researches , in order to let the cnn taking position into account ? thank you .",25963,,,2019-05-29T12:42:35.913,how to add some data input in a cnn ?,convolutional-neural-networks autoencoders pytorch image-segmentation,1,2,
3743,12551,1,,2019-05-27T11:45:00.813,0,26,"i 'm about to create an openai gym environment for a flight simulator . i 'm wondering , how to cope with the fact , that the result and reward for some action needs a considerable time to advance through the system due to the inherent time constants . in the easy example gym - environments ( e. g. cartpole , or some games ) the step can anstantly be executed and the resulting reard can be calculated . in my continuous control system ( aka flight simulator ) , there is some reaction time needed , until i can calculate the result from my action . e. g. when i pull the stick , it takes some time , until the aircraft lifts it 's nose . so there is a considerable delay ( maybe in the seconds ballpark ) from commanding the action to the environment , and the earliest observation of that result and it 's corresponding reward . how can i cope with that . as far as i understood , the env.step(action ) function blocks till it comes back with a new observation and a corresponding reward . how can i cope with long lasting reward caclulations ? is it possible to have overlapping actions somehow ? e. g. command a new action every 100ms , but get the reward for that action only 1 second later . in this case there would be always 10 rewards pending . i hope i made my point clear . do n't hesitate to ask for further details in the comments . any hints are welcome . is there anything to read out in the wild dealing with a similar issue ? cheers , felix",25972,,,2019-05-27T15:08:59.973,openai gym interface when reward calculation is delayed ? ( continuous control with considerable reaction time ),reinforcement-learning open-ai gym,1,0,
3744,12552,1,,2019-05-27T11:49:55.327,3,61,"i am trying to formulate and solve the following problem of image mutation . suppose i am trying to insert an object image into a "" background "" image of several objects , and i will need to look for a "" sweet spot "" to insert the image : i am tentatively trying to formulate the problem into a reinforcement learning process , with the following elements : 0 . initial stage : a background image where the location of objects within the image has been marked ( let 's suppose we have a perfect object detector ) another image of a new object , let 's say , a human 1 . action space : location ( x , y ) for the object image to be inserted ; in that sense the action space is quite large . 2 . environment : each step i will have a new image to "" learn from "" . an oracle function f returns 1 or 0 ( roughly one computation of f takes 30 seconds ) . this function tells me the latest synthesized image hits the "" sweet spot "" or not ( 1 means hit ) . if so , i will stop the search and return the image . 3 . constraint : the newly inserted object should n't overlap with the original objects in the figure . while my gut feeling is that this problem is somehow similar to the classic "" maze escape "" problem which can be solved well with reinforcement learning , the action space seems quite large in this problem . so here are my questions : in case i would like to formulate this "" beautify "" image problem into a "" deep "" reinforcement learning problem , how can i learn from such large action space ? or is it really suitable for a reinforcement learning process ? can i somehow subsume the "" non - overlapping "" constraint into the oracle function f ? if so , how should i decide the reward score ? any principled or empirical way of deciding so ?",25973,,,2019-05-28T20:01:54.500,beautify an image with reinforcement learning,reinforcement-learning deep-rl image-generation,1,0,
3745,12554,1,,2019-05-27T13:43:21.327,0,6,"i have a dataframe df_train of shape ( 11808 , 1 ) that looks as follows : datum menge 2018 - 01 - 01 00:00:00 19.5 2018 - 01 - 01 00:15:00 19.0 2018 - 01 - 01 00:30:00 19.5 2018 - 01 - 01 00:45:00 19.5 2018 - 01 - 01 01:00:00 21.0 2018 - 01 - 01 01:15:00 19.5 2018 - 01 - 01 01:30:00 20.0 2018 - 01 - 01 01:45:00 23.0 2018 - 01 - 01 02:00:00 20.5 2018 - 01 - 01 02:15:00 20.5 and a second df nan_df of shape ( 3071 , 1 ) that looks as follows : datum menge 2018 - 05 - 04 00:15:00 nan 2018 - 05 - 04 00:30:00 nan 2018 - 05 - 04 00:45:00 nan 2018 - 05 - 04 01:00:00 nan 2018 - 05 - 04 01:15:00 nan 2018 - 05 - 04 01:30:00 nan 2018 - 05 - 04 01:45:00 nan 2018 - 05 - 04 02:00:00 nan 2018 - 05 - 04 02:15:00 nan the nan values in the nan_df need to be predicted using time series forecasting . what i have done : the code below divides the df df_train and runs the arima model on that to predict the values for the test set import pandas as pd from pandas import datetime import matplotlib.pyplot as plt from statsmodels.tsa.arima_model import arima from sklearn.metrics import mean_squared_error def parser(x ) : return datetime.strptime(x,'%m/%d/%y % h:%m ' ) df = pd.read_csv('time_series.csv',index_col = 1,parse_dates = [ 1 ] , date_parser = parser ) df = df.drop(['unnamed : 0'],axis=1 ) df_train = df.dropna ( ) def startarimaforecasting(actual , p , d , q ) : model = arima(actual , order=(p , d , q ) ) model_fit = model.fit(disp=0 ) prediction = model_fit.forecast()[0 ] return prediction numberofelements = len(df_train ) trainingsize = int(numberofelements * 0.7 ) trainingdata = df_train[0 : trainingsize ] trainingdata = trainingdata.values testdata = df_train[trainingsize : numberofelements ] testdata = testdata.values # new arrays to store actual and predictions actual = [ x for x in trainingdata ] predictions = list ( ) # in a for loop , predict values using arima model for timepoint in range(len(testdata ) ) : actualvalue = testdata[timepoint ] prediction = startarimaforecasting(actual , 3 , 1 , 0 ) print('actual=%f , predicted=%f ' % ( actualvalue , prediction ) ) predictions.append(prediction ) actual.append(actualvalue ) error = mean_squared_error(testdata , predictions ) print('test mean squared error ( smaller the better fit ) : % .3f ' % error ) # plot plt.plot(testdata ) plt.plot(predictions , color='red ' ) plt.show ( ) now , i wanted to do the same to predict the nan values in the nan_df , this time using the entire df_train dataframe and i did it as follows : x = df_train.copy().values nan_df = df.iloc[11809 : , : ] .values real = [ x for x in x ] nan_predictions = list ( ) # in a for loop , predict values using arima model for timepoint in range(len(nan_df ) ) : nan_actualvalue = nan_df[timepoint ] nan_prediction = startarimaforecasting(real , 3 , 1 , 0 ) print('real=%f , predicted=%f ' % ( nan_actualvalue , nan_prediction ) ) nan_predictions.append(nan_prediction ) real.append(nan_actualvalue ) when i do this , i get the following error : traceback ( most recent call last ) : file "" & lt;ipython - input-42 - 33f3e242230d&gt ; "" , line 4 , in & lt;module&gt ; nan_prediction = startarimaforecasting(real , 3 , 1 , 0 ) file "" & lt;ipython - input-1 - 043dac0dd994&gt ; "" , line 17 , in startarimaforecasting model_fit = model.fit(disp=0 ) file "" c:\users\kashy\anaconda3\envs\py36\lib\site - packages\statsmodels\tsa\arima_model.py "" , line 1157 , in fit callback , start_ar_lags , * * kwargs ) file "" c:\users\kashy\anaconda3\envs\py36\lib\site - packages\statsmodels\tsa\arima_model.py "" , line 946 , in fit start_ar_lags ) file "" c:\users\kashy\anaconda3\envs\py36\lib\site - packages\statsmodels\tsa\arima_model.py "" , line 562 , in _ fit_start_params start_params = self._fit_start_params_hr(order , start_ar_lags ) file "" c:\users\kashy\anaconda3\envs\py36\lib\site - packages\statsmodels\tsa\arima_model.py "" , line 539 , in _ fit_start_params_hr if p and not np.all(np.abs(np.roots(np.r_[1 , -start_params[k : k + p ] ] file "" c:\users\kashy\anaconda3\envs\py36\lib\site - packages\numpy\lib\polynomial.py "" , line 245 , in roots roots = eigvals(a ) file "" c:\users\kashy\anaconda3\envs\py36\lib\site - packages\numpy\linalg\linalg.py "" , line 1058 , in eigvals _ assertfinite(a ) file "" c:\users\kashy\anaconda3\envs\py36\lib\site - packages\numpy\linalg\linalg.py "" , line 218 , in _ assertfinite raise linalgerror(""array must not contain infs or nans "" ) linalgerror : array must not contain infs or nans so , i would like to know how can i predict the nan values in the nan_df ?",23380,75,2019-05-27T18:13:08.267,2019-05-27T18:13:08.267,how to predict nan ( missing values ) of a dataframe using arima in python ?,machine-learning python forecasting time-series,0,0,
3746,12558,1,12559,2019-05-27T18:48:30.883,0,59,what is the definition of machine learning ? what are the advantages of machine learning ?,24095,2444,2019-05-27T21:08:17.283,2019-05-27T21:08:17.283,what is machine learning ?,machine-learning ai-basics terminology definitions,1,0,1
3747,12561,1,,2019-05-28T00:52:13.103,0,22,"i am building csp nqueens solver and apply the ac-3 algorithm.but,the domain reduction does n't occur and gets more search time.how does it happen?is it a nature of ac-3 algorithm ?",21779,,,2019-05-28T00:52:13.103,can ac-3 algorithm solve n - queens problem ?,algorithm search problem-solving combinatorial-games constraint-satisfaction-problems,0,0,
3748,12562,1,,2019-05-28T02:09:26.493,1,17,"given $ f_1,f_2, .. ,f_n$ as set of final exams of subjects taken by students $ s_1, .. ,s_k$ in h slots such that no student takes two exams in a single slot.here the objective is to maximize the number of exams taken by a student in a single slot . i reduced the problem to graph coloring problem where the nodes of the graph are the final exams of subjects and edges would be the students . the chromatic number of graph would be h. but i am not sure how to represent edges as they can me multiple also as same exams are taken by multiple students and would it have any impact on constraint satisfaction formulation ?",25984,25984,2019-05-28T09:39:02.110,2019-05-28T09:39:02.110,write constraint satisfaction formulation for problem,graphs graph-theory constraint-satisfaction-problems satisfiability,0,0,
3749,12563,1,,2019-05-28T07:32:34.133,1,21,is it better to train one neural network for a dispersed labeled data with large number of classes or first classify data by unsupervised learning then train each part by a separate nn ? i mean by unsupervised learning we help each nn to classify in lower dispersed data with lower number of labels . so for test data the class of data is found by unsupervised learning then the final label is found by the network associated with that class . does this question generally have an answer or it depends on data and needs to be answered in practice ?,25988,,,2019-05-28T07:32:34.133,one end to end neural network or many task - specific ones ?,neural-networks machine-learning deep-learning deep-network,0,0,
3750,12564,1,,2019-05-28T09:16:04.140,1,48,"i want to develop a fraud detection application in the stock market , we have some pattern that defines the anomaly for use of supervised machine learning but there is one question remain : how to use unsupervised machine learning for detecting anomaly like a fraud ?",25452,,,2019-05-28T09:16:04.140,fraud detection in stock market,machine-learning ai-design anomaly-detection,0,0,1
3751,12565,1,,2019-05-28T09:45:44.620,5,153,"when training a neural network , we often run into the issue of overfitting . however , is it possible to put overfitting to use ? basically , my idea is , instead of storing a large dataset in a database , you can just train a neural network on the entire dataset until it overfits as much as possible , then retrieve data "" stored "" in the neural network like it 's a hashing function .",23941,,,2019-05-29T23:16:24.783,is it possible for a neural network to be used to compress data ?,neural-networks overfitting,2,0,
3752,12568,1,,2019-05-28T14:25:06.190,2,38,"for logistic regression , the cost function is defined as : \begin{equation } cost(h_{\theta}(x)-y ) = -ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x ) ) \end{equation } i now have a nonlinear function \begin{equation } h_{\theta}^{(i)}(x)=xe^{-j\theta_i|x|^2 } \end{equation } where $ i$ denotes the $ i$ th training sample . how should i define cost function for this particular nonlinear function ?",25806,2444,2019-05-29T23:13:00.297,2019-05-29T23:13:00.297,how to define cost function for custom nonlinear functions ?,deep-learning deep-network logistic-regression,0,2,1
3753,12569,1,,2019-05-28T14:44:04.090,1,16,"i am tentatively trying to train a deep reinforcement learning model the maze escaping task , and each time it takes one image as the input ( e.g. , a different "" maze "" ) . suppose i have about $ 10k$ different maze images , and the ideal case is that after training $ n$ mazes , my model would do a good job to quickly solve the puzzle in the rest $ 10k$ - $ n$ images . i am writing to inquire some good idea / empirical evidences on how to select a good $ n$ for the training task . and in general , how should i estimate and enhance the ability of "" transfer learning "" of my reinforcement model ? make it more generalized ? any advice or suggestions would be appreciate it very much . thanks .",25973,,,2019-05-28T14:44:04.090,training a reinforcement learning model with multiple images,reinforcement-learning convolutional-neural-networks deep-rl transfer-learning,0,0,
3754,12570,1,,2019-05-28T16:40:29.937,0,31,"in the book "" reinforcement learning : an introduction "" ( 2018 ) sutton and barto define the prediction objective ( ) as follows ( page 199 ) : $ $ where $ v_{\pi}(s)$ is the true value of $ s$ and is the approximation of it . furthermore it is stated that this is "" often used in plots "" . my questions is : how do i get the true value $ v_{\pi}(s)$ ? and if there is a way to obtain the value , why would i need to approximate it ?",21299,,,2019-05-28T18:27:52.987,prediction objective reinforcement learning,reinforcement-learning rl-an-introduction,1,0,
3755,12573,1,,2019-05-29T01:53:08.997,0,19,"i am working on a project that involves using a convnet to identify screws . i am able to train from scratch a convnet based on the first version of the inception network , but shallower ( only 3 inception modules ) , and at the moment classifying only 45 different screws ( the goal is to cover a significant part of a catalog containing ~ 4000 different itens ) . my training set consists of rectangular grayscale images of the screws ( 150 x 300 pixels ) , approx . 700 images for each class . the prototype of this model has been working pretty well with 45 classes ( test set accuracy ~98 % ) , but i am starting to worry about two things : 1 ) many screws in the catalog have similar shapes , but different sizes , so the production model will have to be able to infer the scale of the objects . this is important because future users will image screws with different smartphone cameras , yielding different screw sizes in the images fed to the convnet . i have n't been able to find much about this in the literature . and from what i have read about convnets , they are good at detecting shapes , which mean that two objects , the first 1 meter long and the other 1 centimeter long , would be considered "" equal "" by a convnet if they appeared similar in an image . one ( not very elegant ) solution i imagined would be to include a scale in the training images , by means either of a ruler or a common object ( a coin , for example ) . anyway , i wonder if this problem has a simple solution , since i believe many people might have faced it . 2 ) all of the notorious convnets i know of are trained with the imagenet dataset , which comprises 1000 different classes . my screw dataset will ultimately have more than that . is that an issue ? assuming i have the hardware resources to train very large fully connected layers and softmax output layers , is there an upper bound to the number of classes a convnet can identify ?",26003,,,2019-05-29T01:53:08.997,object size identification and maximum number of classes with convolutional neural networks,convolutional-neural-networks computer-vision object-recognition,0,0,
3756,12576,1,,2019-05-29T08:19:38.493,2,28,"autoencoders are used for unsupervised anomaly detection by at first learning the features of the data set with mainly "" normal "" data points . then new data can be considered anomalous , if the new data has a large reconstruction error , i.e. it was hard to fit the features as in the normal data . but how can autoencoders provide a reconstruction error when unsupervised ? even if the training is supervised by learning to reconstruct the same data , how is the reconstruction error retrieved from new data ?",26009,9947,2019-05-29T08:56:29.960,2019-05-29T08:56:29.960,reconstruction errors in auto encoders after training,unsupervised-learning autoencoders anomaly-detection,1,1,
3757,12577,1,,2019-05-29T08:49:17.190,1,54,"i am a newbie in reinforcement learning working on a college project . the project is related to optimizing the hardware power . i am running proprietary software in linux distribution ( 16.04 ) . the goal is to use reinforcement learning and optimize the power of the system ( keeping the performance degradation of the software as minimum as possible ) . for this , i need to create a custom environment for my reinforcement learning . from reading different materials , i could understand that i need to make my software as a custom environment from where i can retrieve the state features . action space may include instructions to linux to change the power ( i can use some predefined set of power options ) . the proprietary software is a cellular network and the state variables include latency or throughput . to control the power action space , rapl - tools can be used to control cpu power . i just started working on this project and everything seems blurry . what is the best way to make this work ? is there some tutorials or materials that would help me make things clear . is my understanding of creating a custom environment for reinforcement learning true ?",25885,25885,2019-05-29T12:24:29.107,2019-05-29T12:57:00.323,how to create a custom environment for reinforcement learning,reinforcement-learning environment,1,8,
3758,12579,1,12598,2019-05-29T10:08:39.777,1,70,"in the paper , contextual string embeddings for sequence labeling , the authors state that \begin{equation } p(x_{0 : t } ) = \prod_{t=0}^t p(x_t|x_{0 : t-1 } ) \end{equation } they also state that , in the lstm architecture , the conditional probability $ p(x_{t}|x_{0 : t})$ is approximately a function of the network output $ h_t$ . \begin{equation } p(x_{t}|x_{0 : t } ) \approx \prod_{t=0}^{t } p(x_t|h_t;\theta ) \end{equation } why is this equation true ?",26012,2444,2019-05-30T14:57:23.767,2019-05-30T16:46:16.487,why can we approximate the joint probability distribution using the output vector of an lstm ?,machine-learning natural-language-processing lstm probability-distribution sequence-modelling,1,0,
3759,12587,1,,2019-05-29T20:53:45.710,0,6,"i am new to big data theory , and during the past 3 days , i took an official big data course with some of the best instructors available in my country in this domain . the things was little bit obscure for me as i have an engineering background but with no knowledge in ai techniques and domains . after getting an intro on big data and the 5vs ( volume , velocity , ... ) , we got an intro on hadoop and hadoop ecosystem tools ( hive , pig , ... ) . then a simple example on how to run mapreduce java script on small data file . so to make things clear with me , are hive , pig and other hadoop ecosystem tools , are tools to break up my large data files from different sources and servers into fast - readable files , by which we create new tables with our required fields to use them later on in machine learning scripts and feature extractions ? p.s . by fast readable files i mean , using a scripting tools that normal relational database tools like sql and oracle do n't have it on huge data sets ( 1 terrabytes and above ) to manage and get info from it as fast as possible ?",26028,,,2019-05-29T20:53:45.710,are hadoop ecosystem tools main goal is to break up large data sets into fast readable files ?,machine-learning data-science,0,0,
3760,12588,1,,2019-05-29T22:49:36.580,1,22,"basickly , nn is approximator of any statistical relation , but we operate the common sence where entities may be rather strickt known , so maybe its possible to construct ai , representing it as a chain of algorithms ? means , by ( very big and tricky ) common ( turing - complete ) programm ? ?",25836,,,2019-05-29T22:49:36.580,"must strong ai be only neural network based , or its possible with common programm ?",strong-ai,0,4,
3761,12590,1,,2019-05-29T23:12:22.280,1,14,"i 've seen gans that do things like convert an image to a painting or this gan here https://make.girls.moe/#/ that takes in a set of characteristics and generates a waifu with those characteristics . my understanding of a gan is that the generator upsamples random noise and the discriminator detects if an image is within the real or fake . so if the generator say , generates a waifu with the wrong hair color , how would the discriminator know ?",23941,,,2019-05-29T23:12:22.280,how do gans create an image with specific characteristics ?,neural-networks generative-model generative-adversarial-networks image-generation,0,0,
3762,12591,1,,2019-05-30T04:57:42.483,0,5,"i 'm trying to work out an approach to balancing my dataset , which is a subset of a google openimages - some classes are represented orders of magnitude more than others and i am hesistant to simply throw away data . the approach i want to try next is roughly : add a single instance of every image to the training set . crop out instances of the majority class from entire dataset and then add these cropped images to the dataset . keep adding these cropped images to training set until number of instances of 2nd majority class is close to number of instances of 1st majority class . crop out instances of second majority class and repeat ( 2 ) keep going until all images have been cropped down to the minority class and dataset is roughly balanced . this approach would mean that no instance of any class would be repeated more often than any other instance of the same class . my concern is that the minority classes will end up being predominantly represented in training by heavily cropped images . i 'm not sure but i do n't think this would matter in a two - stage object detector but i 'm concerned that it might be a problem for a one - stage detector if , for some classes , it was predominantly trained on images that were cropped to various degrees . the alternative to doing this would be undersampling ie . just throwing away most of the data which i 'm hesistant to do . anyway , it would be really great to get some opinions on this in the context of yolov3 ? is yolov3 sufficiently robust at scale - invariance that training on many examples that are cropped / larger than what they are likely to be at test time ? ( i 'm fairly commited to some sort of oversampling because of the results in this paper which found best results generally from oversampling with cnn classifiers as opposed to undersampling , threshold adjustment etc . https://arxiv.org/pdf/1710.05381.pdf )",21583,,,2019-05-30T04:57:42.483,will balancing dataset of images for object detection for a single - shot od ( yolov3-spp ) by cropping lower the quality of the model ?,datasets object-recognition,0,0,
3763,12593,1,,2019-05-30T05:34:48.933,1,72,the human and animal brain is made of neural networks . is it possible to train an animal so that it becomes as intelligent as a human ?,26033,1671,2019-05-30T18:58:00.110,2019-05-31T20:11:42.437,is it possible to train an animal so that it becomes as intelligent as a human ?,neural-networks philosophy biology soft-question intelligence,2,2,
3764,12594,1,,2019-05-30T09:59:09.537,0,28,"i am tentatively reusing a codebase of pacman to train my own deep reinforcement learning model . while most of the components seems reasonable and understandable to me , there are two things that seem obscure to me : how to decide the size of the replay memory ? currently , since i set the total step of learning as 4000 ( note that in the referred codebase this value is set as 4000000 ) , so i just proportionally decrease the replay_memory_size as 400 . would that make sense ? what is the return value epsilon when calling function piecewiseschedule ? i also proportionally decrease its parameters as follows : epsilon = piecewiseschedule([(0 , 1.0 ) , ( 40 , 1.0 ) , # since we start training at 10000 steps ( 80 , 0.4 ) , ( 200 , 0.2 ) , ( 400 , 0.1 ) , ( 2000 , 0.05 ) ] , outside_value=0.01 ) replay_memory = prioritizedreplaybuffer(replay_memory_size , replay_alpha ) where the original function call is like this : epsilon = piecewiseschedule([(0 , 1.0 ) , ( 10000 , 1.0 ) , # since we start training at 10000 steps ( 20000 , 0.4 ) , ( 50000 , 0.2 ) , ( 100000 , 0.1 ) , ( 500000 , 0.05 ) ] , outside_value=0.01 ) replay_memory = prioritizedreplaybuffer(replay_memory_size , replay_alpha ) and in general , what is the principle ( guideline ) behind setting a good size of "" replay memory "" and calling function piecewiseschedule ? thank you !",25973,12509,2019-05-30T14:32:34.647,2019-05-30T15:50:02.477,understanding the configuration of replay memory and epsilon in deep reinforcement learning,deep-learning reinforcement-learning,1,2,
3765,12596,1,,2019-05-30T10:52:10.080,2,26,"i 'd like to build an application for tracking the position of a given animal ( e.g. a cat ) in a series of images . is there any off - the - shelf api i could use ? azure has some vision apis , but it seems to me they ca n't be used to get the position of something in an image .",26043,2444,2019-05-30T16:32:50.827,2019-05-31T12:18:18.500,which api can i use for tracking the position of animal in one or more images ?,machine-learning computer-vision object-recognition,1,3,
3766,12600,1,,2019-05-30T18:10:08.487,0,33,"how can i implement a gan network for text ( review ) generation ? please , can someone guide me to resource ( code ) to help in text generation ?",26049,2444,2019-05-30T21:21:12.610,2019-05-30T22:11:06.610,how can i implement a gan network for text ( review ) generation ?,deep-learning natural-language-processing generative-adversarial-networks software-evaluation,0,3,
3767,12601,1,12602,2019-05-30T20:29:22.730,2,45,"can you please point me to some resources about image genereation besides gans ? are there any other techniques throughout history ? how did idea of image generation evolved and how it started ? i tried googling "" image generation before gans "" and similar alternatives but without any success .",23918,2444,2019-05-30T21:18:25.587,2019-05-30T21:18:25.587,other deep learning image generation techniques besides gans ?,deep-learning reference-request generative-model image-generation,1,0,
3768,12604,1,12609,2019-05-31T02:47:46.293,2,62,"in the original prioritized experience replay paper , the authors track in every state transition tuple ( see line 6 in algorithm below ) : why do the authors track this at every time step ? also , many blog posts and implementations leave this out ( including i believe the openai implementation on github ) . can someone explain explicitly how is used in this algorithm ? note : i understand the typical use of as a discount factor . but typically gamma remains fixed . which is why i ’m curious as to the need to track it .",16343,16343,2019-05-31T12:13:33.577,2019-05-31T12:13:33.577,why do authors track in prioritized experience replay paper ?,dqn deep-rl experience-replay,1,0,
3769,12605,1,12608,2019-05-31T04:14:01.397,2,48,"this is because all neural network simulations run on computers that use binary . so , is it possible to combine or create conditional statements of 0 and 1 , and optimize with an evolutionary algorithm ? there may be an algorithm that maps input and output to 0 and 1 , or a conditional statement that edits a conditional statement . example of conditional statement of 0 and 1 ) if 11001 then 01110 just as molecules in the primitive earth have been synthesized and developed into living things , begin with the most fundamental ( 0 and 1 , if then ) and develop into intelligence",23500,23500,2019-05-31T07:32:50.217,2019-05-31T08:16:43.323,can we evolve 0 and 1 ?,neural-networks binary,1,2,
3770,12607,1,,2019-05-31T06:06:22.240,2,22,"i read that a mix of "" greedy "" and "" random "" are ideal for stochastic local search ( sls ) , but i 'm not sure why . it mentioned that the greedy finds the local minima and the randomness avoids getting trapped by the minima . what is the minima and how can you get trapped ? also , how does randomness avoid this ? it seems like if it 's truly random there 's always a chance of ending up searching solutions that lead to dead ends multiple times ( which seems like a waste of processing and avoidable ) ?",25721,2444,2019-05-31T13:05:51.307,2019-05-31T15:05:55.600,"why is a mix of greedy and random usually "" best "" for stochastic local search ?",optimization search problem-solving local-search,2,0,
3771,12612,1,12615,2019-05-31T12:54:50.300,4,76,"the mse can be defined as $ ( \hat{y } - y)^2 $ , which should be equal to $ ( y - \hat{y})^2 $ , but i think their derivative is different , so i am confused of what derivative will i use for computing my gradient . can someone explain for me what term to use ?",26070,2444,2019-05-31T16:33:15.313,2019-06-01T23:03:10.153,which function $ ( \hat{y } - y)^2 $ or $ ( y - \hat{y})^2 $ should i use to compute the gradient ?,math gradient-descent loss-functions,3,0,
3772,12616,1,,2019-05-31T14:13:07.597,0,22,"imagine you have access to a dataset of pairs $ ( s , \hat{\pi}(s))$ where s is a state in a high dimension continuous space $ s$ , is a probabilistic distribution on a large discrete space $ d$ ( size around $ 10^{9}$ ) , and is a sample from this distribution . a detail is important : is not a distribution right , it 's who is one . my question is simple : how can an algorithm learn to , given a state s , sample from ?",26071,,,2019-05-31T14:13:07.597,how to learn to sample ?,neural-networks reinforcement-learning autoencoders,0,0,
3773,12619,1,,2019-05-31T15:49:45.397,3,25,"like our human brain , we can first learn ( train ) the handwriting 0 and 1 . after the traing ( and test ) accuray is good enough , we only need to study ( traing ) the hardwriting 2 , instead of cleaning all of learned memory , and relearn handwriting data 0 , 1 , and 2 at the same time . can cnn do the same thing ? can cnn learn something new , but keep the previous memory ? if yes , the efficiency could be high . right now , i have to give all of data at the same time , the efficiency is very very low .",26072,2444,2019-05-31T15:59:50.780,2019-05-31T16:21:04.090,can a cnn be trained incrementally ?,machine-learning convolutional-neural-networks incremental-learning online-learning,1,0,
3774,12622,1,12624,2019-05-31T16:55:03.883,1,37,"i wonder if virtual reality ( vr ) , augmented reality ( ar ) and mixed reality ( mr ) use any machine learning or deep learning ? for example in ar , the virtual objects are brought into the real world , does this process involve any object detection and localization ?",21213,2444,2019-05-31T17:00:01.650,2019-06-01T12:27:06.847,"do vr , ar and mr use any machine learning or deep learning ?",machine-learning deep-learning applications,2,0,
3775,12625,1,12626,2019-05-31T18:59:56.977,0,14,"good day i have a problem where i have 9 data points that are collected every minute for 40 minutes , and by the 40th minute the solution would be either end up being black or white . i would like to set up a neural network ; which would take the live input of every minute ; and i was hoping within the 25 - 30 minute mark to predict the outcome of what the results would be at 40 minute ; which is a classification . i have over 3000 historical runs of this experiment ; each containing 40 rows of 9 columns data per experiment . what network would i need to set up ; so that it can learn from each run at every minute mark per experiment with the results ; and then set it up for live input , when the experiment is running again . i feel like i might need more than one system to accomplish this ; any help in pointing me towards the right path would be greatly appreciated i am using python ( keras ) to try to solve this problem .",26075,,,2019-05-31T19:25:25.697,what type of network for a repeated experiment,neural-networks keras,1,0,
3776,12627,1,,2019-05-31T20:09:58.620,4,31,"i am running a basic dqn on the pong environment . not a cnn , just a 3 layer linear neural net with relus . it seems to work for the most part , but at some point my model suffers from catastrophic performance loss : what is really the reason for that ? what are common ways to avoid this ? clipping the gradients ? what else ? ( reloading from previous successful checkpoints feels more like a hack , rather than a proper solution to this issue . )",26077,,,2019-05-31T20:09:58.620,catastrophic forgetting on pong environment using dqn,deep-learning reinforcement-learning,0,2,
3777,12630,1,,2019-06-01T00:58:30.793,2,39,"i read this article : "" towards autonomous data ferry route design through reinforcement learning "" by daniel henkel and timothy x brown . it specifies an infinite horizon problem where they use as a reward function for td[0 ] the following : \begin{equation } r(s , a ) = - \int_{t_0}^{t_1 } ( ft + n_0)e^{-\beta t } dt \end{equation } where $ n_0 $ and $ f$ are constant , is used to adjust the discount factor and $ t_0 , t_1 $ are the initial and final time . then they proceed to use $ e^{-\beta t}$ as the ( discount factor ) in the td[0 ] update formula and in the policy formula . why is the discount factor in the infinite horizon problem $ e^{-\beta t}$ , and why is it used as in $ v(s)$ update , since it is a variable factor ? also , in the formula of the td[0 ] update they do n't subtract . they do : \begin{equation } v_{t+1}(s ) = v_t(s ) + \alpha ( r(s , a ) + e^{-\beta t_a } v_t(s ' ) ) \end{equation } i really think this is a mistake , and the values of $ v$ will explode without it , even in an infinite horizon problem . am i correct ? is $ - v(s)$ missing inside the brackets ? finally , if someone is willing and has the time to directly read this part of the article and enlighten me on if $ t_0 $ and $ t_1 $ represent the initial and final time of an action or $ t_0 $ is always 0 and $ t_1 $ is the duration of the action , i would appreciate it . i ask this because from what is written in the paper $ t_0 $ seems to be the current time in the simulation , but i 'm afraid that would just decay too fast and after some actions the reward would be close to 0 . it is really not well explained and i 'm a bit confused . thank you for your time if you got this far reading . any guideline answer will be very much appreciated .",24054,2444,2019-06-01T10:41:44.753,2019-06-01T10:41:44.753,infinite horizon in reinforcement learning,reinforcement-learning papers temporal-difference infinite-horizon,0,2,
3778,12631,1,,2019-06-01T01:02:55.923,0,19,i am trying to train a deep learning model to predict an 8 * 2 matrix . the predicted matrix would have complex values and the input matrix would be real numbers . can it be done ? thank you for your time .,26083,,,2019-06-01T08:32:51.337,how can i train a deep learning model to predict a matrix ?,deep-learning convolutional-neural-networks,1,1,
3779,12632,1,,2019-06-01T02:26:03.443,2,19,"i am having a hard time converting line 6 of the prioritized experience replay algorithm from the original paper into plain english ( see below ) : i understand that new transitions ( not visited before ) are given maximal priority . on line 6 this would be done for every transition in an initial pass since the history is initialized as empty on line 2 . i ’m having trouble with the notation $ p_t$ = max $ _ { i&lt;t } p_i$ . can someone please state this in plain english ? if $ t$ = 4 for example , then $ p_t$ = 4 ? how is this equal to max $ _ { i&lt;t } p_i$ . it seems in my contrived example here , max $ _ { i&lt;t } p_i$ would be 3 . i must be misreading this notation .",16343,16343,2019-06-01T02:40:18.333,2019-06-01T10:22:55.153,new transition priorities in prioritized experience replay ?,reinforcement-learning dqn deep-rl experience-replay,1,0,
3780,12637,1,,2019-06-01T12:56:42.367,-1,22,"i am an second year b.tech student . i have basic idea of unity , c # for game development and i recently starting learning ai in python so i am confused which to choose as career .",26090,,,2019-06-01T14:14:37.217,which carrer path to choose ai or game developmen,ai-basics python,1,0,
3781,12639,1,12641,2019-06-01T14:41:50.517,3,82,"what is the use of td(0 ) method when we talk about temporal difference learning ? the weights in the temporal difference learning are updated as given by the equation ( can be referenced as equation number 4 here ) : when lambda = 0 as in td(0 ) , how does the method learn ? as it appears , with lambda = 0 , there will never be a change in weight and hence no learning . when can td(0 ) be used ? when is it helpful ? i can not see any reason to use td(0 ) method . am i missing anything ?",25768,2444,2019-06-01T15:10:33.550,2019-06-01T15:10:33.550,how is td(0 ) method helpful ? what good does it do ?,reinforcement-learning temporal-difference notation,1,2,1
3782,12640,1,,2019-06-01T14:53:15.223,0,14,"i am using deep reinforcement learning to solve a classic maze escaping task , similar to the implementation provided here , except the following three key differences : instead of using a numpy array as the input of a standard maze escaping task , i am feeding the model with an image at each step ; the image is a 1300 * 900 rgb image , so it is not too small . reward : each valid move has a small negative reward ( penalize long move ) each invalid move has a big negative reward ( run into other objects or boundaries ) each blocked move has the minimal reward ( not common ) find the remote detectors ’ defect has a positive reward ( 5 ) i tweaked the parameters of replay memory , reduced the size of the replay memory buffer . regarding the implementation , i basically do not change the agent setup except the above items , and i implemented my env to wrap my customized maze . but the problem is that , the accumulated reward ( first 200 rounds of successful escaping ) does not increase : and the number of steps it takes to escape one maze is also stable somewhat : here are my question , on which aspect i could start to look at to optimize my problem ? or is it still too early and i will need to train more time ?",25973,,,2019-06-01T14:53:15.223,reward does not increase for a maze escaping problem with dqn,deep-learning reinforcement-learning deep-rl,0,0,
3783,12642,1,,2019-06-01T17:30:01.860,0,15,i want to implement super - resolution and deblurring on text documents . which is the best approach ? are there any git - hub links which will help me to start ? i am new to the field . any help would be appreciated . thanks in advance .,21797,,,2019-06-01T17:30:01.860,super resolution on text documents,neural-networks machine-learning deep-learning convolutional-neural-networks,0,0,
